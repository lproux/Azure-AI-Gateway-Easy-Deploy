{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Backend pool Load Balancing lab (with Terraform)\n",
    "![flow](../../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "Playground to try the built-in load balancing [backend pool functionality of APIM](https://learn.microsoft.com/azure/api-management/backends?tabs=bicep) to a list of Azure OpenAI endpoints.\n",
    "\n",
    "Notes:\n",
    "- **This is a typical prioritized PTU with fallback consumption scenario**. The lab specifically showcases how a priority 1 (highest) backend is exhausted before gracefully falling back to two equally-weighted priority 2 backends.\n",
    "- The backend pool uses round-robin by default.\n",
    "- Priority and weight-based routing are supported and can be adjusted by modifying `priority` (the lower the number, the higher the priority) and `weight` variables in the `openai_resources` variable below.\n",
    "- The `retry` API Management policy initiates a retry to an available backend if an HTTP 429 status code is encountered. This is transparent to the caller.\n",
    "\n",
    "### Result\n",
    "![result](result.png)\n",
    "\n",
    "### TOC\n",
    "- [0Ô∏è‚É£ Initialize notebook variables](#0)\n",
    "- [1Ô∏è‚É£ Verify the Azure CLI and the connected Azure subscription](#1)\n",
    "- [2Ô∏è‚É£ Create deployment using Terraform](#2)\n",
    "- [3Ô∏è‚É£ Get the deployment outputs](#3)\n",
    "- [üß™ Test the API using a direct HTTP call](#requests)\n",
    "- [üîç Analyze Load Balancing results](#plot)\n",
    "- [üß™ Test the API using the Azure OpenAI Python SDK](#sdk)\n",
    "- [üóëÔ∏è Clean up resources](#clean)\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [Pandas Library](https://pandas.pydata.org/) and matplotlib installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with Contributor permissions\n",
    "- [Access granted to Azure OpenAI](https://aka.ms/oai/access) or just enable the mock service\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "- [Terraform CLI](https://learn.hashicorp.com/tutorials/terraform/install-cli) installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique random string.\n",
    "- Adjust the location variables according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management)\n",
    "- Adjust the OpenAI model and version according the [availability by region.](https://learn.microsoft.com/azure/ai-services/openai/concepts/models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ \u001b[1;32mNotebook initialized\u001b[0m ‚åö 10:15:12.517562 \n"
     ]
    }
   ],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "import utils\n",
    "\n",
    "deployment_name = os.path.basename(os.path.dirname(globals()['__vsc_ipynb_file__']))\n",
    "resource_group_name = f\"lab-{deployment_name}\" # change the name to match your naming style\n",
    "resource_group_location = \"westeurope\"\n",
    "\n",
    "apim_sku = \"BasicV2_1\"\n",
    "\n",
    "# Prioritize UK South until exhaustion (simulate PTU with TPM), then equally distribute between Sweden and France (consumption fallback)\n",
    "aiservices_config = \"\"\"{\n",
    "  aiservices-uks = {\n",
    "    name     = \"foundry1\",\n",
    "    location = \"uksouth\",\n",
    "    priority = 1\n",
    "    weight   = \"\"\n",
    "  },\n",
    "  aiservices-swc = {\n",
    "    name     = \"foundry2\",\n",
    "    location = \"swedencentral\",\n",
    "    priority = 2,\n",
    "    weight   = 50\n",
    "  },\n",
    "  aiservices-frc = {\n",
    "    name     = \"foundry3\",\n",
    "    location = \"francecentral\",\n",
    "    priority = 2,\n",
    "    weight   = 50\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "model_deployment_name = \"gpt-4o-mini\"\n",
    "model_name            = \"gpt-4o-mini\"\n",
    "model_version         = \"2024-07-18\"\n",
    "model_capacity        = 1\n",
    "model_api_version     = \"2024-10-21\"\n",
    "\n",
    "utils.print_ok('Notebook initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Verify the Azure CLI and the connected Azure subscription\n",
    "\n",
    "The following commands ensure that you have the latest version of the Azure CLI and that the Azure CLI is connected to your Azure subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è \u001b[1;34mRunning: az account show \u001b[0m\n",
      "‚úÖ \u001b[1;32mRetrieved az account\u001b[0m ‚åö 10:15:15.462683 [0m:2s]\n",
      "üëâüèΩ \u001b[1;34mCurrent user: lproux@microsoft.com\u001b[0m\n",
      "üëâüèΩ \u001b[1;34mTenant ID: 2b9d9f47-1fb6-400a-a438-39fe7d768649\u001b[0m\n",
      "üëâüèΩ \u001b[1;34mSubscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = utils.run(\"az account show\", \"Retrieved az account\", \"Failed to get the current az account\")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    current_user = output.json_data['user']['name']\n",
    "    tenant_id = output.json_data['tenantId']\n",
    "    subscription_id = output.json_data['id']\n",
    "\n",
    "    utils.print_info(f\"Current user: {current_user}\")\n",
    "    utils.print_info(f\"Tenant ID: {tenant_id}\")\n",
    "    utils.print_info(f\"Subscription ID: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Create deployment using Terraform\n",
    "\n",
    "This lab uses [Terraform](https://learn.microsoft.com/en-us/azure/developer/terraform/overview) to declarative define all the resources that will be deployed in the specified resource group. Change the variables or the [main.tf](main.tf) directly to try different configurations.\n",
    "\n",
    "`openAIModelCapacity` is set intentionally low to `8` (8k tokens per minute) to trigger the retry logic in the load balancer (transparent to the user) as well as the priority failover from priority 1 to 2.\n",
    "\n",
    "Setup the variables for Terraform. Te following code will generate a terraform file called `terraform.tfvars` with the variables needed to deploy the resources.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ \u001b[1;32mterraform.tfvars file created\u001b[0m ‚åö 10:15:15.476931 \n"
     ]
    }
   ],
   "source": [
    "# Define the Terraform variables\n",
    "terraform_variables = f\"\"\"\n",
    "resource_group_name     = \"{resource_group_name}\"\n",
    "resource_group_location = \"{resource_group_location}\"\n",
    "apim_sku                = \"{apim_sku}\"\n",
    "model_deployment_name   = \"{model_deployment_name}\"\n",
    "model_name              = \"{model_name}\"\n",
    "model_version           = \"{model_version}\"\n",
    "model_capacity          = \"{model_capacity}\"\n",
    "model_api_version       = \"{model_api_version}\"\n",
    "aiservices_config       = {aiservices_config}\n",
    "\"\"\"\n",
    "\n",
    "# Write the variables to the terraform.tfvars file\n",
    "with open(\"terraform.tfvars\", \"w\") as terraform_variables_file:\n",
    "    terraform_variables_file.write(terraform_variables)\n",
    "\n",
    "utils.print_ok('terraform.tfvars file created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terraform commands `terraform init` and `terraform apply` will be executed in order to deploy the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è \u001b[1;34mRunning: terraform init \u001b[0m\n",
      "‚ùå \u001b[1;33mInitialization failed\u001b[0m ‚åö 10:15:15.514828 [0m:0s] 'terraform' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "\n",
      "‚öôÔ∏è \u001b[1;34mRunning: terraform apply -auto-approve -var-file=terraform.tfvars \u001b[0m\n",
      "‚ùå \u001b[1;33mDeployment failed\u001b[0m ‚åö 10:15:15.540166 [0m:0s] 'terraform' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intialize terraform\n",
    "output = utils.run(\n",
    "    f\"terraform init\",\n",
    "    f\"Initialization succeeded\",\n",
    "    f\"Initialization failed\",\n",
    ")\n",
    "\n",
    "# Specify the target subscription for Terraform\n",
    "os.environ['ARM_SUBSCRIPTION_ID'] = subscription_id\n",
    "\n",
    "# Run the deployment\n",
    "output = utils.run(\n",
    "    f\"terraform apply -auto-approve -var-file=terraform.tfvars\",\n",
    "    f\"Deployment succeeded\",\n",
    "    f\"Deployment failed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "Retrieve the required outputs from the Terraform deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è \u001b[1;34mRunning: terraform output apim_resource_gateway_url \u001b[0m\n",
      "üëâüèΩ \u001b[1;34mapim_resource_gateway_url: {}\u001b[0m\n",
      "‚öôÔ∏è \u001b[1;34mRunning: terraform output apim_subscription_key \u001b[0m\n",
      "üëâüèΩ \u001b[1;34mapim_subscription_key: {}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "apim_resource_gateway_url = utils.run(\"terraform output apim_resource_gateway_url\").json_data\n",
    "utils.print_info(f\"apim_resource_gateway_url: {apim_resource_gateway_url}\")\n",
    "\n",
    "apim_subscription_key = utils.run(\"terraform output apim_subscription_key\").json_data\n",
    "utils.print_info(f\"apim_subscription_key: {apim_subscription_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n",
    "Requests is an elegant and simple HTTP library for Python that will be used here to make raw API requests and inspect the responses. \n",
    "\n",
    "You will not see HTTP 429s returned as API Management's `retry` policy will select an available backend. If no backends are viable, an HTTP 503 will be returned.\n",
    "\n",
    "Tip: Use the [tracing tool](../../tools/tracing.ipynb) to track the behavior of the backend pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Run 1/20:\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '{}/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21': No scheme supplied. Perhaps you meant https://{}/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMissingSchema\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ñ∂Ô∏è Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mruns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m response_time = time.time() - start_time\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚åö \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\.venv\\Lib\\site-packages\\requests\\sessions.py:637\u001b[39m, in \u001b[36mSession.post\u001b[39m\u001b[34m(self, url, data, json, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    627\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    628\u001b[39m \n\u001b[32m    629\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    634\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\.venv\\Lib\\site-packages\\requests\\sessions.py:575\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[32m    563\u001b[39m req = Request(\n\u001b[32m    564\u001b[39m     method=method.upper(),\n\u001b[32m    565\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m     hooks=hooks,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m prep = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m proxies = proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    579\u001b[39m settings = \u001b[38;5;28mself\u001b[39m.merge_environment_settings(\n\u001b[32m    580\u001b[39m     prep.url, proxies, stream, verify, cert\n\u001b[32m    581\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\.venv\\Lib\\site-packages\\requests\\sessions.py:484\u001b[39m, in \u001b[36mSession.prepare_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    481\u001b[39m     auth = get_netrc_auth(request.url)\n\u001b[32m    483\u001b[39m p = PreparedRequest()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\.venv\\Lib\\site-packages\\requests\\models.py:367\u001b[39m, in \u001b[36mPreparedRequest.prepare\u001b[39m\u001b[34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_method(method)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_headers(headers)\n\u001b[32m    369\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_cookies(cookies)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\.venv\\Lib\\site-packages\\requests\\models.py:438\u001b[39m, in \u001b[36mPreparedRequest.prepare_url\u001b[39m\u001b[34m(self, url, params)\u001b[39m\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(*e.args)\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[32m    439\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: No scheme supplied. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    440\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    441\u001b[39m     )\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: No host supplied\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mMissingSchema\u001b[39m: Invalid URL '{}/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21': No scheme supplied. Perhaps you meant https://{}/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21?"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 100\n",
    "url = f\"{apim_resource_gateway_url}/openai/deployments/{model_deployment_name}/chat/completions?api-version={model_api_version}\"\n",
    "messages = {\"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "]}\n",
    "api_runs = []\n",
    "\n",
    "# Initialize a session for connection pooling and set any default headers\n",
    "session = requests.Session()\n",
    "session.headers.update({'api-key': apim_subscription_key})\n",
    "\n",
    "try:\n",
    "    for i in range(runs):\n",
    "        print(f\"‚ñ∂Ô∏è Run {i+1}/{runs}:\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = session.post(url, json = messages)\n",
    "        response_time = time.time() - start_time\n",
    "        print(f\"‚åö {response_time:.2f} seconds\")\n",
    "\n",
    "        utils.print_response_code(response)\n",
    "\n",
    "        if \"x-ms-region\" in response.headers:\n",
    "            print(f\"x-ms-region: \\x1b[1;32m{response.headers.get(\"x-ms-region\")}\\x1b[0m\") # this header is useful to determine the region of the backend that served the request\n",
    "            api_runs.append((response_time, response.headers.get(\"x-ms-region\")))\n",
    "\n",
    "        if (response.status_code == 200):\n",
    "            data = json.loads(response.text)\n",
    "            print(f\"Token usage: {json.dumps(dict(data.get(\"usage\")), indent = 4)}\\n\")\n",
    "            print(f\"üí¨ {data.get(\"choices\")[0].get(\"message\").get(\"content\")}\\n\")\n",
    "        else:\n",
    "            print(f\"{response.text}\\n\")\n",
    "\n",
    "        time.sleep(sleep_time_ms/1000)\n",
    "finally:\n",
    "    # Close the session to release the connection\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Load Balancing results\n",
    "\n",
    "The priority 1 backend will be used until TPM exhaustion sets in, then distribution will occur near equally across the two priority 2 backends with 50/50 weights.  \n",
    "\n",
    "Please note that the first request of the lab can take a bit longer and should be discounted in terms of duration.\n",
    "\n",
    "In case the dependencies were not installed, you can install theme with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle as pltRectangle\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 7]\n",
    "df = pd.DataFrame(api_runs, columns = ['Response Time', 'Region'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "# Define a color map for each region\n",
    "color_map = {'UK South': 'lightpink', 'France Central': 'lightblue', 'Sweden Central': 'lightyellow'}  # Add more regions and colors as needed\n",
    "\n",
    "# Plot the dataframe with colored bars\n",
    "ax = df.plot(kind = 'bar', x = 'Run', y = 'Response Time', color = [color_map.get(region, 'gray') for region in df['Region']], legend = False)\n",
    "\n",
    "# Add legend\n",
    "legend_labels = [pltRectangle((0, 0), 1, 1, color = color_map.get(region, 'gray')) for region in df['Region'].unique()]\n",
    "ax.legend(legend_labels, df['Region'].unique())\n",
    "\n",
    "plt.title('Load Balancing results')\n",
    "plt.xlabel('Run #')\n",
    "plt.ylabel('Response Time')\n",
    "plt.xticks(rotation = 0)\n",
    "\n",
    "average = df['Response Time'].mean()\n",
    "plt.axhline(y = average, color = 'r', linestyle = '--', label = f'Average: {average:.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n",
    "Repeat the same test using the Python SDK to ensure compatibility. Note that we do not know what region served the response; we only see that we obtained a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 100\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = apim_resource_gateway_url,\n",
    "    api_key = apim_subscription_key,\n",
    "    api_version = model_api_version\n",
    ")\n",
    "\n",
    "for i in range(runs):\n",
    "    print(f\"‚ñ∂Ô∏è Run {i+1}/{runs}:\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    raw_response = client.chat.completions.with_raw_response.create(\n",
    "        model = model_name,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "        ])\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    print(f\"‚åö {response_time:.2f} seconds\")\n",
    "    print(f\"x-ms-region: \\x1b[1;32m{raw_response.headers.get(\"x-ms-region\")}\\x1b[0m\") # this header is useful to determine the region of the backend that served the request\n",
    "\n",
    "    response = raw_response.parse()\n",
    "\n",
    "    if response.usage:\n",
    "        print(f\"Token usage:\\n   Total tokens: {response.usage.total_tokens}\\n   Prompt tokens: {response.usage.prompt_tokens}\\n   Completion tokens: {response.usage.completion_tokens}\\n\")\n",
    "\n",
    "    print(f\"üí¨ {response.choices[0].message.content}\\n\")\n",
    "\n",
    "    time.sleep(sleep_time_ms/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
