{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master AI Gateway Workshop\n",
    "\n",
    "**One-Click Deployment for Azure AI Gateway with MCP Integration**\n",
    "\n",
    "## Deployment Flow\n",
    "1. **Bootstrap** - Minimal configuration (no master-lab.env needed)\n",
    "2. **Deploy** - Azure resources via Bicep\n",
    "3. **Generate** - Create master-lab.env from deployment outputs\n",
    "4. **Configure** - Load complete configuration\n",
    "5. **Run** - MCP servers, policies, exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEPENDENCY INSTALLATION FOR WSL/DEBIAN\n",
      "================================================================================\n",
      "\n",
      "⚠️  If you're using WSL (Windows Subsystem for Linux) or Debian:\n",
      "\n",
      "   → Skip this cell and use Cell 9 instead!\n",
      "\n",
      "   Cell 9 automatically handles:\n",
      "   ✅ Externally-managed-environment errors\n",
      "   ✅ WSL filesystem limitations\n",
      "   ✅ Python 3.12 compatibility\n",
      "   ✅ Automatic package installation to user directory\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Cell 9 is located a few cells down.\n",
      "Look for: '(-1.2) Dependencies Install (Smart Multi-Environment)'\n",
      "\n",
      "Just run Cell 9, then restart your kernel!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Python 3.12 Setup - WSL/Debian Users: Use Cell 9 Instead!\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEPENDENCY INSTALLATION FOR WSL/DEBIAN\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"⚠️  If you're using WSL (Windows Subsystem for Linux) or Debian:\")\n",
    "print()\n",
    "print(\"   → Skip this cell and use Cell 9 instead!\")\n",
    "print()\n",
    "print(\"   Cell 9 automatically handles:\")\n",
    "print(\"   ✅ Externally-managed-environment errors\")\n",
    "print(\"   ✅ WSL filesystem limitations\")\n",
    "print(\"   ✅ Python 3.12 compatibility\")\n",
    "print(\"   ✅ Automatic package installation to user directory\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Cell 9 is located a few cells down.\")\n",
    "print(\"Look for: '(-1.2) Dependencies Install (Smart Multi-Environment)'\")\n",
    "print()\n",
    "print(\"Just run Cell 9, then restart your kernel!\")\n",
    "print()\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Bootstrap & Initial Setup\n",
    "\n",
    "**Important**: These cells run WITHOUT master-lab.env (it doesn't exist yet!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 003: Environment Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Workspace: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "Python: 3.12.3\n"
     ]
    }
   ],
   "source": [
    "# Cell 003: Environment Detection\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_CODESPACE = bool(os.getenv('CODESPACE_NAME'))\n",
    "WORKSPACE_ROOT = Path.cwd()\n",
    "\n",
    "print(f\"Environment: {'GitHub Codespace' if IS_CODESPACE else 'Local'}\")\n",
    "print(f\"Workspace: {WORKSPACE_ROOT}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 005: Bootstrap Configuration (Minimal)\n",
    "Load ONLY subscription, resource group, location from bootstrap.env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detecting notebook directory...\n",
      "    Current working directory: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "    Platform: linux\n",
      "    Environment: WSL (Windows Subsystem for Linux)\n",
      "[OK] Method 1: Found in current directory\n",
      "\n",
      "[OK] Notebook directory: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "[OK] Changed working directory to: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "[OK] Loading from: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/bootstrap.env\n",
      "\n",
      "Bootstrap Configuration:\n",
      "  Subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  Resource Group: lab-master-lab\n",
      "  Location: eastus2\n",
      "\n",
      "[OK] Bootstrap configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 005: Load Bootstrap Configuration (minimal)\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get notebook directory (works in WSL and Windows)\n",
    "NOTEBOOK_DIR = None\n",
    "\n",
    "print(\"[*] Detecting notebook directory...\")\n",
    "print(f\"    Current working directory: {Path.cwd()}\")\n",
    "print(f\"    Platform: {sys.platform}\")\n",
    "\n",
    "# Detect if running in WSL\n",
    "IS_WSL = 'microsoft' in str(Path('/proc/version').read_text()).lower() if Path('/proc/version').exists() else False\n",
    "if IS_WSL:\n",
    "    print(\"    Environment: WSL (Windows Subsystem for Linux)\")\n",
    "else:\n",
    "    print(f\"    Environment: Native {sys.platform}\")\n",
    "\n",
    "# Method 1: Check if we're in the right directory already\n",
    "if (Path.cwd() / 'bootstrap.env').exists() or (Path.cwd() / 'bootstrap.env.template').exists():\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "    print(f\"[OK] Method 1: Found in current directory\")\n",
    "\n",
    "# Method 2: Use known absolute path (WSL-aware)\n",
    "if NOTEBOOK_DIR is None:\n",
    "    if IS_WSL:\n",
    "        # WSL path format\n",
    "        known_path = Path('/mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab')\n",
    "    else:\n",
    "        # Windows path format\n",
    "        known_path = Path(r'C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab')\n",
    "\n",
    "    print(f\"[*] Method 2: Checking known path: {known_path}\")\n",
    "\n",
    "    if known_path.exists():\n",
    "        NOTEBOOK_DIR = known_path\n",
    "        print(f\"[OK] Method 2: Using known path\")\n",
    "    else:\n",
    "        print(f\"    Path does not exist\")\n",
    "\n",
    "# Method 3: Search parent directories\n",
    "if NOTEBOOK_DIR is None:\n",
    "    print(f\"[*] Method 3: Searching parent directories...\")\n",
    "    current = Path.cwd()\n",
    "    for level in range(5):\n",
    "        print(f\"    Checking: {current}\")\n",
    "        if (current / 'bootstrap.env').exists() or (current / 'bootstrap.env.template').exists():\n",
    "            NOTEBOOK_DIR = current\n",
    "            print(f\"[OK] Method 3: Found at level {level}\")\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "# Method 4: Navigate from current directory if we see AI-Gateway\n",
    "if NOTEBOOK_DIR is None:\n",
    "    print(f\"[*] Method 4: Looking for AI-Gateway in current directory...\")\n",
    "    current = Path.cwd()\n",
    "\n",
    "    # Check if AI-Gateway exists in current dir\n",
    "    ai_gateway = current / 'AI-Gateway'\n",
    "    if ai_gateway.exists() and ai_gateway.is_dir():\n",
    "        master_lab = ai_gateway / 'labs' / 'master-lab'\n",
    "        print(f\"    Found AI-Gateway, checking: {master_lab}\")\n",
    "        if master_lab.exists() and ((master_lab / 'bootstrap.env').exists() or (master_lab / 'bootstrap.env.template').exists()):\n",
    "            NOTEBOOK_DIR = master_lab\n",
    "            print(f\"[OK] Method 4: Found via AI-Gateway navigation\")\n",
    "\n",
    "# Method 5: Search for master-lab folder in tree\n",
    "if NOTEBOOK_DIR is None:\n",
    "    print(f\"[*] Method 5: Searching for master-lab folder...\")\n",
    "    current = Path.cwd()\n",
    "\n",
    "    # Check current and all parents\n",
    "    for parent in [current] + list(current.parents)[:5]:\n",
    "        if parent.name == 'master-lab':\n",
    "            if (parent / 'bootstrap.env').exists() or (parent / 'bootstrap.env.template').exists():\n",
    "                NOTEBOOK_DIR = parent\n",
    "                print(f\"[OK] Method 5: Found master-lab folder: {parent}\")\n",
    "                break\n",
    "\n",
    "        # Also check if master-lab exists as subdirectory\n",
    "        master_lab_candidates = list(parent.glob('**/master-lab'))\n",
    "        for candidate in master_lab_candidates[:3]:  # Check first 3 matches\n",
    "            if (candidate / 'bootstrap.env').exists() or (candidate / 'bootstrap.env.template').exists():\n",
    "                NOTEBOOK_DIR = candidate\n",
    "                print(f\"[OK] Method 5: Found master-lab via glob: {candidate}\")\n",
    "                break\n",
    "\n",
    "        if NOTEBOOK_DIR:\n",
    "            break\n",
    "\n",
    "if NOTEBOOK_DIR is None:\n",
    "    # Last resort: Show what's available\n",
    "    print(\"\\n[!] DEBUG: Current directory contents:\")\n",
    "    try:\n",
    "        items = list(Path.cwd().iterdir())\n",
    "        for item in items[:20]:\n",
    "            marker = \"DIR\" if item.is_dir() else \"   \"\n",
    "            print(f\"    [{marker}] {item.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error listing: {e}\")\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Cannot locate notebook directory.\\n\"\n",
    "        f\"Current directory: {Path.cwd()}\\n\"\n",
    "        f\"Platform: {sys.platform} ({'WSL' if IS_WSL else 'Native'})\\n\"\n",
    "        \"Expected to find: bootstrap.env or bootstrap.env.template\\n\"\n",
    "        \"\\n\"\n",
    "        \"Possible solutions:\\n\"\n",
    "        \"1. Change to the notebook directory first:\\n\"\n",
    "        \"   import os\\n\"\n",
    "        \"   os.chdir(r'C:\\\\Users\\\\lproux\\\\Documents\\\\GitHub\\\\MCP-servers-internalMSFT-and-external\\\\AI-Gateway\\\\labs\\\\master-lab')\\n\"\n",
    "        \"   # or in WSL:\\n\"\n",
    "        \"   os.chdir('/mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab')\\n\"\n",
    "        \"\\n\"\n",
    "        \"2. Or create bootstrap.env.template in the current directory\"\n",
    "    )\n",
    "\n",
    "# Change to notebook directory\n",
    "os.chdir(NOTEBOOK_DIR)\n",
    "print(f\"\\n[OK] Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"[OK] Changed working directory to: {Path.cwd()}\")\n",
    "\n",
    "@dataclass\n",
    "class BootstrapConfig:\n",
    "    subscription_id: str = \"\"\n",
    "    resource_group: str = \"ai-gateway-workshop\"\n",
    "    location: str = \"eastus2\"\n",
    "    deploy_suffix: str = \"\"\n",
    "\n",
    "# Use absolute path for bootstrap.env\n",
    "bootstrap_file = NOTEBOOK_DIR / 'bootstrap.env'\n",
    "if not bootstrap_file.exists():\n",
    "    print(f\"[WARN] bootstrap.env not found at: {bootstrap_file}\")\n",
    "    bootstrap_file = NOTEBOOK_DIR / 'bootstrap.env.template'\n",
    "    print(f\"[INFO] Using template: {bootstrap_file}\")\n",
    "\n",
    "# Load ONLY bootstrap values\n",
    "bootstrap = BootstrapConfig()\n",
    "if bootstrap_file.exists():\n",
    "    print(f\"[OK] Loading from: {bootstrap_file}\")\n",
    "    for line in bootstrap_file.read_text().splitlines():\n",
    "        if '=' in line and not line.strip().startswith('#'):\n",
    "            key, value = line.split('=', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            if hasattr(bootstrap, key.lower()):\n",
    "                setattr(bootstrap, key.lower(), value)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Bootstrap file not found at: {bootstrap_file}\\n\"\n",
    "        f\"Please create bootstrap.env\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBootstrap Configuration:\")\n",
    "print(f\"  Subscription: {bootstrap.subscription_id or 'NOT SET'}\")\n",
    "print(f\"  Resource Group: {bootstrap.resource_group}\")\n",
    "print(f\"  Location: {bootstrap.location}\")\n",
    "\n",
    "# Validate\n",
    "if not bootstrap.subscription_id:\n",
    "    raise ValueError(\n",
    "        \"SUBSCRIPTION_ID must be set in bootstrap.env\\n\"\n",
    "        f\"File location: {bootstrap_file}\\n\"\n",
    "        \"Please edit the file and add your Azure subscription ID.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n[OK] Bootstrap configuration loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 007: Dependencies Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cell_3_41f69468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEPENDENCY INSTALLATION\n",
      "================================================================================\n",
      "\n",
      "Python: 3.12.3\n",
      "Path:   /usr/bin/python\n",
      "In virtual environment: False\n",
      "System Python: True\n",
      "Externally managed: True\n",
      "\n",
      "⚠️  Externally-managed system Python detected\n",
      "   Using --user flag to install to user site-packages\n",
      "\n",
      "================================================================================\n",
      "[1/2] Installing python-dotenv (critical for environment loading)...\n",
      "      ⚠️  Warning: \u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally \n",
      "\n",
      "[2/2] Installing from: requirements-py312.txt\n",
      "      (Python 3.12+ - no pyautogen)\n",
      "\n",
      "      Running pip install...\n",
      "      Command: /usr/bin/python -m pip install --user -r requirements-py312.txt\n",
      "\n",
      "      \u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "      \n",
      "      \u001b[31m×\u001b[0m This environment is externally managed\n",
      "      \u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "      \u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "      \u001b[31m   \u001b[0m install.\n",
      "      \u001b[31m   \u001b[0m\n",
      "      \u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "      \u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "      \u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "      \u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "      \u001b[31m   \u001b[0m\n",
      "      \u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "      \u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "      \u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "      \u001b[31m   \u001b[0m\n",
      "      \u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "      \n",
      "      \u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "      \u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\n",
      "      ⚠️  pip exited with code 1\n",
      "      Some packages may have failed - check output above\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "✅ Packages installed to: /home/lproux/.local/lib/python3.12/site-packages\n",
      "   Using --user flag (externally-managed system)\n",
      "\n",
      "ℹ️  Note: Python 3.12+ detected\n",
      "   - AutoGen 0.2.x skipped (not compatible)\n",
      "   - All other packages installed successfully\n",
      "   - Cells 8, 105, 111 can be skipped (AutoGen setup)\n",
      "\n",
      "Next steps:\n",
      "  1. Restart kernel if needed (Kernel → Restart Kernel)\n",
      "  2. Continue with the labs!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# (-1.2) Dependencies Install (Smart Multi-Environment)\n",
    "import sys\n",
    "import subprocess\n",
    "import pathlib\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEPENDENCY INSTALLATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check Python version\n",
    "py_version = sys.version_info\n",
    "print(f'\\nPython: {py_version.major}.{py_version.minor}.{py_version.micro}')\n",
    "print(f'Path:   {sys.executable}')\n",
    "\n",
    "# 2. Detect environment\n",
    "in_venv = sys.prefix != sys.base_prefix\n",
    "is_system_python = '/usr/bin/python' in sys.executable or '/usr/local/bin/python' in sys.executable\n",
    "externally_managed = is_system_python and py_version.major == 3 and py_version.minor >= 11\n",
    "\n",
    "print(f'In virtual environment: {in_venv}')\n",
    "print(f'System Python: {is_system_python}')\n",
    "print(f'Externally managed: {externally_managed}')\n",
    "\n",
    "# 3. Determine pip install strategy\n",
    "pip_args = [sys.executable, '-m', 'pip', 'install']\n",
    "\n",
    "if in_venv:\n",
    "    # In a virtual environment - install normally\n",
    "    print('\\n✅ Virtual environment detected - installing packages normally')\n",
    "    extra_args = []\n",
    "elif externally_managed:\n",
    "    # System Python with PEP 668 (externally-managed-environment)\n",
    "    print('\\n⚠️  Externally-managed system Python detected')\n",
    "    print('   Using --user flag to install to user site-packages')\n",
    "    extra_args = ['--user']\n",
    "else:\n",
    "    # Other cases (older Python, non-Debian systems)\n",
    "    print('\\n✅ Installing packages normally')\n",
    "    extra_args = []\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 4. Install python-dotenv first (CRITICAL - needed by subsequent cells)\n",
    "print('[1/2] Installing python-dotenv (critical for environment loading)...')\n",
    "cmd_dotenv = pip_args + extra_args + ['-q', 'python-dotenv>=1.0.0']\n",
    "\n",
    "try:\n",
    "    r = subprocess.run(cmd_dotenv, capture_output=True, text=True, timeout=60)\n",
    "    if r.returncode == 0:\n",
    "        print('      ✅ python-dotenv installed')\n",
    "    else:\n",
    "        print(f'      ⚠️  Warning: {r.stderr.strip()[:100]}')\n",
    "        # Try without -q for better error messages\n",
    "        if '--user' not in extra_args and not in_venv:\n",
    "            print('      Retrying with --user flag...')\n",
    "            cmd_dotenv_retry = pip_args + ['--user', 'python-dotenv>=1.0.0']\n",
    "            r2 = subprocess.run(cmd_dotenv_retry, capture_output=True, text=True, timeout=60)\n",
    "            if r2.returncode == 0:\n",
    "                print('      ✅ python-dotenv installed with --user')\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('      ⚠️  Installation timeout (network issue?)')\n",
    "except Exception as e:\n",
    "    print(f'      ⚠️  Error: {e}')\n",
    "\n",
    "print()\n",
    "\n",
    "# 5. Determine which requirements file to use\n",
    "REQ_FILE = pathlib.Path('requirements.txt')\n",
    "REQ_FILE_PY312 = pathlib.Path('requirements-py312.txt')\n",
    "\n",
    "# Use Python 3.12-specific requirements if available and Python >= 3.12\n",
    "if py_version.minor >= 12 and REQ_FILE_PY312.exists():\n",
    "    install_file = REQ_FILE_PY312\n",
    "    print(f'[2/2] Installing from: {install_file}')\n",
    "    print('      (Python 3.12+ - no pyautogen)')\n",
    "elif REQ_FILE.exists():\n",
    "    req_content = REQ_FILE.read_text()\n",
    "\n",
    "    # If Python >= 3.12 but no py312 requirements, create temp file without pyautogen\n",
    "    if py_version.minor >= 12:\n",
    "        print('[2/2] Python 3.12+ detected - filtering out pyautogen...')\n",
    "\n",
    "        temp_req = pathlib.Path('.requirements-temp.txt')\n",
    "        lines = []\n",
    "        for line in req_content.splitlines():\n",
    "            # Skip pyautogen but keep comments\n",
    "            if 'pyautogen' not in line.lower() or line.strip().startswith('#'):\n",
    "                lines.append(line)\n",
    "        temp_req.write_text('\\n'.join(lines))\n",
    "        install_file = temp_req\n",
    "        print(f'      Installing from: {install_file} (filtered)')\n",
    "    else:\n",
    "        install_file = REQ_FILE\n",
    "        print(f'[2/2] Installing from: {install_file}')\n",
    "else:\n",
    "    print('[2/2] ❌ No requirements file found')\n",
    "    install_file = None\n",
    "\n",
    "# 6. Install all dependencies\n",
    "if install_file:\n",
    "    cmd = pip_args + extra_args + ['-r', str(install_file)]\n",
    "\n",
    "    print()\n",
    "    print('      Running pip install...')\n",
    "    print(f'      Command: {\" \".join(shlex.quote(str(c)) for c in cmd)}')\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Run with real-time output\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "\n",
    "        # Print output in real-time (truncated)\n",
    "        line_count = 0\n",
    "        for line in process.stdout:\n",
    "            line_count += 1\n",
    "            # Only print first 20 and last 10 lines to avoid flooding\n",
    "            if line_count <= 20:\n",
    "                print(f'      {line.rstrip()}')\n",
    "            elif line_count == 21:\n",
    "                print('      ... (truncating output) ...')\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        print()\n",
    "        if process.returncode == 0:\n",
    "            print('      ✅ All dependencies installed successfully!')\n",
    "        else:\n",
    "            print(f'      ⚠️  pip exited with code {process.returncode}')\n",
    "            print('      Some packages may have failed - check output above')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'      ❌ Error during installation: {e}')\n",
    "\n",
    "    # Clean up temp file\n",
    "    if install_file.name == '.requirements-temp.txt' and install_file.exists():\n",
    "        install_file.unlink()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 7. Summary\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if in_venv:\n",
    "    print(f\"✅ Packages installed to: {sys.prefix}\")\n",
    "    print(\"   You're using a virtual environment (recommended!)\")\n",
    "elif extra_args and '--user' in extra_args:\n",
    "    import site\n",
    "    print(f\"✅ Packages installed to: {site.USER_SITE}\")\n",
    "    print(\"   Using --user flag (externally-managed system)\")\n",
    "else:\n",
    "    print(f\"✅ Packages installed to: {sys.prefix}\")\n",
    "\n",
    "if py_version.minor >= 12:\n",
    "    print()\n",
    "    print(\"ℹ️  Note: Python 3.12+ detected\")\n",
    "    print(\"   - AutoGen 0.2.x skipped (not compatible)\")\n",
    "    print(\"   - All other packages installed successfully\")\n",
    "    print(\"   - Cells 8, 105, 111 can be skipped (AutoGen setup)\")\n",
    "\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Restart kernel if needed (Kernel → Restart Kernel)\")\n",
    "print(\"  2. Continue with the labs!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 009: Azure Authentication & Service Principal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az resolved: /usr/bin/az (reason=ranked selection)\n",
      "[azure] az version: azure-cli                         2.78.0 *\n",
      "[azure] Using existing SUBSCRIPTION_ID from environment: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[azure] SP credentials already present; skipping creation\n",
      "  SUBSCRIPTION_ID=d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  AZURE_CLIENT_ID=4a5d0f1a-578e-479a-8ba9-05770ae9ce6b\n",
      "  AZURE_TENANT_ID=2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "  AZURE_CLIENT_SECRET=***\n",
      "[msal] MSAL cache flush helpers loaded\n",
      "[msal] Available functions: flush_msal_cache(), az_with_msal_retry()\n",
      "[endpoint] Existing OPENAI_ENDPOINT found; using as-is\n",
      "[endpoint] OPENAI_ENDPOINT = https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "[endpoint] Derived convenience vars: OPENAI_API_BASE, OPENAI_MODELS_URL\n"
     ]
    }
   ],
   "source": [
    "# (-1.3) Azure CLI & Service Principal Setup (Consolidated v2)\n",
    "import json, os, shutil, subprocess, sys, time\n",
    "from pathlib import Path\n",
    "AZ_CREDS_FILE=Path('.azure-credentials.env')\n",
    "\n",
    "OS_RELEASE = {}\n",
    "try:\n",
    "    if Path('/etc/os-release').exists():\n",
    "        for line in Path('/etc/os-release').read_text().splitlines():\n",
    "            if '=' in line:\n",
    "                k,v=line.split('=',1)\n",
    "                OS_RELEASE[k]=v.strip().strip('\"')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ARCH_LINUX = OS_RELEASE.get('ID') == 'arch'\n",
    "CODESPACES = bool(os.environ.get('CODESPACES')) or bool(os.environ.get('CODESPACE_NAME'))\n",
    "# Retry delay between Azure CLI timeout retries (override with AZ_RETRY_DELAY_SEC env var)\n",
    "retry_delay_sec = float(os.environ.get('AZ_RETRY_DELAY_SEC', '3'))\n",
    "\n",
    "def resolve_az_cli():\n",
    "    # 1. Explicit override\n",
    "    override=os.environ.get('AZURE_CLI_PATH')\n",
    "    if override and Path(override).exists():\n",
    "        return override, 'env AZURE_CLI_PATH'\n",
    "    candidates = []\n",
    "    # which-based\n",
    "    for name in ['az','az.cmd','az.exe']:\n",
    "        p=shutil.which(name)\n",
    "        if p: candidates.append(p)\n",
    "    # Common Linux / macOS locations\n",
    "    candidates += [\n",
    "        '/usr/bin/az', '/usr/local/bin/az', '/snap/bin/az', '/opt/homebrew/bin/az'\n",
    "    ]\n",
    "    # Codespaces typical path (if pip user install)\n",
    "    if CODESPACES:\n",
    "        candidates.append(str(Path.home()/'.local/bin/az'))\n",
    "    # Windows typical install locations\n",
    "    candidates += [\n",
    "        'C:/Program Files (x86)/Microsoft SDKs/Azure/CLI2/wbin/az.cmd',\n",
    "        'C:/Program Files/Microsoft SDKs/Azure/CLI2/wbin/az.cmd'\n",
    "    ]\n",
    "    # Home azure-cli shim\n",
    "    home_cli = Path.home()/'.azure-cli/az'\n",
    "    candidates.append(str(home_cli))\n",
    "    # Remove non-existing\n",
    "    existing=[c for c in candidates if c and Path(c).exists()]\n",
    "    if not existing:\n",
    "        # Last-resort: if a pip install put az inside .venv Scripts\n",
    "        venv_az = Path(sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "        if venv_az.exists():\n",
    "            return str(venv_az), 'venv fallback'\n",
    "        return None, 'not found'\n",
    "    # Rank: prefer system-level (exclude .venv & Scripts) then shortest path\n",
    "    def rank(p):\n",
    "        p_low=p.lower()\n",
    "        penalty = 1000 if ('.venv' in p_low or 'scripts' in p_low) else 0\n",
    "        return penalty, len(p)\n",
    "    existing.sort(key=rank)\n",
    "    chosen=existing[0]\n",
    "    return chosen, 'ranked selection'\n",
    "\n",
    "az_cli, reason = resolve_az_cli()\n",
    "print(f'[azure] az resolved: {az_cli or \"NOT FOUND\"} (reason={reason})')\n",
    "if not az_cli:\n",
    "    if ARCH_LINUX:\n",
    "        print('[azure] Arch Linux detected. Install Azure CLI: sudo pacman -S azure-cli')\n",
    "    else:\n",
    "        print('[azure] Install Azure CLI: https://learn.microsoft.com/cli/azure/install-azure-cli')\n",
    "    raise SystemExit('Azure CLI not found.')\n",
    "\n",
    "os.environ['AZ_CLI']=az_cli\n",
    "# Quick version check with short timeout\n",
    "try:\n",
    "    ver=subprocess.run([az_cli,'--version'],capture_output=True,text=True,timeout=4)\n",
    "    if ver.returncode==0:\n",
    "        first_line=ver.stdout.splitlines()[0] if ver.stdout else ''\n",
    "        print('[azure] az version:', first_line)\n",
    "    else:\n",
    "        print('[azure] az --version exit', ver.returncode)\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('[azure] WARN: az version check timed out (continuing)')\n",
    "except Exception as e:\n",
    "    print('[azure] WARN: az version check error:', e)\n",
    "\n",
    "# Subscription discovery (robust with timeout retries)\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')  # existing env takes precedence\n",
    "sub_proc = None\n",
    "if not subscription_id:\n",
    "    attempts = 2\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        try:\n",
    "            timeout_sec = 8 if attempt == 1 else 20  # longer second attempt\n",
    "            sub_proc = subprocess.run(\n",
    "                [az_cli, 'account', 'show', '--output', 'json'],\n",
    "                capture_output=True, text=True, timeout=timeout_sec\n",
    "            )\n",
    "            if sub_proc.returncode == 0:\n",
    "                try:\n",
    "                    sub = json.loads(sub_proc.stdout)\n",
    "                    subscription_id = sub.get('id')\n",
    "                    print('[azure] Active subscription:', subscription_id)\n",
    "                    if subscription_id:\n",
    "                        os.environ.setdefault('SUBSCRIPTION_ID', subscription_id)\n",
    "                except Exception as e:\n",
    "                    print('[azure] Parse error account show:', e)\n",
    "                break\n",
    "            else:\n",
    "                print(f'[azure] account show failed (rc={sub_proc.returncode}): {sub_proc.stderr[:200]}')\n",
    "                break  # non-timeout failure; do not retry\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f'[azure] account show timed out (attempt {attempt}/{attempts}, timeout={timeout_sec}s)')\n",
    "            if attempt < attempts:\n",
    "                time.sleep(retry_delay_sec)  # use existing retry delay variable\n",
    "            else:\n",
    "                print('[azure] ERROR: account show timed out; skipping subscription discovery')\n",
    "else:\n",
    "    print('[azure] Using existing SUBSCRIPTION_ID from environment:', subscription_id)\n",
    "\n",
    "# Ensure Service Principal\n",
    "sp_env_keys=['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID']\n",
    "creds_present=all(os.environ.get(k) for k in sp_env_keys)\n",
    "if creds_present:\n",
    "    print('[azure] SP credentials already present; skipping creation')\n",
    "elif AZ_CREDS_FILE.exists():\n",
    "    print('[azure] Loading existing credentials file')\n",
    "    for line in AZ_CREDS_FILE.read_text().splitlines():\n",
    "        if line.strip() and '=' in line:\n",
    "            k,v=line.split('=',1); os.environ.setdefault(k.strip(),v.strip())\n",
    "else:\n",
    "    if not os.environ.get('SUBSCRIPTION_ID'):\n",
    "        print('[azure] Cannot create SP: missing SUBSCRIPTION_ID')\n",
    "    else:\n",
    "        print('[azure] Creating new service principal (Contributor)')\n",
    "        sp_cmd=[az_cli,'ad','sp','create-for-rbac','--name','ai-gateway-sp','--role','Contributor','--scopes',f\"/subscriptions/{os.environ.get('SUBSCRIPTION_ID','')}\",\"--sdk-auth\"]\n",
    "        r=subprocess.run(sp_cmd,capture_output=True,text=True,timeout=40)\n",
    "        if r.returncode!=0:\n",
    "            print('[azure] SP creation failed:', r.stderr[:300])\n",
    "        else:\n",
    "            data=json.loads(r.stdout)\n",
    "            mapping={'clientId':'AZURE_CLIENT_ID','clientSecret':'AZURE_CLIENT_SECRET','tenantId':'AZURE_TENANT_ID','subscriptionId':'SUBSCRIPTION_ID'}\n",
    "            for src,dst in mapping.items():\n",
    "                if src in data:\n",
    "                    os.environ[dst]=data[src]\n",
    "            lines=[f'{k}={os.environ[k]}' for k in mapping.values() if k in os.environ]\n",
    "            AZ_CREDS_FILE.write_text('\\n'.join(lines))\n",
    "            print('[azure] SP created & credentials saved (.azure-credentials.env)')\n",
    "\n",
    "# Masked summary\n",
    "for k in ['SUBSCRIPTION_ID','AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET']:\n",
    "    v=os.environ.get(k)\n",
    "    if not v: continue\n",
    "    masked='***' if 'SECRET' in k else v\n",
    "    print(f'  {k}={masked}')\n",
    "\n",
    "\n",
    "# (-1.3b) MSAL Cache Flush Helper\n",
    "\"\"\"Helper function to flush MSAL cache when Azure CLI encounters MSAL corruption.\n",
    "\n",
    "The MSAL error 'Can't get attribute NormalizedResponse' indicates cache corruption.\n",
    "This helper safely clears the MSAL cache and retries Azure CLI operations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def flush_msal_cache():\n",
    "    \"\"\"Flush MSAL cache directories to resolve cache corruption.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if cache was flushed successfully\n",
    "    \"\"\"\n",
    "    msal_cache_dirs = [\n",
    "        Path.home() / '.azure' / 'msal_token_cache.bin',\n",
    "        Path.home() / '.azure' / 'msal_token_cache.json',\n",
    "        Path.home() / '.azure' / 'msal_http_cache',\n",
    "        Path.home() / '.azure' / 'service_principal_entries.bin',\n",
    "    ]\n",
    "    \n",
    "    flushed = []\n",
    "    for cache_path in msal_cache_dirs:\n",
    "        try:\n",
    "            if cache_path.exists():\n",
    "                if cache_path.is_file():\n",
    "                    cache_path.unlink()\n",
    "                    flushed.append(str(cache_path))\n",
    "                elif cache_path.is_dir():\n",
    "                    shutil.rmtree(cache_path)\n",
    "                    flushed.append(str(cache_path))\n",
    "        except Exception as e:\n",
    "            print(f'[msal] Warning: Could not remove {cache_path}: {e}')\n",
    "    \n",
    "    if flushed:\n",
    "        print(f'[msal] Flushed {len(flushed)} cache entries')\n",
    "        return True\n",
    "    else:\n",
    "        print('[msal] No cache entries found to flush')\n",
    "        return False\n",
    "\n",
    "def az_with_msal_retry(az_cli, command_args, **kwargs):\n",
    "    \"\"\"Execute Azure CLI command with automatic MSAL cache flush on error.\n",
    "    \n",
    "    Args:\n",
    "        az_cli: Path to az CLI executable\n",
    "        command_args: List of command arguments (e.g., ['account', 'show'])\n",
    "        **kwargs: Additional arguments for subprocess.run()\n",
    "    \n",
    "    Returns:\n",
    "        subprocess.CompletedProcess: Result of the command\n",
    "    \"\"\"\n",
    "    # Ensure capture_output and text are set\n",
    "    kwargs.setdefault('capture_output', True)\n",
    "    kwargs.setdefault('text', True)\n",
    "    kwargs.setdefault('timeout', 30)\n",
    "    \n",
    "    # First attempt\n",
    "    result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "    \n",
    "    # Check for MSAL error\n",
    "    if result.returncode != 0 and 'NormalizedResponse' in result.stderr:\n",
    "        print('[msal] MSAL cache corruption detected, flushing cache...')\n",
    "        flush_msal_cache()\n",
    "        \n",
    "        # Re-login if needed\n",
    "        print('[msal] Re-authenticating...')\n",
    "        login_result = subprocess.run(\n",
    "            [az_cli, 'login'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if login_result.returncode == 0:\n",
    "            print('[msal] Re-authentication successful, retrying command...')\n",
    "            # Retry the original command\n",
    "            result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "        else:\n",
    "            print(f'[msal] Re-authentication failed: {login_result.stderr[:200]}')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('[msal] MSAL cache flush helpers loaded')\n",
    "print('[msal] Available functions: flush_msal_cache(), az_with_msal_retry()')\n",
    "\n",
    "\n",
    "# (-1.4) Endpoint Normalizer & Derived Variables\n",
    "\"\"\"\n",
    "Derives OPENAI_ENDPOINT and related derived variables if missing.\n",
    "Logic priority:\n",
    "1. Use explicit OPENAI_ENDPOINT if set (leave unchanged).\n",
    "2. Else if APIM_GATEWAY_URL + INFERENCE_API_PATH present -> compose.\n",
    "3. Else attempt Foundry style endpoints (AZURE_OPENAI_ENDPOINT, AI_FOUNDRY_ENDPOINT).\n",
    "Persist back to master-lab.env if value was newly derived.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os, re\n",
    "env_path=Path('master-lab.env')\n",
    "text=env_path.read_text() if env_path.exists() else ''\n",
    "get=lambda k: os.environ.get(k) or re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE).group(1).strip() if re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE) else ''\n",
    "openai_endpoint=get('OPENAI_ENDPOINT')\n",
    "modified=False\n",
    "if openai_endpoint:\n",
    "    print('[endpoint] Existing OPENAI_ENDPOINT found; using as-is')\n",
    "else:\n",
    "    apim=get('APIM_GATEWAY_URL')\n",
    "    path_var=get('INFERENCE_API_PATH') or '/inference'\n",
    "    if apim:\n",
    "        openai_endpoint=apim.rstrip('/')+path_var\n",
    "        print('[endpoint] Derived from APIM_GATEWAY_URL + INFERENCE_API_PATH')\n",
    "        modified=True\n",
    "    else:\n",
    "        fallback=get('AZURE_OPENAI_ENDPOINT') or get('AI_FOUNDRY_ENDPOINT')\n",
    "        if fallback:\n",
    "            openai_endpoint=fallback.rstrip('/')\n",
    "            print('[endpoint] Derived from Foundry/Azure fallback endpoint')\n",
    "            modified=True\n",
    "        else:\n",
    "            print('[endpoint] Unable to derive endpoint; please set OPENAI_ENDPOINT manually in master-lab.env')\n",
    "if openai_endpoint:\n",
    "    os.environ['OPENAI_ENDPOINT']=openai_endpoint\n",
    "    print('[endpoint] OPENAI_ENDPOINT =', openai_endpoint)\n",
    "    if modified and env_path.exists():\n",
    "        # update file\n",
    "        lines=[]\n",
    "        found=False\n",
    "        for line in text.splitlines():\n",
    "            if line.startswith('OPENAI_ENDPOINT='):\n",
    "                lines.append(f'OPENAI_ENDPOINT={openai_endpoint}')\n",
    "                found=True\n",
    "            else:\n",
    "                lines.append(line)\n",
    "        if not found:\n",
    "            lines.append(f'OPENAI_ENDPOINT={openai_endpoint}')\n",
    "        env_path.write_text('\\n'.join(lines))\n",
    "        print('[endpoint] Persisted derived endpoint to master-lab.env')\n",
    "# Convenience derived variables (could be referenced later)\n",
    "os.environ.setdefault('OPENAI_API_BASE', openai_endpoint)\n",
    "os.environ.setdefault('OPENAI_MODELS_URL', openai_endpoint.rstrip('/') + '/models')\n",
    "print('[endpoint] Derived convenience vars: OPENAI_API_BASE, OPENAI_MODELS_URL')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 011: Core Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[az] version: azure-cli                         2.78.0 *\n",
      "[az] account: ME-MngEnvMCAP592090-lproux-1 d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[shim] AzureOpenAI shim ready.\n",
      "[deploy] helpers ready\n",
      "[policy] Missing env vars; set: APIM_SERVICE\n",
      "⚠️  MCP Client already initialized. Skipping re-initialization.\n",
      "\n",
      "Available Data Sources:\n",
      "  ✓ Excel MCP: http://excel-mcp-master.eastus.azurecontainer.io:8000\n",
      "  ✓ Docs MCP: http://docs-mcp-master.eastus.azurecontainer.io:8000\n",
      "  ✓ GitHub API (APIM): https://apim-pavavy6pu5hpa.azure-api.net/github\n",
      "  ✓ Weather API (APIM): https://apim-pavavy6pu5hpa.azure-api.net/weather\n",
      "[AzureOps] CLI: /usr/bin/az\n",
      "[AzureOps] login status: OK\n",
      "[AzureOps] version: azure-cli                         2.78.0 *\n",
      "[AzureOps] strategy: sdk\n"
     ]
    }
   ],
   "source": [
    "# (-1.5) Unified az() Helper & Login Check\n",
    "\"\"\"Provides a cached az CLI executor with:\n",
    "- Path reuse via AZ_CLI env (expects (-1.3) run first)\n",
    "- Automatic login prompt if account show fails and no service principal creds\n",
    "- Timeout controls & JSON parsing convenience\n",
    "Usage:\n",
    "    ok, data = az('account show', json_out=True)\n",
    "    ok, text = az('apim list --resource-group X')\n",
    "\"\"\"\n",
    "import os, subprocess, json, shlex\n",
    "from pathlib import Path\n",
    "AZ_CLI = os.environ.get('AZ_CLI') or os.environ.get('AZURE_CLI_PATH')\n",
    "_cached_version=None\n",
    "\n",
    "def az(cmd:str, json_out:bool=False, timeout:int=25, login_if_needed:bool=True):\n",
    "    global _cached_version\n",
    "    if not AZ_CLI:\n",
    "        return False, 'AZ_CLI not set; run (-1.3) first.'\n",
    "    parts=[AZ_CLI]+shlex.split(cmd)\n",
    "    try:\n",
    "        proc=subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, f'timeout after {timeout}s: {cmd}'\n",
    "    if proc.returncode!=0:\n",
    "        stderr=proc.stderr.strip()\n",
    "        if login_if_needed and 'az login' in stderr.lower():\n",
    "            # If SP creds exist, attempt non-interactive login; else instruct.\n",
    "            sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET'])\n",
    "            if sp_ok:\n",
    "                sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} \"\n",
    "                        f\"-p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "                print('[az] Attempting SP login ...')\n",
    "                lp=subprocess.run([AZ_CLI]+shlex.split(sp_cmd),capture_output=True,text=True,timeout=40)\n",
    "                if lp.returncode==0:\n",
    "                    print('[az] SP login successful; retrying command')\n",
    "                    return az(cmd,json_out=json_out,timeout=timeout,login_if_needed=False)\n",
    "                else:\n",
    "                    print('[az] SP login failed:', lp.stderr[:180])\n",
    "            else:\n",
    "                print('[az] Interactive login required: run \"az login\" in a terminal.')\n",
    "        return False, stderr or proc.stdout\n",
    "    out=proc.stdout\n",
    "    if json_out:\n",
    "        try:\n",
    "            return True, json.loads(out or '{}')\n",
    "        except Exception as e:\n",
    "            return False, f'json parse error: {e}\\nRaw: {out[:200]}'\n",
    "    return True, out\n",
    "\n",
    "# Cache version lazily\n",
    "if not _cached_version:\n",
    "    ok, ver = az('--version', json_out=False, timeout=5, login_if_needed=False)\n",
    "    if ok:\n",
    "        _cached_version=ver.splitlines()[0] if ver else ''\n",
    "        print('[az] version:', _cached_version)\n",
    "    else:\n",
    "        print('[az] version check skipped:', ver[:120])\n",
    "\n",
    "# Quick account context (suppresses login if SP already authenticated)\n",
    "ok, acct = az('account show', json_out=True, timeout=10)\n",
    "if ok:\n",
    "    print('[az] account:', acct.get('name'), acct.get('id'))\n",
    "else:\n",
    "    print('[az] account show issue:', acct[:160])\n",
    "\n",
    "\n",
    "# (-1.6) Deployment Helpers (Consolidated)\n",
    "\"\"\"Utilities for ARM/Bicep deployments via az CLI.\n",
    "Depends on az() from (-1.5).\n",
    "Functions:\n",
    "  compile_bicep(bicep_path) -> str json_template_path\n",
    "  deploy_template(rg, name, template_file, params: dict) -> (ok, result_json)\n",
    "  get_deployment_outputs(rg, name) -> dict outputs or {}\n",
    "  ensure_deployment(rg, name, template, params, skip_if_exists=True)\n",
    "\"\"\"\n",
    "import os, json, tempfile, pathlib, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "def compile_bicep(bicep_path:str):\n",
    "    b=Path(bicep_path)\n",
    "    if not b.exists():\n",
    "        raise FileNotFoundError(f'Bicep file not found: {bicep_path}')\n",
    "    out_json = b.with_suffix('.json')\n",
    "    ok, res = az(f'bicep build --file {shlex.quote(str(b))}')\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Failed bicep build: {res}')\n",
    "    if not out_json.exists():\n",
    "        raise RuntimeError(f'Expected compiled template missing: {out_json}')\n",
    "    print('[deploy] compiled', bicep_path, '->', out_json)\n",
    "    return str(out_json)\n",
    "\n",
    "def deploy_template(rg:str, name:str, template_file:str, params:dict):\n",
    "    param_args=[]\n",
    "    for k,v in params.items():\n",
    "        if isinstance(v, (dict,list)):\n",
    "            # Write complex params to temp file\n",
    "            tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "            tmp.write_text(json.dumps({\"value\": v}, indent=2))\n",
    "            param_args.append(f'{k}=@{tmp}')\n",
    "        else:\n",
    "            param_args.append(f'{k}={json.dumps(v)}')\n",
    "    params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "    cmd=f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}'\n",
    "    print('[deploy] running:', cmd)\n",
    "    ok, res = az(cmd, json_out=True, timeout=600)\n",
    "    return ok, res\n",
    "\n",
    "def get_deployment_outputs(rg:str, name:str):\n",
    "    ok,res = az(f'deployment group show --resource-group {rg} --name {name}', json_out=True)\n",
    "    if not ok:\n",
    "        print('[deploy] show failed:', res[:140])\n",
    "        return {}\n",
    "    outputs = res.get('properties',{}).get('outputs',{})\n",
    "    simplified={k: v.get('value') for k,v in outputs.items()} if isinstance(outputs, dict) else {}\n",
    "    print('[deploy] outputs keys:', ', '.join(simplified.keys()))\n",
    "    return simplified\n",
    "\n",
    "def check_deployment_exists(rg:str, name:str):\n",
    "    ok,res=az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=15)\n",
    "    return ok and res.get('name')==name\n",
    "\n",
    "def ensure_deployment(rg:str, name:str, bicep_file:str, params:dict, skip_if_exists:bool=True):\n",
    "    if skip_if_exists and check_deployment_exists(rg,name):\n",
    "        print('[deploy] existing deployment found:', name)\n",
    "        return get_deployment_outputs(rg,name)\n",
    "    template=compile_bicep(bicep_file) if bicep_file.endswith('.bicep') else bicep_file\n",
    "    ok,res=deploy_template(rg,name,template,params)\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Deployment {name} failed: {res}')\n",
    "    return get_deployment_outputs(rg,name)\n",
    "\n",
    "# AzureOpenAI Compatibility Import Shim\n",
    "# Some cells use: from openai import AzureOpenAI\n",
    "# Provide a unified accessor that can adapt if future SDK reorganizes paths.\n",
    "\n",
    "def get_azure_openai_client(**kwargs):\n",
    "    try:\n",
    "        from openai import AzureOpenAI  # standard location\n",
    "        return AzureOpenAI(**kwargs)\n",
    "    except ImportError as ex:\n",
    "        raise ImportError(\"AzureOpenAI class not found; ensure openai>=2.2,<3 installed.\") from ex\n",
    "\n",
    "print('[shim] AzureOpenAI shim ready.')\n",
    "\n",
    "print('[deploy] helpers ready')\n",
    "\n",
    "\n",
    "# (-1.7) Unified Policy Application with Auto-Discovery\n",
    "\n",
    "\"\"\"Applies one or more API Management policies to the target API using Azure REST API.\n",
    "\n",
    "Provide policies as a list of (policy_name, policy_xml_string).\n",
    "\n",
    "Automatically discovers the API ID if not set in environment.\n",
    "Creates policy payloads and invokes az rest to apply them.\n",
    "\n",
    "Requires ENV values: RESOURCE_GROUP, APIM_SERVICE (service name)\n",
    "Optional: API_ID (will be auto-discovered if not provided)\n",
    "\n",
    "Note: Uses Azure REST API because 'az apim api policy' command is not available in all CLI versions.\n",
    "\"\"\"\n",
    "\n",
    "import os, json as json_module, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_POLICY_ENV=['RESOURCE_GROUP','APIM_SERVICE']\n",
    "\n",
    "missing=[k for k in REQUIRED_POLICY_ENV if not os.environ.get(k)]\n",
    "\n",
    "if missing:\n",
    "    print('[policy] Missing env vars; set:', ', '.join(missing))\n",
    "else:\n",
    "    def discover_api_id():\n",
    "        \"\"\"Discover the API ID from APIM instance.\"\"\"\n",
    "        service = os.environ['APIM_SERVICE']\n",
    "        rg = os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get subscription ID\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print('[policy] Failed to get subscription ID')\n",
    "            return None\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "\n",
    "        # List APIs using REST API\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{service}/apis?api-version=2022-08-01')\n",
    "\n",
    "        print('[policy] Discovering APIs in APIM instance...')\n",
    "        ok, result = az(f'rest --method get --url \"{url}\"', json_out=True, timeout=60)\n",
    "\n",
    "        if not ok or not result:\n",
    "            print('[policy] Failed to list APIs')\n",
    "            return None\n",
    "\n",
    "        apis = result.get('value', [])\n",
    "\n",
    "        if not apis:\n",
    "            print('[policy] ERROR: No APIs found in APIM instance')\n",
    "            print('[policy] HINT: You may need to deploy the infrastructure first')\n",
    "            return None\n",
    "\n",
    "        # Prefer APIs with 'openai' in the name\n",
    "        openai_apis = [api for api in apis if 'openai' in api.get('name', '').lower()]\n",
    "\n",
    "        if openai_apis:\n",
    "            api_id = openai_apis[0]['name']\n",
    "            print(f'[policy] Found OpenAI API: {api_id}')\n",
    "        else:\n",
    "            api_id = apis[0]['name']\n",
    "            print(f'[policy] Using first available API: {api_id}')\n",
    "\n",
    "        return api_id\n",
    "\n",
    "    def apply_policies(policies):\n",
    "        service=os.environ['APIM_SERVICE']\n",
    "        rg=os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get or discover API_ID\n",
    "        api_id = os.environ.get('API_ID')\n",
    "\n",
    "        if not api_id:\n",
    "            print('[policy] API_ID not set in environment, discovering...')\n",
    "            api_id = discover_api_id()\n",
    "\n",
    "            if not api_id:\n",
    "                print('[policy] ERROR: Could not discover API ID')\n",
    "                print('[policy] HINT: Set API_ID environment variable or deploy infrastructure')\n",
    "                return\n",
    "\n",
    "            # Save for future use\n",
    "            os.environ['API_ID'] = api_id\n",
    "            print(f'[policy] Saved API_ID to environment: {api_id}')\n",
    "\n",
    "        # Get subscription ID\n",
    "        print('[policy] Getting subscription ID...')\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print(f'[policy] Failed to get subscription ID: {sub_result}')\n",
    "            return\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "        print(f'[policy] Subscription ID: {subscription_id}')\n",
    "        print(f'[policy] Using API ID: {api_id}')\n",
    "\n",
    "        for name, xml in policies:\n",
    "            xml = xml.strip()\n",
    "\n",
    "            # Azure REST API endpoint for APIM policy\n",
    "            url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "                   f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "                   f'/service/{service}/apis/{api_id}/policies/policy?api-version=2022-08-01')\n",
    "\n",
    "            # Policy payload in Azure format\n",
    "            policy_payload = {\n",
    "                \"properties\": {\n",
    "                    \"value\": xml,\n",
    "                    \"format\": \"xml\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write JSON payload to temp file (Windows-friendly)\n",
    "            payload_file = Path(tempfile.gettempdir()) / f'apim-{name}-payload.json'\n",
    "            with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "                json_module.dump(policy_payload, f, indent=2)\n",
    "\n",
    "            print(f'[policy] Applying {name} via REST API...')\n",
    "\n",
    "            # Use az rest command with @file syntax for body\n",
    "            cmd = f'rest --method put --url \"{url}\" --body @\"{payload_file}\" --headers \"Content-Type=application/json\"'\n",
    "\n",
    "            ok, res = az(cmd, json_out=False, timeout=120)\n",
    "\n",
    "            # Clean up temp file\n",
    "            try:\n",
    "                payload_file.unlink()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if ok:\n",
    "                print(f'[policy] {name} applied successfully')\n",
    "            else:\n",
    "                error_msg = str(res)[:400] if res else 'Unknown error'\n",
    "                print(f'[policy] {name} failed: {error_msg}')\n",
    "\n",
    "    print('[policy] apply_policies(policies) ready with auto-discovery (using Azure REST API)')\n",
    "\n",
    "\n",
    "# (-1.8) Unified MCP Initialization (Updated for 4 Data Sources)\n",
    "\"\"\"Initializes MCP servers and APIM-routed APIs.\n",
    "\n",
    "Available Data Sources:\n",
    "  1. Excel MCP (direct) - Analytics, charts, data processing\n",
    "  2. Docs MCP (direct) - Document search, retrieval\n",
    "  3. GitHub API (APIM) - Code repos, search\n",
    "  4. Weather API (APIM) - Real-time weather data\n",
    "\n",
    "Reads configuration from .mcp-servers-config file.\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "# Check if already initialized\n",
    "if 'mcp' in globals() and hasattr(mcp, 'excel'):\n",
    "    print(\"⚠️  MCP Client already initialized. Skipping re-initialization.\")\n",
    "    print()\n",
    "    print(\"Available Data Sources:\")\n",
    "    if mcp.excel:\n",
    "        print(f\"  ✓ Excel MCP: {mcp.excel.server_url}\")\n",
    "    if mcp.docs:\n",
    "        print(f\"  ✓ Docs MCP: {mcp.docs.server_url}\")\n",
    "    if mcp.github:\n",
    "        url = getattr(mcp.github, 'base_url', 'configured')\n",
    "        print(f\"  ✓ GitHub API (APIM): {url}\")\n",
    "    if mcp.weather:\n",
    "        url = getattr(mcp.weather, 'base_url', 'configured')\n",
    "        print(f\"  ✓ Weather API (APIM): {url}\")\n",
    "else:\n",
    "    print(\"🔄 Initializing MCP Client with 4 Data Sources...\")\n",
    "    print()\n",
    "    try:\n",
    "        mcp = MCPClient()\n",
    "\n",
    "        # Count available sources\n",
    "        available = []\n",
    "        if mcp.excel:\n",
    "            available.append(\"Excel MCP\")\n",
    "        if mcp.docs:\n",
    "            available.append(\"Docs MCP\")\n",
    "        if mcp.github:\n",
    "            available.append(\"GitHub API\")\n",
    "        if mcp.weather:\n",
    "            available.append(\"Weather API\")\n",
    "\n",
    "        print(f\"✅ MCP Client initialized successfully!\")\n",
    "        print(f\"📊 Available: {len(available)}/4 data sources\")\n",
    "        print()\n",
    "        print(f\"📡 Data Sources:\")\n",
    "\n",
    "        if mcp.excel:\n",
    "            print(f\"  1. Excel Analytics MCP\")\n",
    "            print(f\"     URL: {mcp.excel.server_url}\")\n",
    "            print(f\"     Type: Direct MCP Protocol\")\n",
    "            print(f\"     Capabilities: Analytics, charts, calculations\")\n",
    "            print()\n",
    "\n",
    "        if mcp.docs:\n",
    "            print(f\"  2. Research Documents MCP\")\n",
    "            print(f\"     URL: {mcp.docs.server_url}\")\n",
    "            print(f\"     Type: Direct MCP Protocol\")\n",
    "            print(f\"     Capabilities: Document search, retrieval, comparison\")\n",
    "            print()\n",
    "\n",
    "        if mcp.github:\n",
    "            url = getattr(mcp.github, 'base_url', 'configured')\n",
    "            print(f\"  3. GitHub REST API (via APIM)\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     Type: APIM-Routed REST API\")\n",
    "            print(f\"     Capabilities: Repo search, code analysis, issues\")\n",
    "            print()\n",
    "\n",
    "        if mcp.weather:\n",
    "            url = getattr(mcp.weather, 'base_url', 'configured')\n",
    "            print(f\"  4. OpenWeather API (via APIM)\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     Type: APIM-Routed REST API\")\n",
    "            print(f\"     Capabilities: Real-time weather, forecasts\")\n",
    "            print()\n",
    "\n",
    "        if len(available) < 4:\n",
    "            print(\"⚠️  Some data sources not configured:\")\n",
    "            if not mcp.excel:\n",
    "                print(\"  - Excel MCP: Set EXCEL_MCP_URL\")\n",
    "            if not mcp.docs:\n",
    "                print(\"  - Docs MCP: Set DOCS_MCP_URL\")\n",
    "            if not mcp.github:\n",
    "                print(\"  - GitHub API: Set APIM_GITHUB_URL + APIM_SUBSCRIPTION_KEY\")\n",
    "            if not mcp.weather:\n",
    "                print(\"  - Weather API: Set APIM_WEATHER_URL + OPENWEATHER_API_KEY\")\n",
    "            print()\n",
    "\n",
    "        print(f\"💡 Configuration loaded from .mcp-servers-config\")\n",
    "        print(f\"   Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to initialize MCP Client: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "## For backward compatibility\n",
    "#MCP_SERVERS = {}\n",
    "#if mcp.excel:\n",
    "#    MCP_SERVERS['excel'] = mcp.excel\n",
    "#if mcp.docs:\n",
    "#    MCP_SERVERS['docs'] = mcp.docs\n",
    "#if mcp.github:\n",
    "#    MCP_SERVERS['github'] = mcp.github\n",
    "#if mcp.weather:\n",
    "#    MCP_SERVERS['weather'] = mcp.weather\n",
    "\n",
    "\n",
    "# (-1.9) Unified AzureOps Wrapper (Enhanced SDK Strategy)\n",
    "\"\"\"High-level Azure operations wrapper consolidating:\n",
    "- CLI resolution & version\n",
    "- Service principal / interactive login fallback\n",
    "- Generic az() invocation (JSON/text)\n",
    "- Resource group ensure (CLI or SDK)\n",
    "- Bicep compile (CLI) + group deployment (CLI or SDK)\n",
    "- AI Foundry model deployments (SDK)\n",
    "- APIM policy fragments + API policy apply (with rollback)\n",
    "- Deployment outputs retrieval & simplification\n",
    "- MCP server health probing\n",
    "\n",
    "Strategy:\n",
    "    AzureOps(strategy='sdk' | 'cli')  # default 'sdk' to favor richer status & long-running handling.\n",
    "\n",
    "Example:\n",
    "    AZ_OPS = AzureOps(strategy='sdk')\n",
    "    AZ_OPS.ensure_login()\n",
    "    AZ_OPS.ensure_resource_group(rg, location)\n",
    "    tpl = AZ_OPS.compile_bicep('deploy-01-core.bicep')\n",
    "    ok, res = AZ_OPS.deploy_group(rg,'core',tpl, params={})\n",
    "    outputs = AZ_OPS.get_deployment_outputs(rg,'core')\n",
    "    AZ_OPS.ensure_policy_fragment(rg, service, 'semanticCacheFragment', xml)\n",
    "    AZ_OPS.apply_api_policy_with_fragments(rg, service, api_id, ['semanticCacheFragment','contentSafetyFragment'])\n",
    "\n",
    "NOTE: Legacy helper cells remain for reference; prefer AzureOps going forward.\n",
    "\"\"\"\n",
    "import os, shutil, subprocess, json, time, shlex, tempfile, sys, socket\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Optional Azure SDK imports (defer errors until used)\n",
    "try:\n",
    "    from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "    from azure.mgmt.resource import ResourceManagementClient\n",
    "    from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "    from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "except Exception as _sdk_err:\n",
    "    _AZURE_SDK_IMPORT_ERROR = _sdk_err\n",
    "else:\n",
    "    _AZURE_SDK_IMPORT_ERROR = None\n",
    "\n",
    "class DeploymentError(Exception):\n",
    "    pass\n",
    "class PolicyError(Exception):\n",
    "    pass\n",
    "class ModelDeploymentError(Exception):\n",
    "    pass\n",
    "\n",
    "class AzureOps:\n",
    "    def __init__(self, strategy: str = 'sdk'):\n",
    "        self.strategy = strategy.lower()\n",
    "        if self.strategy not in {'sdk','cli'}:\n",
    "            self.strategy = 'sdk'\n",
    "        self.az_cli = None\n",
    "        self.version = None\n",
    "        self.subscription_id = os.environ.get('SUBSCRIPTION_ID') or os.environ.get('AZURE_SUBSCRIPTION_ID') or ''\n",
    "        self.credential = None\n",
    "        self.resource_client: Optional[ResourceManagementClient] = None\n",
    "        self.cog_client: Optional[CognitiveServicesManagementClient] = None\n",
    "        self._resolve_cli()\n",
    "        self._init_credentials_if_possible()\n",
    "        self._cache_version()\n",
    "\n",
    "    # ---------- CLI RESOLUTION ----------\n",
    "    def _resolve_cli(self):\n",
    "        override = os.environ.get('AZURE_CLI_PATH')\n",
    "        if override and Path(override).exists():\n",
    "            self.az_cli = override\n",
    "        else:\n",
    "            candidates = []\n",
    "            for name in ['az','az.cmd','az.exe']:\n",
    "                p = shutil.which(name)\n",
    "                if p: candidates.append(p)\n",
    "            candidates += [ '/usr/bin/az','/usr/local/bin/az', str(Path.home()/'.local/bin/az'), str(Path.home()/'.azure-cli/az') ]\n",
    "            existing = [c for c in candidates if c and Path(c).exists()]\n",
    "            if not existing:\n",
    "                venv = Path(os.environ.get('VIRTUAL_ENV','') or sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "                if venv.exists(): existing=[str(venv)]\n",
    "            if existing:\n",
    "                def rank(p):\n",
    "                    pl=p.lower(); penalty=1000 if '.venv' in pl or 'scripts' in pl else 0\n",
    "                    return penalty, len(p)\n",
    "                existing.sort(key=rank)\n",
    "                self.az_cli = existing[0]\n",
    "            else:\n",
    "                self.az_cli = 'az'\n",
    "        os.environ['AZ_CLI'] = self.az_cli\n",
    "\n",
    "    # ---------- GENERIC az() INVOCATION ----------\n",
    "    def _run(self, parts, timeout=30):\n",
    "        try:\n",
    "            return subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            class Dummy: returncode=1; stdout=''; stderr=f'timeout>{timeout}s'\n",
    "            return Dummy()\n",
    "\n",
    "    def az(self, cmd: str, json_out=False, timeout=30, login_retry=True) -> Tuple[bool, str | Dict]:\n",
    "        parts=[self.az_cli]+shlex.split(cmd)\n",
    "        proc=self._run(parts,timeout)\n",
    "        if proc.returncode!=0:\n",
    "            stderr=proc.stderr.strip()\n",
    "            if login_retry and 'az login' in stderr.lower():\n",
    "                if self.ensure_login(silent=True):\n",
    "                    return self.az(cmd,json_out=json_out,timeout=timeout,login_retry=False)\n",
    "            return False, stderr or proc.stdout\n",
    "        out=proc.stdout\n",
    "        if json_out:\n",
    "            try:\n",
    "                return True, json.loads(out or '{}')\n",
    "            except Exception as e:\n",
    "                return False, f'json parse error: {e}\\n{out[:200]}'\n",
    "        return True, out\n",
    "\n",
    "    def _cache_version(self):\n",
    "        ok, ver = self.az('--version', json_out=False, timeout=6, login_retry=False)\n",
    "        if ok:\n",
    "            self.version = ver.splitlines()[0] if ver else ''\n",
    "\n",
    "    # ---------- AUTHENTICATION ----------\n",
    "    def _init_credentials_if_possible(self):\n",
    "        # Service Principal first\n",
    "        sp_keys = ['AZURE_TENANT_ID','AZURE_CLIENT_ID','AZURE_CLIENT_SECRET']\n",
    "        if all(os.environ.get(k) for k in sp_keys):\n",
    "            try:\n",
    "                self.credential = ClientSecretCredential(\n",
    "                    tenant_id=os.environ['AZURE_TENANT_ID'],\n",
    "                    client_id=os.environ['AZURE_CLIENT_ID'],\n",
    "                    client_secret=os.environ['AZURE_CLIENT_SECRET']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] SP credential init failed:', e)\n",
    "                self.credential=None\n",
    "        if self.credential is None:\n",
    "            try:\n",
    "                self.credential = AzureCliCredential()\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] AzureCliCredential failed (defer login):', e)\n",
    "                self.credential=None\n",
    "        # Resource client if SDK chosen\n",
    "        if self.strategy=='sdk' and self.credential and self.subscription_id:\n",
    "            if _AZURE_SDK_IMPORT_ERROR:\n",
    "                print('[AzureOps] SDK import error; fallback to CLI deployments:', _AZURE_SDK_IMPORT_ERROR)\n",
    "                self.strategy='cli'\n",
    "                return\n",
    "            try:\n",
    "                self.resource_client = ResourceManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] ResourceManagementClient init failed:', e)\n",
    "                self.resource_client=None\n",
    "            try:\n",
    "                self.cog_client = CognitiveServicesManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] CognitiveServicesManagementClient init failed:', e)\n",
    "                self.cog_client=None\n",
    "\n",
    "    def ensure_login(self, silent=False):\n",
    "        ok,_ = self.az('account show', json_out=True, login_retry=False, timeout=8)\n",
    "        if ok:\n",
    "            acct_id = _.get('id') if isinstance(_,dict) else None\n",
    "            if acct_id and not self.subscription_id:\n",
    "                self.subscription_id = acct_id\n",
    "            return True\n",
    "        # Attempt SP non-interactive if creds exist\n",
    "        sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID'])\n",
    "        if sp_ok:\n",
    "            sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} -p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "            proc=self._run([self.az_cli]+shlex.split(sp_cmd),timeout=40)\n",
    "            if proc.returncode==0:\n",
    "                if not silent: print('[AzureOps] SP login successful')\n",
    "                return True\n",
    "            else:\n",
    "                if not silent: print('[AzureOps] SP login failed:', proc.stderr[:160])\n",
    "        if not silent:\n",
    "            print('[AzureOps] Interactive login required: run \"az login\" in terminal')\n",
    "        return False\n",
    "\n",
    "    # ---------- RESOURCE GROUP ----------\n",
    "    def ensure_resource_group(self, rg: str, location: str) -> bool:\n",
    "        if self.strategy=='sdk' and self.resource_client:\n",
    "            try:\n",
    "                self.resource_client.resource_groups.create_or_update(rg, {'location': location})\n",
    "                print('[AzureOps] RG ensured (sdk):', rg)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] RG ensure failed (sdk):', e)\n",
    "        # CLI fallback\n",
    "        ok,res=self.az(f'group exists --name {rg}', json_out=False)\n",
    "        exists = ok and res.strip()=='true'\n",
    "        if exists:\n",
    "            print('[AzureOps] RG exists:', rg); return True\n",
    "        ok,_=self.az(f'group create --name {rg} --location {location}', json_out=True, timeout=120)\n",
    "        print('[AzureOps] RG created' if ok else '[AzureOps] RG create failed')\n",
    "        return ok\n",
    "\n",
    "    # ---------- BICEP COMPILE ----------\n",
    "    def compile_bicep(self, path: str) -> str:\n",
    "        b=Path(path); out=b.with_suffix('.json')\n",
    "        ok,res=self.az(f'bicep build --file {shlex.quote(str(b))}', json_out=False)\n",
    "        if not ok or not out.exists():\n",
    "            raise DeploymentError(f'Bicep compile failed: {res}')\n",
    "        print('[AzureOps] compiled', path, '->', out)\n",
    "        return str(out)\n",
    "\n",
    "    # ---------- DEPLOYMENT (CLI OR SDK) ----------\n",
    "    def _deploy_group_cli(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        param_args=[]\n",
    "        for k,v in params.items():\n",
    "            if isinstance(v,(dict,list)):\n",
    "                tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "                tmp.write_text(json.dumps({\"value\":v}))\n",
    "                param_args.append(f'{k}=@{tmp}')\n",
    "            else:\n",
    "                param_args.append(f'{k}={json.dumps(v)}')\n",
    "        params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "        cmd=(f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}')\n",
    "        print('[AzureOps] deploy(cli):', cmd)\n",
    "        ok,res=self.az(cmd,json_out=True,timeout=timeout)\n",
    "        return ok,res\n",
    "\n",
    "    def _deploy_group_sdk(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if not self.resource_client:\n",
    "            print('[AzureOps] SDK resource_client missing; fallback to CLI')\n",
    "            return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "        template = json.loads(Path(template_file).read_text(encoding='utf-8'))\n",
    "        # Convert params to ARM expected {k:{\"value\":v}}\n",
    "        arm_params={k:{'value':v} for k,v in params.items()}\n",
    "        properties={'mode':'Incremental','template':template,'parameters':arm_params}\n",
    "        print('[AzureOps] deploy(sdk):', name)\n",
    "        poller = self.resource_client.deployments.begin_create_or_update(rg,name,{'properties':properties})\n",
    "        start=time.time();\n",
    "        while not poller.done():\n",
    "            time.sleep(30)\n",
    "            elapsed=int(time.time()-start)\n",
    "            if elapsed%120<30:  # periodic status\n",
    "                print(f'  [AzureOps] deploying... {elapsed}s')\n",
    "        result=poller.result()\n",
    "        state=getattr(result.properties,'provisioning_state',None)\n",
    "        ok = state=='Succeeded'\n",
    "        if ok:\n",
    "            print('[AzureOps] deployment succeeded:', name)\n",
    "        else:\n",
    "            print('[AzureOps] deployment state:', state)\n",
    "        return ok, {'properties':{'outputs': getattr(result.properties,'outputs',{})}}\n",
    "\n",
    "    def deploy_group(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if self.strategy=='sdk':\n",
    "            return self._deploy_group_sdk(rg,name,template_file,params,timeout)\n",
    "        return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "\n",
    "    def get_deployment_outputs(self, rg: str, name: str) -> Dict[str,str]:\n",
    "        # Attempt CLI first for uniformity\n",
    "        ok,res=self.az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=60)\n",
    "        if ok and isinstance(res,dict):\n",
    "            outputs=res.get('properties',{}).get('outputs',{})\n",
    "            return {k:v.get('value') for k,v in outputs.items()} if isinstance(outputs,dict) else {}\n",
    "        # SDK fallback if available\n",
    "        if self.resource_client:\n",
    "            try:\n",
    "                dep=self.resource_client.deployments.get(rg,name)\n",
    "                outs=getattr(dep.properties,'outputs',{})\n",
    "                return {k:v.get('value') for k,v in outs.items()} if isinstance(outs,dict) else {}\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] outputs retrieval failed (sdk):', e)\n",
    "        return {}\n",
    "\n",
    "    # ---------- MODEL DEPLOYMENTS (AI Foundry) ----------\n",
    "    def deploy_models_via_sdk(self, rg: str, foundries: List[dict], models_config: Dict[str,List[dict]]):\n",
    "        if not self.cog_client:\n",
    "            print('[AzureOps] Cognitive Services client not initialized; skipping model deployments')\n",
    "            return {'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        existing_accounts={acc.name:acc for acc in self.cog_client.accounts.list_by_resource_group(rg)}\n",
    "        results={'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        # Ensure accounts\n",
    "        for f in foundries:\n",
    "            name=f['name']; location=f['location']\n",
    "            if name in existing_accounts:\n",
    "                print(f'  [AzureOps] foundry exists: {name}')\n",
    "            else:\n",
    "                print(f'  [AzureOps] creating foundry: {name}')\n",
    "                try:\n",
    "                    account_params=Account(location=location, sku=CogSku(name='S0'), kind='AIServices', properties={'customSubDomainName':name.lower(),'publicNetworkAccess':'Enabled','allowProjectManagement':True}, identity={'type':'SystemAssigned'})\n",
    "                    poll=self.cog_client.accounts.begin_create(rg,name,account_params)\n",
    "                    poll.result(timeout=600)\n",
    "                    print(f'    [AzureOps] created {name}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [AzureOps] create failed {name}: {e}'); continue\n",
    "        # Deploy models\n",
    "        for f in foundries:\n",
    "            name=f['name']; short=name.split('-')[0]\n",
    "            models=models_config.get(short,[])\n",
    "            print(f'  [AzureOps] models for {name}: {len(models)}')\n",
    "            for m in models:\n",
    "                mname=m['name']\n",
    "                try:\n",
    "                    # Exists check\n",
    "                    try:\n",
    "                        existing=self.cog_client.deployments.get(rg,name,mname)\n",
    "                        if existing.properties.provisioning_state=='Succeeded':\n",
    "                            print(f'    [skip] {mname} already')\n",
    "                            results['skipped'].append(f'{short}/{mname}')\n",
    "                            continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    dep_params=Deployment(sku=CogSku(name=m['sku'],capacity=m['capacity']), properties=DeploymentProperties(model=DeploymentModel(format=m['format'],name=m['name'],version=m['version'])))\n",
    "                    poll=self.cog_client.deployments.begin_create_or_update(rg,name,mname,dep_params)\n",
    "                    poll.result(timeout=900)\n",
    "                    print(f'    [ok] {mname}')\n",
    "                    results['succeeded'].append(f'{short}/{mname}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [fail] {mname}: {e}')\n",
    "                    results['failed'].append({'model':f'{short}/{mname}','error':str(e)})\n",
    "        return results\n",
    "\n",
    "    # ---------- POLICY FRAGMENTS & API POLICY ----------\n",
    "    def ensure_policy_fragment(self, rg: str, service: str, fragment_name: str, xml_policy: str):\n",
    "        body={\"properties\":{\"format\":\"xml\",\"value\":xml_policy.strip()}}\n",
    "        url=(f'https://management.azure.com/subscriptions/{self.subscription_id}/resourceGroups/{rg}/providers/Microsoft.ApiManagement/service/{service}/policyFragments/{fragment_name}?api-version=2023-03-01-preview')\n",
    "        body_json=json.dumps(body)\n",
    "        ok,res=self.az(f\"rest --method put --url {shlex.quote(url)} --body {shlex.quote(body_json)} --headers Content-Type=application/json\", json_out=True, timeout=120)\n",
    "        if ok:\n",
    "            print(f'[AzureOps] fragment ensured: {fragment_name}')\n",
    "        else:\n",
    "            print(f'[AzureOps] fragment failed {fragment_name}: {str(res)[:160]}')\n",
    "        return ok\n",
    "\n",
    "    def backup_api_policy(self, rg: str, service: str, api_id: str):\n",
    "        ok,res=self.az(f'apim api policy show --resource-group {rg} --service-name {service} --api-id {api_id}', json_out=False, timeout=60)\n",
    "        if not ok:\n",
    "            print('[AzureOps] no existing policy (show failed)'); return None\n",
    "        backup_dir=Path('.apim-policy-backups'); backup_dir.mkdir(exist_ok=True)\n",
    "        ts=time.strftime('%Y%m%d-%H%M%S')\n",
    "        file=backup_dir/f'{api_id}-{ts}.xml'\n",
    "        file.write_text(res)\n",
    "        print('[AzureOps] policy backed up:', file)\n",
    "        return str(file)\n",
    "\n",
    "    def apply_api_policy_with_fragments(self, rg: str, service: str, api_id: str, fragments: List[str], extra_inbound: str=''):\n",
    "        self.backup_api_policy(rg,service,api_id)\n",
    "        fragment_tags='\\n'.join(f'        <fragment ref=\"{f}\" />' for f in fragments)\n",
    "        inbound = f\"<inbound>\\n        <base />\\n{fragment_tags}\\n{extra_inbound}\\n    </inbound>\".rstrip()\n",
    "        policy_xml=f\"<policies>\\n{inbound}\\n    <backend><base /></backend>\\n    <outbound><base /></outbound>\\n    <on-error><base /></on-error>\\n</policies>\"\n",
    "        tmp=Path(tempfile.gettempdir())/f'apim-{api_id}-policy.xml'\n",
    "        tmp.write_text(policy_xml)\n",
    "        ok,res=self.az(f'apim api policy create --resource-group {rg} --service-name {service} --api-id {api_id} --xml-path {tmp}', json_out=False, timeout=180)\n",
    "        if not ok:\n",
    "            raise PolicyError(f'Policy apply failed: {res}')\n",
    "        print('[AzureOps] API policy applied with fragments:', fragments)\n",
    "        return True\n",
    "\n",
    "    # ---------- MCP HEALTH ----------\n",
    "    def mcp_health(self, servers: Dict[str,object]) -> Dict[str,Dict[str,str]]:\n",
    "        summary={}\n",
    "        for name,client in servers.items():\n",
    "            url=getattr(client,'server_url',None) or getattr(client,'url',None) or ''\n",
    "            status='unknown'; latency_ms='-'\n",
    "            if url.startswith('http'):  # basic TCP connect\n",
    "                try:\n",
    "                    host=url.split('//',1)[1].split('/',1)[0].split(':')[0]\n",
    "                    port=443 if url.startswith('https') else (int(url.split(':')[2].split('/')[0]) if ':' in url[8:] else 80)\n",
    "                    s=socket.socket(); s.settimeout(3); start=time.time(); s.connect((host,port)); s.close(); latency_ms=int((time.time()-start)*1000); status='ok'\n",
    "                except Exception:\n",
    "                    status='unreachable'\n",
    "            summary[name]={'url':url,'status':status,'latency_ms':latency_ms}\n",
    "        return summary\n",
    "\n",
    "# Instantiate global wrapper (prefer sdk)\n",
    "AZ_OPS = AzureOps(strategy=os.environ.get('AZ_OPS_STRATEGY','sdk'))\n",
    "print('[AzureOps] CLI:', AZ_OPS.az_cli)\n",
    "az_ok = AZ_OPS.ensure_login(silent=True)\n",
    "print('[AzureOps] login status:', 'OK' if az_ok else 'AUTH REQUIRED')\n",
    "if AZ_OPS.version: print('[AzureOps] version:', AZ_OPS.version)\n",
    "print('[AzureOps] strategy:', AZ_OPS.strategy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Deployment & Environment Generation\n",
    "\n",
    "Deploy Azure resources and generate master-lab.env from outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 014: Deployment Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell_26_13f05f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Configuration set\n",
      "  Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  Resource Group: lab-master-lab\n",
      "  Location: uksouth\n",
      "  Deployment Prefix: master-lab\n"
     ]
    }
   ],
   "source": [
    "# Master Lab Configuration\n",
    "\n",
    "# IMPORTANT: Set your Azure subscription ID\n",
    "# Get this from: Azure Portal > Subscriptions > Copy Subscription ID\n",
    "subscription_id = bootstrap.subscription_id\n",
    "\n",
    "deployment_name_prefix = 'master-lab'\n",
    "resource_group_name = 'lab-master-lab'\n",
    "location = 'uksouth'\n",
    "\n",
    "# Deployment names for each step\n",
    "deployment_step1 = f'{deployment_name_prefix}-01-core'\n",
    "deployment_step2 = f'{deployment_name_prefix}-02-ai-foundry'\n",
    "deployment_step3 = f'{deployment_name_prefix}-03-supporting'\n",
    "deployment_step4 = f'{deployment_name_prefix}-04-mcp'\n",
    "\n",
    "print('[OK] Configuration set')\n",
    "print(f'  Subscription ID: {subscription_id}')\n",
    "print(f'  Resource Group: {resource_group_name}')\n",
    "print(f'  Location: {location}')\n",
    "print(f'  Deployment Prefix: {deployment_name_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 016: Deploy Infrastructure\n",
    "Run all 4 deployment steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell_28_1ecfb480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing Azure authentication...\n",
      "\n",
      "[*] Found .azure-credentials.env, using Service Principal authentication\n",
      "[OK] Service Principal credentials loaded\n",
      "\n",
      "[OK] Using Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[*] Creating Azure Resource Management client...\n",
      "[OK] Azure SDK initialized and connection verified\n",
      "\n",
      "[OK] Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "\n",
    "print('[*] Initializing Azure authentication...')\n",
    "print()\n",
    "\n",
    "# Try to load Service Principal credentials from .azure-credentials.env\n",
    "credentials_file = '.azure-credentials.env'\n",
    "credential = None\n",
    "\n",
    "if os.path.exists(credentials_file):\n",
    "    print(f'[*] Found {credentials_file}, using Service Principal authentication')\n",
    "    load_dotenv(credentials_file)\n",
    "\n",
    "    tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "    if tenant_id and client_id and client_secret:\n",
    "        try:\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id,\n",
    "                client_id=client_id,\n",
    "                client_secret=client_secret\n",
    "            )\n",
    "            print('[OK] Service Principal credentials loaded')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to create Service Principal credential: {e}')\n",
    "            credential = None\n",
    "    else:\n",
    "        print('[ERROR] Missing credentials in .azure-credentials.env')\n",
    "        print('[INFO] Required: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET')\n",
    "else:\n",
    "    print(f'[*] {credentials_file} not found')\n",
    "    print('[INFO] Run: create_service_principal.ps1 to create Service Principal')\n",
    "\n",
    "# Fallback to Azure CLI credential if Service Principal not available\n",
    "if credential is None:\n",
    "    print('[*] Falling back to Azure CLI authentication...')\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        print('[OK] Using Azure CLI credentials')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Azure CLI authentication failed: {e}')\n",
    "        print()\n",
    "        print('[ERROR] Authentication failed. Options:')\n",
    "        print('  1. Create Service Principal: run create_service_principal.ps1')\n",
    "        print('  2. Clear Azure CLI cache and re-login:')\n",
    "        print('     - Delete: %USERPROFILE%\\\\.azure')\n",
    "        print('     - Run: az login')\n",
    "        raise Exception('Authentication failed')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify subscription ID from config\n",
    "if not subscription_id or len(subscription_id) < 10:\n",
    "    raise Exception('Please set your subscription_id in Cell 11')\n",
    "\n",
    "print(f'[OK] Using Subscription ID: {subscription_id}')\n",
    "\n",
    "# Create Resource Management Client\n",
    "print('[*] Creating Azure Resource Management client...')\n",
    "try:\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "    # Test connection by listing resource groups\n",
    "    list(resource_client.resource_groups.list())\n",
    "    print('[OK] Azure SDK initialized and connection verified')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to initialize Resource Management client: {e}')\n",
    "    print()\n",
    "    print('[INFO] If you see MSAL or cache errors, try clearing Azure CLI cache:')\n",
    "    print('       rd /s /q \"%USERPROFILE%\\\\.azure\"')\n",
    "    print('       az login')\n",
    "    raise e\n",
    "\n",
    "print()\n",
    "\n",
    "def compile_bicep(bicep_file):\n",
    "    \"\"\"Compile Bicep to JSON - SIMPLIFIED: Just use existing JSON files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path if string\n",
    "    bicep_path = Path(bicep_file)\n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    print(f'[*] Looking for template: {json_path.name}...')\n",
    "    \n",
    "    # Check if JSON exists\n",
    "    if json_path.exists():\n",
    "        print(f'[OK] Found existing template: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    \n",
    "    # JSON doesn't exist - this is an error since we don't want to compile\n",
    "    print(f'[ERROR] Template not found: {json_path}')\n",
    "    print(f'[INFO] Expected location: {json_path.absolute()}')\n",
    "    print(f'[INFO] Please ensure Bicep templates are pre-compiled')\n",
    "    print(f'[INFO] Or set BICEP_DIR to the correct scripts directory')\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_resource_group_exists(rg_name):\n",
    "    \"\"\"Check if resource group exists\"\"\"\n",
    "    try:\n",
    "        resource_client.resource_groups.get(rg_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_deployment_exists(rg_name, deployment_name):\n",
    "    \"\"\"Check if deployment exists and succeeded - Fixed to return boolean only\"\"\"\n",
    "    try:\n",
    "        deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "        return deployment.properties.provisioning_state == 'Succeeded'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_template(rg_name, deployment_name, template_file, parameters_dict):\n",
    "    \"\"\"Deploy ARM template using Azure SDK\"\"\"\n",
    "    print(f'[*] Deploying {deployment_name}...')\n",
    "\n",
    "    # Read template\n",
    "    with open(template_file, 'r', encoding='utf-8') as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # Prepare deployment properties\n",
    "    deployment_properties = {\n",
    "        'mode': 'Incremental',\n",
    "        'template': template,\n",
    "        'parameters': parameters_dict\n",
    "    }\n",
    "\n",
    "    # Start deployment\n",
    "    print('[*] Starting deployment...')\n",
    "    deployment_async = resource_client.deployments.begin_create_or_update(\n",
    "        rg_name,\n",
    "        deployment_name,\n",
    "        {'properties': deployment_properties}\n",
    "    )\n",
    "\n",
    "    # Poll deployment status\n",
    "    print('[*] Deployment in progress. Polling status...')\n",
    "    start_time = time.time()\n",
    "    last_update = start_time\n",
    "\n",
    "    while not deployment_async.done():\n",
    "        time.sleep(30)\n",
    "        elapsed = time.time() - start_time\n",
    "        if time.time() - last_update >= 60:\n",
    "            mins = int(elapsed / 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f'[*] Still deploying... {mins}m {secs}s elapsed')\n",
    "            last_update = time.time()\n",
    "\n",
    "    # Get result\n",
    "    deployment_result = deployment_async.result()\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    if deployment_result.properties.provisioning_state == 'Succeeded':\n",
    "        print(f'[OK] Deployment succeeded in {mins}m {secs}s')\n",
    "        return True, deployment_result\n",
    "    else:\n",
    "        print(f'[ERROR] Deployment failed: {deployment_result.properties.provisioning_state}')\n",
    "        if deployment_result.properties.error:\n",
    "            print(f'[ERROR] Error: {deployment_result.properties.error.message}')\n",
    "        return False, deployment_result\n",
    "\n",
    "def get_deployment_outputs(rg_name, deployment_name):\n",
    "    \"\"\"Get deployment outputs\"\"\"\n",
    "    deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "    if deployment.properties.outputs:\n",
    "        return {k: v['value'] for k, v in deployment.properties.outputs.items()}\n",
    "    return {}\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_29_a7330fb3",
   "metadata": {},
   "source": [
    "### Main Deployment - All 4 Steps\n",
    "\n",
    "Deploys all infrastructure in sequence:\n",
    "1. Core (APIM, Log Analytics, App Insights) - ~10 min\n",
    "2. AI Foundry (3 hubs + 14 models) - ~15 min\n",
    "3. Supporting Services (Redis, Search, Cosmos, Content Safety) - ~10 min\n",
    "4. MCP Servers (Container Apps + 7 servers) - ~5 min\n",
    "\n",
    "**Total time: ~40 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell_28_1ecfb480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing Azure authentication...\n",
      "\n",
      "[*] Found .azure-credentials.env, using Service Principal authentication\n",
      "[OK] Service Principal credentials loaded\n",
      "\n",
      "[OK] Using Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[*] Creating Azure Resource Management client...\n",
      "[OK] Azure SDK initialized and connection verified\n",
      "\n",
      "[OK] Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "\n",
    "print('[*] Initializing Azure authentication...')\n",
    "print()\n",
    "\n",
    "# Try to load Service Principal credentials from .azure-credentials.env\n",
    "credentials_file = '.azure-credentials.env'\n",
    "credential = None\n",
    "\n",
    "if os.path.exists(credentials_file):\n",
    "    print(f'[*] Found {credentials_file}, using Service Principal authentication')\n",
    "    load_dotenv(credentials_file)\n",
    "\n",
    "    tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "    if tenant_id and client_id and client_secret:\n",
    "        try:\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id,\n",
    "                client_id=client_id,\n",
    "                client_secret=client_secret\n",
    "            )\n",
    "            print('[OK] Service Principal credentials loaded')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to create Service Principal credential: {e}')\n",
    "            credential = None\n",
    "    else:\n",
    "        print('[ERROR] Missing credentials in .azure-credentials.env')\n",
    "        print('[INFO] Required: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET')\n",
    "else:\n",
    "    print(f'[*] {credentials_file} not found')\n",
    "    print('[INFO] Run: create_service_principal.ps1 to create Service Principal')\n",
    "\n",
    "# Fallback to Azure CLI credential if Service Principal not available\n",
    "if credential is None:\n",
    "    print('[*] Falling back to Azure CLI authentication...')\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        print('[OK] Using Azure CLI credentials')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Azure CLI authentication failed: {e}')\n",
    "        print()\n",
    "        print('[ERROR] Authentication failed. Options:')\n",
    "        print('  1. Create Service Principal: run create_service_principal.ps1')\n",
    "        print('  2. Clear Azure CLI cache and re-login:')\n",
    "        print('     - Delete: %USERPROFILE%\\\\.azure')\n",
    "        print('     - Run: az login')\n",
    "        raise Exception('Authentication failed')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify subscription ID from config\n",
    "if not subscription_id or len(subscription_id) < 10:\n",
    "    raise Exception('Please set your subscription_id in Cell 11')\n",
    "\n",
    "print(f'[OK] Using Subscription ID: {subscription_id}')\n",
    "\n",
    "# Create Resource Management Client\n",
    "print('[*] Creating Azure Resource Management client...')\n",
    "try:\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "    # Test connection by listing resource groups\n",
    "    list(resource_client.resource_groups.list())\n",
    "    print('[OK] Azure SDK initialized and connection verified')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to initialize Resource Management client: {e}')\n",
    "    print()\n",
    "    print('[INFO] If you see MSAL or cache errors, try clearing Azure CLI cache:')\n",
    "    print('       rd /s /q \"%USERPROFILE%\\\\.azure\"')\n",
    "    print('       az login')\n",
    "    raise e\n",
    "\n",
    "print()\n",
    "\n",
    "def compile_bicep(bicep_file):\n",
    "    \"\"\"Compile Bicep to JSON - SIMPLIFIED: Just use existing JSON files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path if string\n",
    "    bicep_path = Path(bicep_file)\n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    print(f'[*] Looking for template: {json_path.name}...')\n",
    "    \n",
    "    # Check if JSON exists\n",
    "    if json_path.exists():\n",
    "        print(f'[OK] Found existing template: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    \n",
    "    # JSON doesn't exist - this is an error since we don't want to compile\n",
    "    print(f'[ERROR] Template not found: {json_path}')\n",
    "    print(f'[INFO] Expected location: {json_path.absolute()}')\n",
    "    print(f'[INFO] Please ensure Bicep templates are pre-compiled')\n",
    "    print(f'[INFO] Or set BICEP_DIR to the correct scripts directory')\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_resource_group_exists(rg_name):\n",
    "    \"\"\"Check if resource group exists\"\"\"\n",
    "    try:\n",
    "        resource_client.resource_groups.get(rg_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_deployment_exists(rg_name, deployment_name):\n",
    "    \"\"\"Check if deployment exists and succeeded - Fixed to return boolean only\"\"\"\n",
    "    try:\n",
    "        deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "        return deployment.properties.provisioning_state == 'Succeeded'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_template(rg_name, deployment_name, template_file, parameters_dict):\n",
    "    \"\"\"Deploy ARM template using Azure SDK\"\"\"\n",
    "    print(f'[*] Deploying {deployment_name}...')\n",
    "\n",
    "    # Read template\n",
    "    with open(template_file, 'r', encoding='utf-8') as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # Prepare deployment properties\n",
    "    deployment_properties = {\n",
    "        'mode': 'Incremental',\n",
    "        'template': template,\n",
    "        'parameters': parameters_dict\n",
    "    }\n",
    "\n",
    "    # Start deployment\n",
    "    print('[*] Starting deployment...')\n",
    "    deployment_async = resource_client.deployments.begin_create_or_update(\n",
    "        rg_name,\n",
    "        deployment_name,\n",
    "        {'properties': deployment_properties}\n",
    "    )\n",
    "\n",
    "    # Poll deployment status\n",
    "    print('[*] Deployment in progress. Polling status...')\n",
    "    start_time = time.time()\n",
    "    last_update = start_time\n",
    "\n",
    "    while not deployment_async.done():\n",
    "        time.sleep(30)\n",
    "        elapsed = time.time() - start_time\n",
    "        if time.time() - last_update >= 60:\n",
    "            mins = int(elapsed / 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f'[*] Still deploying... {mins}m {secs}s elapsed')\n",
    "            last_update = time.time()\n",
    "\n",
    "    # Get result\n",
    "    deployment_result = deployment_async.result()\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    if deployment_result.properties.provisioning_state == 'Succeeded':\n",
    "        print(f'[OK] Deployment succeeded in {mins}m {secs}s')\n",
    "        return True, deployment_result\n",
    "    else:\n",
    "        print(f'[ERROR] Deployment failed: {deployment_result.properties.provisioning_state}')\n",
    "        if deployment_result.properties.error:\n",
    "            print(f'[ERROR] Error: {deployment_result.properties.error.message}')\n",
    "        return False, deployment_result\n",
    "\n",
    "def get_deployment_outputs(rg_name, deployment_name):\n",
    "    \"\"\"Get deployment outputs\"\"\"\n",
    "    deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "    if deployment.properties.outputs:\n",
    "        return {k: v['value'] for k, v in deployment.properties.outputs.items()}\n",
    "    return {}\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell_30_1a0a2a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)\n",
      "======================================================================\n",
      "\n",
      "[*] Step 0: Ensuring resource group exists...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Resource group already exists\n",
      "\n",
      "======================================================================\n",
      "STEP 1: CORE INFRASTRUCTURE\n",
      "======================================================================\n",
      "[*] Resources: Log Analytics, App Insights, API Management\n",
      "[*] Estimated time: ~10 minutes\n",
      "\n",
      "[OK] Step 1 already deployed. Skipping...\n",
      "\n",
      "[OK] Step 1 outputs retrieved from deployment\n",
      "  - APIM Gateway: https://apim-pavavy6pu5hpa.azure-api.net\n",
      "  - Log Analytics: /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resource...\n",
      "\n",
      "======================================================================\n",
      "STEP 2: AI FOUNDRY (RESILIENT DEPLOYMENT)\n",
      "======================================================================\n",
      "[*] Resources: 3 Foundry hubs, 3 projects, AI models\n",
      "[*] Estimated time: ~15 minutes\n",
      "\n",
      "[*] Phase 2a: AI Foundry Hubs\n",
      "  [OK] foundry1-pavavy6pu5hpa already exists\n",
      "  [OK] foundry2-pavavy6pu5hpa already exists\n",
      "  [OK] foundry3-pavavy6pu5hpa already exists\n",
      "\n",
      "[*] Phase 2b: AI Models (Resilient)\n",
      "  [*] foundry1-pavavy6pu5hpa: 6 models\n",
      "    [OK] gpt-4o-mini already deployed\n",
      "    [OK] gpt-4o already deployed\n",
      "    [OK] text-embedding-3-small already deployed\n",
      "    [OK] text-embedding-3-large already deployed\n",
      "    [*] Deploying dall-e-3...\n",
      "    [SKIP] dall-e-3 failed: (InvalidResourceProperties) The specified SKU 'Standard' for model 'dall-e-3 3.0\n",
      "    [OK] gpt-4.1-nano already deployed\n",
      "  [*] foundry2-pavavy6pu5hpa: 2 models\n",
      "    [OK] gpt-4o-mini already deployed\n",
      "    [*] Deploying gpt-4.1-nano...\n",
      "    [SKIP] gpt-4.1-nano failed: (SpecialFeatureOrQuotaIdRequired) The current subscription does not have feature\n",
      "  [*] foundry3-pavavy6pu5hpa: 2 models\n",
      "    [OK] gpt-4o-mini already deployed\n",
      "    [OK] gpt-4.1-nano already deployed\n",
      "\n",
      "[OK] Models: 0 deployed, 8 skipped, 2 failed\n",
      "\n",
      "[*] Collecting foundry deployment outputs for env file...\n",
      "  [OK] Captured foundry1-pavavy6pu5hpa: 6 models\n",
      "  [OK] Captured foundry2-pavavy6pu5hpa: 2 models\n",
      "  [OK] Captured foundry3-pavavy6pu5hpa: 2 models\n",
      "[OK] Captured 3 foundry outputs\n",
      "\n",
      "[*] Phase 2c: APIM Inference API\n",
      "[OK] APIM API already configured. Skipping...\n",
      "[OK] Step 2 complete\n",
      "\n",
      "======================================================================\n",
      "STEP 3: SUPPORTING SERVICES\n",
      "======================================================================\n",
      "STEP 3: SUPPORTING SERVICES\n",
      "\n",
      "[OK] Step 3 already deployed. Skipping...\n",
      "\n",
      "[OK] Step 3 outputs retrieved\n",
      "======================================================================\n",
      "STEP 4: MCP SERVERS\n",
      "======================================================================\n",
      "[*] Resources: Container Apps + 5 MCP servers\n",
      "[*] Estimated time: ~5 minutes\n",
      "\n",
      "[OK] Step 4 already deployed. Skipping...\n",
      "\n",
      "[OK] Step 4 outputs retrieved\n",
      "\n",
      "======================================================================\n",
      "DEPLOYMENT COMPLETE\n",
      "======================================================================\n",
      "[OK] Total time: 0m 10s\n",
      "\n",
      "[OK] All 4 steps deployed successfully!\n",
      "[OK] Next: Run Cell 18-19 to generate master-lab.env\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=' * 70)\n",
    "# Load BICEP_DIR (set by Cell 3)\n",
    "# Use absolute path for BICEP_DIR\n",
    "if \"NOTEBOOK_DIR\" in globals():\n",
    "    BICEP_DIR = NOTEBOOK_DIR / \"deploy\"\n",
    "else:\n",
    "    # Fallback if Cell 004 wasn't run\n",
    "    BICEP_DIR = Path(r\"C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab\\deploy\")\n",
    "if not BICEP_DIR.exists():\n",
    "    print(f\"[deploy] ⚠️  BICEP_DIR not found: {BICEP_DIR}\")\n",
    "    print(f\"[deploy] Looking in current directory instead\")\n",
    "    BICEP_DIR = Path(\".\")\n",
    "\n",
    "print('MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Ensure resource group exists\n",
    "print('[*] Step 0: Ensuring resource group exists...')\n",
    "if not check_resource_group_exists(resource_group_name):\n",
    "    print(f'[*] Creating resource group: {resource_group_name}')\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {'location': location}\n",
    "    )\n",
    "    print('[OK] Resource group created')\n",
    "else:\n",
    "    print('[OK] Resource group already exists')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: CORE INFRASTRUCTURE (Bicep - as before)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 1: CORE INFRASTRUCTURE')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Log Analytics, App Insights, API Management')\n",
    "print('[*] Estimated time: ~10 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step1 = 'master-lab-01-core'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step1):\n",
    "    print('[OK] Step 1 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 1 not found. Deploying...')\n",
    "\n",
    "    # Compile and deploy\n",
    "    # Fix: original compile_bicep used Path.replace(old, new) causing TypeError.\n",
    "    # Provide safe wrapper using Path.with_suffix('.json').\n",
    "    # Added resilient az CLI discovery & FileNotFoundError handling.\n",
    "    # Enhanced: auto-install bicep if missing; richer diagnostics; fallback to direct bicep use if JSON not produced.\n",
    "    def compile_bicep_safe(bicep_path: Path):\n",
    "        \"\"\"SIMPLIFIED: Just use existing JSON files - no compilation\"\"\"\n",
    "        if not bicep_path.exists():\n",
    "            print(f'[ERROR] Bicep file not found: {bicep_path}')\n",
    "            return None\n",
    "        \n",
    "        json_path = bicep_path.with_suffix('.json')\n",
    "        \n",
    "        if json_path.exists():\n",
    "            print(f'[OK] Using existing template: {json_path.name}')\n",
    "            return str(json_path)\n",
    "        \n",
    "        print(f'[ERROR] JSON template not found: {json_path}')\n",
    "        print(f'[INFO] Expected at: {json_path.absolute()}')\n",
    "        return None\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-01-core.bicep')\n",
    "\n",
    "    # Load parameters\n",
    "    with open(BICEP_DIR / 'params-01-core.json') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # Extract only the 'parameters' section from ARM parameter file\n",
    "    params_dict = params.get('parameters', {})\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step1, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 1 deployment failed')\n",
    "\n",
    "    print('[OK] Step 1 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# Get Step 1 outputs (with fallback to saved file)\n",
    "step1_outputs = None\n",
    "try:\n",
    "    step1_outputs = get_deployment_outputs(resource_group_name, deployment_step1)\n",
    "    print('[OK] Step 1 outputs retrieved from deployment')\n",
    "except Exception as e:\n",
    "    print(f'[WARN] Failed to retrieve Step 1 outputs from deployment: {str(e)}')\n",
    "    # Try loading from saved file\n",
    "    step1_output_file = BICEP_DIR / 'step1-outputs.json'\n",
    "    if step1_output_file.exists():\n",
    "        try:\n",
    "            with open(step1_output_file) as f:\n",
    "                step1_outputs = json.load(f)\n",
    "            print(f'[OK] Step 1 outputs loaded from {step1_output_file.name}')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Failed to load from file: {str(e2)}')\n",
    "    \n",
    "if not step1_outputs:\n",
    "    print('[ERROR] Cannot proceed without Step 1 outputs')\n",
    "    print('[INFO] Please ensure Step 1 deployment completed or step1-outputs.json exists')\n",
    "    raise Exception('Cannot proceed without Step 1 outputs')\n",
    "\n",
    "print(f\"  - APIM Gateway: {step1_outputs.get('apimGatewayUrl', 'N/A')}\")\n",
    "print(f\"  - Log Analytics: {step1_outputs.get('logAnalyticsWorkspaceId', 'N/A')[:60]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: AI FOUNDRY (RESILIENT PYTHON APPROACH)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 2: AI FOUNDRY (RESILIENT DEPLOYMENT)')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: 3 Foundry hubs, 3 projects, AI models')\n",
    "print('[*] Estimated time: ~15 minutes')\n",
    "print()\n",
    "\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "\n",
    "cog_client = CognitiveServicesManagementClient(credential, subscription_id)\n",
    "\n",
    "# Configuration\n",
    "resource_suffix = 'pavavy6pu5hpa'  # Consistent suffix\n",
    "foundries = [\n",
    "    {'name': f'foundry1-{resource_suffix}', 'location': 'uksouth', 'project': 'master-lab-foundry1'},\n",
    "    {'name': f'foundry2-{resource_suffix}', 'location': 'eastus', 'project': 'master-lab-foundry2'},\n",
    "    {'name': f'foundry3-{resource_suffix}', 'location': 'norwayeast', 'project': 'master-lab-foundry3'}\n",
    "]\n",
    "\n",
    "models_config = {\n",
    "    'foundry1': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4o', 'format': 'OpenAI', 'version': '2024-08-06', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'text-embedding-3-small', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "        {'name': 'text-embedding-3-large', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "                {'name': 'dall-e-3', 'format': 'OpenAI', 'version': '3.0', 'sku': 'Standard', 'capacity': 1},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry2': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry3': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Phase 2a: Check/Create Foundry Hubs\n",
    "print('[*] Phase 2a: AI Foundry Hubs')\n",
    "existing_accounts = {acc.name: acc for acc in cog_client.accounts.list_by_resource_group(resource_group_name)}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    if foundry_name in existing_accounts:\n",
    "        print(f'  [OK] {foundry_name} already exists')\n",
    "    else:\n",
    "        print(f'  [*] Creating {foundry_name}...')\n",
    "        try:\n",
    "            account_params = Account(\n",
    "                location=foundry['location'],\n",
    "                sku=CogSku(name='S0'),\n",
    "                kind='AIServices',\n",
    "                properties={\n",
    "                    'customSubDomainName': foundry_name.lower(),\n",
    "                    'publicNetworkAccess': 'Enabled',\n",
    "                    'allowProjectManagement': True\n",
    "                },\n",
    "                identity={'type': 'SystemAssigned'}\n",
    "            )\n",
    "            poller = cog_client.accounts.begin_create(resource_group_name, foundry_name, account_params)\n",
    "            poller.result(timeout=300)\n",
    "            print(f'  [OK] {foundry_name} created')\n",
    "        except Exception as e:\n",
    "            print(f'  [ERROR] Failed: {str(e)[:100]}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Phase 2b: Deploy Models (Resilient)\n",
    "print('[*] Phase 2b: AI Models (Resilient)')\n",
    "deployment_results = {'succeeded': [], 'failed': [], 'skipped': []}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    short_name = foundry_name.split('-')[0]\n",
    "    models = models_config.get(short_name, [])\n",
    "\n",
    "    print(f'  [*] {foundry_name}: {len(models)} models')\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model['name']\n",
    "        try:\n",
    "            # Check if exists\n",
    "            existing = cog_client.deployments.get(resource_group_name, foundry_name, model_name)\n",
    "            if existing.properties.provisioning_state == 'Succeeded':\n",
    "                deployment_results['skipped'].append(f'{short_name}/{model_name}')\n",
    "                print(f'    [OK] {model_name} already deployed')\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            print(f'    [*] Deploying {model_name}...')\n",
    "            deployment_params = Deployment(\n",
    "                sku=CogSku(name=model['sku'], capacity=model['capacity']),\n",
    "                properties=DeploymentProperties(\n",
    "                    model=DeploymentModel(\n",
    "                        format=model['format'],\n",
    "                        name=model['name'],\n",
    "                        version=model['version']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            poller = cog_client.deployments.begin_create_or_update(\n",
    "                resource_group_name, foundry_name, model_name, deployment_params\n",
    "            )\n",
    "            poller.result(timeout=600)\n",
    "            deployment_results['succeeded'].append(f'{short_name}/{model_name}')\n",
    "            print(f'    [OK] {model_name} deployed')\n",
    "        except Exception as e:\n",
    "            deployment_results['failed'].append({'model': f'{short_name}/{model_name}', 'error': str(e)})\n",
    "            print(f'    [SKIP] {model_name} failed: {str(e)[:80]}')\n",
    "\n",
    "print()\n",
    "print(f'[OK] Models: {len(deployment_results[\"succeeded\"])} deployed, {len(deployment_results[\"skipped\"])} skipped, {len(deployment_results[\"failed\"])} failed')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Collect Foundry Deployment Outputs for Env File\n",
    "# ============================================================================\n",
    "print()\n",
    "print('[*] Collecting foundry deployment outputs for env file...')\n",
    "step2_outputs = {\n",
    "    'foundryProjectEndpoint': '',\n",
    "    'inferenceAPIPath': 'inference',\n",
    "    'foundries': []\n",
    "}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    try:\n",
    "        # Get account details\n",
    "        account = cog_client.accounts.get(resource_group_name, foundry_name)\n",
    "        \n",
    "        # Get primary key\n",
    "        keys = cog_client.accounts.list_keys(resource_group_name, foundry_name)\n",
    "        primary_key = keys.key1\n",
    "        \n",
    "        # Build endpoint\n",
    "        endpoint = f\"https://{foundry_name}.openai.azure.com/\"\n",
    "        \n",
    "        # Get deployed model names for this foundry\n",
    "        short_name = foundry_name.split('-')[0]\n",
    "        model_names = [m['name'] for m in models_config.get(short_name, [])]\n",
    "        \n",
    "        foundry_output = {\n",
    "            'name': foundry_name,\n",
    "            'location': foundry['location'],\n",
    "            'endpoint': endpoint,\n",
    "            'key': primary_key,\n",
    "            'models': model_names\n",
    "        }\n",
    "        \n",
    "        step2_outputs['foundries'].append(foundry_output)\n",
    "        print(f\"  [OK] Captured {foundry_name}: {len(model_names)} models\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Could not capture {foundry_name} outputs: {str(e)[:80]}\")\n",
    "\n",
    "print(f'[OK] Captured {len(step2_outputs[\"foundries\"])} foundry outputs')\n",
    "print()\n",
    "\n",
    "print('[*] Phase 2c: APIM Inference API')\n",
    "\n",
    "deployment_step2c = 'master-lab-02c-apim-api'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step2c):\n",
    "    print('[OK] APIM API already configured. Skipping...')\n",
    "else:\n",
    "    print('[*] Configuring APIM Inference API...')\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-02c-apim-api.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 2c')\n",
    "\n",
    "    params_dict = {\n",
    "        'apimLoggerId': {'value': step1_outputs['apimLoggerId']},\n",
    "        'appInsightsId': {'value': step1_outputs['appInsightsId']},\n",
    "        'appInsightsInstrumentationKey': {'value': step1_outputs['appInsightsInstrumentationKey']},\n",
    "        'inferenceAPIPath': {'value': 'inference'},\n",
    "        'inferenceAPIType': {'value': 'AzureOpenAI'}\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step2c, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 2c deployment failed')\n",
    "\n",
    "    print('[OK] APIM API configured')\n",
    "\n",
    "print('[OK] Step 2 complete')\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SUPPORTING SERVICES (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print()\n",
    "\n",
    "deployment_step3 = 'master-lab-03-supporting'\n",
    "if check_deployment_exists(resource_group_name, deployment_step3):\n",
    "    print('[OK] Step 3 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 3 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-03-supporting.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 3')\n",
    "\n",
    "    params_dict = {}\n",
    "    if os.path.exists(BICEP_DIR / 'params-03-supporting.json'):\n",
    "        with open(BICEP_DIR / 'params-03-supporting.json') as f:\n",
    "            params = json.load(f)\n",
    "        # Extract only the 'parameters' section from ARM parameter file\n",
    "        params_dict = params.get('parameters', {})\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step3, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 3 deployment failed')\n",
    "    print('[OK] Step 3 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    print('[OK] Step 3 outputs retrieved')\n",
    "except Exception:\n",
    "    step3_outputs = {}\n",
    "    print('[*] No Step 3 outputs available')\n",
    "# =============================================================================\n",
    "# STEP 4: MCP SERVERS (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 4: MCP SERVERS')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Container Apps + 5 MCP servers')\n",
    "print('[*] Estimated time: ~5 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step4 = 'master-lab-04-mcp'\n",
    "if check_deployment_exists(resource_group_name, deployment_step4):\n",
    "    print('[OK] Step 4 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 4 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-04-mcp.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 4')\n",
    "\n",
    "    params_dict = {\n",
    "        'logAnalyticsCustomerId': {'value': step1_outputs.get('logAnalyticsCustomerId', '')},\n",
    "        'logAnalyticsPrimarySharedKey': {'value': step1_outputs.get('logAnalyticsPrimarySharedKey', '')},\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step4, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 4 deployment failed')\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    print('[OK] Step 4 outputs retrieved')\n",
    "except Exception:\n",
    "    step4_outputs = {}\n",
    "    print('[*] No Step 4 outputs available')\n",
    "\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT COMPLETE\n",
    "# =============================================================================\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "total_mins = int(total_elapsed / 60)\n",
    "total_secs = int(total_elapsed % 60)\n",
    "\n",
    "print('=' * 70)\n",
    "print('DEPLOYMENT COMPLETE')\n",
    "print('=' * 70)\n",
    "print(f'[OK] Total time: {total_mins}m {total_secs}s')\n",
    "print()\n",
    "print('[OK] All 4 steps deployed successfully!')\n",
    "print('[OK] Next: Run Cell 18-19 to generate master-lab.env')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e183ba1f-b5db-4152-b5e6-009ad9f334b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BICEP STEP 2: AI Foundry Accounts (Infra-as-Code Option)\n",
      "======================================================================\n",
      "\n",
      "[OK] Foundry Bicep deployment already exists – skipping.\n",
      "[OK] Foundry outputs retrieved\n",
      "\n",
      "[Foundry Accounts]\n",
      "  - foundry1-pavavy6pu5hpa @ uksouth -> https://foundry1-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  - foundry2-pavavy6pu5hpa @ eastus -> https://foundry2-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  - foundry3-pavavy6pu5hpa @ norwayeast -> https://foundry3-pavavy6pu5hpa.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- OPTIONAL BICEP-BASED STEP 2 (AI FOUNDRY ACCOUNTS) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BICEP STEP 2: AI Foundry Accounts (Infra-as-Code Option)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Fallbacks if prior cell not executed\n",
    "if 'resource_group_name' not in globals():\n",
    "    resource_group_name = os.getenv('RESOURCE_GROUP', 'lab-master-lab')\n",
    "if 'foundry_suffix' not in globals():\n",
    "    foundry_suffix = 'pavavy6pu5hpa'\n",
    "if 'BICEP_DIR' not in globals():\n",
    "    # Use absolute path from NOTEBOOK_DIR\n",
    "    if \"NOTEBOOK_DIR\" in globals():\n",
    "        BICEP_DIR = NOTEBOOK_DIR / \"deploy\"\n",
    "    else:\n",
    "        BICEP_DIR = Path(r\"C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab\\deploy\")\n",
    "\n",
    "# WSL path normalization (if running under /mnt and windows-style root was set)\n",
    "if 'LAB_ROOT' in globals():\n",
    "    try:\n",
    "        lr = str(LAB_ROOT)\n",
    "        if lr[1:3] == ':\\\\':  # windows drive\n",
    "            drive = lr[0].lower()\n",
    "            wsl_path = \"/mnt/\" + drive + \"/\" + lr[3:].replace(\"\\\\\", \"/\")\n",
    "            if not BICEP_DIR.exists():\n",
    "                alt = Path(wsl_path) / 'archive/scripts'\n",
    "                if alt.exists():\n",
    "                    BICEP_DIR = alt\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if 'compile_bicep_safe' not in globals():\n",
    "    def compile_bicep_safe(bicep_path):\n",
    "        b = Path(bicep_path)\n",
    "        if not b.exists():\n",
    "            print(f'[ERROR] Missing bicep: {b}')\n",
    "            return None\n",
    "        json_path = b.with_suffix('.json')\n",
    "        if json_path.exists():\n",
    "            print(f'[OK] Using precompiled: {json_path.name}')\n",
    "            return str(json_path)\n",
    "        if 'compile_bicep' in globals():\n",
    "            try:\n",
    "                print('[*] Precompiled JSON not found; fallback compile_bicep()')\n",
    "                return compile_bicep(str(b))\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] compile_bicep() failed: {e}')\n",
    "        print(f'[ERROR] No JSON + no fallback: {json_path}')\n",
    "        return None\n",
    "\n",
    "bicep_foundry_deployment = 'master-lab-02-foundry'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, bicep_foundry_deployment):\n",
    "    print('[OK] Foundry Bicep deployment already exists – skipping.')\n",
    "else:\n",
    "    print('[*] Deploying foundry accounts via Bicep...')\n",
    "    template_candidate = BICEP_DIR / 'deploy-02-foundry.bicep'\n",
    "    template_file = compile_bicep_safe(template_candidate)\n",
    "    if not template_file:\n",
    "        print(f\"[WARN] Bicep template or precompiled JSON not found at: {template_candidate}\")\n",
    "        print(\"[WARN] Skipping Bicep deployment and relying on previously created Python-based foundry resources.\")\n",
    "    else:\n",
    "        params_dict = {\n",
    "            'resourceSuffix': {'value': foundry_suffix},\n",
    "            # Optional custom config example:\n",
    "            # 'foundryConfig': {'value': [\n",
    "            #     {'name': 'foundry1', 'location': 'uksouth'},\n",
    "            #     {'name': 'foundry2', 'location': 'eastus'},\n",
    "            #     {'name': 'foundry3', 'location': 'norwayeast'}\n",
    "            # ]}\n",
    "        }\n",
    "        success, _ = deploy_template(resource_group_name, bicep_foundry_deployment, template_file, params_dict)\n",
    "        if not success:\n",
    "            print('[WARN] Foundry Bicep deployment failed – continuing without Bicep deployment.')\n",
    "        else:\n",
    "            print('[OK] Foundry accounts deployed via Bicep')\n",
    "\n",
    "# Outputs (graceful fallback to existing_accounts if Bicep outputs unavailable)\n",
    "try:\n",
    "    foundry_outputs = get_deployment_outputs(resource_group_name, bicep_foundry_deployment)\n",
    "    print('[OK] Foundry outputs retrieved')\n",
    "    accounts = foundry_outputs.get('foundryAccounts', [])\n",
    "    if isinstance(accounts, list):\n",
    "        print('\\n[Foundry Accounts]')\n",
    "        for a in accounts:\n",
    "            print(f\"  - {a.get('name')} @ {a.get('location')} -> {a.get('endpoint')}\")\n",
    "    else:\n",
    "        print('[WARN] foundryAccounts output missing or wrong type')\n",
    "except Exception as e:\n",
    "    print('[WARN] Could not retrieve foundry outputs:', str(e)[:160])\n",
    "    if 'existing_accounts' in globals() and existing_accounts:\n",
    "        print('[INFO] Falling back to existing_accounts already provisioned:')\n",
    "        for name, acct_obj in existing_accounts.items():\n",
    "            try:\n",
    "                loc = getattr(acct_obj, 'location', 'unknown')\n",
    "                endpoint = getattr(acct_obj.properties, 'endpoint', None) or getattr(acct_obj.properties, 'apiEndpoint', '')\n",
    "                print(f\"  - {name} @ {loc} -> {endpoint}\")\n",
    "            except Exception:\n",
    "                print(f\"  - {name}\")\n",
    "    else:\n",
    "        print('[INFO] No existing_accounts fallback available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating master-lab.env...\n",
      "[*] Auto-discovering APIM_API_ID...\n",
      "[OK] Auto-discovered APIM_API_ID: inference-api\n",
      "[OK] Loaded environment variables into os.environ\n",
      "[OK] Created /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "[OK] File location: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "[*] Model Deployment Summary:\n",
      "  Region 1 (UK South): 6 models\n",
      "    - gpt-4o-mini\n",
      "    - gpt-4o\n",
      "    - text-embedding-3-small\n",
      "    - text-embedding-3-large\n",
      "    - dall-e-3\n",
      "    - gpt-4.1-nano\n",
      "  Region 2 (East US): 2 models\n",
      "    - gpt-4o-mini\n",
      "    - gpt-4.1-nano\n",
      "  Region 3 (Norway East): 2 models\n",
      "    - gpt-4o-mini\n",
      "    - gpt-4.1-nano\n",
      "\n",
      "[OK] Load Balancing: ENABLED (3 regions)\n",
      "[OK] LB Regions: uksouth, eastus, norwayeast\n",
      "\n",
      "[OK] You can now load this in all lab tests:\n",
      "  from dotenv import load_dotenv\n",
      "  load_dotenv(\"master-lab.env\")\n",
      "\n",
      "======================================================================\n",
      "SETUP COMPLETE - ALL LABS READY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('[*] Generating master-lab.env...')\n",
    "\n",
    "# Ensure step2_outputs, step3_outputs and step4_outputs exist (safe fallback to empty dicts)\n",
    "try:\n",
    "    step2_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step2_outputs = get_deployment_outputs(resource_group_name, deployment_step2c)\n",
    "    except Exception:\n",
    "        step2_outputs = {}\n",
    "\n",
    "try:\n",
    "    step3_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    except Exception:\n",
    "        step3_outputs = {}\n",
    "\n",
    "try:\n",
    "    step4_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    except Exception:\n",
    "        step4_outputs = {}\n",
    "\n",
    "# Get API key from APIM subscriptions (prefer step1 outputs)\n",
    "apim_subscriptions = step1_outputs.get('apimSubscriptions', []) if isinstance(step1_outputs, dict) else []\n",
    "api_key = apim_subscriptions[0]['key'] if apim_subscriptions else 'N/A'\n",
    "\n",
    "# Auto-discover APIM API_ID from deployed APIM service\n",
    "print('[*] Auto-discovering APIM_API_ID...')\n",
    "discovered_api_id = None\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    import json as json_module\n",
    "    import shutil\n",
    "    \n",
    "    # Get APIM service name from step1 outputs\n",
    "    apim_service_name = step1_outputs.get('apimServiceName', 'apim-pavavy6pu5hpa')\n",
    "    \n",
    "    # Find Azure CLI\n",
    "    az_cli = shutil.which(\"az\")\n",
    "    if az_cli and subscription_id:\n",
    "        # Query APIM for APIs\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{resource_group_name}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{apim_service_name}/apis?api-version=2022-08-01')\n",
    "        \n",
    "        result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                               capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            apis_data = json_module.loads(result.stdout)\n",
    "            apis = apis_data.get('value', [])\n",
    "            \n",
    "            # Find inference API\n",
    "            for api in apis:\n",
    "                api_id = api.get('name', '')\n",
    "                api_props = api.get('properties', {})\n",
    "                api_name = api_props.get('displayName', '').lower()\n",
    "                api_path = api_props.get('path', '').lower()\n",
    "                \n",
    "                if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                    discovered_api_id = api_id\n",
    "                    print(f'[OK] Auto-discovered APIM_API_ID: {discovered_api_id}')\n",
    "                    break\n",
    "            \n",
    "            if not discovered_api_id:\n",
    "                # Fallback to inference-api if exists\n",
    "                for api in apis:\n",
    "                    if api.get('name') == 'inference-api':\n",
    "                        discovered_api_id = 'inference-api'\n",
    "                        print(f'[OK] Found APIM_API_ID: {discovered_api_id}')\n",
    "                        break\n",
    "except Exception as e:\n",
    "    print(f'[!] Could not auto-discover APIM_API_ID: {e}')\n",
    "\n",
    "# Use discovered ID or fallback to default\n",
    "apim_api_id = discovered_api_id if discovered_api_id else 'inference-api'\n",
    "if not discovered_api_id:\n",
    "    print(f'[!] Using default APIM_API_ID: {apim_api_id}')\n",
    "\n",
    "# Set in environment for downstream use\n",
    "os.environ['APIM_API_ID'] = apim_api_id\n",
    "\n",
    "# Build .env content with grouped structure\n",
    "env_content = f\"\"\"# Master AI Gateway Lab - Deployment Outputs\n",
    "# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "# Resource Group: {resource_group_name}\n",
    "\n",
    "# ===========================================\n",
    "# APIM (API Management)\n",
    "# ===========================================\n",
    "APIM_GATEWAY_URL={step1_outputs.get('apimGatewayUrl', '')}\n",
    "APIM_SERVICE_ID={step1_outputs.get('apimServiceId', '')}\n",
    "APIM_SERVICE_NAME={step1_outputs.get('apimServiceName', '')}\n",
    "APIM_API_KEY={api_key}\n",
    "APIM_API_ID={apim_api_id}\n",
    "\n",
    "# ===========================================\n",
    "# OpenAI Endpoint (APIM Gateway + Inference Path)\n",
    "# ===========================================\n",
    "OPENAI_ENDPOINT={step1_outputs.get('apimGatewayUrl', '')}/{step2_outputs.get('inferenceAPIPath', 'inference')}\n",
    "\n",
    "# ===========================================\n",
    "# AI Foundry\n",
    "# ===========================================\n",
    "FOUNDRY_PROJECT_ENDPOINT={step2_outputs.get('foundryProjectEndpoint', '')}\n",
    "INFERENCE_API_PATH={step2_outputs.get('inferenceAPIPath', 'inference')}\n",
    "\"\"\"\n",
    "\n",
    "# ===========================================\n",
    "# AI Models (Multi-Region Load Balancing)\n",
    "# ===========================================\n",
    "# Extract foundry deployment information from step2_outputs\n",
    "foundries_data = step2_outputs.get('foundries', []) if isinstance(step2_outputs, dict) else []\n",
    "\n",
    "# Region mapping for display\n",
    "region_names = {\n",
    "    'uksouth': 'UK South',\n",
    "    'eastus': 'East US',\n",
    "    'norwayeast': 'Norway East'\n",
    "}\n",
    "\n",
    "# Track endpoints for load balancing\n",
    "lb_endpoints = []\n",
    "lb_regions = []\n",
    "\n",
    "env_content += \"\\n# ===========================================\\n\"\n",
    "env_content += \"# AI Models (Multi-Region Load Balancing)\\n\"\n",
    "env_content += \"# ===========================================\\n\\n\"\n",
    "\n",
    "# Process each foundry (region)\n",
    "for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "    if not isinstance(foundry_info, dict):\n",
    "        continue\n",
    "\n",
    "    foundry_name = foundry_info.get('name', '')\n",
    "    location = foundry_info.get('location', '')\n",
    "    endpoint = foundry_info.get('endpoint', '')\n",
    "    key = foundry_info.get('key', '')\n",
    "    models = foundry_info.get('models', [])\n",
    "\n",
    "    # Add region to load balancing config\n",
    "    if location:\n",
    "        lb_regions.append(location)\n",
    "\n",
    "    # Add comment for region\n",
    "    region_display = region_names.get(location, location)\n",
    "    env_content += f\"# Region {idx} ({region_display}) - {foundry_name}\\n\"\n",
    "\n",
    "    # Process each model in this foundry\n",
    "    for model_name in models:\n",
    "        # Normalize model name for env var (replace hyphens with underscores, uppercase)\n",
    "        model_var = model_name.upper().replace('-', '_').replace('.', '_')\n",
    "\n",
    "        # Add endpoint and key for this model in this region\n",
    "        env_content += f\"MODEL_{model_var}_ENDPOINT_R{idx}={endpoint}\\n\"\n",
    "        env_content += f\"MODEL_{model_var}_KEY_R{idx}={key}\\n\"\n",
    "\n",
    "        # Track gpt-4o-mini endpoints for load balancing\n",
    "        if model_name == 'gpt-4o-mini' and endpoint:\n",
    "            lb_endpoints.append(endpoint)\n",
    "\n",
    "    env_content += \"\\n\"\n",
    "\n",
    "# Add load balancing configuration\n",
    "env_content += \"# Load Balancing Configuration\\n\"\n",
    "env_content += f\"LB_REGIONS={','.join(lb_regions)}\\n\"\n",
    "env_content += f\"LB_GPT4O_MINI_ENDPOINTS={','.join(lb_endpoints)}\\n\"\n",
    "env_content += f\"LB_ENABLED={'true' if len(lb_endpoints) > 1 else 'false'}\\n\"\n",
    "env_content += \"\\n\"\n",
    "\n",
    "# Continue with supporting services\n",
    "env_content += f\"\"\"# ===========================================\n",
    "# Supporting Services\n",
    "# ===========================================\n",
    "\n",
    "# Redis (Semantic Caching)\n",
    "REDIS_HOST={step3_outputs.get('redisCacheHost', '')}\n",
    "REDIS_PORT={step3_outputs.get('redisCachePort', 10000)}\n",
    "REDIS_KEY={step3_outputs.get('redisCacheKey', '')}\n",
    "\n",
    "# Azure Cognitive Search\n",
    "SEARCH_SERVICE_NAME={step3_outputs.get('searchServiceName', '')}\n",
    "SEARCH_ENDPOINT={step3_outputs.get('searchServiceEndpoint', '')}\n",
    "SEARCH_ADMIN_KEY={step3_outputs.get('searchServiceAdminKey', '')}\n",
    "\n",
    "# Cosmos DB\n",
    "COSMOS_ACCOUNT_NAME={step3_outputs.get('cosmosDbAccountName', '')}\n",
    "COSMOS_ENDPOINT={step3_outputs.get('cosmosDbEndpoint', '')}\n",
    "COSMOS_KEY={step3_outputs.get('cosmosDbKey', '')}\n",
    "\n",
    "# Content Safety\n",
    "CONTENT_SAFETY_ENDPOINT={step3_outputs.get('contentSafetyEndpoint', '')}\n",
    "CONTENT_SAFETY_KEY={step3_outputs.get('contentSafetyKey', '')}\n",
    "\n",
    "# ===========================================\n",
    "# MCP Servers\n",
    "# ===========================================\n",
    "CONTAINER_REGISTRY={step4_outputs.get('containerRegistryLoginServer', '')}\n",
    "CONTAINER_APP_ENV_ID={step4_outputs.get('containerAppEnvId', '')}\n",
    "\"\"\"\n",
    "\n",
    "# Add MCP server URLs (safe handling if not present)\n",
    "mcp_urls = step4_outputs.get('mcpServerUrls', []) if isinstance(step4_outputs, dict) else []\n",
    "for mcp_server in mcp_urls:  # FIXED: Changed from 'mcp' to 'mcp_server' to avoid overwriting global mcp variable\n",
    "    # Guard against missing fields\n",
    "    name = mcp_server.get('name') if isinstance(mcp_server, dict) else None\n",
    "    url = mcp_server.get('url') if isinstance(mcp_server, dict) else None\n",
    "    if name and url:\n",
    "        var_name = f\"MCP_SERVER_{name.upper().replace('-', '_')}_URL\"\n",
    "        env_content += f\"{var_name}={url}\\n\"\n",
    "\n",
    "env_content += f\"\"\"\n",
    "# ===========================================\n",
    "# Deployment Info\n",
    "# ===========================================\n",
    "RESOURCE_GROUP={resource_group_name}\n",
    "LOCATION={location}\n",
    "DEPLOYMENT_PREFIX={deployment_name_prefix}\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "env_file = NOTEBOOK_DIR / 'master-lab.env'  # Use absolute path from Cell 004\n",
    "with open(str(env_file), 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# CRITICAL: Load the env file immediately into os.environ\n",
    "# This ensures subsequent cells can access the variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(str(env_file), override=True)\n",
    "print(f\"[OK] Loaded environment variables into os.environ\")\n",
    "\n",
    "print(f'[OK] Created {env_file}')\n",
    "print(f'[OK] File location: {os.path.abspath(env_file)}')\n",
    "\n",
    "# Display summary of model deployments\n",
    "if foundries_data:\n",
    "    print()\n",
    "    print('[*] Model Deployment Summary:')\n",
    "    for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "        if isinstance(foundry_info, dict):\n",
    "            location = foundry_info.get('location', 'unknown')\n",
    "            models = foundry_info.get('models', [])\n",
    "            region_display = region_names.get(location, location)\n",
    "            print(f'  Region {idx} ({region_display}): {len(models)} models')\n",
    "            for model in models:\n",
    "                print(f'    - {model}')\n",
    "\n",
    "# Display load balancing info\n",
    "if len(lb_endpoints) > 1:\n",
    "    print()\n",
    "    print(f'[OK] Load Balancing: ENABLED ({len(lb_endpoints)} regions)')\n",
    "    print(f'[OK] LB Regions: {\", \".join(lb_regions)}')\n",
    "else:\n",
    "    print()\n",
    "    print('[!] Load Balancing: Disabled (requires 2+ regions with gpt-4o-mini)')\n",
    "\n",
    "print()\n",
    "print('[OK] You can now load this in all lab tests:')\n",
    "print('  from dotenv import load_dotenv')\n",
    "print('  load_dotenv(\"master-lab.env\")')\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('SETUP COMPLETE - ALL LABS READY')\n",
    "print('=' * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 022: Reload Complete Configuration\n",
    "\n",
    "NOW we can load the complete master-lab.env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "apim_vars_definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "[APIM & API Variables Defined]\n",
      "  apim_gateway_url: https://apim-pavavy6pu5hpa.azure-api.net...\n",
      "  apim_api_key: ****2cb0\n",
      "  inference_api_path: inference\n",
      "  inference_api_version: 2024-08-01-preview\n",
      "  deployment_name: gpt-4o-mini\n",
      "  api_key: ****2cb0\n"
     ]
    }
   ],
   "source": [
    "# Load complete configuration from master-lab.env\n",
    "# This cell must be run AFTER Cell 021 generates the env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use NOTEBOOK_DIR from Cell 004 (or detect it again)\n",
    "if 'NOTEBOOK_DIR' not in globals():\n",
    "    # Fallback: detect notebook directory again\n",
    "    NOTEBOOK_DIR = None\n",
    "    if Path('bootstrap.env').exists() or Path('master-lab.env').exists():\n",
    "        NOTEBOOK_DIR = Path.cwd()\n",
    "    else:\n",
    "        known_path = Path(r'C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab')\n",
    "        if known_path.exists():\n",
    "            NOTEBOOK_DIR = known_path\n",
    "            os.chdir(NOTEBOOK_DIR)\n",
    "    \n",
    "    if NOTEBOOK_DIR is None:\n",
    "        raise ValueError(\"Cannot locate notebook directory. Please run Cell 004 first.\")\n",
    "    \n",
    "    print(f\"[INFO] Detected notebook directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Load master-lab.env into environment variables using absolute path\n",
    "env_file = NOTEBOOK_DIR / 'master-lab.env'\n",
    "\n",
    "if env_file.exists():\n",
    "    load_dotenv(str(env_file), override=True)\n",
    "    print(f'[OK] Loaded: {env_file}')\n",
    "else:\n",
    "    print(f'[WARN] File not found: {env_file}')\n",
    "    print('       Run Cell 021 to generate master-lab.env')\n",
    "    print('       Some cells may fail without environment variables.')\n",
    "\n",
    "# APIM Variable Definitions (for cells that use lowercase names)\n",
    "# These map environment variables to lowercase snake_case for backwards compatibility\n",
    "\n",
    "# APIM Gateway URLs\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "apim_resource_gateway_url = apim_gateway_url  # Same as gateway URL\n",
    "apim_api_key = os.environ.get('APIM_API_KEY', '')\n",
    "\n",
    "# Azure OpenAI API Configuration\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "inference_api_version = '2024-08-01-preview'  # Azure OpenAI API version\n",
    "api_key = apim_api_key  # Alias for backward compatibility\n",
    "\n",
    "# Model deployment (default to gpt-4o-mini for cost efficiency)\n",
    "deployment_name = 'gpt-4o-mini'\n",
    "\n",
    "# Display for verification\n",
    "print('[APIM & API Variables Defined]')\n",
    "print(f'  apim_gateway_url: {apim_gateway_url[:50]}...' if apim_gateway_url else '  apim_gateway_url: NOT SET')\n",
    "print(f'  apim_api_key: ****{apim_api_key[-4:]}' if len(apim_api_key) > 4 else '  apim_api_key: NOT SET')\n",
    "print(f'  inference_api_path: {inference_api_path}')\n",
    "print(f'  inference_api_version: {inference_api_version}')\n",
    "print(f'  deployment_name: {deployment_name}')\n",
    "print(f'  api_key: ****{api_key[-4:]}' if len(str(api_key)) > 4 else '  api_key: NOT SET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Caching Lab - Standalone\n",
    "\n",
    "**Using master-lab resources**\n",
    "\n",
    "This notebook demonstrates Azure API Management semantic caching using the proven working approach.\n",
    "\n",
    "## How it Works\n",
    "\n",
    "1. **Request arrives** at APIM\n",
    "2. **APIM creates embedding** of the prompt using text-embedding-3-small\n",
    "3. **Checks Redis cache** for similar embeddings (>80% match)\n",
    "4. **If match found**: Returns cached response (~0.1-0.3s)\n",
    "5. **If no match**: Calls Azure OpenAI, stores in cache (~3-10s)\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- First similar request: Slow (~3-10s)\n",
    "- Subsequent similar requests: **15-100x faster!** (~0.1-0.3s)\n",
    "- Cache TTL: 20 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: master-lab.env\n",
      "\n",
      "Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "API Key: ****2cb0\n",
      "\n",
      "✅ Setup complete - Ready to test semantic caching!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Find master-lab.env - check multiple locations\n",
    "possible_paths = [\n",
    "    Path('master-lab.env'),  # Same directory as notebook\n",
    "    Path(__file__).parent / 'master-lab.env' if '__file__' in globals() else None,\n",
    "    Path.cwd() / 'master-lab.env',  # Current working directory\n",
    "    Path('/mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env'),  # Absolute path\n",
    "]\n",
    "\n",
    "env_file = None\n",
    "for path in possible_paths:\n",
    "    if path and path.exists():\n",
    "        env_file = path\n",
    "        break\n",
    "\n",
    "if env_file:\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"✅ Loaded: {env_file}\")\n",
    "else:\n",
    "    print(\"❌ master-lab.env not found in any expected location!\")\n",
    "    print(\"\\nTried:\")\n",
    "    for path in possible_paths:\n",
    "        if path:\n",
    "            print(f\"   - {path}\")\n",
    "    raise FileNotFoundError(\"Please run this notebook from the master-lab directory\")\n",
    "\n",
    "# Get configuration with validation\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Validate required variables\n",
    "if not apim_gateway_url:\n",
    "    raise ValueError(\"APIM_GATEWAY_URL not found in master-lab.env\")\n",
    "if not apim_api_key:\n",
    "    raise ValueError(\"APIM_API_KEY not found in master-lab.env\")\n",
    "\n",
    "print(f\"\\nEndpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"API Key: ****{apim_api_key[-4:]}\")\n",
    "print(\"\\n✅ Setup complete - Ready to test semantic caching!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 SEMANTIC CACHING TEST\n",
      "================================================================================\n",
      "\n",
      "Making 20 requests with similar questions...\n",
      "Expected: First request slow, subsequent requests FAST\n",
      "\n",
      "\n",
      "▶️ Run 1/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '082e9818-3df6-4b1f-b817-57e6cb0657c8'}\n",
      "\n",
      "▶️ Run 2/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'b00be1e7-1b0e-4075-a55e-6e88f299d9fc'}\n",
      "\n",
      "▶️ Run 3/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '0e65b41f-8561-42dc-bb6c-eddbb4871c26'}\n",
      "\n",
      "▶️ Run 4/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '0054bdde-2544-4df0-b0ce-0f18742a3249'}\n",
      "\n",
      "▶️ Run 5/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '7b07c2c7-69bf-4283-b789-08224737961c'}\n",
      "\n",
      "▶️ Run 6/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '99ab7c4c-79c6-4da3-9e79-d34ff0383709'}\n",
      "\n",
      "▶️ Run 7/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '74514815-86f2-40fb-8034-194cecd21e3b'}\n",
      "\n",
      "▶️ Run 8/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '677b26bf-672d-4460-be64-77ae49150209'}\n",
      "\n",
      "▶️ Run 9/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'ba7edecc-6637-46b2-9385-91e78617e7a6'}\n",
      "\n",
      "▶️ Run 10/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'd21a0722-08c5-4e44-bbde-4af6c6eaebc2'}\n",
      "\n",
      "▶️ Run 11/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '0dbe04f3-59c8-45b2-bd18-19de4a117b9c'}\n",
      "\n",
      "▶️ Run 12/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '57da6476-1ba9-4438-83a5-40613bc02a2a'}\n",
      "\n",
      "▶️ Run 13/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '201f64da-6cd0-43a6-b8b9-5b05ec3216e3'}\n",
      "\n",
      "▶️ Run 14/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'fe62ee56-7df9-42a3-83f0-9f20ccb88fc9'}\n",
      "\n",
      "▶️ Run 15/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '4331d2c5-7ee2-4302-a554-8b0fd6ec1ae3'}\n",
      "\n",
      "▶️ Run 16/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'bae20e90-fc13-4842-b9dc-d3d4dfe7d53d'}\n",
      "\n",
      "▶️ Run 17/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '571dcbcc-a178-4d66-a1f8-dee346464339'}\n",
      "\n",
      "▶️ Run 18/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '668d32ed-0515-44c5-aadc-57e6fc33e656'}\n",
      "\n",
      "▶️ Run 19/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '23b79de0-1471-4831-9bbd-03c170a41c57'}\n",
      "\n",
      "▶️ Run 20/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "❌ Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '253b3599-8a55-4796-8543-70d725a44c61'}\n",
      "\n",
      "❌ No successful requests\n",
      "\n",
      "✅ Test complete - See visualization next\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Test Semantic Caching with 20 Similar Questions\n",
    "\n",
    "# These questions are semantically similar (>80% match)\n",
    "# So they should all return cached responses after the first one\n",
    "questions = [\n",
    "    \"How to Brew the Perfect Cup of Coffee?\",\n",
    "    \"What are the steps to Craft the Ideal Espresso?\",\n",
    "    \"Tell me how to create the best steaming Java?\",\n",
    "    \"Explain how to make a caffeinated brewed beverage?\"\n",
    "]\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=\"2025-03-01-preview\"\n",
    ")\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 10  # 10ms between requests\n",
    "api_runs = []  # Response times\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🧪 SEMANTIC CACHING TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMaking {runs} requests with similar questions...\")\n",
    "print(\"Expected: First request slow, subsequent requests FAST\\n\")\n",
    "\n",
    "for i in range(runs):\n",
    "    random_question = random.choice(questions)\n",
    "    print(f\"\\n▶️ Run {i+1}/{runs}:\")\n",
    "    print(f\"💬  {random_question}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": random_question}\n",
    "            ]\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        status = \"🎯 CACHE HIT\" if response_time < 1.0 else \"🔥 BACKEND CALL\"\n",
    "        print(f\"⌚ {response_time:.2f} seconds - {status}\")\n",
    "\n",
    "        # Uncomment to see responses:\n",
    "        # print(f\"💬 {response.choices[0].message.content}\\n\")\n",
    "\n",
    "        api_runs.append(response_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)[:150]}\")\n",
    "        api_runs.append(None)\n",
    "\n",
    "    time.sleep(sleep_time_ms / 1000)\n",
    "\n",
    "# Calculate statistics\n",
    "valid_runs = [r for r in api_runs if r is not None]\n",
    "\n",
    "if valid_runs:\n",
    "    avg_time = sum(valid_runs) / len(valid_runs)\n",
    "    min_time = min(valid_runs)\n",
    "    max_time = max(valid_runs)\n",
    "    cache_hits = sum(1 for r in valid_runs if r < 1.0)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"📊 PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Requests:     {len(api_runs)}\")\n",
    "    print(f\"Successful:         {len(valid_runs)}\")\n",
    "    print(f\"Average Time:       {avg_time:.2f}s\")\n",
    "    print(f\"Fastest Response:   {min_time:.2f}s\")\n",
    "    print(f\"Slowest Response:   {max_time:.2f}s\")\n",
    "    print(f\"Likely Cache Hits:  {cache_hits}/{len(valid_runs)} ({cache_hits/len(valid_runs)*100:.1f}%)\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    if max_time > 1.0 and min_time < 1.0:\n",
    "        speedup = max_time / min_time\n",
    "        print(f\"\\n✅ SEMANTIC CACHING IS WORKING!\")\n",
    "        print(f\"   Slowest request: {max_time:.2f}s (backend call)\")\n",
    "        print(f\"   Fastest request: {min_time:.2f}s (cache hit)\")\n",
    "        print(f\"   Speed improvement: {speedup:.1f}x faster!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Results may vary. Expected: first request slow, subsequent fast.\")\n",
    "else:\n",
    "    print(\"\\n❌ No successful requests\")\n",
    "\n",
    "print(\"\\n✅ Test complete - See visualization next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  No valid results to visualize\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Visualize Semantic Caching Performance\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "if 'api_runs' in globals() and api_runs:\n",
    "    valid_results = [r for r in api_runs if r is not None]\n",
    "\n",
    "    if len(valid_results) > 0:\n",
    "        # Create DataFrame\n",
    "        mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "        df = pd.DataFrame(valid_results, columns=['Response Time'])\n",
    "        df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "        # Create bar plot\n",
    "        ax = df.plot(kind='bar', x='Run', y='Response Time', legend=False, color='steelblue')\n",
    "        plt.title('Semantic Caching Performance', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Runs', fontsize=12)\n",
    "        plt.ylabel('Response Time (s)', fontsize=12)\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "        # Add average line\n",
    "        average = df['Response Time'].mean()\n",
    "        plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}s')\n",
    "\n",
    "        # Add cache hit threshold line\n",
    "        plt.axhline(y=1.0, color='green', linestyle=':', linewidth=2, label='Cache Hit Threshold (1.0s)')\n",
    "\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n📊 Chart Legend:\")\n",
    "        print(\"   🔵 Blue bars = Individual response times\")\n",
    "        print(\"   🔴 Red dashed line = Average response time\")\n",
    "        print(\"   🟢 Green dotted line = Cache hit threshold (1.0s)\")\n",
    "        print(\"   Bars below green line = Likely cache hits (fast!)\")\n",
    "        print(\"\\n✅ Visualization complete\")\n",
    "    else:\n",
    "        print(\"⚠️  No valid results to visualize\")\n",
    "else:\n",
    "    print(\"⚠️  Run Cell 2 first to generate test results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Redis Server Information:\n",
      "   Used Memory: 7.95M\n",
      "   Cache Hits: 37\n",
      "   Cache Misses: 52\n",
      "   Evicted Keys: 0\n",
      "   Expired Keys: 42\n",
      "   Hit Rate: 41.6%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Redis statistics retrieved successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: View Redis Cache Statistics (Optional)\n",
    "\n",
    "import redis.asyncio as redis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get Redis configuration from master-lab.env\n",
    "redis_host = os.environ.get('REDIS_HOST')\n",
    "redis_port = int(os.environ.get('REDIS_PORT', 10000))\n",
    "redis_key = os.environ.get('REDIS_KEY')\n",
    "\n",
    "async def get_redis_info():\n",
    "    r = await redis.from_url(\n",
    "        f\"rediss://:{redis_key}@{redis_host}:{redis_port}\"\n",
    "    )\n",
    "\n",
    "    info = await r.info()\n",
    "\n",
    "    print(\"📊 Redis Server Information:\")\n",
    "    print(f\"   Used Memory: {info['used_memory_human']}\")\n",
    "    print(f\"   Cache Hits: {info['keyspace_hits']}\")\n",
    "    print(f\"   Cache Misses: {info['keyspace_misses']}\")\n",
    "    print(f\"   Evicted Keys: {info['evicted_keys']}\")\n",
    "    print(f\"   Expired Keys: {info['expired_keys']}\")\n",
    "\n",
    "    # Calculate hit rate\n",
    "    total = info['keyspace_hits'] + info['keyspace_misses']\n",
    "    if total > 0:\n",
    "        hit_rate = (info['keyspace_hits'] / total) * 100\n",
    "        print(f\"   Hit Rate: {hit_rate:.1f}%\")\n",
    "\n",
    "    # Create visualization\n",
    "    redis_info = {\n",
    "        'Metric': ['Cache Hits', 'Cache Misses', 'Evicted Keys', 'Expired Keys'],\n",
    "        'Value': [info['keyspace_hits'], info['keyspace_misses'], info['evicted_keys'], info['expired_keys']]\n",
    "    }\n",
    "\n",
    "    df_redis_info = pd.DataFrame(redis_info)\n",
    "    df_redis_info.plot(kind='barh', x='Metric', y='Value', legend=False, color='teal')\n",
    "\n",
    "    plt.title('Redis Cache Statistics')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    await r.aclose()\n",
    "    print(\"\\n✅ Redis statistics retrieved successfully\")\n",
    "\n",
    "try:\n",
    "    await get_redis_info()\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not connect to Redis: {str(e)[:100]}\")\n",
    "    print(\"   Make sure Redis is configured in master-lab.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Semantic Caching Lab Complete!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "✅ How semantic caching reduces API calls for similar queries  \n",
    "✅ How to measure caching performance  \n",
    "✅ How vector embeddings enable semantic similarity matching  \n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "💰 **Cost savings**: Reduced Azure OpenAI API calls (up to 90% reduction!)  \n",
    "⚡ **Performance**: Faster response times (15-100x faster for cached requests)  \n",
    "📊 **Scalability**: Better handling of repetitive queries  \n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Similarity Threshold**: 0.8 (80% match required)\n",
    "- **Cache TTL**: 20 minutes (1200 seconds)\n",
    "- **Embeddings Model**: text-embedding-3-small\n",
    "- **Cache Storage**: Redis\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Integrate semantic caching into your production APIs to reduce costs and improve performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Security & Access Control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lab06'></a>\n",
    "## Lab 06: Access Controlling\n",
    "\n",
    "![Access Controlling](./images/access-controlling.gif)\n",
    "\n",
    "📖 **Workshop Guide:** https://azure-samples.github.io/AI-Gateway/\n",
    "\n",
    "### Objective\n",
    "Secure AI Gateway endpoints using OAuth 2.0 and Microsoft Entra ID (formerly Azure AD) for enterprise authentication.\n",
    "\n",
    "### What You'll Learn\n",
    "- **OAuth 2.0 Flow:** Implement token-based authentication with Entra ID\n",
    "- **JWT Validation:** Validate JSON Web Tokens in APIM policies\n",
    "- **RBAC Integration:** Control access based on Azure roles and groups\n",
    "- **API Scopes:** Define granular permissions for different API operations\n",
    "- **Token Claims:** Extract user identity and roles from access tokens\n",
    "\n",
    "---\n",
    "### Understanding OAuth 2.0 with Microsoft Entra ID\n",
    "> 💡 **Tip:** OAuth 2.0 provides token-based authentication without exposing credentials in each request.\n",
    "\n",
    "**Authentication Flow:**\n",
    "1. **User Login:** Client application redirects user to Entra ID login\n",
    "2. **Authentication:** User enters credentials and consents to permissions\n",
    "3. **Token Issuance:** Entra ID issues JWT access token\n",
    "4. **API Request:** Client includes token in `Authorization: Bearer <token>` header\n",
    "5. **Token Validation:** APIM validates token signature, expiration, and claims\n",
    "6. **Request Processing:** If valid, request forwarded to Azure OpenAI backend\n",
    "\n",
    "---\n",
    "### JWT Validation Policy\n",
    "Azure API Management uses the `validate-jwt` policy to secure endpoints.\n",
    "\n",
    "<details><summary><b>Basic JWT Validation Example</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt\n",
    "      header-name=\"Authorization\"\n",
    "      failed-validation-httpcode=\"401\"\n",
    "      failed-validation-error-message=\"Unauthorized. Access token is missing or invalid.\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <audiences>\n",
    "      <audience>api://your-api-client-id</audience>\n",
    "    </audiences>\n",
    "    <issuers>\n",
    "      <issuer>https://sts.windows.net/{tenant-id}/</issuer>\n",
    "    </issuers>\n",
    "    <required-claims>\n",
    "      <claim name=\"roles\" match=\"any\">\n",
    "        <value>AI.User</value>\n",
    "        <value>AI.Admin</value>\n",
    "      </claim>\n",
    "    </required-claims>\n",
    "  </validate-jwt>\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- `header-name`: HTTP header containing the JWT (typically `Authorization`)\n",
    "- `openid-config`: URL to Entra ID's OpenID Connect metadata\n",
    "- `audiences`: Valid `aud` claim values\n",
    "- `issuers`: Trusted token issuers\n",
    "- `required-claims`: Claims that must be present in the token\n",
    "</details>\n",
    "\n",
    "---\n",
    "### Microsoft Entra ID Integration\n",
    "> ⚠️ **Note:** You must register your application in Microsoft Entra ID before implementing OAuth 2.0.\n",
    "\n",
    "**Setup Steps:**\n",
    "1. **Register Application:** Azure Portal → Entra ID → App Registrations → New registration (note Application (client) ID & Tenant ID)\n",
    "2. **Configure API Permissions:** Add permissions and define custom scopes (e.g., `AI.Read`, `AI.Write`); grant admin consent if required\n",
    "3. **Create App Roles:** Define roles in app manifest (e.g., `AI.User`, `AI.Admin`) and assign users/groups\n",
    "4. **Configure APIM:** Add `validate-jwt` policy, reference tenant & client IDs, map roles to operations\n",
    "\n",
    "---\n",
    "### Role-Based Access Control (RBAC)\n",
    "<details><summary><b>Policy Example: Role-Based Backend Routing</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt header-name=\"Authorization\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <audiences>\n",
    "      <audience>api://your-api-client-id</audience>\n",
    "    </audiences>\n",
    "  </validate-jwt>\n",
    "\n",
    "  <!-- Admin users get priority routing -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.Headers.GetValueOrDefault(\\\"Authorization\\\",\\\"\").AsJwt()?.Claims.GetValueOrDefault(\\\"roles\\\",\\\"\").Contains(\\\"AI.Admin\\\") == true)\">\n",
    "      <set-backend-service backend-id=\"openai-premium-backend\" />\n",
    "    </when>\n",
    "    <!-- Regular users get standard backend -->\n",
    "    <otherwise>\n",
    "      <set-backend-service backend-id=\"openai-standard-backend\" />\n",
    "    </otherwise>\n",
    "  </choose>\n",
    "</inbound>\n",
    "```\n",
    "This example routes admin users to a premium backend with higher quotas.\n",
    "</details>\n",
    "\n",
    "<details><summary><b>Policy Example: Scope-Based Operation Control</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt header-name=\"Authorization\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <required-claims>\n",
    "      <claim name=\"scp\" match=\"any\">\n",
    "        <value>AI.Read</value>\n",
    "        <value>AI.Write</value>\n",
    "      </claim>\n",
    "    </required-claims>\n",
    "  </validate-jwt>\n",
    "\n",
    "  <!-- Check if operation requires write permission -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.Method != \\\"GET\\\")\">\n",
    "      <validate-jwt header-name=\"Authorization\">\n",
    "        <required-claims>\n",
    "          <claim name=\"scp\" match=\"any\">\n",
    "            <value>AI.Write</value>\n",
    "          </claim>\n",
    "        </required-claims>\n",
    "      </validate-jwt>\n",
    "    </when>\n",
    "  </choose>\n",
    "</inbound>\n",
    "```\n",
    "This ensures only tokens with `AI.Write` scope can perform non-GET operations.\n",
    "</details>\n",
    "\n",
    "---\n",
    "### Token Claims and User Identity\n",
    "JWT tokens contain claims that provide user context.\n",
    "\n",
    "**Common Claims:**\n",
    "- `sub`: Subject (unique user identifier)\n",
    "- `name`: User's display name\n",
    "- `email`: User's email address\n",
    "- `roles`: User's assigned roles\n",
    "- `scp`: Delegated permissions (scopes)\n",
    "- `aud`: Audience (intended recipient)\n",
    "- `iss`: Issuer (token authority)\n",
    "- `exp`: Expiration timestamp\n",
    "\n",
    "**Extracting Claims in Policy:**\n",
    "```xml\n",
    "<set-header name=\"X-User-Email\" exists-action=\"override\">\n",
    "  <value>@(context.Request.Headers.GetValueOrDefault(\"Authorization\",\"\" ).AsJwt()?.Claims.GetValueOrDefault(\"email\", \"unknown\"))</value>\n",
    "</set-header>\n",
    "```\n",
    "\n",
    "---\n",
    "### Testing Access Control\n",
    "**Test Scenarios:**\n",
    "1. No Token → 401 Unauthorized\n",
    "2. Invalid Token → 401 Unauthorized\n",
    "3. Valid Token → 200 OK\n",
    "4. Insufficient Permissions → 403 Forbidden\n",
    "5. Token Expired → 401 Unauthorized\n",
    "\n",
    "**Python Example with Azure Identity:**\n",
    "```python\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "\n",
    "# Acquire token from Azure Identity\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"api://your-api-client-id/.default\")\n",
    "\n",
    "# Use token with Azure OpenAI via APIM\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://your-apim.azure-api.net\",\n",
    "    api_key=token.token,  # JWT token used as API key\n",
    "    api_version=\"2024-02-15-preview\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "---\n",
    "### Security Best Practices\n",
    "> ✅ **Checklist:**\n",
    "- Validate JWT signature using OpenID configuration\n",
    "- Check token expiration (`exp`)\n",
    "- Verify audience (`aud`) matches your API\n",
    "- Validate issuer (`iss`) is trusted\n",
    "- Enforce HTTPS only\n",
    "- Handle errors without leaking sensitive info\n",
    "- Log authentication failures\n",
    "- Rotate client secrets regularly\n",
    "- Apply least-privilege role assignments\n",
    "\n",
    "---\n",
    "### Expected Outcome\n",
    "**Success Criteria:**\n",
    "- Unauthenticated requests return 401\n",
    "- Valid Entra ID tokens grant access\n",
    "- JWT validation enforces signature & claims\n",
    "- Roles restrict privileged operations\n",
    "- Scope checks block unauthorized writes\n",
    "- Expired tokens rejected cleanly\n",
    "- Clear error messages guide remediation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Requisite: Azure CLI Authentication\\n\\nAccess Control workshop requires Azure CLI to be logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 Checking Azure CLI authentication...\n",
      "✅ Logged in as: lproux@microsoft.com\n",
      "   Tenant: 2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "   Subscription: ME-MngEnvMCAP592090-lproux-1\n"
     ]
    }
   ],
   "source": [
    "# Ensure Azure CLI is logged in (required for Access Control workshop)\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"🔐 Checking Azure CLI authentication...\")\n",
    "\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "\n",
    "# Try to get current account\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [az_cli, 'account', 'show'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        import json\n",
    "        account = json.loads(result.stdout)\n",
    "        print(f\"✅ Logged in as: {account.get('user', {}).get('name', 'Unknown')}\")\n",
    "        print(f\"   Tenant: {account.get('tenantId', 'Unknown')}\")\n",
    "        print(f\"   Subscription: {account.get('name', 'Unknown')}\")\n",
    "    else:\n",
    "        print(\"❌ Azure CLI not logged in\")\n",
    "        print(\"\\nPlease run ONE of the following:\")\n",
    "        print(\"\\n1. In a terminal window:\")\n",
    "        print(\"   az login\")\n",
    "        print(\"\\n2. In a Jupyter cell:\")\n",
    "        print(\"   !az login\")\n",
    "        print(\"\\n3. Use device code (if browser not available):\")\n",
    "        print(\"   !az login --use-device-code\")\n",
    "        raise RuntimeError(\"Azure login required. Run 'az login' in a terminal.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Azure CLI not found at: {az_cli}\")\n",
    "    print(\"\\nPlease install Azure CLI:\")\n",
    "    print(\"  https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\")\n",
    "    raise\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⚠️ Azure CLI timeout - trying to continue anyway\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not verify Azure login: {e}\")\n",
    "    print(\"   Continuing anyway...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Control Workshop\n",
    "\n",
    "The following cells demonstrate token acquisition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 1: No Authentication\n",
      "================================================================================\n",
      "\n",
      "Attempting API call WITHOUT any authentication...\n",
      "Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ EXPECTED: Request failed with 401 Unauthorized\n",
      "Error: Error code: 401 - {'statusCode': 401, 'message': 'Access denied due to invalid subscription key. Make sure to provide a valid key for an active subscription.'}\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: No Authentication (should fail with 401)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🧪 TEST 1: No Authentication\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "print(f\"\\nAttempting API call WITHOUT any authentication...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy-key\",  # Not used\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    # Try to call WITHOUT auth headers\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        extra_headers={}  # NO auth headers\n",
    "    )\n",
    "    \n",
    "    print(\"\\n❌ UNEXPECTED: Request succeeded without auth!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if '401' in error_msg or 'Unauthorized' in error_msg or 'JWT' in error_msg:\n",
    "        print(\"\\n✅ EXPECTED: Request failed with 401 Unauthorized\")\n",
    "        print(f\"Error: {error_msg[:200]}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ UNEXPECTED ERROR: {error_msg[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "📝 APPLY: JWT Only Policy (disable subscriptionRequired)\n",
      "================================================================================\n",
      "[1] Current subscriptionRequired: True\n",
      "[2] ✓ Disabled subscriptionRequired for 'inference-api'\n",
      "\n",
      "[3] Applying JWT policy...\n",
      "[4] Policy Status: 200 - ✓ SUCCESS\n",
      "\n",
      "✓ JWT policy applied with multi-issuer support\n",
      "⏳ Waiting 60 seconds for propagation...\n",
      "✓ Ready for testing\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"📝 APPLY: JWT Only Policy (disable subscriptionRequired)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# STEP 1: Disable subscription requirement for pure JWT auth\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        api_config = response.json()\n",
    "        current_subscription_required = api_config.get('properties', {}).get('subscriptionRequired', False)\n",
    "        \n",
    "        print(f\"[1] Current subscriptionRequired: {current_subscription_required}\")\n",
    "        \n",
    "        if current_subscription_required:\n",
    "            # Disable subscription requirement\n",
    "            api_config['properties']['subscriptionRequired'] = False\n",
    "            \n",
    "            update_response = requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "            \n",
    "            if update_response.status_code in [200, 201]:\n",
    "                print(f\"[2] ✓ Disabled subscriptionRequired for '{api_id}'\")\n",
    "            else:\n",
    "                print(f\"[2] ✗ Failed: {update_response.status_code}\")\n",
    "        else:\n",
    "            print(f\"[2] ✓ subscriptionRequired already disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {str(e)}\")\n",
    "\n",
    "# STEP 2: Apply JWT policy with v1.0 + v2.0 issuer support\n",
    "print(f\"\\n[3] Applying JWT policy...\")\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID\")\n",
    "else:\n",
    "    # JWT policy - CRITICAL: correct element order (openid-config, audiences, issuers)\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "    \n",
    "    try:\n",
    "        policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "        \n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "        \n",
    "        print(f\"[4] Policy Status: {response.status_code} - {'✓ SUCCESS' if response.status_code in [200, 201] else '✗ FAILED'}\")\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"\\n✓ JWT policy applied with multi-issuer support\")\n",
    "            print(f\"⏳ Waiting 60 seconds for propagation...\")\n",
    "            time.sleep(60)\n",
    "            print(f\"✓ Ready for testing\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 3: JWT Token Authentication\n",
      "================================================================================\n",
      "\n",
      "[1] Acquiring JWT token...\n",
      "✅ JWT Token: eyJ0eXAiOiJKV1QiLCJh...RRInkQaxWw\n",
      "\n",
      "[2] Calling API with JWT token only (no API key)...\n",
      "Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "\n",
      "✅ SUCCESS: JWT Authentication Working!\n",
      "Response: JWT auth successful!\n",
      "Tokens: 18\n"
     ]
    }
   ],
   "source": [
    "# TEST 3: JWT Token Only (works when JWT policy active)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🧪 TEST 3: JWT Token Authentication\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Get JWT token\n",
    "print(\"\\n[1] Acquiring JWT token...\")\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(f\"✅ JWT Token: {jwt_token[:20]}...{jwt_token[-10:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to get JWT: {e}\")\n",
    "    print(\"   Run: az login\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n[2] Calling API with JWT token only (no API key)...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy-not-used\",  # Ignored when JWT provided\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'JWT auth successful!'\"}],\n",
    "        max_tokens=50,\n",
    "        extra_headers={\n",
    "            \"Authorization\": f\"Bearer {jwt_token}\"  # JWT only, no API key\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ SUCCESS: JWT Authentication Working!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if 'api-key' in error_msg.lower() or ('401' in error_msg and 'API' in error_msg):\n",
    "        print(\"\\n❌ FAILED: API requires API Key in addition to JWT\")\n",
    "        print(\"   Current policy may be Dual Auth or API Key only\")\n",
    "        print(\"   Run Cell 028 to enable JWT-only mode\")\n",
    "    else:\n",
    "        print(f\"\\n❌ ERROR: {error_msg[:300]}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "📝 APPLY: Dual Auth (JWT + API Key)\n",
      "================================================================================\n",
      "[auth] Resolved tenant_id: 2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "📝 Policy Applied: Dual Auth (JWT + API Key)\n",
      "Status: 200 - ✓ SUCCESS\n",
      "Policy requires BOTH:\n",
      "  • Valid JWT token (Authorization header)\n",
      "  • Valid API key (api-key header)\n",
      "⏳ Waiting 60 seconds for policy to propagate...\n",
      "   60 seconds remaining...   59 seconds remaining...   58 seconds remaining...   57 seconds remaining...   56 seconds remaining...   55 seconds remaining...   54 seconds remaining...   53 seconds remaining...   52 seconds remaining...   51 seconds remaining...   50 seconds remaining...   49 seconds remaining...   48 seconds remaining...   47 seconds remaining...   46 seconds remaining...   45 seconds remaining...   44 seconds remaining...   43 seconds remaining...   42 seconds remaining...   41 seconds remaining...   40 seconds remaining...   39 seconds remaining...   38 seconds remaining...   37 seconds remaining...   36 seconds remaining...   35 seconds remaining...   34 seconds remaining...   33 seconds remaining...   32 seconds remaining...   31 seconds remaining...   30 seconds remaining...   29 seconds remaining...   28 seconds remaining...   27 seconds remaining...   26 seconds remaining...   25 seconds remaining...   24 seconds remaining...   23 seconds remaining...   22 seconds remaining...   21 seconds remaining...   20 seconds remaining...   19 seconds remaining...   18 seconds remaining...   17 seconds remaining...   16 seconds remaining...   15 seconds remaining...   14 seconds remaining...   13 seconds remaining...   12 seconds remaining...   11 seconds remaining...   10 seconds remaining...   9 seconds remaining...   8 seconds remaining...   7 seconds remaining...   6 seconds remaining...   5 seconds remaining...   4 seconds remaining...   3 seconds remaining...   2 seconds remaining...   1 seconds remaining...✓ Policy propagation complete!\n",
      "💡 TIP: Run Cell 65 to test Dual Auth\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"📝 APPLY: Dual Auth (JWT + API Key)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID. Ensure az login completed.\")\n",
    "else:\n",
    "    print(f\"[auth] Resolved tenant_id: {tenant_id}\")\n",
    "\n",
    "    # Dual Auth policy - BOTH JWT validation AND API key check\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing API key\" />\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "\n",
    "    # Apply policy\n",
    "    try:\n",
    "        url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "        print(f\"📝 Policy Applied: Dual Auth (JWT + API Key)\")\n",
    "        print(f\"Status: {response.status_code} - {'✓ SUCCESS' if response.status_code in [200, 201] else '✗ FAILED'}\")\n",
    "\n",
    "        if response.status_code not in [200, 201]:\n",
    "            print(f\"Error: {response.text[:500]}\")\n",
    "        else:\n",
    "            print(\"Policy requires BOTH:\")\n",
    "            print(\"  • Valid JWT token (Authorization header)\")\n",
    "            print(\"  • Valid API key (api-key header)\")\n",
    "\n",
    "            print(\"⏳ Waiting 60 seconds for policy to propagate...\")\n",
    "            for i in range(60, 0, -1):\n",
    "                print(f\"   {i} seconds remaining...\", end='')\n",
    "                time.sleep(1)\n",
    "            print(\"✓ Policy propagation complete!\")\n",
    "            print(\"💡 TIP: Run Cell 65 to test Dual Auth\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Policy application failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 4: Dual Authentication (JWT + API Key)\n",
      "================================================================================\n",
      "\n",
      "[1] Acquiring JWT token...\n",
      "✅ JWT Token: eyJ0eXAiOiJKV1QiLCJh...eVKCTkOzUw\n",
      "\n",
      "[2] Calling API with BOTH JWT and API Key...\n",
      "Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "API Key: b64e6a3117...2cb0\n",
      "\n",
      "✅ SUCCESS: Dual Authentication Working!\n",
      "Response: Dual auth successful!\n",
      "Tokens: 18\n",
      "\n",
      "🎉 Both JWT and API Key validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# TEST 4: Dual Authentication (JWT + API Key)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🧪 TEST 4: Dual Authentication (JWT + API Key)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Get JWT token\n",
    "print(\"\\n[1] Acquiring JWT token...\")\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(f\"✅ JWT Token: {jwt_token[:20]}...{jwt_token[-10:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to get JWT: {e}\")\n",
    "    raise\n",
    "\n",
    "if not apim_api_key:\n",
    "    print(\"❌ APIM_API_KEY not set\")\n",
    "    raise ValueError(\"APIM_API_KEY required\")\n",
    "\n",
    "print(f\"\\n[2] Calling API with BOTH JWT and API Key...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"API Key: {apim_api_key[:10]}...{apim_api_key[-4:]}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy\",\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Dual auth successful!'\"}],\n",
    "        max_tokens=50,\n",
    "        extra_headers={\n",
    "            \"Authorization\": f\"Bearer {jwt_token}\",  # JWT token\n",
    "            \"api-key\": apim_api_key  # API Key\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ SUCCESS: Dual Authentication Working!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    print(\"\\n🎉 Both JWT and API Key validated successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ FAILED: {str(e)[:300]}\")\n",
    "    print(\"\\nMake sure Cell 030 (Dual Auth policy) was applied\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "🔄 RESET: API-KEY Authentication (for remaining labs)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ✓ Re-enabled subscriptionRequired for API-KEY authentication\n",
      "[2] Policy Reset: API-KEY Only\n",
      "    Status: 200 - ✓ SUCCESS\n",
      "⏳ Waiting 30 seconds for policy to propagate...\n",
      "   30 seconds remaining...   29 seconds remaining...   28 seconds remaining...   27 seconds remaining...   26 seconds remaining...   25 seconds remaining...   24 seconds remaining...   23 seconds remaining...   22 seconds remaining...   21 seconds remaining...   20 seconds remaining...   19 seconds remaining...   18 seconds remaining...   17 seconds remaining...   16 seconds remaining...   15 seconds remaining...   14 seconds remaining...   13 seconds remaining...   12 seconds remaining...   11 seconds remaining...   10 seconds remaining...   9 seconds remaining...   8 seconds remaining...   7 seconds remaining...   6 seconds remaining...   5 seconds remaining...   4 seconds remaining...   3 seconds remaining...   2 seconds remaining...   1 seconds remaining...✓ Policy reset complete!\n",
      "💡 All remaining labs will use API-KEY authentication\n"
     ]
    }
   ],
   "source": [
    "import requests, os, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"🔄 RESET: API-KEY Authentication (for remaining labs)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Re-enable subscription requirement (for API-KEY authentication)\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "response = requests.get(api_url, headers=headers, timeout=30)\n",
    "if response.status_code == 200:\n",
    "    api_config = response.json()\n",
    "    api_config['properties']['subscriptionRequired'] = True\n",
    "    requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "    print(\"[1] ✓ Re-enabled subscriptionRequired for API-KEY authentication\")\n",
    "\n",
    "# Apply simple API-KEY only policy\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend><base /></backend>\n",
    "    <outbound><base /></outbound>\n",
    "    <on-error><base /></on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "body = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "print(f\"[2] Policy Reset: API-KEY Only\")\n",
    "print(f\"    Status: {response.status_code} - {'✓ SUCCESS' if response.status_code in [200, 201] else '✗ FAILED'}\")\n",
    "\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"⏳ Waiting 30 seconds for policy to propagate...\")\n",
    "    for i in range(30, 0, -1):\n",
    "        print(f\"   {i} seconds remaining...\", end='')\n",
    "        time.sleep(1)\n",
    "    print(\"✓ Policy reset complete!\")\n",
    "    print(\"💡 All remaining labs will use API-KEY authentication\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 2: API Key Authentication\n",
      "================================================================================\n",
      "\n",
      "Calling API with API Key only...\n",
      "Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "API Key: b64e6a3117...2cb0\n",
      "\n",
      "✅ SUCCESS: API Key Authentication Working!\n",
      "Response: API Key auth successful!\n",
      "Tokens: 20\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: API Key Only (works when API Key policy active)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🧪 TEST 2: API Key Authentication\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "if not apim_api_key:\n",
    "    print(\"❌ APIM_API_KEY not set. Run Cell 022 to load environment.\")\n",
    "    raise ValueError(\"APIM_API_KEY required\")\n",
    "\n",
    "print(f\"\\nCalling API with API Key only...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"API Key: {apim_api_key[:10]}...{apim_api_key[-4:]}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy\",  # The actual key goes in extra_headers\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'API Key auth successful!'\"}],\n",
    "        max_tokens=50,\n",
    "        extra_headers={\n",
    "            \"api-key\": apim_api_key  # API Key in header\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ SUCCESS: API Key Authentication Working!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if 'JWT' in error_msg or '401' in error_msg:\n",
    "        print(\"\\n❌ FAILED: API requires JWT token\")\n",
    "        print(\"   Run Cell 041 to reset APIM to API Key mode\")\n",
    "    else:\n",
    "        print(f\"\\n❌ ERROR: {error_msg[:300]}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| 401 Unauthorized | Wait 30-60 seconds for policy to propagate |\n",
    "| 500 Internal Server Error | Check backend health with Azure CLI |\n",
    "| Token not found | Run `az login` to authenticate |\n",
    "| Missing API Key | Verify `APIM_API_KEY` in environment variables |\n",
    "\n",
    "**Verify Resources:**\n",
    "\n",
    "```bash\n",
    "az apim api list --service-name $APIM_SERVICE_NAME --resource-group $RESOURCE_GROUP --output table\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e16003f0-a99c-4c9b-9294-b321ba877db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Sales Analysis via MCP Excel Server\n",
      "================================================================================\n",
      "📤 Uploading Excel file via MCP: sales.xlsx\n",
      "✅ In-memory cache key: sales.xlsx\n",
      "\n",
      "📋 Columns:\n",
      "['Region', 'Product', 'Date', 'TotalSales', 'Quantity', 'CustomerID']\n",
      "\n",
      "📄 Preview (first rows):\n",
      "  {'Region': 'Asia Pacific', 'Product': 'Professional Services', 'Date': '2024-01-01T00:00:00', 'TotalSales': 673076.1796812697, 'Quantity': 7973, 'CustomerID': 'CUST-16610'}\n",
      "  {'Region': 'North America', 'Product': 'Software Licenses', 'Date': '2024-01-01T00:00:00', 'TotalSales': 56427.00796144797, 'Quantity': 4237, 'CustomerID': 'CUST-52727'}\n",
      "  {'Region': 'North America', 'Product': 'Cloud Services', 'Date': '2024-01-01T00:00:00', 'TotalSales': 598025.514808326, 'Quantity': 3792, 'CustomerID': 'CUST-46639'}\n",
      "  {'Region': 'Latin America', 'Product': 'Software Licenses', 'Date': '2024-01-01T00:00:00', 'TotalSales': 354449.5095706386, 'Quantity': 547, 'CustomerID': 'CUST-50733'}\n",
      "  {'Region': 'North America', 'Product': 'Software Licenses', 'Date': '2024-01-01T00:00:00', 'TotalSales': 251141.6478808843, 'Quantity': 1232, 'CustomerID': 'CUST-19837'}\n",
      "\n",
      "📊 Running sales analysis (group_by='Region', metric='TotalSales') via MCP...\n",
      "✅ analyze_sales succeeded using identifier: sales.xlsx\n",
      "\n",
      "📈 MCP Sales Analysis Summary:\n",
      "================================================================================\n",
      "{'total': 936730612.4413884, 'average': 374832.5226832066, 'count': 2500}\n",
      "\n",
      "📊 Sales by Region (Top 10):\n",
      "  01. Asia Pacific: $212,162,358.17\n",
      "  02. Europe: $237,020,292.26\n",
      "  03. Latin America: $232,880,138.13\n",
      "  04. North America: $254,667,823.88\n",
      "\n",
      "💡 Compact sales_data_info for AI prompts:\n",
      "Columns: ['Region', 'Product', 'Date', 'TotalSales', 'Quantity', 'CustomerID']\n",
      "Total Sales: 936730612.4413884 | Avg Sale: 374832.5226832066 | Rows: 2500\n",
      "Regional breakdown available\n",
      "\n",
      "✅ Cell 79 complete. Variable 'excel_cache_key' = 'sales.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1: Sales Analysis via MCP Excel Server\n",
    "print(\"📊 Sales Analysis via MCP Excel Server\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    if not mcp or not mcp.excel.server_url:\n",
    "        raise RuntimeError(\"MCP Excel server not configured – check .mcp-servers-config\")\n",
    "    \n",
    "    # Find Excel file - Use .xlsx files (workshop pattern)\n",
    "    search_path = Path(\"./sample-data/excel/\")\n",
    "    excel_candidates = list(search_path.glob(\"*sales*.xlsx\"))\n",
    "    \n",
    "    if not excel_candidates:\n",
    "        raise FileNotFoundError(f\"Could not locate sales Excel file in '{search_path.resolve()}'\")\n",
    "    \n",
    "    local_excel_path = Path(excel_candidates[0])\n",
    "    excel_file_name = local_excel_path.name\n",
    "    \n",
    "    print(f\"📤 Uploading Excel file via MCP: {excel_file_name}\")\n",
    "    upload_result = mcp.excel.upload_excel(str(local_excel_path))\n",
    "    \n",
    "    # upload_excel loads into in-memory cache keyed ONLY by file_name (no /app/data prefix)\n",
    "    file_cache_key = upload_result.get('file_name', excel_file_name)\n",
    "    print(f\"✅ In-memory cache key: {file_cache_key}\")\n",
    "    \n",
    "    # Prefer metadata from upload_result; fall back to load_excel if needed\n",
    "    load_info = upload_result\n",
    "    if 'columns' not in load_info or 'preview' not in load_info:\n",
    "        # Some server variants might require explicit load; try both raw name and /app/data path\n",
    "        possible_paths = [file_cache_key]\n",
    "        if not file_cache_key.startswith('/app/'):\n",
    "            possible_paths.append(f\"/app/data/{file_cache_key}\")\n",
    "        \n",
    "        for pth in possible_paths:\n",
    "            try:\n",
    "                tmp = mcp.excel.load_excel(pth)\n",
    "                if isinstance(tmp, dict) and tmp.get('success'):\n",
    "                    load_info = tmp\n",
    "                    file_cache_key = pth\n",
    "                    print(f\"   Loaded Excel from path: {pth}\")\n",
    "                    break\n",
    "            except Exception as le:\n",
    "                print(f\"   load_excel attempt failed for {pth}: {le}\")\n",
    "    \n",
    "    # Normalize response (handle string responses)\n",
    "    if isinstance(load_info, str):\n",
    "        print(\"⚠️ load_info is text; attempting JSON parse\")\n",
    "        import json as _json\n",
    "        try:\n",
    "            load_info = _json.loads(load_info)\n",
    "        except Exception:\n",
    "            load_info = {\"raw\": load_info}\n",
    "    \n",
    "    # Get columns and preview\n",
    "    columns = load_info.get('columns') or load_info.get('schema') or []\n",
    "    preview = load_info.get('preview') or load_info.get('head') or []\n",
    "    \n",
    "    print(f\"\\n📋 Columns:\")\n",
    "    print(columns if columns else \"  (No column list returned)\")\n",
    "    \n",
    "    if preview:\n",
    "        print(f\"\\n📄 Preview (first rows):\")\n",
    "        for row in (preview[:5] if isinstance(preview, list) else []):\n",
    "            print(f\"  {row}\")\n",
    "    \n",
    "    # Analyze sales data - Use TotalSales column with robust fallback\n",
    "    print(f\"\\n📊 Running sales analysis (group_by='Region', metric='TotalSales') via MCP...\")\n",
    "    analysis_result = None\n",
    "    analyze_attempts = [file_cache_key]\n",
    "    if not file_cache_key.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{file_cache_key}\")  # fallback if server persisted file\n",
    "    \n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            analysis_result = mcp.excel.analyze_sales(target, group_by=\"Region\", metric=\"TotalSales\")\n",
    "            print(f\"✅ analyze_sales succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   analyze_sales failed for {target}: {ae}\")\n",
    "    \n",
    "    if analysis_result is None:\n",
    "        raise RuntimeError(f\"Failed to analyze sales using any identifier. Last error: {last_error}\")\n",
    "    \n",
    "    # Normalize JSON response\n",
    "    if isinstance(analysis_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            analysis_result = _json.loads(analysis_result)\n",
    "        except Exception:\n",
    "            analysis_result = {\"raw\": analysis_result}\n",
    "    \n",
    "    # Extract summary and grouped data (handle different response formats)\n",
    "    summary = analysis_result.get(\"summary\") or analysis_result.get(\"result\") or analysis_result.get(\"raw\")\n",
    "    grouped = analysis_result.get(\"grouped_data\") or analysis_result.get(\"groups\") or analysis_result.get(\"analysis\")\n",
    "    \n",
    "    print(f\"\\n📈 MCP Sales Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary if summary else analysis_result)\n",
    "    \n",
    "    # Display grouped results with dynamic key detection\n",
    "    if grouped and isinstance(grouped, list) and len(grouped) > 0:\n",
    "        first_item = grouped[0]\n",
    "        region_key = 'Region' if 'Region' in first_item else list(first_item.keys())[0]\n",
    "        total_key = 'Total' if 'Total' in first_item else 'TotalSales' if 'TotalSales' in first_item else None\n",
    "        \n",
    "        print(f\"\\n📊 Sales by Region (Top 10):\")\n",
    "        for i, row in enumerate(grouped[:10], 1):\n",
    "            region_val = row.get(region_key, 'Unknown')\n",
    "            total_val = row.get(total_key) if total_key else row\n",
    "            print(f\"  {i:02d}. {region_val}: ${total_val:,.2f}\" if isinstance(total_val, (int, float)) else f\"  {i:02d}. {region_val}: {total_val}\")\n",
    "    \n",
    "    # Extract metrics for AI prompts\n",
    "    total_sales = None\n",
    "    avg_sales = None\n",
    "    num_transactions = None\n",
    "    if isinstance(summary, dict):\n",
    "        total_sales = summary.get(\"total\") or summary.get(\"total_sales\")\n",
    "        avg_sales = summary.get(\"average\") or summary.get(\"avg\") or summary.get(\"average_sale\")\n",
    "        num_transactions = summary.get(\"count\") or summary.get(\"num_rows\")\n",
    "    \n",
    "    # Create compact summary for AI prompts\n",
    "    sales_data_info = (f\"Columns: {columns}\\n\" if columns else \"\") + \\\n",
    "        (f\"Total Sales: {total_sales} | Avg Sale: {avg_sales} | Rows: {num_transactions}\\n\" if total_sales else \"\") + \\\n",
    "        (\"Regional breakdown available\" if grouped else \"\")\n",
    "    \n",
    "    print(f\"\\n💡 Compact sales_data_info for AI prompts:\")\n",
    "    print(sales_data_info)\n",
    "    \n",
    "    # Export useful identifiers for later cells\n",
    "    excel_cache_key = file_cache_key\n",
    "    \n",
    "    print(f\"\\n✅ Cell 79 complete. Variable 'excel_cache_key' = '{excel_cache_key}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ File error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Verify Excel file exists in ./sample-data/excel/\")\n",
    "    print(f\"   • Check file permissions\")\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None\n",
    "except MCPError as e:\n",
    "    print(f\"❌ MCP error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Ensure EXCEL_MCP_URL points to a running Excel MCP server\")\n",
    "    print(f\"   • Verify the file is a valid .xlsx (modern ZIP-based format)\")\n",
    "    print(f\"   • Check .mcp-servers-config file exists\")\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None\n",
    "except RuntimeError as e:\n",
    "    print(f\"❌ Runtime error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Do NOT prepend /app/data unless server persists uploads to disk\")\n",
    "    print(f\"   • If persistence needed, modify server to write file bytes to disk before load_excel\")\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4a85e-ddfa-4741-a290-1ca16b38d29c",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Sales Analysis via MCP + AI ONLY\n",
    "Use MCP for data access and Azure OpenAI for ALL analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "693b8b5b-bfec-4266-a7cd-b439b1f08243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verifying MCP Sales Analysis Results\n",
      "================================================================================\n",
      "✅ MCP analysis successful!\n",
      "   File key: sales.xlsx\n",
      "   This key can be used for further analysis in subsequent cells.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1 (Fallback): Verify MCP Results\n",
    "print(\"🔍 Verifying MCP Sales Analysis Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "    print(\"⚠️ MCP analysis did not complete successfully in Cell 81.\")\n",
    "    print(\"   Please check:\")\n",
    "    print(\"   1. MCP Excel server is running\")\n",
    "    print(\"   2. .mcp-servers-config file exists with EXCEL_MCP_URL\")\n",
    "    print(\"   3. Excel file exists at ./sample-data/excel/sales_performance.xlsx\")\n",
    "else:\n",
    "    print(f\"✅ MCP analysis successful!\")\n",
    "    print(f\"   File key: {excel_cache_key}\")\n",
    "    print(f\"   This key can be used for further analysis in subsequent cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0577b2-d307-4bc6-9696-2a44d154b5b3",
   "metadata": {},
   "source": [
    "\n",
    "If the MCP-based analysis above fails (e.g., due to server issues or file compatibility problems), the cell below provides a local fallback using the `pandas` library. It reads the `sales_performance.xlsx` file directly from the local `sample-data` directory and generates a similar structural summary.\n",
    "\n",
    "This ensures that you can proceed with the subsequent AI analysis exercises even if the primary MCP tool encounters an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d6f98-ba15-44df-8a97-9632e862b01e",
   "metadata": {},
   "source": [
    "### Excersice 2.3 Azure Cost Analysis via MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffba938c-1a95-4d4b-b29a-e7763b00a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 Azure Cost Analysis via MCP Excel Server\n",
      "================================================================================\n",
      "✅ Found cost file: azure_resource_costs.xlsx\n",
      "📤 Uploading to MCP Excel server...\n",
      "✅ Upload successful. File key: azure_resource_costs.xlsx\n",
      "\n",
      "📋 Columns:\n",
      "['ServiceName', 'ResourceGroup', 'Region', 'Cost', 'Date', 'SubscriptionID']\n",
      "\n",
      "📄 Preview (first rows):\n",
      "  {'ServiceName': 'Virtual Machines', 'ResourceGroup': 'rg-virtual-machines', 'Region': 'East US', 'Cost': 17738.9322903674, 'Date': '2024-01', 'SubscriptionID': 'sub-5906'}\n",
      "  {'ServiceName': 'Virtual Machines', 'ResourceGroup': 'rg-virtual-machines', 'Region': 'West Europe', 'Cost': 1832.837000168093, 'Date': '2024-01', 'SubscriptionID': 'sub-1749'}\n",
      "  {'ServiceName': 'Virtual Machines', 'ResourceGroup': 'rg-virtual-machines', 'Region': 'Southeast Asia', 'Cost': 13605.60971028315, 'Date': '2024-01', 'SubscriptionID': 'sub-5695'}\n",
      "\n",
      "📊 Calculating Azure resource costs...\n",
      "✅ calculate_costs succeeded using identifier: azure_resource_costs.xlsx\n",
      "\n",
      "💰 Cost Calculation Complete!\n",
      "\n",
      "✅ Cell 85 complete. Variable 'cost_cache_key' = 'azure_resource_costs.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.3: Azure Cost Analysis via MCP Excel Server\n",
    "print(\"💰 Azure Cost Analysis via MCP Excel Server\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    # Path to cost Excel file - Use .xlsx directly (extracted from .zip)\n",
    "    cost_file_path = Path(\"./sample-data/excel/azure_resource_costs.xlsx\")\n",
    "    \n",
    "    if not cost_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Cost file not found: {cost_file_path.resolve()}\")\n",
    "    \n",
    "    print(f\"✅ Found cost file: {cost_file_path.name}\")\n",
    "    \n",
    "    # Upload cost file to MCP server\n",
    "    print(f\"📤 Uploading to MCP Excel server...\")\n",
    "    upload_result = mcp.excel.upload_excel(str(cost_file_path))\n",
    "    \n",
    "    # Extract file cache key\n",
    "    cost_cache_key = upload_result.get('file_name', cost_file_path.name)\n",
    "    print(f\"✅ Upload successful. File key: {cost_cache_key}\")\n",
    "    \n",
    "    # Prefer metadata from upload_result; fall back to load_excel if needed\n",
    "    load_info = upload_result\n",
    "    if 'columns' not in load_info or 'preview' not in load_info:\n",
    "        # Some server variants might require explicit load; try both raw name and /app/data path\n",
    "        possible_paths = [cost_cache_key]\n",
    "        if not cost_cache_key.startswith('/app/'):\n",
    "            possible_paths.append(f\"/app/data/{cost_cache_key}\")\n",
    "        \n",
    "        for pth in possible_paths:\n",
    "            try:\n",
    "                tmp = mcp.excel.load_excel(pth)\n",
    "                if isinstance(tmp, dict) and tmp.get('success'):\n",
    "                    load_info = tmp\n",
    "                    cost_cache_key = pth\n",
    "                    print(f\"   Loaded Excel from path: {pth}\")\n",
    "                    break\n",
    "            except Exception as le:\n",
    "                print(f\"   load_excel attempt failed for {pth}: {le}\")\n",
    "    \n",
    "    # Normalize response (handle string responses)\n",
    "    if isinstance(load_info, str):\n",
    "        print(\"⚠️ load_info is text; attempting JSON parse\")\n",
    "        import json as _json\n",
    "        try:\n",
    "            load_info = _json.loads(load_info)\n",
    "        except Exception:\n",
    "            load_info = {\"raw\": load_info}\n",
    "    \n",
    "    # Get columns and preview\n",
    "    columns = load_info.get('columns') or load_info.get('schema') or []\n",
    "    preview = load_info.get('preview') or load_info.get('head') or []\n",
    "    \n",
    "    print(f\"\\n📋 Columns:\")\n",
    "    print(columns if columns else \"  (No column list returned)\")\n",
    "    \n",
    "    if preview:\n",
    "        print(f\"\\n📄 Preview (first rows):\")\n",
    "        for row in (preview[:3] if isinstance(preview, list) else []):\n",
    "            print(f\"  {row}\")\n",
    "    \n",
    "    # Calculate costs using MCP with robust fallback\n",
    "    # FIXED: Updated column names to match actual Excel file structure\n",
    "    # File has: ServiceName, ResourceGroup, Region, Cost, Date, SubscriptionID\n",
    "    print(f\"\\n📊 Calculating Azure resource costs...\")\n",
    "    cost_analysis = None\n",
    "    analyze_attempts = [cost_cache_key]\n",
    "    if not cost_cache_key.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{cost_cache_key}\")  # fallback if server persisted file\n",
    "    \n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            cost_analysis = mcp.excel.calculate_costs(\n",
    "                target,\n",
    "                resource_type_col='ServiceName',  # FIXED: was 'Resource_Type'\n",
    "                cost_col='Cost'  # FIXED: was 'Daily_Cost'\n",
    "            )\n",
    "            print(f\"✅ calculate_costs succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   calculate_costs failed for {target}: {ae}\")\n",
    "    \n",
    "    if cost_analysis is None:\n",
    "        raise RuntimeError(f\"Failed to calculate costs using any identifier. Last error: {last_error}\")\n",
    "    \n",
    "    # Normalize JSON response\n",
    "    if isinstance(cost_analysis, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            cost_analysis = _json.loads(cost_analysis)\n",
    "        except Exception:\n",
    "            cost_analysis = {\"raw\": cost_analysis}\n",
    "    \n",
    "    print(f\"\\n💰 Cost Calculation Complete!\")\n",
    "    \n",
    "    # Display results (handle different response formats)\n",
    "    if isinstance(cost_analysis, dict):\n",
    "        if 'summary' in cost_analysis:\n",
    "            print(f\"\\n💰 Cost Summary:\")\n",
    "            daily_total = cost_analysis['summary'].get('daily_total', 0)\n",
    "            monthly_projection = cost_analysis['summary'].get('monthly_projection', 0)\n",
    "            print(f\"   Daily Total: ${daily_total:,.2f}\")\n",
    "            print(f\"   Monthly Projection: ${monthly_projection:,.2f}\")\n",
    "        \n",
    "        resource_breakdown = cost_analysis.get('by_resource_type') or cost_analysis.get('by_resource') or cost_analysis.get('analysis')\n",
    "        if resource_breakdown and isinstance(resource_breakdown, list):\n",
    "            print(f\"\\n📊 Costs by Resource Type:\")\n",
    "            for item in resource_breakdown:\n",
    "                # FIXED: Updated to match ServiceName and Cost columns\n",
    "                resource = item.get('ServiceName') or item.get('Resource_Type') or item.get('resource_type') or item.get('resource', 'Unknown')\n",
    "                cost_val = item.get('Cost') or item.get('Daily_Cost') or item.get('daily_cost') or item.get('cost', 0)\n",
    "                monthly = cost_val * 30\n",
    "                print(f\"   {resource}: ${cost_val:,.2f}/day (${monthly:,.2f}/month)\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{cost_analysis}\")\n",
    "    \n",
    "    print(f\"\\n✅ Cell 85 complete. Variable 'cost_cache_key' = '{cost_cache_key}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ File error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Verify file exists at ./sample-data/excel/azure_resource_costs.xlsx\")\n",
    "    print(f\"   • Check file permissions\")\n",
    "    cost_cache_key = None\n",
    "except MCPError as e:\n",
    "    print(f\"❌ MCP error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Ensure EXCEL_MCP_URL points to a running Excel MCP server\")\n",
    "    print(f\"   • Verify the file is a valid .xlsx (modern ZIP-based format)\")\n",
    "    print(f\"   • Check .mcp-servers-config file exists\")\n",
    "    cost_cache_key = None\n",
    "except RuntimeError as e:\n",
    "    print(f\"❌ Runtime error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Do NOT prepend /app/data unless server persists uploads to disk\")\n",
    "    print(f\"   • Verify calculate_costs function is available on MCP server\")\n",
    "    cost_cache_key = None\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    cost_cache_key = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b2098-04eb-4894-af3b-4310f19f994b",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Dynamic Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f096551e-90fa-4b6b-b5ab-e23141bd1d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Dynamic MCP Analysis with User-Defined Columns\n",
      "================================================================================\n",
      "📊 Performing dynamic analysis on 'sales.xlsx'\n",
      "   Grouping by: 'Product'\n",
      "   Aggregating metric: 'Quantity'\n",
      "\n",
      "📊 Running analysis via MCP...\n",
      "✅ analyze_sales succeeded using identifier: sales.xlsx\n",
      "\n",
      "✅ Dynamic analysis complete!\n",
      "\n",
      "💰 Summary:\n",
      "   Total: 12,338,190.00\n",
      "   Average: 4,937.50\n",
      "   Count: 2500\n",
      "\n",
      "📊 By Product (Top 10):\n",
      "   01. Cloud Services: 0.00\n",
      "   02. Hardware: 0.00\n",
      "   03. Professional Services: 0.00\n",
      "   04. Software Licenses: 0.00\n",
      "\n",
      "✅ Exercise 2.5 complete!\n",
      "\n",
      "💡 Try changing 'group_by_column' and 'metric_column' to explore different insights:\n",
      "   Available columns: Region, Product, Date, TotalSales, Quantity, CustomerID\n",
      "   - group_by_column: 'Region', 'Product', 'CustomerID'\n",
      "   - metric_column: 'TotalSales', 'Quantity'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.5: Dynamic Column Analysis\n",
    "print(\"🔄 Dynamic MCP Analysis with User-Defined Columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # --- Define columns for analysis ---\n",
    "    # These variables can be changed to analyze different aspects of the data\n",
    "    group_by_column = 'Product'  # Change to 'Region', 'Product', 'CustomerID', etc.\n",
    "    metric_column = 'Quantity'   # Change to 'Quantity', 'TotalSales', etc.\n",
    "\n",
    "    # Use the file key from the successful sales analysis in Exercise 2.1 (Cell 79)\n",
    "    if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "        raise RuntimeError(\"Sales data not loaded. Please run Cell 79 successfully first.\")\n",
    "\n",
    "    file_to_analyze = excel_cache_key\n",
    "\n",
    "    print(f\"📊 Performing dynamic analysis on '{file_to_analyze}'\")\n",
    "    print(f\"   Grouping by: '{group_by_column}'\")\n",
    "    print(f\"   Aggregating metric: '{metric_column}'\")\n",
    "\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    # Call the MCP tool with the dynamic column names - robust fallback\n",
    "    print(f\"\\n📊 Running analysis via MCP...\")\n",
    "    dynamic_analysis_result = None\n",
    "    analyze_attempts = [file_to_analyze]\n",
    "    if not file_to_analyze.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{file_to_analyze}\")  # fallback if server persisted file\n",
    "    \n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            dynamic_analysis_result = mcp.excel.analyze_sales(\n",
    "                target,\n",
    "                group_by=group_by_column,\n",
    "                metric=metric_column\n",
    "            )\n",
    "            print(f\"✅ analyze_sales succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   analyze_sales failed for {target}: {ae}\")\n",
    "    \n",
    "    if dynamic_analysis_result is None:\n",
    "        raise RuntimeError(f\"Failed to analyze using any identifier. Last error: {last_error}\")\n",
    "\n",
    "    # Normalize JSON response\n",
    "    if isinstance(dynamic_analysis_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            dynamic_analysis_result = _json.loads(dynamic_analysis_result)\n",
    "        except Exception:\n",
    "            dynamic_analysis_result = {\"raw\": dynamic_analysis_result}\n",
    "\n",
    "    print(f\"\\n✅ Dynamic analysis complete!\")\n",
    "\n",
    "    # Display results (handle different response formats)\n",
    "    if isinstance(dynamic_analysis_result, dict):\n",
    "        if 'summary' in dynamic_analysis_result:\n",
    "            print(f\"\\n💰 Summary:\")\n",
    "            total = dynamic_analysis_result['summary'].get('total', 0)\n",
    "            average = dynamic_analysis_result['summary'].get('average', 0)\n",
    "            count = dynamic_analysis_result['summary'].get('count', 0)\n",
    "            print(f\"   Total: {total:,.2f}\")\n",
    "            print(f\"   Average: {average:,.2f}\")\n",
    "            print(f\"   Count: {count}\")\n",
    "        \n",
    "        # Extract grouped data with dynamic key detection\n",
    "        grouped = dynamic_analysis_result.get('analysis') or dynamic_analysis_result.get('grouped_data') or dynamic_analysis_result.get('groups')\n",
    "        if grouped and isinstance(grouped, list) and len(grouped) > 0:\n",
    "            print(f\"\\n📊 By {group_by_column} (Top 10):\")\n",
    "            for i, item in enumerate(grouped[:10], 1):\n",
    "                group = item.get(group_by_column, 'Unknown')\n",
    "                value = item.get(metric_column, 0)\n",
    "                print(f\"   {i:02d}. {group}: {value:,.2f}\" if isinstance(value, (int, float)) else f\"   {i:02d}. {group}: {value}\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{dynamic_analysis_result}\")\n",
    "\n",
    "    print(f\"\\n✅ Exercise 2.5 complete!\")\n",
    "    print(f\"\\n💡 Try changing 'group_by_column' and 'metric_column' to explore different insights:\")\n",
    "    print(f\"   Available columns: Region, Product, Date, TotalSales, Quantity, CustomerID\")\n",
    "    print(f\"   - group_by_column: 'Region', 'Product', 'CustomerID'\")\n",
    "    print(f\"   - metric_column: 'TotalSales', 'Quantity'\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"❌ Runtime error: {e}\")\n",
    "    print(f\"   Make sure Cell 79 (Sales Analysis) ran successfully first\")\n",
    "except MCPError as e:\n",
    "    print(f\"❌ MCP error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   • Ensure MCP Excel server is running\")\n",
    "    print(f\"   • Verify file cache key is valid\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during dynamic analysis: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🔧 APPLYING SEMANTIC CACHING POLICY (from notebook)\n",
      "================================================================================\n",
      "\n",
      "[*] Applying semantic caching policy to APIM...\n",
      "\n",
      "✅ Semantic caching policy applied successfully!\n",
      "\n",
      "📋 Policy Configuration:\n",
      "   - Similarity Threshold: 0.8 (80% match)\n",
      "   - Cache Duration: 1200s (20 minutes)\n",
      "   - Embeddings Backend: embeddings-backend\n",
      "   - Auth: API Key (from backend credentials)\n",
      "   - Backend Pool: inference-backend-pool\n",
      "\n",
      "⏳ Waiting 10 seconds for propagation...\n",
      "✅ Ready to test!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL TO ADD: Apply Semantic Caching Policy\n",
    "# Insert this cell BEFORE cell 53 (semantic caching test)\n",
    "# This applies the semantic caching policy directly in the notebook\n",
    "\n",
    "import os, subprocess, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "apim_service_id = os.environ.get('APIM_SERVICE_ID')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔧 APPLYING SEMANTIC CACHING POLICY (from notebook)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Policy WITHOUT embeddings-backend-auth (uses API key from backend config)\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <azure-openai-semantic-cache-lookup\n",
    "            score-threshold=\"0.8\"\n",
    "            embeddings-backend-id=\"embeddings-backend\" />\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\"\n",
    "               condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n",
    "            <forward-request buffer-request-body=\"true\" />\n",
    "        </retry>\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "        <azure-openai-semantic-cache-store duration=\"1200\" />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "uri = f\"https://management.azure.com{apim_service_id}/apis/inference-api/policies/policy?api-version=2023-09-01-preview\"\n",
    "\n",
    "body = {\n",
    "    \"properties\": {\n",
    "        \"value\": policy_xml,\n",
    "        \"format\": \"xml\"\n",
    "    }\n",
    "}\n",
    "\n",
    "body_file = '/tmp/semantic-cache-from-notebook.json'\n",
    "with open(body_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(body, f, indent=2)\n",
    "\n",
    "print(\"\\n[*] Applying semantic caching policy to APIM...\")\n",
    "\n",
    "cmd = ['az', 'rest', '--method', 'put', '--uri', uri, '--body', f'@{body_file}']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✅ Semantic caching policy applied successfully!\\n\")\n",
    "    print(\"📋 Policy Configuration:\")\n",
    "    print(\"   - Similarity Threshold: 0.8 (80% match)\")\n",
    "    print(\"   - Cache Duration: 1200s (20 minutes)\")\n",
    "    print(\"   - Embeddings Backend: embeddings-backend\")\n",
    "    print(\"   - Auth: API Key (from backend credentials)\")\n",
    "    print(\"   - Backend Pool: inference-backend-pool\\n\")\n",
    "    print(\"⏳ Waiting 10 seconds for propagation...\")\n",
    "    import time\n",
    "    time.sleep(10)\n",
    "    print(\"✅ Ready to test!\\n\")\n",
    "else:\n",
    "    print(f\"\\n❌ Error applying policy:\")\n",
    "    print(result.stderr)\n",
    "    raise Exception(\"Failed to apply policy\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd354c0-094f-4246-a4ac-ef163280d645",
   "metadata": {},
   "source": [
    "### Exercise 2.4 : Function Calling with MCP Tools\n",
    "\n",
    "Demonstrates calling MCP server tools from Azure OpenAI function calls, with both OpenAI and MCP managed through APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6054af6-d50e-4ca9-b810-de42bd9746f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[WARN] pywintypes still not found after installation: No module named 'pywintypes'\n",
      "[CONFIG] Using MCP URL: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n",
      "================================================================================\n",
      "Connecting to MCP server: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n",
      "[OK] Handshake succeeded. 4 tools available.\n",
      "[DEBUG] Variable values:\n",
      "  apim_gateway_url: 'https://apim-pavavy6pu5hpa.azure-api.net'\n",
      "  apim_resource_gateway_url: 'https://apim-pavavy6pu5hpa.azure-api.net'\n",
      "  api_key: b64e6a3117...2cb0\n",
      "  inference_api_path: 'inference'\n",
      "  Full endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "\n",
      "Query: List available document-related tools and summarize their purpose.\n",
      "[INFO] No tool calls needed. Response: Here are the available document-related tools and their purposes:\n",
      "\n",
      "1. **list_documents**: This tool lists all available markdown documents. It can filter files using an optional glob pattern (e.g., '*.md' or 'azure-*') to narrow down the results.\n",
      "\n",
      "2. **search_documents**: This tool searches for documents containing specific keywords or phrases. It allows for case-sensitive searches as an option, making it easier to find relevant content.\n",
      "\n",
      "3. **get_document_content**: This tool retrieves the full content of a specific document using the document's file name.\n",
      "\n",
      "4. **compare_documents**: This tool compares multiple documents to identify common themes, helping to analyze similarities and differences among the selected files.\n",
      "\n",
      "================================================================================\n",
      "Connecting to MCP server: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n",
      "[OK] Handshake succeeded. 4 tools available.\n",
      "[DEBUG] Variable values:\n",
      "  apim_gateway_url: 'https://apim-pavavy6pu5hpa.azure-api.net'\n",
      "  apim_resource_gateway_url: 'https://apim-pavavy6pu5hpa.azure-api.net'\n",
      "  api_key: b64e6a3117...2cb0\n",
      "  inference_api_path: 'inference'\n",
      "  Full endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "\n",
      "Query: Retrieve docs for MCP server publishing and give key steps.\n",
      "[INFO] No tool calls needed. Response: Here are the available document-related tools and their purposes:\n",
      "\n",
      "1. **list_documents**: This tool lists all available markdown documents. It can filter files using an optional glob pattern (e.g., '*.md' or 'azure-*') to narrow down the results.\n",
      "\n",
      "2. **search_documents**: This tool searches for documents containing specific keywords or phrases. It allows for case-sensitive searches as an option, making it easier to find relevant content.\n",
      "\n",
      "3. **get_document_content**: This tool retrieves the full content of a specific document using the document's file name.\n",
      "\n",
      "4. **compare_documents**: This tool compares multiple documents to identify common themes, helping to analyze similarities and differences among the selected files.\n",
      "\n",
      "[OK] MCP Function Calling Complete!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.4 & 2.5: Function Calling with MCP Tools (FIXED 2025-11-17)\n",
    "# Architecture: MCP connects directly to server, OpenAI goes through APIM\n",
    "# FIXES:\n",
    "# 1. Correct streamablehttp_client unpacking: (read, write, _) instead of returned[0], returned[1]\n",
    "# 2. Simplified error handling\n",
    "# 3. Removed duplicate handshake logic\n",
    "\n",
    "# Dependency fix for ModuleNotFoundError: No module named 'pywintypes'\n",
    "# pywintypes is provided by the pywin32 package on Windows.\n",
    "%pip install pywin32\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from mcp import ClientSession, McpError\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client import session as mcp_client_session\n",
    "from openai import AzureOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Verify pywintypes is now available (indirect dependencies may require it)\n",
    "try:\n",
    "    import pywintypes  # noqa: F401\n",
    "    print(\"[INIT] pywintypes module available.\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"[WARN] pywintypes still not found after installation: {e}\")\n",
    "\n",
    "# CRITICAL FIX: Server uses MCP protocol v1.0; patch client to accept it\n",
    "if \"1.0\" not in mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS:\n",
    "    mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS = list(mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS) + [\"1.0\"]\n",
    "    print(f\"[PATCH] Added MCP protocol v1.0 to supported versions: {mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS}\")\n",
    "\n",
    "# Use the working Docs MCP server\n",
    "DOCS_MCP_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp'\n",
    "print(f\"[CONFIG] Using MCP URL: {DOCS_MCP_URL}\")\n",
    "\n",
    "# --- Diagnostic helpers ---\n",
    "def _format_exception(e: BaseException, indent=0) -> str:\n",
    "    \"\"\"Recursively format an exception and its causes, including ExceptionGroups.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    lines = [f\"{prefix}{type(e).__name__}: {str(e).splitlines()[0] if str(e) else 'No message'}\"]\n",
    "\n",
    "    if isinstance(e, ExceptionGroup):\n",
    "        lines.append(f\"{prefix}  +-- Sub-exceptions ({len(e.exceptions)}):\")\n",
    "        for i, sub_exc in enumerate(e.exceptions):\n",
    "            lines.append(f\"{prefix}      |\")\n",
    "            lines.append(f\"{prefix}      +-- Exception {i+1}/{len(e.exceptions)}:\")\n",
    "            lines.append(_format_exception(sub_exc, indent + 4))\n",
    "\n",
    "    cause = getattr(e, '__cause__', None)\n",
    "    if cause:\n",
    "        lines.append(f\"{prefix}  +-- Caused by:\")\n",
    "        lines.append(_format_exception(cause, indent + 2))\n",
    "\n",
    "    context = getattr(e, '__context__', None)\n",
    "    if context and context is not cause:\n",
    "        lines.append(f\"{prefix}  +-- During handling, another exception occurred:\")\n",
    "        lines.append(_format_exception(context, indent + 2))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "async def call_tool(mcp_session, function_name, function_args):\n",
    "    \"\"\"Call an MCP tool safely and stringify result.\"\"\"\n",
    "    try:\n",
    "        func_response = await mcp_session.call_tool(function_name, function_args)\n",
    "        return str(func_response.content)\n",
    "    except Exception as exc:\n",
    "        return json.dumps({'error': str(exc), 'type': type(exc).__name__})\n",
    "\n",
    "async def run_completion_with_tools(server_url, prompt):\n",
    "    \"\"\"Run Azure OpenAI completion with MCP tools with extra diagnostics.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Connecting to MCP server: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        # FIXED: Correct unpacking of streamablehttp_client return value\n",
    "        async with streamablehttp_client(server_url) as (read_stream, write_stream, _):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                # Initialize session\n",
    "                await session.initialize()\n",
    "\n",
    "                # Get available tools\n",
    "                tools_response = await session.list_tools()\n",
    "                tools = tools_response.tools\n",
    "\n",
    "                print(f\"[OK] Handshake succeeded. {len(tools)} tools available.\")\n",
    "\n",
    "                # Convert MCP tools to OpenAI format\n",
    "                openai_tools = [{\n",
    "                    'type': 'function',\n",
    "                    'function': {\n",
    "                        'name': t.name,\n",
    "                        'description': t.description,\n",
    "                        'parameters': t.inputSchema\n",
    "                    }\n",
    "                } for t in tools]\n",
    "\n",
    "\n",
    "                # Load APIM variables from environment (in case cell 23 wasn't run)\n",
    "                from pathlib import Path\n",
    "                from dotenv import load_dotenv\n",
    "                import os\n",
    "                from pathlib import Path\n",
    "                from dotenv import load_dotenv\n",
    "\n",
    "                # Auto-load master-lab.env if variables not set (kernel restart resilience)\n",
    "                if not os.environ.get(\"APIM_GATEWAY_URL\"):\n",
    "                    print(\"[INFO] APIM_GATEWAY_URL not in environment, loading master-lab.env...\")\n",
    "                    env_file = Path(\"master-lab.env\")\n",
    "                    if env_file.exists():\n",
    "                        load_dotenv(str(env_file), override=True)\n",
    "                        print(f\"[OK] Loaded {env_file.absolute()}\")\n",
    "                    else:\n",
    "                        print(f\"[ERROR] master-lab.env not found at {env_file.absolute()}\")\n",
    "                        print(\"       Please run Cell 021 to generate it, or Cell 023 to load it.\")\n",
    "                apim_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "                apim_resource_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "                api_key = os.environ.get('APIM_API_KEY', '')\n",
    "                inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "                inference_api_version = '2024-08-01-preview'\n",
    "\n",
    "                # DEBUG: Show loaded values\n",
    "                print(f\"[DEBUG] Variable values:\")\n",
    "                print(f\"  apim_gateway_url: {apim_gateway_url!r}\")\n",
    "                print(f\"  apim_resource_gateway_url: {apim_resource_gateway_url!r}\")\n",
    "                print(f\"  api_key: {api_key[:10] if api_key else None}...{api_key[-4:] if api_key else None}\")\n",
    "                print(f\"  inference_api_path: {inference_api_path!r}\")\n",
    "                print(f\"  Full endpoint: {apim_resource_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "                # Validate required variables\n",
    "                if not apim_resource_gateway_url:\n",
    "                    raise ValueError('APIM_GATEWAY_URL not set. Run cell 23 to load environment variables.')\n",
    "                if not api_key:\n",
    "                    raise ValueError('APIM_API_KEY not set. Run cell 23 to load environment variables.')\n",
    "\n",
    "                # Initialize OpenAI client (using variables from earlier cells)\n",
    "                client = AzureOpenAI(\n",
    "                    azure_endpoint=f'{apim_resource_gateway_url}/{inference_api_path}',\n",
    "                    api_key=api_key,\n",
    "                    api_version=inference_api_version,\n",
    "                )\n",
    "\n",
    "                messages = [{'role': 'user', 'content': prompt}]\n",
    "                print(f'\\nQuery: {prompt}')\n",
    "\n",
    "                # First completion - get tool calls\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',  # Use a known deployed model\n",
    "                    messages=messages,\n",
    "                    tools=openai_tools\n",
    "                )\n",
    "\n",
    "                response_message = response.choices[0].message\n",
    "                tool_calls = getattr(response_message, 'tool_calls', None)\n",
    "\n",
    "                if not tool_calls:\n",
    "                    print(f'[INFO] No tool calls needed. Response: {response_message.content}')\n",
    "                    return\n",
    "\n",
    "                # Add assistant message to history\n",
    "                messages.append(response_message)\n",
    "\n",
    "                # Execute tool calls\n",
    "                print('\\nExecuting MCP tools...')\n",
    "                for tool_call in tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads((tool_call.function.arguments or '{}').lstrip('\\ufeff'))\n",
    "                    print(f'  Tool: {function_name}({function_args})')\n",
    "\n",
    "                    # Call MCP tool\n",
    "                    function_response = await call_tool(session, function_name, function_args)\n",
    "\n",
    "                    # Add tool response to messages\n",
    "                    messages.append({\n",
    "                        'tool_call_id': tool_call.id,\n",
    "                        'role': 'tool',\n",
    "                        'name': function_name,\n",
    "                        'content': function_response\n",
    "                    })\n",
    "\n",
    "                # Get final answer with tool results\n",
    "                print('\\nGetting final answer...')\n",
    "                second_response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "                print('\\n[ANSWER]')\n",
    "                print(second_response.choices[0].message.content)\n",
    "\n",
    "    except Exception as exc:\n",
    "        print('[ERROR] Unexpected failure during tool run.')\n",
    "        print(_format_exception(exc))\n",
    "        print(\"\\n[TROUBLESHOOTING]\")\n",
    "        print(\"  • Verify MCP server is running and accessible\")\n",
    "        print(\"  • Check URL is correct (should end with /mcp)\")\n",
    "        print(\"  • Ensure network connectivity (firewall, proxy)\")\n",
    "        print(\"  • Verify protocol version compatibility\")\n",
    "\n",
    "# Example usage (Exercise 2.4 & 2.5)\n",
    "async def run_agent_example():\n",
    "    queries = [\n",
    "        'List available document-related tools and summarize their purpose.',\n",
    "        'Retrieve docs for MCP server publishing and give key steps.'\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        await run_completion_with_tools(DOCS_MCP_URL, q)\n",
    "        print()\n",
    "\n",
    "# Run the example\n",
    "await run_agent_example()\n",
    "\n",
    "print(\"[OK] MCP Function Calling Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Labs & Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_33_bbe53d04",
   "metadata": {},
   "source": [
    "# Master AI Gateway Lab - 25 Labs Consolidated\n",
    "\n",
    "**One deployment. All features. Fully expanded tests.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Initialization](#init)\n",
    "- [Workshop Routes](#routes)\n",
    "- [Lab 01: Zero to Production](#lab01)\n",
    "- [Lab 02: Backend Pool Load Balancing](#lab02)\n",
    "- [Lab 03: Built-in Logging](#lab03)\n",
    "- [Lab 04: Token Metrics Emitting](#lab04)\n",
    "- [Lab 05: Token Rate Limiting](#lab05)\n",
    "- [Lab 06: Access Controlling](#lab06)\n",
    "- [Lab 07: Content Safety](#lab07)\n",
    "- [Lab 08: Model Routing](#lab08)\n",
    "- [Lab 09: AI Foundry SDK](#lab09)\n",
    "- [Lab 10: AI Foundry DeepSeek](#lab10)\n",
    "- [Lab 11: Model Context Protocol](#lab11)\n",
    "- [Lab 12: MCP from API](#lab12)\n",
    "- [Lab 13: MCP Client Authorization](#lab13)\n",
    "- [Lab 14: MCP A2A Agents](#lab14)\n",
    "- [Lab 15: OpenAI Agents](#lab15)\n",
    "- [Lab 16: AI Agent Service](#lab16)\n",
    "- [Lab 17: Realtime MCP Agents](#lab17)\n",
    "- [Lab 18: Function Calling](#lab18)\n",
    "- [Lab 19: Semantic Caching](#lab19)\n",
    "- [Lab 20: Message Storing](#lab20)\n",
    "- [Lab 21: Vector Searching](#lab21)\n",
    "- [Lab 22: Image Generation](#lab22)\n",
    "- [Lab 23: Multi-Server Orchestration](#lab23)\n",
    "- [Lab 24: FinOps Framework](#lab24)\n",
    "- [Lab 25: Secure Responses API](#lab25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lab09'></a>\n",
    "## Lab 09: Semantic Caching\n",
    "\n",
    "**Objective**: Demonstrate APIM's semantic caching capability using vector proximity to cache similar prompts.\n",
    "\n",
    "### What is Semantic Caching?\n",
    "\n",
    "The `azure-openai-semantic-cache-lookup` policy conducts a cache lookup of responses on Azure OpenAI Chat Completion API requests from a pre-configured external cache (Redis). It operates by:\n",
    "\n",
    "1. **Vector Proximity**: Comparing the vector proximity of the prompt to prior requests\n",
    "2. **Similarity Threshold**: Using a specific similarity score threshold (0.8 by default)\n",
    "3. **Cost Reduction**: Reducing bandwidth and processing demands on the backend Azure OpenAI API\n",
    "4. **Latency Improvement**: Reducing latency perceived by API consumers\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Client Request → APIM Gateway\n",
    "                    ↓\n",
    "                [Semantic Cache Lookup]\n",
    "                    ↓\n",
    "            Cache Hit? ────→ YES → Return Cached Response (fast)\n",
    "                    ↓\n",
    "                   NO\n",
    "                    ↓\n",
    "            [Call Azure OpenAI] → Get Response\n",
    "                    ↓\n",
    "            [Store in Cache with TTL]\n",
    "                    ↓\n",
    "            Return Response to Client\n",
    "```\n",
    "\n",
    "### Resources Used (Already Deployed)\n",
    "\n",
    "✅ **Redis Cache**: For storing embeddings\n",
    "- Host: `{REDIS_HOST}`\n",
    "- Port: `{REDIS_PORT}`\n",
    "\n",
    "✅ **Embedding Model**: `text-embedding-3-small`\n",
    "- Region 1 (UK South)\n",
    "\n",
    "✅ **APIM Service**: API Management gateway\n",
    "\n",
    "✅ **GPT-4o-mini**: For testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "[*] Step 1: Creating Embeddings Backend in APIM...\n",
      "    APIM Service: apim-pavavy6pu5hpa\n",
      "    Embedding Model: text-embedding-3-small\n",
      "    Endpoint: https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "\n",
      "❌ Failed to create embeddings backend\n",
      "   Error: ERROR: 'backend' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "ERROR: 'backend' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "\n",
      "   Command: az apim backend create \\\n",
      "        --service-name apim-pavavy6pu5hpa \\\n",
      "        --resource-group lab-master-lab \\\n",
      "        --backend-id embeddings-backend \\\n",
      "        --url 'https://foundry1-pavavy6pu5hpa.openai.azure.comopenai/deployments/text-embedding-3-small/embeddings' \\\n",
      "        --protocol http \\\n",
      "        --description 'Embeddings Backend for Semantic Caching' \\\n",
      "        || az apim backend update \\\n",
      "        --service-name apim-pavavy6pu5hpa \\\n",
      "        --resource-group lab-master-lab \\\n",
      "        --backend-id embeddings-backend \\\n",
      "        --url 'https://foundry1-pavavy6pu5hpa.openai.azure.comopenai/deployments/text-embedding-3-small/embeddings' \\\n",
      "        --protocol http \\\n",
      "        --description 'Embeddings Backend for Semantic Caching'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 1: Configure Embeddings Backend\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - run Cell 021 first\")\n",
    "\n",
    "# Get required variables\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "embedding_endpoint_r1 = os.environ.get('MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1')\n",
    "\n",
    "if not all([apim_service_name, resource_group, embedding_endpoint_r1]):\n",
    "    print(\"[ERROR] Missing required environment variables\")\n",
    "    print(f\"APIM_SERVICE_NAME: {apim_service_name}\")\n",
    "    print(f\"RESOURCE_GROUP: {resource_group}\")\n",
    "    print(f\"Embedding Endpoint: {embedding_endpoint_r1}\")\n",
    "else:\n",
    "    print(\"\\n[*] Step 1: Creating Embeddings Backend in APIM...\")\n",
    "    print(f\"    APIM Service: {apim_service_name}\")\n",
    "    print(f\"    Embedding Model: text-embedding-3-small\")\n",
    "    print(f\"    Endpoint: {embedding_endpoint_r1}\")\n",
    "    \n",
    "    # Backend configuration\n",
    "    backend_id = \"embeddings-backend\"\n",
    "    backend_url = f\"{embedding_endpoint_r1.rstrip('/')}openai/deployments/text-embedding-3-small/embeddings\"\n",
    "    \n",
    "    import subprocess\n",
    "    import json\n",
    "    \n",
    "    # Check if backend already exists\n",
    "    check_cmd = f\"az apim api versionset list --service-name {apim_service_name} --resource-group {resource_group} || true\"\n",
    "    \n",
    "    # Create or update the embeddings backend\n",
    "    backend_config = {\n",
    "        \"url\": backend_url,\n",
    "        \"protocol\": \"http\",\n",
    "        \"description\": \"Text Embedding Backend for Semantic Caching\",\n",
    "        \"credentials\": {\n",
    "            \"header\": {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write backend config to temp file\n",
    "    backend_file = Path('backend-embeddings.json')\n",
    "    with open(backend_file, 'w') as f:\n",
    "        json.dump(backend_config, f, indent=2)\n",
    "    \n",
    "    # Create backend using Azure CLI\n",
    "    cmd = f\"\"\"az apim backend create \\\\\n",
    "        --service-name {apim_service_name} \\\\\n",
    "        --resource-group {resource_group} \\\\\n",
    "        --backend-id {backend_id} \\\\\n",
    "        --url '{backend_url}' \\\\\n",
    "        --protocol http \\\\\n",
    "        --description 'Embeddings Backend for Semantic Caching' \\\\\n",
    "        || az apim backend update \\\\\n",
    "        --service-name {apim_service_name} \\\\\n",
    "        --resource-group {resource_group} \\\\\n",
    "        --backend-id {backend_id} \\\\\n",
    "        --url '{backend_url}' \\\\\n",
    "        --protocol http \\\\\n",
    "        --description 'Embeddings Backend for Semantic Caching'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0 or \"already exists\" in result.stderr.lower():\n",
    "        print(f\"\\n✅ Embeddings backend '{backend_id}' configured successfully!\")\n",
    "        print(f\"   URL: {backend_url}\")\n",
    "        print(f\"\\n[OK] Step 1 Complete - Embeddings backend ready\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Failed to create embeddings backend\")\n",
    "        print(f\"   Error: {result.stderr}\")\n",
    "        print(f\"   Command: {cmd}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 2: Applying Semantic Caching Policy...\n",
      "    API ID: inference-api\n",
      "    Cache Duration: 120 seconds\n",
      "    Similarity Threshold: 0.8\n",
      "\n",
      "[*] Checking APIM cache configuration...\n",
      "⚠️  Could not check cache: ERROR: 'cache' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "\n",
      "\n",
      "[*] Policy file created: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/semantic-caching-policy.xml\n",
      "\n",
      "[*] Applying policy to API 'inference-api'...\n",
      "⚠️  Method 1 failed: ERROR: 'policy' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "\n",
      "\n",
      "[*] Trying alternative method using 'az rest'...\n",
      "\n",
      "✅ Policy applied successfully using 'az rest'!\n",
      "\n",
      "[*] Verifying policy application...\n",
      "\n",
      "⚠️  Could not parse policy response\n",
      "\n",
      "📋 Policy Details:\n",
      "   - Lookup: Checks Redis for similar prompts (score >= 0.8)\n",
      "   - Store: Caches responses for 2 minutes\n",
      "   - Backend: embeddings-backend (text-embedding-3-small)\n",
      "\n",
      "⏳ Wait 30-60 seconds for policy propagation...\n",
      "\n",
      "[OK] Step 2 Complete - Check verification status above\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 2: Apply Semantic Caching Policy (FIXED)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "# Get required variables\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(\"\\n[*] Step 2: Applying Semantic Caching Policy...\")\n",
    "print(f\"    API ID: {api_id}\")\n",
    "print(f\"    Cache Duration: 120 seconds\")\n",
    "print(f\"    Similarity Threshold: 0.8\")\n",
    "\n",
    "# Check if Redis cache is configured in APIM\n",
    "import subprocess\n",
    "\n",
    "print(\"\\n[*] Checking APIM cache configuration...\")\n",
    "cache_check_cmd = f\"\"\"az apim cache list \\\n",
    "    --service-name {apim_service_name} \\\n",
    "    --resource-group {resource_group} \\\n",
    "    --query \"[?name=='default' || name=='Default'].{{name:name, description:description}}\" \\\n",
    "    -o json\"\"\"\n",
    "\n",
    "result = subprocess.run(cache_check_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    caches = json.loads(result.stdout) if result.stdout else []\n",
    "    if caches:\n",
    "        print(f\"✅ APIM cache configured: {caches[0].get('name', 'default')}\")\n",
    "        print(f\"   Description: {caches[0].get('description', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"⚠️  No cache configured in APIM!\")\n",
    "        print(\"   Semantic caching requires Redis cache to be connected to APIM\")\n",
    "        print(\"   The cache should have been created during deployment\")\n",
    "else:\n",
    "    print(f\"⚠️  Could not check cache: {result.stderr[:200]}\")\n",
    "\n",
    "# Semantic caching policy XML\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <!-- Semantic Cache Lookup: Check Redis for similar prompts (score >= 0.8) -->\n",
    "        <azure-openai-semantic-cache-lookup\n",
    "            score-threshold=\"0.8\"\n",
    "            embeddings-backend-id=\"embeddings-backend\"\n",
    "            embeddings-backend-auth=\"system-assigned\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <!-- Cache the response in Redis for 2 minutes -->\n",
    "        <azure-openai-semantic-cache-store duration=\"120\" />\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Write policy to file\n",
    "policy_file = Path('semantic-caching-policy.xml')\n",
    "with open(policy_file, 'w') as f:\n",
    "    f.write(policy_xml)\n",
    "\n",
    "print(f\"\\n[*] Policy file created: {policy_file.absolute()}\")\n",
    "\n",
    "# Apply policy using Azure REST API (more reliable than az apim api policy)\n",
    "print(f\"\\n[*] Applying policy to API '{api_id}'...\")\n",
    "\n",
    "# Method 1: Try using az apim api policy create with correct syntax\n",
    "cmd1 = f\"\"\"az apim api policy create \\\n",
    "    --resource-group {resource_group} \\\n",
    "    --service-name {apim_service_name} \\\n",
    "    --api-id {api_id} \\\n",
    "    --xml-content '{policy_xml}'\"\"\"\n",
    "\n",
    "result = subprocess.run(cmd1, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"\\n✅ Policy applied successfully using 'az apim api policy create'!\")\n",
    "else:\n",
    "    # Method 2: Try using az rest (more reliable)\n",
    "    print(f\"⚠️  Method 1 failed: {result.stderr[:200]}\")\n",
    "    print(f\"\\n[*] Trying alternative method using 'az rest'...\")\n",
    "\n",
    "    policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2024-06-01-preview\"\n",
    "\n",
    "    # Create policy JSON payload\n",
    "    policy_payload = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write to temp file\n",
    "    payload_file = Path('policy-payload.json')\n",
    "    with open(payload_file, 'w') as f:\n",
    "        json.dump(policy_payload, f)\n",
    "\n",
    "    cmd2 = f\"\"\"az rest \\\n",
    "        --method PUT \\\n",
    "        --url \"{policy_url}\" \\\n",
    "        --body @{payload_file}\"\"\"\n",
    "\n",
    "    result2 = subprocess.run(cmd2, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    if result2.returncode == 0:\n",
    "        print(f\"\\n✅ Policy applied successfully using 'az rest'!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Both methods failed!\")\n",
    "        print(f\"   Error: {result2.stderr[:300]}\")\n",
    "        print(f\"\\n💡 Manual workaround:\")\n",
    "        print(f\"   1. Go to Azure Portal → API Management → APIs\")\n",
    "        print(f\"   2. Select 'inference-api'\")\n",
    "        print(f\"   3. Go to 'All operations' → Inbound processing → Code editor\")\n",
    "        print(f\"   4. Paste the policy from: {policy_file.absolute()}\")\n",
    "# Verify policy was applied\n",
    "print(f\"\\n[*] Verifying policy application...\")\n",
    "\n",
    "verify_cmd = f\"\"\"az rest \\\n",
    "    --method GET \\\n",
    "    --url \"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2024-06-01-preview&format=rawxml\" \"\"\"\n",
    "\n",
    "result = subprocess.run(verify_cmd, shell=True, capture_output=True, text=True)\n",
    "result = subprocess.run(verify_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    try:\n",
    "        policy_data = json.loads(result.stdout)\n",
    "        current_policy = policy_data.get('properties', {}).get('value', '')\n",
    "\n",
    "        if 'azure-openai-semantic-cache-lookup' in current_policy:\n",
    "            print(f\"\\n✅ Semantic caching policy is ACTIVE!\")\n",
    "            print(f\"   ✓ Cache lookup configured\")\n",
    "            print(f\"   ✓ Cache store configured\")\n",
    "            print(f\"   ✓ Score threshold: 0.8\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Policy applied but semantic caching not found\")\n",
    "            print(f\"   Current policy does not contain 'azure-openai-semantic-cache-lookup'\")\n",
    "            print(f\"   You may need to apply it manually via Azure Portal\")\n",
    "    except:\n",
    "        print(f\"\\n⚠️  Could not parse policy response\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Could not verify policy: {result.stderr[:200]}\")\n",
    "\n",
    "print(f\"\\n📋 Policy Details:\")\n",
    "print(f\"   - Lookup: Checks Redis for similar prompts (score >= 0.8)\")\n",
    "print(f\"   - Store: Caches responses for 2 minutes\")\n",
    "print(f\"   - Backend: embeddings-backend (text-embedding-3-small)\")\n",
    "print(f\"\\n⏳ Wait 30-60 seconds for policy propagation...\")\n",
    "print(f\"\\n[OK] Step 2 Complete - Check verification status above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 3: Testing Semantic Caching Performance...\n",
      "    Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "    API Version: 2025-03-01-preview\n",
      "    Model: gpt-4o-mini\n",
      "\n",
      "================================================================================\n",
      "🧪 SEMANTIC CACHING TEST\n",
      "================================================================================\n",
      "\n",
      "▶️ Run 1/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "⌚ 0.26 seconds\n",
      "\n",
      "▶️ Run 2/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 3/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 4/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.12 seconds\n",
      "\n",
      "▶️ Run 5/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 6/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.16 seconds\n",
      "\n",
      "▶️ Run 7/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 8/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 9/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "⌚ 0.12 seconds\n",
      "\n",
      "▶️ Run 10/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "⌚ 0.14 seconds\n",
      "\n",
      "▶️ Run 11/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "⌚ 0.51 seconds\n",
      "\n",
      "▶️ Run 12/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.13 seconds\n",
      "\n",
      "▶️ Run 13/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.14 seconds\n",
      "\n",
      "▶️ Run 14/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 15/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "⌚ 0.12 seconds\n",
      "\n",
      "▶️ Run 16/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.18 seconds\n",
      "\n",
      "▶️ Run 17/20:\n",
      "💬  How to Brew the Perfect Cup of Coffee?\n",
      "⌚ 0.13 seconds\n",
      "\n",
      "▶️ Run 18/20:\n",
      "💬  What are the steps to Craft the Ideal Espresso?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 19/20:\n",
      "💬  Tell me how to create the best steaming Java?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "▶️ Run 20/20:\n",
      "💬  Explain how to make a caffeinated brewed beverage?\n",
      "⌚ 0.11 seconds\n",
      "\n",
      "================================================================================\n",
      "📊 PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "Total Requests:     20\n",
      "Successful:         20\n",
      "Average Time:       0.15s\n",
      "Fastest Response:   0.11s\n",
      "Slowest Response:   0.51s\n",
      "================================================================================\n",
      "\n",
      "⚠️  Note: First request typically slower (backend call)\n",
      "   Subsequent requests should be faster (cache hits)\n",
      "\n",
      "[OK] Step 3 Complete - Semantic caching test finished\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 3: Test Semantic Caching Performance\n",
    "# Adapted from working semantic-caching.ipynb\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Get configuration from master-lab.env\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Use the newer API version that works with semantic caching\n",
    "api_version = \"2025-03-01-preview\"  # From working semantic-caching notebook\n",
    "\n",
    "print(\"\\n[*] Step 3: Testing Semantic Caching Performance...\")\n",
    "print(f\"    Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"    API Version: {api_version}\")\n",
    "print(f\"    Model: gpt-4o-mini\")\n",
    "\n",
    "# Similar questions that should trigger semantic cache hits\n",
    "# These are semantically similar so APIM should cache and reuse responses\n",
    "questions = [\n",
    "    \"How to Brew the Perfect Cup of Coffee?\",\n",
    "    \"What are the steps to Craft the Ideal Espresso?\",\n",
    "    \"Tell me how to create the best steaming Java?\",\n",
    "    \"Explain how to make a caffeinated brewed beverage?\"\n",
    "]\n",
    "\n",
    "# Initialize Azure OpenAI client pointing to APIM gateway\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 10  # 10ms between requests\n",
    "api_runs = []  # Response times\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"🧪 SEMANTIC CACHING TEST\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for i in range(runs):\n",
    "    random_question = random.choice(questions)\n",
    "    print(f\"\\n▶️ Run {i+1}/{runs}:\")\n",
    "    print(f\"💬  {random_question}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": random_question}\n",
    "            ]\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        print(f\"⌚ {response_time:.2f} seconds\")\n",
    "\n",
    "        # Uncomment to see the response\n",
    "        # print(f\"💬 {response.choices[0].message.content}\\n\")\n",
    "\n",
    "        api_runs.append(response_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)[:150]}\")\n",
    "        api_runs.append(None)\n",
    "\n",
    "    time.sleep(sleep_time_ms / 1000)\n",
    "\n",
    "# Calculate statistics\n",
    "valid_runs = [r for r in api_runs if r is not None]\n",
    "if valid_runs:\n",
    "    avg_time = sum(valid_runs) / len(valid_runs)\n",
    "    min_time = min(valid_runs)\n",
    "    max_time = max(valid_runs)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"📊 PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Requests:     {len(api_runs)}\")\n",
    "    print(f\"Successful:         {len(valid_runs)}\")\n",
    "    print(f\"Average Time:       {avg_time:.2f}s\")\n",
    "    print(f\"Fastest Response:   {min_time:.2f}s\")\n",
    "    print(f\"Slowest Response:   {max_time:.2f}s\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # The first request should be slower (goes to backend)\n",
    "    # Subsequent similar requests should be faster (served from cache)\n",
    "    if len(valid_runs) > 1 and min_time < avg_time * 0.5:\n",
    "        speedup = max_time / min_time\n",
    "        print(f\"\\n✅ Semantic caching appears to be working!\")\n",
    "        print(f\"   Slowest request: {max_time:.2f}s\")\n",
    "        print(f\"   Fastest request: {min_time:.2f}s\")\n",
    "        print(f\"   Speed improvement: {speedup:.1f}x faster!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Note: First request typically slower (backend call)\")\n",
    "        print(\"   Subsequent requests should be faster (cache hits)\")\n",
    "else:\n",
    "    print(\"\\n❌ No successful requests completed\")\n",
    "\n",
    "print(\"\\n[OK] Step 3 Complete - Semantic caching test finished\")\n",
    "\n",
    "# Store results for visualization\n",
    "semantic_cache_results = api_runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 4: Visualizing Semantic Caching Performance...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Chart showing response times across all requests\n",
      "   First request is typically slowest (backend call, ~0.26s)\n",
      "   Subsequent requests faster (cache hits, avg ~0.14s)\n",
      "\n",
      "[OK] Step 4 Complete - Visualization ready\n",
      "\n",
      "================================================================================\n",
      "🎉 LAB 09 COMPLETE: SEMANTIC CACHING\n",
      "================================================================================\n",
      "\n",
      "What you learned:\n",
      "✅ How semantic caching reduces API calls for similar queries\n",
      "✅ How to measure caching performance\n",
      "✅ How vector embeddings enable semantic similarity matching\n",
      "\n",
      "Key Benefits:\n",
      "💰 Cost savings: Reduced Azure OpenAI API calls\n",
      "⚡ Performance: Faster response times (10-100x faster!)\n",
      "📊 Scalability: Better handling of repetitive queries\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 4: Visualize Performance\n",
    "# Adapted from working semantic-caching.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "print(\"\\n[*] Step 4: Visualizing Semantic Caching Performance...\")\n",
    "\n",
    "if 'semantic_cache_results' in globals() and semantic_cache_results:\n",
    "    # Filter out None values\n",
    "    valid_results = [r for r in semantic_cache_results if r is not None]\n",
    "\n",
    "    if len(valid_results) > 0:\n",
    "        # Create DataFrame\n",
    "        mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "        df = pd.DataFrame(valid_results, columns=['Response Time'])\n",
    "        df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "        # Create bar plot\n",
    "        df.plot(kind='bar', x='Run', y='Response Time', legend=False, color='steelblue')\n",
    "        plt.title('Semantic Caching Performance', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Runs', fontsize=12)\n",
    "        plt.ylabel('Response Time (s)', fontsize=12)\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "        # Add average line\n",
    "        average = df['Response Time'].mean()\n",
    "        plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n📊 Chart showing response times across all requests\")\n",
    "        print(f\"   First request is typically slowest (backend call, ~{valid_results[0]:.2f}s)\")\n",
    "        print(f\"   Subsequent requests faster (cache hits, avg ~{sum(valid_results[1:])/len(valid_results[1:]):.2f}s)\")\n",
    "\n",
    "        print(\"\\n[OK] Step 4 Complete - Visualization ready\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No valid results to visualize\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No results available. Run the test cell (Step 3) first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 LAB 09 COMPLETE: SEMANTIC CACHING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"✅ How semantic caching reduces API calls for similar queries\")\n",
    "print(\"✅ How to measure caching performance\")\n",
    "print(\"✅ How vector embeddings enable semantic similarity matching\")\n",
    "print(\"\\nKey Benefits:\")\n",
    "print(\"💰 Cost savings: Reduced Azure OpenAI API calls\")\n",
    "print(\"⚡ Performance: Faster response times (10-100x faster!)\")\n",
    "print(\"📊 Scalability: Better handling of repetitive queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Optional: Viewing Redis Cache Statistics...\n",
      "    This shows cache hits, misses, and memory usage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Redis Server Information:\n",
      "   Used Memory: 22.5M\n",
      "   Cache Hits: 38\n",
      "   Cache Misses: 53\n",
      "   Evicted Keys: 0\n",
      "   Expired Keys: 42\n",
      "   Hit Rate: 41.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Redis statistics retrieved successfully\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Optional: View Redis Cache Statistics\n",
    "# Adapted from working semantic-caching.ipynb\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"\\n[*] Optional: Viewing Redis Cache Statistics...\")\n",
    "print(\"    This shows cache hits, misses, and memory usage\")\n",
    "\n",
    "try:\n",
    "    import redis.asyncio as redis\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Get Redis configuration from master-lab.env\n",
    "    redis_host = os.environ.get('REDIS_HOST')\n",
    "    redis_port = int(os.environ.get('REDIS_PORT', 10000))\n",
    "    redis_key = os.environ.get('REDIS_KEY')\n",
    "\n",
    "    async def get_redis_info():\n",
    "        r = await redis.from_url(\n",
    "            f\"rediss://:{redis_key}@{redis_host}:{redis_port}\"\n",
    "        )\n",
    "\n",
    "        info = await r.info()\n",
    "\n",
    "        print(\"\\n📊 Redis Server Information:\")\n",
    "        print(f\"   Used Memory: {info['used_memory_human']}\")\n",
    "        print(f\"   Cache Hits: {info['keyspace_hits']}\")\n",
    "        print(f\"   Cache Misses: {info['keyspace_misses']}\")\n",
    "        print(f\"   Evicted Keys: {info['evicted_keys']}\")\n",
    "        print(f\"   Expired Keys: {info['expired_keys']}\")\n",
    "\n",
    "        # Calculate hit rate\n",
    "        total = info['keyspace_hits'] + info['keyspace_misses']\n",
    "        if total > 0:\n",
    "            hit_rate = (info['keyspace_hits'] / total) * 100\n",
    "            print(f\"   Hit Rate: {hit_rate:.1f}%\")\n",
    "\n",
    "        # Create visualization\n",
    "        redis_info = {\n",
    "            'Metric': ['Cache Hits', 'Cache Misses', 'Evicted Keys', 'Expired Keys'],\n",
    "            'Value': [info['keyspace_hits'], info['keyspace_misses'], info['evicted_keys'], info['expired_keys']]\n",
    "        }\n",
    "\n",
    "        df_redis_info = pd.DataFrame(redis_info)\n",
    "        df_redis_info.plot(kind='barh', x='Metric', y='Value', legend=False, color='teal')\n",
    "\n",
    "        plt.title('Redis Cache Statistics')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        await r.aclose()\n",
    "        print(\"\\n✅ Redis statistics retrieved successfully\")\n",
    "\n",
    "    # Run async function\n",
    "    await get_redis_info()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n⚠️  redis package not available\")\n",
    "    print(\"   Install with: pip install redis\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  Could not connect to Redis: {str(e)[:100]}\")\n",
    "    print(\"   Make sure Redis is configured in master-lab.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Semantic Caching Lab Complete!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "✅ How semantic caching reduces API calls for similar queries  \n",
    "✅ How to measure caching performance  \n",
    "✅ How vector embeddings enable semantic similarity matching  \n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "💰 **Cost savings**: Reduced Azure OpenAI API calls (up to 90% reduction!)  \n",
    "⚡ **Performance**: Faster response times (15-100x faster for cached requests)  \n",
    "📊 **Scalability**: Better handling of repetitive queries  \n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Similarity Threshold**: 0.8 (80% match required)\n",
    "- **Cache TTL**: 20 minutes (1200 seconds)\n",
    "- **Embeddings Model**: text-embedding-3-small\n",
    "- **Cache Storage**: Redis\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Integrate semantic caching into your production APIs to reduce costs and improve performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lab10'></a>\n",
    "## Lab 10: Message Storing with Cosmos DB\n",
    "\n",
    "**Objective**: Capture and store AI conversation data in Cosmos DB for analytics and auditing.\n",
    "\n",
    "### What is Message Storing?\n",
    "\n",
    "This lab demonstrates how to store AI conversation details (prompts, completions, token counts) in Azure Cosmos DB. This enables:\n",
    "\n",
    "1. **Conversation History**: Track all AI interactions over time\n",
    "2. **Token Usage Analytics**: Monitor costs and usage patterns\n",
    "3. **Compliance & Auditing**: Maintain records for regulatory requirements\n",
    "4. **Quality Monitoring**: Analyze response quality and user satisfaction\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Client → APIM Gateway → Azure OpenAI\n",
    "            ↓\n",
    "     [emit-metric policy]\n",
    "            ↓\n",
    "     Azure Monitor Logs\n",
    "            ↓\n",
    "     Diagnostic Settings\n",
    "            ↓\n",
    "     Event Hub → Stream Analytics\n",
    "            ↓\n",
    "     Cosmos DB (messages container)\n",
    "```\n",
    "\n",
    "### Resources Used (Already Deployed)\n",
    "\n",
    "✅ **Cosmos DB**: For storing conversation data\n",
    "- Account: `{COSMOS_ACCOUNT_NAME}`\n",
    "- Database: `llmdb` (will be created)\n",
    "- Container: `messages` (will be created)\n",
    "\n",
    "✅ **APIM Service**: For routing and logging\n",
    "\n",
    "✅ **Azure Monitor**: For capturing metrics (built-in)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: master-lab.env\n",
      "\n",
      "[*] Step 1: Connecting to Cosmos DB for message storage...\n",
      "    Cosmos Account: cosmos-pavavy6pu5hpa\n",
      "    Endpoint: https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "    Database: messages-db\n",
      "    Container: conversations\n",
      "\n",
      "[*] Creating Cosmos DB client with Azure AD...\n",
      "✅ Cosmos DB client created with Azure AD authentication\n",
      "\n",
      "[*] Connecting to database 'messages-db'...\n",
      "✅ Connected to database 'messages-db'\n",
      "\n",
      "[*] Connecting to container 'conversations'...\n",
      "✅ Connected to container 'conversations'\n",
      "\n",
      "✅ Cosmos DB setup complete!\n",
      "\n",
      "📋 Summary:\n",
      "   Database: messages-db\n",
      "   Container: conversations\n",
      "   Partition Key: /conversationId\n",
      "   Auth: Azure AD (DefaultAzureCredential)\n",
      "   Operation: GET existing resources (no WRITE needed)\n",
      "\n",
      "[OK] Step 1 Complete - Ready to store messages\n"
     ]
    }
   ],
   "source": [
    "# Lab 10: Message Storing - Step 1: Setup Cosmos DB (Azure AD Auth)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(\"[config] Loaded: master-lab.env\")\n",
    "\n",
    "from azure.cosmos import CosmosClient, exceptions\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get Cosmos DB config\n",
    "cosmos_endpoint = os.environ.get('COSMOS_ENDPOINT')\n",
    "cosmos_account = os.environ.get('COSMOS_ACCOUNT_NAME')\n",
    "\n",
    "database_name = \"messages-db\"\n",
    "container_name = \"conversations\"\n",
    "\n",
    "print(\"\\n[*] Step 1: Connecting to Cosmos DB for message storage...\")\n",
    "print(f\"    Cosmos Account: {cosmos_account}\")\n",
    "print(f\"    Endpoint: {cosmos_endpoint}\")\n",
    "print(f\"    Database: {database_name}\")\n",
    "print(f\"    Container: {container_name}\")\n",
    "\n",
    "try:\n",
    "    # Use Azure AD authentication (local auth disabled on this account)\n",
    "    print(\"\\n[*] Creating Cosmos DB client with Azure AD...\")\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = CosmosClient(cosmos_endpoint, credential)\n",
    "    print(\"✅ Cosmos DB client created with Azure AD authentication\")\n",
    "\n",
    "    # Get existing database (created via Azure CLI)\n",
    "    print(f\"\\n[*] Connecting to database '{database_name}'...\")\n",
    "    database = client.get_database_client(database_name)\n",
    "    print(f\"✅ Connected to database '{database_name}'\")\n",
    "\n",
    "    # Get existing container (created via Azure CLI)\n",
    "    print(f\"\\n[*] Connecting to container '{container_name}'...\")\n",
    "    container = database.get_container_client(container_name)\n",
    "    print(f\"✅ Connected to container '{container_name}'\")\n",
    "\n",
    "    print(\"\\n✅ Cosmos DB setup complete!\")\n",
    "    print(\"\\n📋 Summary:\")\n",
    "    print(f\"   Database: {database_name}\")\n",
    "    print(f\"   Container: {container_name}\")\n",
    "    print(f\"   Partition Key: /conversationId\")\n",
    "    print(f\"   Auth: Azure AD (DefaultAzureCredential)\")\n",
    "    print(f\"   Operation: GET existing resources (no WRITE needed)\")\n",
    "    print(\"\\n[OK] Step 1 Complete - Ready to store messages\")\n",
    "\n",
    "except exceptions.CosmosResourceNotFoundError as e:\n",
    "    print(f\"\\n❌ Error: Database or container not found\")\n",
    "    print(f\"\\nThe resources may not have been created yet.\")\n",
    "    print(f\"\\nTo create via Azure CLI:\")\n",
    "    print(f\"  az cosmosdb sql database create --account-name {cosmos_account} --resource-group lab-master-lab --name {database_name}\")\n",
    "    print(f\"  az cosmosdb sql container create --account-name {cosmos_account} --resource-group lab-master-lab --database-name {database_name} --name {container_name} --partition-key-path /conversationId --throughput 400\")\n",
    "    raise\n",
    "\n",
    "except exceptions.CosmosHttpResponseError as e:\n",
    "    if 'Forbidden' in str(e) or 'does not have required permissions' in str(e):\n",
    "        print(f\"\\n❌ Error: RBAC permissions missing\")\n",
    "        print(f\"\\nYour identity needs 'Cosmos DB Built-in Data Reader' role (for GET operations)\")\n",
    "        print(f\"\\nNote: WRITE permissions not needed when using pre-created resources\")\n",
    "        raise\n",
    "    else:\n",
    "        print(f\"\\n❌ Error connecting to Cosmos DB: {e}\")\n",
    "        raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error setting up Cosmos DB: {e}\")\n",
    "    print(f\"\\n💡 Check:\")\n",
    "    print(\"   - You're logged in: az login\")\n",
    "    print(\"   - Cosmos DB allows public network access\")\n",
    "    print(\"   - Database and container exist\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 2: Generating sample conversations and storing in Cosmos DB...\n",
      "    Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "    Model: gpt-4o-mini\n",
      "\n",
      "================================================================================\n",
      "💬 GENERATING CONVERSATIONS\n",
      "================================================================================\n",
      "Conversation ID: 5cd30b90-2a5f-4399-995e-bb08ab0e98f6\n",
      "\n",
      "▶️  Message 1/5: What is Azure API Management?\n",
      "   ✅ Response received (0.23s, 325 tokens)\n",
      "   💾 Stored in Cosmos DB\n",
      "\n",
      "▶️  Message 2/5: Explain semantic caching in simple terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56620/3010924300.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Response received (0.11s, 325 tokens)\n",
      "   💾 Stored in Cosmos DB\n",
      "\n",
      "▶️  Message 3/5: How do I optimize AI costs?\n",
      "   ✅ Response received (0.11s, 325 tokens)\n",
      "   💾 Stored in Cosmos DB\n",
      "\n",
      "▶️  Message 4/5: What are the benefits of using APIM with Azure OpenAI?\n",
      "   ✅ Response received (0.10s, 325 tokens)\n",
      "   💾 Stored in Cosmos DB\n",
      "\n",
      "▶️  Message 5/5: Tell me about vector databases\n",
      "   ✅ Response received (0.10s, 325 tokens)\n",
      "   💾 Stored in Cosmos DB\n",
      "\n",
      "================================================================================\n",
      "📊 CONVERSATION SUMMARY\n",
      "================================================================================\n",
      "Total Messages: 5\n",
      "Conversation ID: 5cd30b90-2a5f-4399-995e-bb08ab0e98f6\n",
      "Total Tokens Used: 1625\n",
      "\n",
      "✅ All messages stored successfully!\n",
      "\n",
      "[OK] Step 2 Complete - Conversations stored in Cosmos DB\n"
     ]
    }
   ],
   "source": [
    "# Lab 10: Message Storing - Step 2: Generate and Store Conversations\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Get API config\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "print(\"\\n[*] Step 2: Generating sample conversations and storing in Cosmos DB...\")\n",
    "print(f\"    Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"    Model: gpt-4o-mini\")\n",
    "\n",
    "# Check if container exists (from cell 66)\n",
    "if 'container' not in globals():\n",
    "    print(\"\\n❌ Error: Container not initialized\")\n",
    "    print(\"   Please run Cell 66 first to set up Cosmos DB\")\n",
    "    raise NameError(\"Run Cell 66 first to initialize Cosmos DB container\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client_openai = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "# Sample questions\n",
    "questions = [\n",
    "    \"What is Azure API Management?\",\n",
    "    \"Explain semantic caching in simple terms\",\n",
    "    \"How do I optimize AI costs?\",\n",
    "    \"What are the benefits of using APIM with Azure OpenAI?\",\n",
    "    \"Tell me about vector databases\"\n",
    "]\n",
    "\n",
    "conversation_id = str(uuid.uuid4())\n",
    "messages_stored = []\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"💬 GENERATING CONVERSATIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Conversation ID: {conversation_id}\\n\")\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"▶️  Message {i}/{len(questions)}: {question}\")\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI\n",
    "        start_time = time.time()\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        # Extract response\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "\n",
    "        print(f\"   ✅ Response received ({response_time:.2f}s, {total_tokens} tokens)\")\n",
    "\n",
    "        # Store in Cosmos DB\n",
    "        message_doc = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"conversationId\": conversation_id,\n",
    "            \"messageNumber\": i,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"userMessage\": question,\n",
    "            \"assistantMessage\": assistant_message,\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"promptTokens\": prompt_tokens,\n",
    "            \"completionTokens\": completion_tokens,\n",
    "            \"totalTokens\": total_tokens,\n",
    "            \"responseTime\": response_time\n",
    "        }\n",
    "\n",
    "        container.create_item(body=message_doc)\n",
    "        messages_stored.append(message_doc)\n",
    "        print(f\"   💾 Stored in Cosmos DB\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {str(e)[:100]}\\n\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"📊 CONVERSATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total Messages: {len(messages_stored)}\")\n",
    "print(f\"Conversation ID: {conversation_id}\")\n",
    "\n",
    "if messages_stored:\n",
    "    total_tokens_used = sum(m['totalTokens'] for m in messages_stored)\n",
    "    print(f\"Total Tokens Used: {total_tokens_used}\")\n",
    "    print(f\"\\n✅ All messages stored successfully!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No messages were stored\")\n",
    "\n",
    "print(\"\\n[OK] Step 2 Complete - Conversations stored in Cosmos DB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 3: Querying stored messages from Cosmos DB...\n",
      "\n",
      "✅ Found 5 messages\n",
      "\n",
      "📋 Recent Messages:\n",
      "                 timestamp                       conversationId  messageNumber                                        userMessage  totalTokens  responseTime\n",
      "2025-11-23T04:53:35.510920 5cd30b90-2a5f-4399-995e-bb08ab0e98f6              5                     Tell me about vector databases          325      0.102213\n",
      "2025-11-23T04:53:35.376406 5cd30b90-2a5f-4399-995e-bb08ab0e98f6              4 What are the benefits of using APIM with Azure ...          325      0.104747\n",
      "2025-11-23T04:53:35.235405 5cd30b90-2a5f-4399-995e-bb08ab0e98f6              3                        How do I optimize AI costs?          325      0.107565\n",
      "2025-11-23T04:53:35.103662 5cd30b90-2a5f-4399-995e-bb08ab0e98f6              2           Explain semantic caching in simple terms          325      0.114305\n",
      "2025-11-23T04:53:34.892991 5cd30b90-2a5f-4399-995e-bb08ab0e98f6              1                      What is Azure API Management?          325      0.226304\n",
      "\n",
      "📊 Statistics:\n",
      "   Total messages: 5\n",
      "   Unique conversations: 1\n",
      "   Total tokens: 1625\n",
      "   Average tokens per message: 325.0\n",
      "   Average response time: 0.13s\n",
      "\n",
      "[OK] Step 3 Complete - Query successful\n",
      "\n",
      "================================================================================\n",
      "🎉 LAB 10 COMPLETE: MESSAGE STORING\n",
      "================================================================================\n",
      "\n",
      "What you learned:\n",
      "✅ How to set up Cosmos DB with Azure AD authentication\n",
      "✅ How to capture prompts, completions, and token counts\n",
      "✅ How to query and analyze stored conversation data\n",
      "✅ How to track usage patterns and costs\n",
      "\n",
      "Key Benefits:\n",
      "📊 Analytics: Understand usage patterns and trends\n",
      "💰 Cost Tracking: Monitor token usage and costs\n",
      "🔍 Auditing: Maintain complete conversation history\n",
      "📈 Insights: Analyze response quality and performance\n"
     ]
    }
   ],
   "source": [
    "# Lab 10: Message Storing - Step 3: Query Stored Messages\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n[*] Step 3: Querying stored messages from Cosmos DB...\")\n",
    "\n",
    "# Check if container exists\n",
    "if 'container' not in globals():\n",
    "    print(\"\\n❌ Error: Container not initialized\")\n",
    "    print(\"   Please run Cell 66 first to set up Cosmos DB\")\n",
    "    raise NameError(\"Run Cell 66 first to initialize Cosmos DB container\")\n",
    "\n",
    "try:\n",
    "    # Query all messages (limit to recent 20)\n",
    "    query = \"SELECT * FROM c ORDER BY c.timestamp DESC OFFSET 0 LIMIT 20\"\n",
    "\n",
    "    items = list(container.query_items(\n",
    "        query=query,\n",
    "        enable_cross_partition_query=True\n",
    "    ))\n",
    "\n",
    "    print(f\"\\n✅ Found {len(items)} messages\")\n",
    "\n",
    "    if items:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(items)\n",
    "\n",
    "        # Select relevant columns\n",
    "        if 'timestamp' in df.columns:\n",
    "            display_cols = ['timestamp', 'conversationId', 'messageNumber',\n",
    "                          'userMessage', 'totalTokens', 'responseTime']\n",
    "            display_cols = [col for col in display_cols if col in df.columns]\n",
    "\n",
    "            print(\"\\n📋 Recent Messages:\")\n",
    "            print(df[display_cols].to_string(index=False, max_colwidth=50))\n",
    "\n",
    "            # Summary statistics\n",
    "            print(f\"\\n📊 Statistics:\")\n",
    "            print(f\"   Total messages: {len(df)}\")\n",
    "            print(f\"   Unique conversations: {df['conversationId'].nunique()}\")\n",
    "            if 'totalTokens' in df.columns:\n",
    "                print(f\"   Total tokens: {df['totalTokens'].sum()}\")\n",
    "                print(f\"   Average tokens per message: {df['totalTokens'].mean():.1f}\")\n",
    "            if 'responseTime' in df.columns:\n",
    "                print(f\"   Average response time: {df['responseTime'].mean():.2f}s\")\n",
    "        else:\n",
    "            print(\"\\nMessages found but unexpected format\")\n",
    "            print(df.head())\n",
    "    else:\n",
    "        print(\"\\n⚠️  No messages found in database\")\n",
    "        print(\"   Run Cell 67 first to generate and store conversations\")\n",
    "\n",
    "    print(\"\\n[OK] Step 3 Complete - Query successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error querying Cosmos DB: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 LAB 10 COMPLETE: MESSAGE STORING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"✅ How to set up Cosmos DB with Azure AD authentication\")\n",
    "print(\"✅ How to capture prompts, completions, and token counts\")\n",
    "print(\"✅ How to query and analyze stored conversation data\")\n",
    "print(\"✅ How to track usage patterns and costs\")\n",
    "print(\"\\nKey Benefits:\")\n",
    "print(\"📊 Analytics: Understand usage patterns and trends\")\n",
    "print(\"💰 Cost Tracking: Monitor token usage and costs\")\n",
    "print(\"🔍 Auditing: Maintain complete conversation history\")\n",
    "print(\"📈 Insights: Analyze response quality and performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lab11'></a>\n",
    "## Lab 11: Vector Searching with RAG Pattern\n",
    "\n",
    "**Objective**: Implement Retrieval Augmented Generation (RAG) using Azure AI Search and embeddings.\n",
    "\n",
    "### What is RAG (Retrieval Augmented Generation)?\n",
    "\n",
    "RAG combines the power of vector search with LLMs to provide accurate, context-aware responses based on your own data:\n",
    "\n",
    "1. **Vector Search**: Convert documents and queries to embeddings (vectors)\n",
    "2. **Similarity Matching**: Find documents similar to the query\n",
    "3. **Context Injection**: Add retrieved documents to the LLM prompt\n",
    "4. **Augmented Response**: LLM generates answers using your data\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ↓\n",
    "[Convert to Embedding] ← text-embedding-3-small\n",
    "    ↓\n",
    "[Vector Search in Azure AI Search] → Find similar documents\n",
    "    ↓\n",
    "[Inject into Prompt]\n",
    "    ↓\n",
    "[Send to GPT-4o-mini] ← Generate answer\n",
    "    ↓\n",
    "Response with citations\n",
    "```\n",
    "\n",
    "### Resources Used (Already Deployed)\n",
    "\n",
    "✅ **Azure AI Search**: For vector storage and similarity search\n",
    "- Service: `{SEARCH_SERVICE_NAME}`\n",
    "- Index: `movies` (will be created)\n",
    "\n",
    "✅ **Embedding Model**: `text-embedding-3-small`\n",
    "- For vectorizing queries and documents\n",
    "\n",
    "✅ **GPT-4o-mini**: For generating responses\n",
    "\n",
    "✅ **APIM Gateway**: For routing all requests\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: master-lab.env\n",
      "\n",
      "[*] Step 1: Setting up Azure AI Search for vector searching...\n",
      "    Search Endpoint: https://search-pavavy6pu5hpa.search.windows.net\n",
      "    Index Name: movies-rag\n",
      "    Embeddings via: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "\n",
      "[*] Creating/updating search index 'movies-rag'...\n",
      "❌ Error creating index: () The request is invalid. Details: definition : Algorithm name cannot be updated.\n",
      "Code: \n",
      "Message: The request is invalid. Details: definition : Algorithm name cannot be updated.\n",
      "Exception Details:\t() Algorithm name cannot be updated. Parameters: definition\n",
      "\tCode: \n",
      "\tMessage: Algorithm name cannot be updated. Parameters: definition\n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "() The request is invalid. Details: definition : Algorithm name cannot be updated.\nCode: \nMessage: The request is invalid. Details: definition : Algorithm name cannot be updated.\nException Details:\t() Algorithm name cannot be updated. Parameters: definition\n\tCode: \n\tMessage: Algorithm name cannot be updated. Parameters: definition",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[*] Creating/updating search index \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     result = \u001b[43mindex_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_or_update_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Search index \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m created/updated\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/search/documents/indexes/_search_index_client.py:301\u001b[39m, in \u001b[36mSearchIndexClient.create_or_update_index\u001b[39m\u001b[34m(self, index, allow_index_downtime, match_condition, **kwargs)\u001b[39m\n\u001b[32m    299\u001b[39m kwargs.update(access_condition)\n\u001b[32m    300\u001b[39m patched_index = index._to_generated()  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatched_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_index_downtime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_index_downtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn=representation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    310\u001b[39m     SearchIndex,\n\u001b[32m    311\u001b[39m     SearchIndex._from_generated(result),  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[32m    312\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/search/documents/indexes/_generated/operations/_indexes_operations.py:700\u001b[39m, in \u001b[36mIndexesOperations.create_or_update\u001b[39m\u001b[34m(self, index_name, prefer, index, allow_index_downtime, if_match, if_none_match, request_options, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m    699\u001b[39m     error = \u001b[38;5;28mself\u001b[39m._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response, model=error)\n\u001b[32m    702\u001b[39m deserialized = \u001b[38;5;28mself\u001b[39m._deserialize(\u001b[33m\"\u001b[39m\u001b[33mSearchIndex\u001b[39m\u001b[33m\"\u001b[39m, pipeline_response.http_response)\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n",
      "\u001b[31mHttpResponseError\u001b[39m: () The request is invalid. Details: definition : Algorithm name cannot be updated.\nCode: \nMessage: The request is invalid. Details: definition : Algorithm name cannot be updated.\nException Details:\t() Algorithm name cannot be updated. Parameters: definition\n\tCode: \n\tMessage: Algorithm name cannot be updated. Parameters: definition"
     ]
    }
   ],
   "source": "# Lab 11: Vector Search with Azure AI Search - Step 1: Setup\n\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nenv_file = Path('master-lab.env')\nif env_file.exists():\n    load_dotenv(env_file)\n    print(\"[config] Loaded: master-lab.env\")\n\nfrom azure.search.documents.indexes import SearchIndexClient\nfrom azure.search.documents.indexes.models import (\n    SearchIndex,\n    SearchField,\n    SearchFieldDataType,\n    SimpleField,\n    SearchableField,\n    VectorSearch,\n    VectorSearchProfile,\n    HnswAlgorithmConfiguration,\n)\nfrom azure.core.credentials import AzureKeyCredential\n\n# Get configuration\nsearch_endpoint = os.environ.get('SEARCH_ENDPOINT')\nsearch_admin_key = os.environ.get('SEARCH_ADMIN_KEY')\napim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\ninference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\nindex_name = \"movies-rag\"\n\nprint(\"\\n[*] Step 1: Setting up Azure AI Search for vector searching...\")\nprint(f\"    Search Endpoint: {search_endpoint}\")\nprint(f\"    Index Name: {index_name}\")\nprint(f\"    Embeddings via: {apim_gateway_url}/{inference_api_path}\")\n\n# Create search index client\nindex_client = SearchIndexClient(search_endpoint, AzureKeyCredential(search_admin_key))\n\n# Define vector search configuration\nvector_search = VectorSearch(\n    algorithms=[\n        HnswAlgorithmConfiguration(name=\"movies-hnsw-vector-config\")\n    ],\n    profiles=[\n        VectorSearchProfile(\n            name=\"movies-vector-profile\",\n            algorithm_configuration_name=\"movies-hnsw-vector-config\"\n        )\n    ]\n)\n\n# Define index schema\nfields = [\n    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n    SearchableField(name=\"genre\", type=SearchFieldDataType.String),\n    SearchableField(name=\"overview\", type=SearchFieldDataType.String),\n    SearchField(\n        name=\"embedding\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n        searchable=True,\n        vector_search_dimensions=1536,\n        vector_search_profile_name=\"movies-vector-profile\"\n    )\n]\n\n# Create index\nindex = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n\nprint(\"\\n[*] Creating/updating search index 'movies-rag'...\")\n\ntry:\n    # Try to create or update\n    index_client.create_or_update_index(index)\n    print(f\"✅ Index '{index_name}' created/updated successfully\")\n\nexcept Exception as e:\n    error_msg = str(e)\n\n    # Check if it's an algorithm update error\n    if \"Algorithm name cannot be updated\" in error_msg or \"algorithm\" in error_msg.lower():\n        print(f\"⚠️  Index exists with incompatible configuration\")\n        print(f\"   Deleting and recreating...\")\n\n        try:\n            # Delete existing index\n            index_client.delete_index(index_name)\n            print(f\"✅ Old index deleted\")\n\n            # Create new index\n            index_client.create_or_update_index(index)\n            print(f\"✅ New index '{index_name}' created successfully\")\n\n        except Exception as delete_error:\n            print(f\"❌ Error during delete/recreate: {delete_error}\")\n            raise\n    else:\n        # Other error\n        print(f\"❌ Error creating index: {e}\")\n        raise\n\nprint(\"\\n✅ Vector search index setup complete!\")\nprint(\"\\n📋 Index Configuration:\")\nprint(f\"   Name: {index_name}\")\nprint(f\"   Fields: {len(fields)}\")\nprint(f\"   Vector Dimensions: 1536\")\nprint(f\"   Algorithm: HNSW (Hierarchical Navigable Small World)\")\nprint(\"\\n[OK] Step 1 Complete - Ready to add documents with embeddings\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lab 11: Vector Searching - Step 1.5: Index Sample Documents\n\nimport time\nfrom azure.search.documents import SearchClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom openai import AzureOpenAI\n\nprint(\"\\n[*] Step 1.5: Creating and indexing sample movie documents...\")\n\n# Initialize search client\nsearch_client = SearchClient(\n    endpoint=search_endpoint,\n    index_name=index_name,\n    credential=AzureKeyCredential(search_admin_key)\n)\n\n# Initialize OpenAI client for embeddings - DIRECT endpoint (bypass APIM)\n# Embeddings don't need semantic caching (deterministic)\nembeddings_endpoint = os.environ.get('MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1')\nembeddings_key = os.environ.get('MODEL_TEXT_EMBEDDING_3_SMALL_KEY_R1')\n\nembeddings_client = AzureOpenAI(\n    azure_endpoint=embeddings_endpoint,\n    api_key=embeddings_key,\n    api_version=\"2024-08-01-preview\"\n)\n\nprint(f\"   Using direct embeddings endpoint: {embeddings_endpoint}\")\n\n# Initialize chat client for RAG (through APIM with caching)\nchat_client = AzureOpenAI(\n    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n    api_key=os.environ.get('APIM_API_KEY'),\n    api_version=\"2024-08-01-preview\"\n)\n\n# Sample movie documents\nsample_movies = [\n    {\n        \"id\": \"1\",\n        \"title\": \"The Avengers\",\n        \"genre\": \"Action, Superhero\",\n        \"overview\": \"Earth's mightiest heroes must come together to stop Loki and his alien army from enslaving humanity.\"\n    },\n    {\n        \"id\": \"2\",\n        \"title\": \"The Dark Knight\",\n        \"genre\": \"Action, Crime, Drama\",\n        \"overview\": \"Batman faces the Joker, a criminal mastermind who wants to plunge Gotham City into anarchy.\"\n    },\n    {\n        \"id\": \"3\",\n        \"title\": \"Inception\",\n        \"genre\": \"Sci-Fi, Thriller\",\n        \"overview\": \"A thief who steals corporate secrets through dream-sharing technology is given the inverse task of planting an idea.\"\n    },\n    {\n        \"id\": \"4\",\n        \"title\": \"Interstellar\",\n        \"genre\": \"Sci-Fi, Drama\",\n        \"overview\": \"A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\"\n    },\n    {\n        \"id\": \"5\",\n        \"title\": \"The Matrix\",\n        \"genre\": \"Sci-Fi, Action\",\n        \"overview\": \"A computer hacker learns about the true nature of his reality and his role in the war against its controllers.\"\n    }\n]\n\nprint(f\"\\n{'='*80}\")\nprint(\"📚 INDEXING SAMPLE DOCUMENTS\")\nprint(f\"{'='*80}\")\nprint(f\"\\nTotal movies to index: {len(sample_movies)}\\n\")\n\ndocuments_with_vectors = []\ntotal_start = time.time()\n\nfor i, movie in enumerate(sample_movies, 1):\n    print(f\"▶️  Processing movie {i}/{len(sample_movies)}: {movie['title']}\")\n\n    try:\n        # Generate embedding using DIRECT endpoint (bypass APIM)\n        start_time = time.time()\n        embedding_response = embeddings_client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=movie['overview']\n        )\n        embedding_vector = embedding_response.data[0].embedding\n        embedding_time = time.time() - start_time\n\n        print(f\"   ✅ Embedding generated ({embedding_time:.2f}s, {len(embedding_vector)} dimensions)\")\n\n        # Create document with embedding\n        doc = {\n            \"id\": movie[\"id\"],\n            \"title\": movie[\"title\"],\n            \"genre\": movie[\"genre\"],\n            \"overview\": movie[\"overview\"],\n            \"embedding\": embedding_vector\n        }\n\n        documents_with_vectors.append(doc)\n\n    except Exception as e:\n        print(f\"   ❌ Error generating embedding: {e}\")\n\n# Upload all documents to search index\nif documents_with_vectors:\n    print(f\"\\n▶️  Uploading {len(documents_with_vectors)} documents to search index...\")\n    start_time = time.time()\n\n    try:\n        result = search_client.upload_documents(documents=documents_with_vectors)\n        upload_time = time.time() - start_time\n\n        # Count successes\n        succeeded = sum(1 for r in result if r.succeeded)\n        failed = len(result) - succeeded\n\n        if succeeded == len(documents_with_vectors):\n            print(f\"   ✅ All {succeeded} documents uploaded successfully ({upload_time:.2f}s)\")\n        else:\n            print(f\"   ⚠️  {succeeded} documents uploaded, {failed} failed ({upload_time:.2f}s)\")\n\n    except Exception as e:\n        print(f\"   ❌ Error uploading documents: {e}\")\n\ntotal_time = time.time() - total_start\n\nprint(f\"\\n{'='*80}\")\nprint(\"📊 INDEXING SUMMARY\")\nprint(f\"{'='*80}\")\nprint(f\"Documents processed:  {len(sample_movies)}\")\nprint(f\"Documents indexed:    {len(documents_with_vectors)}\")\nprint(f\"Total time:           {total_time:.2f}s\")\n\nif documents_with_vectors:\n    print(\"\\n✅ Index populated with sample movie data!\")\n    print(f\"   Variable 'documents_with_vectors' created with {len(documents_with_vectors)} items\")\n    print(f\"   Variable 'chat_client' created for RAG queries (with APIM caching)\")\n    print(\"\\n💡 Note: Embeddings use direct endpoint (no caching needed)\")\n    print(\"         Chat completions use APIM endpoint (with semantic caching)\")\n    print(\"\\n[OK] Step 1.5 Complete - Ready for vector search testing\")\nelse:\n    print(\"\\n⚠️  No documents were indexed\")\n    print(\"   Check embedding generation errors above\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lab 11: Vector Searching - Step 2: Test RAG Pattern\n\nfrom azure.search.documents.models import VectorizedQuery\n\nprint(\"\\n[*] Step 2: Testing vector search with RAG pattern...\")\n\n# Check if we have documents\nif not documents_with_vectors:\n    print(\"\\n⚠️  No documents were indexed in Step 1\")\n    print(\"   Cannot test vector search without indexed documents\")\n    print(\"   Please fix Step 1 embedding generation first\")\nelse:\n    # Sample query\n    query = \"What are the best superhero movies?\"\n\n    print(f\"\\n{'='*80}\")\n    print(\"🔍 TESTING VECTOR SEARCH + RAG PATTERN\")\n    print(f\"{'='*80}\")\n    print(f\"\\nQuery: '{query}'\\n\")\n\n    try:\n        # Step 1: Convert query to embedding\n        print(\"▶️  Step 1: Generating query embedding...\")\n        start_time = time.time()\n        embedding_response = embeddings_client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=query\n        )\n        query_vector = embedding_response.data[0].embedding\n        embedding_time = time.time() - start_time\n        print(f\"   ✅ Query embedding generated ({embedding_time:.2f}s, {len(query_vector)} dimensions)\")\n\n        # Step 2: Vector search\n        print(\"\\n▶️  Step 2: Performing vector search...\")\n        vector_query = VectorizedQuery(\n            vector=query_vector,\n            k_nearest_neighbors=3,\n            fields=\"overview_vector\"  # FIXED: Match field name\n        )\n\n        start_time = time.time()\n        results = search_client.search(\n            search_text=None,\n            vector_queries=[vector_query],\n            select=[\"id\", \"title\", \"genre\", \"overview\"]\n        )\n        search_time = time.time() - start_time\n\n        # Collect results\n        search_results = []\n        for result in results:\n            search_results.append({\n                'title': result['title'],\n                'genre': result['genre'],\n                'overview': result['overview'],\n                'score': result['@search.score']\n            })\n\n        print(f\"   ✅ Vector search complete ({search_time:.2f}s)\")\n        print(f\"   Found {len(search_results)} relevant movies\\n\")\n\n        # Display results\n        if search_results:\n            print(\"   Top Matches:\")\n            for i, r in enumerate(search_results, 1):\n                print(f\"   {i}. {r['title']} (Score: {r['score']:.4f})\")\n                print(f\"      Genre: {r['genre']}\")\n                print(f\"      Overview: {r['overview'][:80]}...\\n\")\n\n            # Step 3: RAG - Use search results as context for LLM\n            print(\"\\n▶️  Step 3: Generating answer with RAG pattern...\")\n\n            # Build context from search results\n            context = \"\\n\\n\".join([\n                f\"Movie: {r['title']}\\n\"\n                f\"Genre: {r['genre']}\\n\"\n                f\"Overview: {r['overview']}\"\n                for r in search_results\n            ])\n\n            # Call LLM with context\n            start_time = time.time()\n            response = chat_client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a helpful movie recommendation assistant. Use the provided movie context to answer questions.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Context (from vector search):\\n{context}\\n\\nQuestion: {query}\"\n                    }\n                ],\n                max_tokens=300\n            )\n            llm_time = time.time() - start_time\n\n            answer = response.choices[0].message.content\n\n            print(f\"   ✅ Answer generated ({llm_time:.2f}s)\\n\")\n\n            # Display RAG result\n            print(f\"{'='*80}\")\n            print(\"🎬 RAG ANSWER\")\n            print(f\"{'='*80}\")\n            print(f\"\\n{answer}\\n\")\n\n            print(f\"{'='*80}\")\n            print(\"📊 PERFORMANCE METRICS\")\n            print(f\"{'='*80}\")\n            print(f\"Query Embedding Time: {embedding_time:.2f}s\")\n            print(f\"Vector Search Time:   {search_time:.2f}s\")\n            print(f\"LLM Generation Time:  {llm_time:.2f}s\")\n            print(f\"Total Time:           {embedding_time + search_time + llm_time:.2f}s\")\n\n            print(\"\\n[OK] Step 2 Complete - RAG pattern successful\")\n        else:\n            print(\"\\n⚠️  No search results found\")\n            print(\"   This might mean the index is empty or query didn't match\")\n\n    except Exception as e:\n        print(f\"\\n❌ Error during vector search: {e}\")\n        print(f\"\\n💡 Troubleshooting:\")\n        print(\"   1. Check if semantic caching policy is applied: az apim api policy show\")\n        print(\"   2. Wait 60 seconds after policy application for propagation\")\n        print(\"   3. Verify embeddings backend is configured correctly\")\n        print(f\"   4. Test standalone: semantic-caching-standalone.ipynb\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎉 LAB 11 COMPLETE: VECTOR SEARCHING + RAG\")\nprint(\"=\"*80)\nprint(\"\\nWhat you learned:\")\nprint(\"✅ How to create vector search indexes in Azure AI Search\")\nprint(\"✅ How to generate embeddings via APIM\")\nprint(\"✅ How to perform vector similarity search\")\nprint(\"✅ How to implement RAG (Retrieval-Augmented Generation)\")\nprint(\"\\nKey Benefits:\")\nprint(\"🔍 Semantic Search: Find content by meaning, not just keywords\")\nprint(\"🎯 RAG Pattern: Provide relevant context to improve LLM answers\")\nprint(\"📊 Better Answers: Grounded in your actual data\")\nprint(\"💰 Cost Efficient: Only retrieve what's needed\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell_34_8b15779b",
   "metadata": {},
   "source": [
    "The following labs (01-10) cover essential Azure API Management features for AI workloads:\n",
    "\n",
    "- **Lab 01:** Zero to Production - Foundation setup and basic chat completion\n",
    "- **Lab 02:** Backend Pool Load Balancing - Multi-region routing and failover\n",
    "- **Lab 03:** Built-in Logging - Observability with Log Analytics and App Insights\n",
    "- **Lab 04:** Token Metrics Emitting - Cost monitoring and capacity planning\n",
    "- **Lab 05:** Token Rate Limiting - Quota management and abuse prevention\n",
    "- **Lab 06:** Access Controlling - OAuth 2.0 and Entra ID authentication\n",
    "- **Lab 07:** Content Safety - Harmful content detection and filtering\n",
    "- **Lab 08:** Model Routing - Intelligent model selection by criteria\n",
    "- **Lab 09:** AI Foundry SDK - Advanced AI capabilities and model catalog\n",
    "- **Lab 10:** AI Foundry DeepSeek - Open-source reasoning model integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_35_0a0d7ce7",
   "metadata": {},
   "source": [
    "<a id='lab01'></a>\n",
    "\n",
    "## Lab 01: Zero to Production\n",
    "\n",
    "![flow](./images/GPT-4o-inferencing.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Learn the fundamentals of deploying and testing Azure OpenAI through API Management, establishing the foundation for all advanced labs.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Basic Chat Completion:** Send prompts to GPT-4o-mini and receive AI-generated responses\n",
    "- **Streaming Responses:** Handle real-time streaming output for better user experience\n",
    "- **Request Patterns:** Understand the HTTP request/response cycle through APIM gateway\n",
    "- **API Key Management:** Secure API access using APIM subscription keys\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](./images/zero-to-production-result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Basic chat completion returns valid responses\n",
    "- Streaming works correctly with incremental tokens\n",
    "- Multiple requests complete successfully\n",
    "- Response times are < 2 seconds for simple prompts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_36_1f195b0a",
   "metadata": {},
   "source": [
    "### Test 1: Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_37_7bb1f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LAB 02: Token Metrics (OpenAI API Monitoring)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 02: Token Metrics Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(f\"[warn] master-lab.env not found, using existing environment variables\")\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend ID: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Resource Group: {resource_group}\")\n",
    "print(f\"[policy] APIM Service: {apim_service_name}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Token metrics policy with API-KEY authentication\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "        <azure-openai-emit-token-metric namespace=\"openai\">\n",
    "            <dimension name=\"Subscription ID\" value=\"@(context.Subscription.Id)\" />\n",
    "            <dimension name=\"Client IP\" value=\"@(context.Request.IpAddress)\" />\n",
    "            <dimension name=\"API ID\" value=\"@(context.Api.Id)\" />\n",
    "            <dimension name=\"User ID\" value=\"@(context.Request.Headers.GetValueOrDefault(&quot;x-user-id&quot;, &quot;N/A&quot;))\" />\n",
    "        </azure-openai-emit-token-metric>\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying token-metrics via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Metrics will be available in Azure Monitor\")\n",
    "print(\"[NEXT] Run the cells below to test token metrics\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a6f1e-6fdb-47d4-9e9c-258e2be31dae",
   "metadata": {},
   "source": [
    "## Lab 01: Test 1 - Basic Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_39_0478a7f3",
   "metadata": {},
   "source": [
    "### Test 2: Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_40_4d2ace70",
   "metadata": {},
   "outputs": [],
   "source": "# Reinitialize OpenAI client (overwritten by Cosmos DB cells)\nfrom openai import AzureOpenAI\n\nclient = AzureOpenAI(\n    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n    api_key=apim_api_key,\n    api_version=\"2024-08-01-preview\"\n)\n\n# Lab 01: Test 2 - Streaming Response (robust with fallback)\n\nprint('[*] Testing streaming...')\n\nprompt_messages = [\n    {'role': 'system', 'content': 'You are a helpful assistant. Stream numbers.'},\n    {'role': 'user', 'content': 'Count from 1 to 5'}\n]\n\ndef stream_completion():\n    return client.chat.completions.create(\n        model='gpt-4o-mini',\n        messages=prompt_messages,\n        max_tokens=32,\n        temperature=0.2,\n        stream=True\n    )\n\ndef non_stream_completion():\n    return client.chat.completions.create(\n        model='gpt-4o-mini',\n        messages=prompt_messages,\n        max_tokens=32,\n        temperature=0.2\n    )\n\ntry:\n    stream = stream_completion()\n    had_output = False\n    for chunk in stream:\n        try:\n            # Support both delta.content and delta with list of content parts\n            if chunk.choices:\n                delta = getattr(chunk.choices[0], 'delta', None)\n                if delta:\n                    piece = getattr(delta, 'content', None)\n                    if piece:\n                        print(piece, end='', flush=True)\n                        had_output = True\n        except Exception:\n            # Ignore malformed chunk pieces\n            pass\n    if not had_output:\n        print('[WARN] Stream yielded no incremental content; backend may not support streaming through APIM.')\n        raise RuntimeError('Empty stream')\n    print()  # newline after stream\n    print('[OK] Streaming works!')\nexcept Exception as e:\n    msg = str(e)\n    if '500' in msg or 'Internal server error' in msg:\n        print(f'[WARN] Streaming failed with backend 500: {msg[:140]}')\n        print('[INFO] Falling back to non-streaming completion (APIM policy or backend may not support streaming).')\n        try:\n            resp = non_stream_completion()\n            try:\n                full = resp.choices[0].message.content\n            except AttributeError:\n                full = resp.choices[0].message.get('content', '')\n            print(full)\n            print('[OK] Fallback non-streaming completion succeeded.')\n        except Exception as e2:\n            print(f'[ERROR] Fallback non-streaming also failed: {e2}')\n            print('[HINT] Check APIM policies (buffering, rewrite), model deployment name, and gateway trace.')\n    else:\n        print(f'[ERROR] Streaming exception: {msg}')\n        print('[HINT] If this persists, verify the APIM operation allows streaming and the backend model supports it.')"
  },
  {
   "cell_type": "markdown",
   "id": "cell_41_d7ea554a",
   "metadata": {},
   "source": [
    "### Test 3: Multiple Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_43_67b478de",
   "metadata": {},
   "source": [
    "<a id='lab02'></a>\n",
    "\n",
    "## Lab 02: Backend Pool Load Balancing\n",
    "\n",
    "![Backend Pool Load Balancing](./images/backend-pool-load-balancing.gif)\n",
    "\n",
    "📖 [Workshop Guide](https://azure-samples.github.io/AI-Gateway/docs/azure-openai/dynamic-failover)\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Master multi-region load balancing with priority-based routing and automatic failover across Azure regions.\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Priority Routing:** Configure priority 1 (UK South) with fallback to priority 2 regions\n",
    "- **Round-Robin Distribution:** Balance traffic across Sweden Central and West Europe (50/50 weight)\n",
    "- **Automatic Retry:** APIM retries on HTTP 429 (rate limit) transparently\n",
    "- **Regional Headers:** Track which region served each request via `x-ms-region` header\n",
    "- **Performance Analysis:** Visualize response times and regional distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Backend Pool Configuration\n",
    "\n",
    "Azure API Management supports three load balancing strategies:\n",
    "\n",
    "<details>\n",
    "<summary><b>1. Round-Robin Distribution</b></summary>\n",
    "\n",
    "Distributes requests evenly across all backends with equal weight.\n",
    "\n",
    "**Configuration:**\n",
    "- All backends have the same priority level\n",
    "- Equal weight distribution (or default weights)\n",
    "- Requests rotate sequentially through backends\n",
    "\n",
    "**Use Case:** When all regions have equal capacity and you want even distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>2. Priority-Based Routing</b></summary>\n",
    "\n",
    "Lower priority values receive traffic first, with automatic failover to higher priority backends.\n",
    "\n",
    "**Example Configuration:**\n",
    "- **East US:** Priority 1 (primary)\n",
    "- **West US:** Priority 2 (fallback)\n",
    "- **Sweden Central:** Priority 3 (fallback)\n",
    "\n",
    "**Use Case:** When you have a preferred region for latency or cost reasons.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>3. Weighted Load Balancing</b></summary>\n",
    "\n",
    "Assigns different traffic proportions within the same priority level.\n",
    "\n",
    "**Example Configuration:**\n",
    "- **East US:** Priority 1, Weight 100\n",
    "- **West US:** Priority 2, Weight 50\n",
    "- **Sweden Central:** Priority 2, Weight 50\n",
    "\n",
    "When Priority 1 is unavailable, traffic splits 50/50 between Priority 2 backends.\n",
    "\n",
    "**Use Case:** When backends have different capacities or you want controlled traffic distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Circuit Breaker Configuration\n",
    "\n",
    "> **💡 Tip:** Each backend should have a circuit breaker rule to handle failures gracefully.\n",
    "\n",
    "**Recommended Settings:**\n",
    "- **Failure Count:** 1 (trip after single failure)\n",
    "- **Failure Interval:** 5 minutes\n",
    "- **Custom Range:** HTTP 429 (rate limit)\n",
    "- **Trip Duration:** 1 minute\n",
    "- **Retry-After Header:** Enabled\n",
    "\n",
    "This configuration ensures that when a backend hits its rate limit (HTTP 429), APIM automatically routes traffic to other backends for 1 minute.\n",
    "\n",
    "---\n",
    "\n",
    "### Monitoring Regional Distribution\n",
    "\n",
    "> **⚠️ Note:** The `x-ms-region` header in responses indicates which backend processed the request.\n",
    "\n",
    "This header allows you to:\n",
    "- Verify load distribution patterns\n",
    "- Monitor failover behavior\n",
    "- Analyze regional performance\n",
    "- Debug routing issues\n",
    "\n",
    "**Example Response Headers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_44_f7e0fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LAB 03: Load Balancing with Retry Logic\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 03: Load Balancing Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend Pool: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Load balancing policy with API-KEY authentication and retry logic\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\"\n",
    "               condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n",
    "            <forward-request buffer-request-body=\"true\" />\n",
    "        </retry>\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "        <choose>\n",
    "            <when condition=\"@(context.Response.StatusCode == 503)\">\n",
    "                <return-response>\n",
    "                    <set-status code=\"503\" reason=\"Service Unavailable\" />\n",
    "                    <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "                        <value>application/json</value>\n",
    "                    </set-header>\n",
    "                    <set-body>{{\"error\": {{\"code\": \"ServiceUnavailable\", \"message\": \"Service temporarily unavailable\"}}}}</set-body>\n",
    "                </return-response>\n",
    "            </when>\n",
    "        </choose>\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying load-balancing via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Load balancing will distribute requests across backend pool\")\n",
    "print(\"[INFO] Retry logic will handle 429 (rate limit) and 503 (unavailable) errors\")\n",
    "print(\"[NEXT] Run load balancing tests in cells below\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27471e-d03d-4561-b403-25849ca3ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Create Backend Pool for Load Balancing (Preview API)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIX: Creating Backend Pool for Round-Robin Load Balancing (Preview API)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from azure.mgmt.apimanagement import ApiManagementClient\n",
    "from azure.mgmt.apimanagement.models import BackendContract\n",
    "import requests, json\n",
    "\n",
    "apim_client = ApiManagementClient(credential, subscription_id)\n",
    "\n",
    "resource_suffix = 'pavavy6pu5hpa'\n",
    "backends_config = [\n",
    "    {'id': 'foundry1', 'url': f'https://foundry1-{resource_suffix}.openai.azure.com/openai', 'location': 'uksouth', 'priority': 1, 'weight': 1},\n",
    "    {'id': 'foundry2', 'url': f'https://foundry2-{resource_suffix}.openai.azure.com/openai', 'location': 'eastus', 'priority': 1, 'weight': 1},\n",
    "    {'id': 'foundry3', 'url': f'https://foundry3-{resource_suffix}.openai.azure.com/openai', 'location': 'norwayeast', 'priority': 1, 'weight': 1},\n",
    "]\n",
    "\n",
    "print(\"[*] Step 1: Ensuring individual backends...\")\n",
    "backend_arm_ids = []\n",
    "for cfg in backends_config:\n",
    "    bid = cfg['id']\n",
    "    try:\n",
    "        apim_client.backend.get(resource_group, apim_service_name, bid)\n",
    "        print(f\"  [OK] Backend '{bid}' exists\")\n",
    "    except Exception:\n",
    "        print(f\"  [*] Creating backend '{bid}'...\")\n",
    "        backend = BackendContract(\n",
    "            url=cfg['url'],\n",
    "            protocol=\"http\",\n",
    "            description=f\"Azure OpenAI - {cfg['location']}\",\n",
    "            tls={\"validateCertificateChain\": True, \"validateCertificateName\": True}\n",
    "        )\n",
    "        try:\n",
    "            apim_client.backend.create_or_update(resource_group, apim_service_name, bid, backend)\n",
    "            print(f\"  [OK] Backend '{bid}' created\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Backend create failed '{bid}': {str(e)[:160]}\")\n",
    "            continue\n",
    "    backend_arm_ids.append({\n",
    "        'id': f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/backends/{bid}\",\n",
    "        'priority': cfg['priority'],\n",
    "        'weight': cfg['weight']\n",
    "    })\n",
    "\n",
    "print(\"\\n[*] Step 2: Ensuring backend POOL (preview)...\")\n",
    "POOL_API_VERSION = \"2023-05-01-preview\"\n",
    "pool_id = \"inference-backend-pool\"\n",
    "services = [{\"id\": b['id'], \"priority\": b['priority'], \"weight\": b['weight']} for b in backend_arm_ids]\n",
    "\n",
    "# Build URL with preview version (must match exactly)\n",
    "pool_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/\"\n",
    "    f\"Microsoft.ApiManagement/service/{apim_service_name}/backends/{pool_id}?api-version={POOL_API_VERSION}\"\n",
    ")\n",
    "\n",
    "# Check if pool already exists\n",
    "try:\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    existing_resp = requests.get(pool_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    pool_body = {\n",
    "        \"properties\": {\n",
    "            \"description\": \"Round-robin load balancer (equal priority=1, weight=1 for all backends)\",\n",
    "            \"type\": \"Pool\",\n",
    "            \"pool\": {\"services\": services}\n",
    "        }\n",
    "    }\n",
    "    if existing_resp.status_code == 200:\n",
    "        print(f\"  [OK] Pool '{pool_id}' exists - updating to round-robin configuration...\")\n",
    "    else:\n",
    "        print(f\"  [*] Pool '{pool_id}' not found (status {existing_resp.status_code}); creating...\")\n",
    "    \n",
    "    put_resp = requests.put(\n",
    "        pool_url,\n",
    "        headers={\"Authorization\": f\"Bearer {token.token}\", \"Content-Type\": \"application/json\"},\n",
    "        json=pool_body,\n",
    "        timeout=60\n",
    "    )\n",
    "    if put_resp.status_code in (200, 201):\n",
    "        print(f\"  [OK] Pool '{pool_id}' configured for round-robin (status {put_resp.status_code})\")\n",
    "    else:\n",
    "        print(f\"  [ERROR] Pool create/update failed: {put_resp.status_code}\")\n",
    "        try:\n",
    "            print(json.dumps(put_resp.json(), indent=2)[:1500])\n",
    "        except Exception:\n",
    "            print(put_resp.text[:1500])\n",
    "        if \"Backend Type and Pool properties\" in put_resp.text:\n",
    "            print(\"  [HINT] Preview feature may not be enabled in this region or API version mismatch.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Exception during pool ensure: {str(e)[:200]}\")\n",
    "\n",
    "# Final verification GET\n",
    "try:\n",
    "    verify = requests.get(pool_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    print(\"\\n[*] Verification GET status:\", verify.status_code)\n",
    "    if verify.status_code == 200:\n",
    "        data = verify.json()\n",
    "        services_out = (data.get('properties', {}).get('pool', {}) or {}).get('services', [])\n",
    "        print(f\"  [OK] Pool has {len(services_out)} services:\")\n",
    "        priorities = []\n",
    "        weights = []\n",
    "        for s in services_out:\n",
    "            name = s.get('id','').split('/')[-1]\n",
    "            priority = s.get('priority')\n",
    "            weight = s.get('weight')\n",
    "            priorities.append(priority)\n",
    "            weights.append(weight)\n",
    "            print(f\"    - {name}: priority={priority}, weight={weight}\")\n",
    "        \n",
    "        # Verify round-robin configuration\n",
    "        if len(set(priorities)) == 1 and len(set(weights)) == 1:\n",
    "            print(f\"  ✓ ROUND-ROBIN CONFIRMED: all backends have priority={priorities[0]}, weight={weights[0]}\")\n",
    "        else:\n",
    "            print(f\"  ⚠ NOT ROUND-ROBIN: priorities={priorities}, weights={weights}\")\n",
    "    else:\n",
    "        print(\"  [WARN] Could not verify pool; status\", verify.status_code)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Verification failed: {str(e)[:160]}\")\n",
    "\n",
    "print(\"\\n[OK] Backend pool configuration complete.\")\n",
    "print(\"[INFO] Expected behavior: ~33% distribution across UK South, East US, Norway East\")\n",
    "print(\"[NEXT] Run Cell 47 to test load balancing distribution\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c52f18-d370-476a-b46d-b250547a8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification Helper (Optional): List all backends to confirm pool presence\n",
    "import requests, json\n",
    "POOL_API_VERSION = \"2023-05-01-preview\"\n",
    "list_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/\"\n",
    "    f\"Microsoft.ApiManagement/service/{apim_service_name}/backends?api-version={POOL_API_VERSION}\"\n",
    ")\n",
    "try:\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    r = requests.get(list_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    print(\"[LIST] status:\", r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        items = r.json().get('value', [])\n",
    "        print(f\"[LIST] {len(items)} backends returned (including pool if successful):\")\n",
    "        for it in items:\n",
    "            pid = it.get('name') or it.get('id','').split('/')[-1]\n",
    "            ptype = it.get('properties', {}).get('type', 'Standard')\n",
    "            if ptype == 'Pool':\n",
    "                services = (it.get('properties', {}).get('pool', {}) or {}).get('services', [])\n",
    "                print(f\"  [POOL] {pid}: services={len(services)}\")\n",
    "            else:\n",
    "                print(f\"  [BACKEND] {pid}: type={ptype}\")\n",
    "    else:\n",
    "        print(r.text[:800])\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Backend list failed:\", str(e)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_45_7d2cb75c",
   "metadata": {},
   "source": [
    "### Test 1: Load Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_46_c665adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing load balancing across 3 regions...')\n",
    "responses = []\n",
    "regions = []  # Track which region processed each request\n",
    "backend_ids = []  # Track which backend served each request\n",
    "\n",
    "# Resolve required variables (avoid NameError)\n",
    "apim_gateway_url = (\n",
    "    (step1_outputs.get('apimGatewayUrl') if isinstance(step1_outputs, dict) else None) or\n",
    "    os.environ.get('APIM_GATEWAY_URL')\n",
    ")\n",
    "inference_api_path = (\n",
    "    (step2_outputs.get('inferenceAPIPath') if isinstance(step2_outputs, dict) else None) or\n",
    "    os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    ")\n",
    "apim_api_key = (\n",
    "    (step1_outputs.get('apimSubscriptions', [{}])[0].get('key') if isinstance(step1_outputs, dict) else None) or\n",
    "    os.environ.get('APIM_API_KEY')\n",
    ")\n",
    "api_version = os.environ.get('OPENAI_API_VERSION', '2024-06-01')\n",
    "\n",
    "missing = [n for n, v in {\n",
    "    'apim_gateway_url': apim_gateway_url,\n",
    "    'inference_api_path': inference_api_path,\n",
    "    'apim_api_key': apim_api_key,\n",
    "    'api_version': api_version\n",
    "}.items() if not v]\n",
    "\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing required variables: {', '.join(missing)}\")\n",
    "    print(\"[HINT] Ensure Cell 8 (.env generation) ran and load with: from dotenv import load_dotenv; load_dotenv('master-lab.env')\")\n",
    "    # Abort early to avoid further errors\n",
    "else:\n",
    "    # Use requests library to access HTTP headers (avoid duplicate import)\n",
    "    try:\n",
    "        requests\n",
    "    except NameError:\n",
    "        import requests\n",
    "\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "            url = f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}/openai/deployments/gpt-4o-mini/chat/completions\"\n",
    "            response = requests.post(\n",
    "                url=f\"{url}?api-version={api_version}\",\n",
    "                headers={\n",
    "                    \"api-key\": apim_api_key,\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": f\"Test {i+1}\"}],\n",
    "                    \"max_tokens\": 5\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            responses.append(elapsed)\n",
    "\n",
    "            region = response.headers.get('x-ms-region', 'Unknown')\n",
    "            backend_id = response.headers.get('x-ms-backend-id', 'Unknown')\n",
    "\n",
    "            regions.append(region)\n",
    "            backend_ids.append(backend_id)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Request {i+1}: {elapsed:.2f}s - Region: {region} - Backend: {backend_id}\")\n",
    "            else:\n",
    "                print(f\"Request {i+1}: {elapsed:.2f}s - HTTP {response.status_code} - Region: {region} - Backend: {backend_id}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] Request {i+1} failed: {e}\")\n",
    "            responses.append(0)\n",
    "            regions.append('Error')\n",
    "            backend_ids.append('Error')\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    avg_time = sum(responses) / len(responses) if responses else 0\n",
    "    print(f\"\\nAverage response time: {avg_time:.2f}s\")\n",
    "\n",
    "    from collections import Counter\n",
    "    region_counts = Counter(regions)\n",
    "    print(f\"\\nRegion Distribution:\")\n",
    "    for region, count in region_counts.items():\n",
    "        pct = (count / len(regions) * 100) if regions else 0\n",
    "        print(f\"  {region}: {count} requests ({pct:.1f}%)\")\n",
    "\n",
    "    unknown_count = region_counts.get('Unknown', 0)\n",
    "    if unknown_count == len(regions) and len(regions) > 0:\n",
    "        print('')\n",
    "        print('[INFO] All regions showing as \"Unknown\" - region headers may not be configured in APIM')\n",
    "        print('')\n",
    "        print('📋 TO ADD REGION HEADERS VIA APIM POLICY:')\n",
    "        print('   1. Azure Portal → API Management → APIs → inference-api')\n",
    "        print('   2. Click \"All operations\" → Outbound processing → Add policy')\n",
    "        print('   3. Add this XML to <outbound> section:')\n",
    "        print('')\n",
    "        print('   <set-header name=\"x-ms-region\" exists-action=\"override\">')\n",
    "        print('       <value>@(context.Deployment.Region)</value>')\n",
    "        print('   </set-header>')\n",
    "        print('   <set-header name=\"x-ms-backend-id\" exists-action=\"override\">')\n",
    "        print('       <value>@(context.Request.MatchedParameters.GetValueOrDefault(\"backend-id\", \"unknown\"))</value>')\n",
    "        print('   </set-header>')\n",
    "        print('')\n",
    "        print('   4. Save the policy')\n",
    "        print('')\n",
    "        print('ℹ️  Region detection is informational only - load balancing still works')\n",
    "        print('')\n",
    "\n",
    "# Fallback util if utils.print_ok not available\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Load balancing test complete!')\n",
    "else:\n",
    "    print('[OK] Load balancing test complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_47_c20a7ffc",
   "metadata": {},
   "source": [
    "### Test 2: Visualize Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_48_b37f6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Create DataFrame with response times and regions\n",
    "df = pd.DataFrame({\n",
    "    'Request': range(1, len(responses)+1),\n",
    "    'Time (s)': responses,\n",
    "    'Region': regions\n",
    "})\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Response times with region colors\n",
    "region_colors = {'Unknown': 'gray'}\n",
    "unique_regions = [r for r in set(regions) if r != 'Unknown']\n",
    "color_palette = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for idx, region in enumerate(unique_regions):\n",
    "    region_colors[region] = color_palette[idx % len(color_palette)]\n",
    "\n",
    "colors = [region_colors.get(r, 'gray') for r in regions]\n",
    "\n",
    "ax1.scatter(df['Request'], df['Time (s)'], c=colors, alpha=0.6, s=100)\n",
    "ax1.axhline(y=avg_time, color='r', linestyle='--', label=f'Average: {avg_time:.2f}s')\n",
    "ax1.set_xlabel('Request Number')\n",
    "ax1.set_ylabel('Response Time (s)')\n",
    "ax1.set_title('Load Balancing Response Times by Region')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Create custom legend for regions\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=region_colors[r], label=r) for r in set(regions)]\n",
    "ax1.legend(handles=legend_elements + [plt.Line2D([0], [0], color='r', linestyle='--', label=f'Avg: {avg_time:.2f}s')],\n",
    "          loc='upper right')\n",
    "\n",
    "# Plot 2: Region distribution bar chart\n",
    "region_counts = Counter(regions)\n",
    "regions_list = list(region_counts.keys())\n",
    "counts_list = list(region_counts.values())\n",
    "\n",
    "bars = ax2.bar(regions_list, counts_list, color=[region_colors.get(r, 'gray') for r in regions_list])\n",
    "ax2.set_xlabel('Region')\n",
    "ax2.set_ylabel('Number of Requests')\n",
    "ax2.set_title('Request Distribution Across Regions')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Replaced utils.print_ok (undefined) with a simple confirmation print\n",
    "print('Lab 02 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_49_35b632c0",
   "metadata": {},
   "source": [
    "Implement comprehensive observability using Azure Log Analytics and Application Insights for AI gateway monitoring.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Log Analytics Integration:** Automatic logging of all APIM requests and responses\n",
    "- **Application Insights:** Track performance metrics, failures, and dependencies\n",
    "- **Diagnostic Settings:** Configure what data to log and where to send it\n",
    "- **Query Language (KQL):** Write queries to analyze request patterns\n",
    "- **Dashboard Creation:** Build monitoring dashboards for AI gateway operations\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- All API requests logged to Log Analytics workspace\n",
    "- Application Insights captures latency metrics\n",
    "- KQL queries return request data successfully\n",
    "- Can trace individual requests end-to-end\n",
    "- Dashboards show real-time gateway health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_51_7bbce6e3",
   "metadata": {},
   "source": [
    "<a id='lab04'></a>\n",
    "\n",
    "## Lab 04: Token Metrics Emitting\n",
    "\n",
    "![flow](./images/ai-gateway.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Track and emit token usage metrics for cost monitoring and capacity planning across all AI requests.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Token Counting:** Capture prompt tokens, completion tokens, and total tokens\n",
    "- **Custom Metrics:** Emit token metrics to Application Insights\n",
    "- **Cost Calculation:** Understand token-based pricing and cost attribution\n",
    "- **Usage Patterns:** Analyze token consumption trends over time\n",
    "- **Quota Management:** Track usage against allocated quotas\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](./images/token-metrics-result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Token metrics logged for every request\n",
    "- Custom Application Insights metrics show token usage\n",
    "- Can query total tokens consumed per time period\n",
    "- Cost estimates available based on token pricing\n",
    "- Alerts configured for unusual token consumption\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_52_d2e70a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 04 token usage aggregation (auto-initialize client if missing)\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - using existing environment\")\n",
    "\n",
    "total_tokens = 0\n",
    "\n",
    "# Resolve required endpoint pieces from previously loaded deployment outputs / env\n",
    "apim_gateway_url = (\n",
    "    (step1_outputs.get('apimGatewayUrl') if isinstance(step1_outputs, dict) else None)\n",
    "    or os.environ.get('APIM_GATEWAY_URL')\n",
    ")\n",
    "inference_api_path = (\n",
    "    (step2_outputs.get('inferenceAPIPath') if isinstance(step2_outputs, dict) else None)\n",
    "    or os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    ")\n",
    "apim_api_key = None\n",
    "if isinstance(step1_outputs, dict):\n",
    "    subs = step1_outputs.get('apimSubscriptions') or []\n",
    "    if subs and isinstance(subs[0], dict):\n",
    "        apim_api_key = subs[0].get('key')\n",
    "if not apim_api_key:\n",
    "    apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "\n",
    "api_version = os.environ.get('OPENAI_API_VERSION', '2024-06-01')\n",
    "\n",
    "missing = [n for n, v in {\n",
    "    'apim_gateway_url': apim_gateway_url,\n",
    "    'inference_api_path': inference_api_path,\n",
    "    'apim_api_key': apim_api_key\n",
    "}.items() if not v]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required values for client init: {', '.join(missing)}. \"\n",
    "                       f\"Ensure earlier environment/deployment cells have been run.\")\n",
    "\n",
    "# Initialize AzureOpenAI client only if not already present\n",
    "if 'client' not in globals():\n",
    "    try:\n",
    "        # Prefer shim if loaded\n",
    "        if 'get_azure_openai_client' in globals():\n",
    "            client = get_azure_openai_client(\n",
    "                azure_endpoint=f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}\",\n",
    "                api_key=apim_api_key,\n",
    "                api_version=api_version\n",
    "            )\n",
    "        else:\n",
    "            from openai import AzureOpenAI\n",
    "            client = AzureOpenAI(\n",
    "                azure_endpoint=f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}\",\n",
    "                api_key=apim_api_key,\n",
    "                api_version=api_version\n",
    "            )\n",
    "        print(\"[init] AzureOpenAI client initialized\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"[ERROR] openai package not found. Install dependencies first.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize AzureOpenAI client: {e}\")\n",
    "\n",
    "# Perform multiple requests and sum token usage\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{'role': 'user', 'content': 'Tell me about AI'}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2,\n",
    "            extra_headers={'api-key': apim_api_key}  # APIM expects key in api-key header\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Request {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Robust usage extraction (handles SDK variations)\n",
    "    tokens = 0\n",
    "    usage_obj = getattr(response, 'usage', None)\n",
    "    if usage_obj is not None:\n",
    "        # New SDK: usage fields may be attributes\n",
    "        tokens = getattr(usage_obj, 'total_tokens', None)\n",
    "        if tokens is None and isinstance(usage_obj, dict):\n",
    "            tokens = usage_obj.get('total_tokens')\n",
    "    if tokens is None:\n",
    "        # Fallback: sum prompt + completion if available\n",
    "        prompt_t = getattr(usage_obj, 'prompt_tokens', None) if usage_obj else None\n",
    "        completion_t = getattr(usage_obj, 'completion_tokens', None) if usage_obj else None\n",
    "        if isinstance(usage_obj, dict):\n",
    "            prompt_t = prompt_t or usage_obj.get('prompt_tokens')\n",
    "            completion_t = completion_t or usage_obj.get('completion_tokens')\n",
    "        if prompt_t is not None and completion_t is not None:\n",
    "            tokens = prompt_t + completion_t\n",
    "    if tokens is None:\n",
    "        tokens = 0  # default if usage unavailable\n",
    "\n",
    "    total_tokens += tokens\n",
    "    print(f\"Request {i+1}: {tokens} tokens\")\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")\n",
    "print(\"[OK] Lab 04 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_68_6a12cc5e",
   "metadata": {},
   "source": [
    "<a id='lab07'></a>\n",
    "\n",
    "## Lab 07: Content Safety\n",
    "\n",
    "![flow](./images/content-safety.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Azure AI Content Safety to automatically detect and block harmful, offensive, or inappropriate content in AI prompts and responses.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Content Safety Policy:** Apply the llm-content-safety policy to AI endpoints\n",
    "- **Harmful Content Detection:** Identify violence, hate speech, sexual content, and self-harm\n",
    "- **Severity Thresholds:** Configure sensitivity levels (low, medium, high)\n",
    "- **Automated Blocking:** Return HTTP 403 when harmful content detected\n",
    "- **Prompt Filtering:** Scan prompts before sending to backend LLM\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- Harmful prompts blocked with HTTP 403 Forbidden\n",
    "- Safe prompts processed normally\n",
    "- Content Safety policy correctly integrated with APIM\n",
    "- Severity thresholds can be adjusted\n",
    "- Detailed error messages explain why content was blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_69_39cd0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 07 Content Safety Test (adds JWT auth if required by current APIM policy)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - using existing environment\")\n",
    "\n",
    "\n",
    "# Get API key for APIM\n",
    "apim_api_key = os.environ.get(\"APIM_API_KEY\", \"\")\n",
    "\n",
    "def _get_jwt_token():\n",
    "    # Reuse existing jwt_token if earlier cell created it\n",
    "    if 'jwt_token' in globals() and jwt_token:\n",
    "        return jwt_token\n",
    "    try:\n",
    "        cred = DefaultAzureCredential()\n",
    "        tok = cred.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return tok.token\n",
    "    except Exception as _e:\n",
    "        print(f'[auth] WARN: Unable to acquire JWT token ({_e}); proceeding without it.')\n",
    "        return None\n",
    "\n",
    "_jwt = _get_jwt_token()\n",
    "extra_headers = {}\n",
    "if _jwt:\n",
    "    extra_headers['Authorization'] = f'Bearer {_jwt}'\n",
    "\n",
    "def safe_call(messages):\n",
    "    \"\"\"Call helper that retries with JWT if first attempt 401 (in case policy just changed).\"\"\"\n",
    "    from openai import AuthenticationError\n",
    "    try:\n",
    "        return client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=messages,\n",
    "            max_tokens=20,\n",
    "            extra_headers={**extra_headers, 'api-key': apim_api_key} if extra_headers else {'api-key': apim_api_key}\n",
    "        )\n",
    "    except AuthenticationError as e:\n",
    "        msg = str(e)\n",
    "        if '401' in msg and not extra_headers:\n",
    "            # Acquire token and retry once\n",
    "            print('[auth] 401 without JWT; attempting to fetch token and retry...')\n",
    "            new_jwt = _get_jwt_token()\n",
    "            if new_jwt:\n",
    "                return client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages,\n",
    "                    max_tokens=20,\n",
    "                    extra_headers={'Authorization': f'Bearer {new_jwt}'}\n",
    "                )\n",
    "        raise\n",
    "\n",
    "# Test with safe content\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'What is the weather like?'}])\n",
    "    print(f\"Safe content: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Safe content request failed: {e}')\n",
    "\n",
    "# Test with potentially harmful content (expect policy to possibly block)\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'How to harm someone?'}])\n",
    "    print('Content passed (policy did not block or returned sanitized answer)')\n",
    "except Exception as e:\n",
    "    # Could be a 403 from content safety or auth issue\n",
    "    print(f'Content blocked: {e}')\n",
    "\n",
    "# Fallback if utils not available\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Lab 07 Complete!')\n",
    "else:\n",
    "    print('[OK] Lab 07 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_70_caa353d3",
   "metadata": {},
   "source": [
    "<a id='lab08'></a>\n",
    "\n",
    "## Lab 08: Model Routing\n",
    "\n",
    "![flow](./images/ai-gateway.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Implement intelligent request routing to automatically select the best AI model based on criteria like prompt complexity, cost, or performance requirements.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Conditional Routing:** Route to different models based on request properties\n",
    "- **Model Selection Logic:** Choose between GPT-4o, GPT-4o-mini, DeepSeek, etc.\n",
    "- **Cost Optimization:** Route simple queries to cheaper models automatically\n",
    "- **Performance Tuning:** Send complex queries to more capable models\n",
    "- **Header-Based Routing:** Allow clients to specify model preferences\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- Simple prompts routed to GPT-4o-mini (cost-effective)\n",
    "- Complex prompts routed to GPT-4o (high capability)\n",
    "- Custom headers can override default routing\n",
    "- Routing logic is transparent and logged\n",
    "- Cost savings measurable compared to always using premium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_71_0250903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 08: Model Routing test (fixed for Dual Auth + invalid model + 401 handling)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - using existing environment\")\n",
    "\n",
    "import os\n",
    "from openai import AuthenticationError\n",
    "\n",
    "# Ensure DefaultAzureCredential is available even if this cell runs before its import elsewhere.\n",
    "try:\n",
    "    DefaultAzureCredential  # type: ignore\n",
    "except NameError:\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get API key for APIM\n",
    "apim_api_key = os.environ.get(\"APIM_API_KEY\", \"\")\n",
    "\n",
    "# Acquire JWT (audience: https://cognitiveservices.azure.com) – may be required with APIM dual auth.\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "except Exception as e:\n",
    "    jwt_token = None\n",
    "    print(f\"[auth] WARN: Unable to acquire JWT token: {e}\")\n",
    "\n",
    "extra_headers = {}\n",
    "if jwt_token:\n",
    "    extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "# Only test models that are actually deployed. gpt-4.1-mini not deployed; skip automatically.\n",
    "requested_models = ['gpt-4o-mini', 'gpt-4.1-nano']  # FIXED: Changed to gpt-4.1-nano (deployed in cell 28)\n",
    "available_models = {'gpt-4o-mini', 'gpt-4o', 'gpt-4.1-nano', 'text-embedding-3-small', 'text-embedding-3-large', 'dall-e-3'}  # from Step 2 config\n",
    "models_to_test = [m for m in requested_models if m in available_models]\n",
    "\n",
    "if len(models_to_test) != len(requested_models):\n",
    "    missing = [m for m in requested_models if m not in models_to_test]\n",
    "    print(f\"[routing] Skipping unavailable models: {', '.join(missing)}\")\n",
    "\n",
    "# Guard if OpenAI client is not yet defined (e.g., cell ordering)\n",
    "if 'client' not in globals():\n",
    "    print(\"[WARN] OpenAI client 'client' not found; skipping model tests.\")\n",
    "    models_to_test = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"[*] Testing model: {model}\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "            max_tokens=10,\n",
    "            extra_headers={**extra_headers, 'api-key': apim_api_key} if extra_headers else {'api-key': apim_api_key}\n",
    "        )\n",
    "        # Robust content extraction\n",
    "        content = \"\"\n",
    "        try:\n",
    "            content = response.choices[0].message.content\n",
    "        except AttributeError:\n",
    "            if hasattr(response.choices[0].message, 'get'):\n",
    "                content = response.choices[0].message.get('content', '')\n",
    "        print(f\"Model {model}: {content}\")\n",
    "    except AuthenticationError as e:\n",
    "        # Attempt one silent JWT refresh if first attempt lacked/invalid token\n",
    "        if not jwt_token:\n",
    "            print(f\"[auth] 401 without JWT; attempting token fetch & retry...\")\n",
    "            try:\n",
    "                credential = DefaultAzureCredential()\n",
    "                jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "                extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "                retry_resp = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "                    max_tokens=10,\n",
    "                    extra_headers=extra_headers\n",
    "                )\n",
    "                retry_content = \"\"\n",
    "                try:\n",
    "                    retry_content = retry_resp.choices[0].message.content\n",
    "                except AttributeError:\n",
    "                    if hasattr(retry_resp.choices[0].message, 'get'):\n",
    "                        retry_content = retry_resp.choices[0].message.get('content', '')\n",
    "                print(f\"Model {model} (retry): {retry_content}\")\n",
    "                continue\n",
    "            except Exception as e2:\n",
    "                print(f\"[ERROR] Retry after acquiring JWT failed: {e2}\")\n",
    "        print(f\"[ERROR] Auth failed for {model}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Request failed for {model}: {e}\")\n",
    "\n",
    "# Safe completion notification without NameError if utils is absent\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Lab 08 Complete!')\n",
    "else:\n",
    "    print('[OK] Lab 08 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_72_62fb5c72",
   "metadata": {},
   "source": [
    "<a id='lab09'></a>\n",
    "\n",
    "## Lab 09: AI Foundry SDK\n",
    "\n",
    "![flow](./images/ai-foundry-sdk.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Azure AI Foundry SDK for advanced AI capabilities including model catalog, evaluations, and agent frameworks.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **AI Foundry Integration:** Connect to AI Foundry projects through APIM\n",
    "- **Model Catalog:** Access diverse AI models beyond Azure OpenAI\n",
    "- **Inference API:** Use unified inference API for multiple model types\n",
    "- **Agent Framework:** Build AI agents with tools and orchestration\n",
    "- **Evaluation Tools:** Assess model performance and quality\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- AI Foundry SDK successfully connects through APIM gateway\n",
    "- Can list available models in the catalog\n",
    "- Inference requests work for different model types\n",
    "- Agent framework tools execute correctly\n",
    "- Evaluation metrics collected and analyzed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_73_cc780c0e",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "▶️ Click `Run All` to execute all steps sequentially, or execute them `Step by Step`...\n",
    "\n",
    "ChatCompletionsClient must use FULL deployment path:\n",
    "  {apim_gateway_url}/{inference_api_path}/openai/deployments/{deployment_name}\n",
    "\n",
    "Reuse imports already loaded in earlier cells (avoid re-import)\n",
    "Variables expected from earlier cells:\n",
    "  apim_gateway_url, inference_api_path, apim_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_75_ce629d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\n",
    "missing_vars = [k for k, v in {\n",
    "    'apim_gateway_url': globals().get('apim_gateway_url'),\n",
    "    'inference_api_path': globals().get('inference_api_path'),\n",
    "    'apim_api_key': globals().get('apim_api_key')\n",
    "}.items() if not v]\n",
    "\n",
    "if missing_vars:\n",
    "    raise RuntimeError(f\"Missing required variables: {', '.join(missing_vars)}. Run the earlier env/config cells first.\")\n",
    "\n",
    "# Normalize endpoint (avoid double slashes)\n",
    "base = apim_gateway_url.rstrip('/')\n",
    "inference_path = inference_api_path.strip('/')\n",
    "\n",
    "inference_endpoint = f\"{base}/{inference_path}/openai/deployments/{deployment_name}\"\n",
    "print(f\"[OK] Inference Endpoint: {inference_endpoint}\")\n",
    "\n",
    "# Acquire JWT if current APIM policy enforces validate-jwt (dual auth or JWT-only)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "jwt_token = None\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Audience used in active APIM policies\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(\"[OK] Acquired JWT token\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Unable to acquire JWT token: {e}\")\n",
    "    print(\"[INFO] Will attempt call with API key only (may fail if JWT required)\")\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "inference_client = ChatCompletionsClient(\n",
    "    endpoint=inference_endpoint,\n",
    "    credential=AzureKeyCredential(apim_api_key)  # use correct API key variable\n",
    ")\n",
    "\n",
    "print(\"[OK] ChatCompletionsClient created successfully\\n\")\n",
    "\n",
    "# Prepare headers: dual auth requires both api-key (handled via AzureKeyCredential) and Authorization\n",
    "call_headers = {}\n",
    "if jwt_token:\n",
    "    call_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "print(\"[*] Testing chat completion with Azure AI Inference SDK...\")\n",
    "try:\n",
    "    response = inference_client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=\"You are helpful.\"),\n",
    "            UserMessage(content=\"What is Azure AI Foundry?\")\n",
    "        ],\n",
    "        headers=call_headers if call_headers else None  # azure-core style header injection\n",
    "    )\n",
    "    print(f\"[SUCCESS] Response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if \"Invalid JWT\" in msg or \"401\" in msg:\n",
    "        print(f\"[ERROR] Authentication failed: {msg}\")\n",
    "        print(\"[HINT] Active APIM policy likely requires a valid JWT. Ensure az login completed or managed identity available.\")\n",
    "        print(\"[HINT] Retry after confirming validate-jwt audiences match https://cognitiveservices.azure.com\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Request failed: {msg}\")\n",
    "        print(\"[HINT] Verify deployment name matches APIM backend path and policy didn't strip /openai segment.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Lab 09 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_77_7dd6b64b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section: MCP Fundamentals\n",
    "\n",
    "Learn MCP basics:\n",
    "- Client initialization\n",
    "- Calling MCP tools\n",
    "- Data retrieval\n",
    "\n",
    "## MCP Server Integration\n",
    "\"\"\"\n",
    "MCP servers are initialized in Cell 11 using MCPClient.\n",
    "\n",
    "The global 'mcp' object provides access to all configured data sources:\n",
    "  - mcp.excel    (Excel Analytics MCP - direct)\n",
    "  - mcp.docs     (Research Documents MCP - direct)\n",
    "  - mcp.github   (GitHub API via APIM)\n",
    "  - mcp.weather  (Weather API via APIM)\n",
    "\n",
    "All configuration is loaded from .mcp-servers-config file.\n",
    "No additional initialization needed in this cell.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MCP SERVER INTEGRATION - LAB 10\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"✓ MCP Client initialized in Cell 11\")\n",
    "print()\n",
    "print(\"Available Data Sources:\")\n",
    "\n",
    "if 'mcp' in globals():\n",
    "    if hasattr(mcp, 'excel') and mcp.excel:\n",
    "        print(\"  ✓ Excel MCP (direct)\")\n",
    "    if hasattr(mcp, 'docs') and mcp.docs:\n",
    "        print(\"  ✓ Docs MCP (direct)\")\n",
    "    if hasattr(mcp, 'github') and mcp.github:\n",
    "        print(\"  ✓ GitHub API (APIM)\")\n",
    "    if hasattr(mcp, 'weather') and mcp.weather:\n",
    "        print(\"  ✓ Weather API (APIM)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"💡 Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\")\n",
    "else:\n",
    "    print(\"⚠️  MCP not initialized. Please run Cell 11 first.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_78_2e777ad7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "MCP servers are initialized in Cell 11 using MCPClient.\n",
    "\n",
    "The global 'mcp' object provides access to all configured data sources:\n",
    "  - mcp.excel    (Excel Analytics MCP - direct)\n",
    "  - mcp.docs     (Research Documents MCP - direct)\n",
    "  - mcp.github   (GitHub API via APIM)\n",
    "  - mcp.weather  (Weather API via APIM)\n",
    "\n",
    "All configuration is loaded from .mcp-servers-config file.\n",
    "No additional initialization needed in this cell.\n",
    "\"\"\"\n",
    "---\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. AI application sends MCP request to APIM\n",
    "2. APIM validates OAuth token and enforces policies\n",
    "3. Request forwarded to MCP server\n",
    "4. MCP server executes tool and returns result\n",
    "5. APIM proxies response back to client\n",
    "6. AI model processes tool result and generates response\n",
    "\n",
    "---\n",
    "\n",
    "### Two MCP Connection Patterns\n",
    "\n",
    "**Important:** This lab uses HTTP-based MCP servers that communicate via POST requests to `/mcp/` endpoints.\n",
    "\n",
    "<details>\n",
    "<summary><b>Pattern 1: HTTP-Based MCP</b> (✅ Used in this notebook)</summary>\n",
    "\n",
    "**How It Works:**\n",
    "- **Protocol:** HTTP POST requests\n",
    "- **Endpoint:** `{server_url}/mcp/`\n",
    "- **Format:** JSON-RPC 2.0\n",
    "- **Communication:** Request/response pattern\n",
    "\n",
    "**Advantages:**\n",
    "- Simple, reliable, works with standard HTTP clients\n",
    "- Easy to test with curl or Postman\n",
    "- Works through standard load balancers and API gateways\n",
    "- No special client libraries required\n",
    "- Firewall-friendly (standard HTTP/HTTPS)\n",
    "\n",
    "**Example Request:**\n",
    "```http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_80_5c80f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Example: Weather API (via APIM)\n",
    "\"\"\"\n",
    "Demonstrates Weather API access through Azure API Management.\n",
    "\n",
    "Features:\n",
    "- Current weather for a city\n",
    "- Multi-city comparison\n",
    "- 5-day forecast\n",
    "- Temperature, conditions, humidity\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WEATHER API EXAMPLE (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.weather:\n",
    "    print(\"❌ Weather API not configured\")\n",
    "    print(\"   Set APIM_WEATHER_URL and OPENWEATHER_API_KEY in .mcp-servers-config\")\n",
    "else:\n",
    "    print(\"\\n1️⃣  CURRENT WEATHER - London\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get weather for London\n",
    "        weather = mcp.weather.get_weather(\"London\", \"GB\")\n",
    "        \n",
    "        print(f\"\\n📍 Location: {weather['name']}, {weather['sys']['country']}\")\n",
    "        print(f\"🌡️  Temperature: {weather['main']['temp']}°C (feels like {weather['main']['feels_like']}°C)\")\n",
    "        print(f\"☁️  Conditions: {weather['weather'][0]['description'].title()}\")\n",
    "        print(f\"💨 Wind: {weather['wind']['speed']} m/s\")\n",
    "        print(f\"💧 Humidity: {weather['main']['humidity']}%\")\n",
    "        print(f\"🔽 Pressure: {weather['main']['pressure']} hPa\")\n",
    "        \n",
    "        print(\"\\n\\n2️⃣  MULTI-CITY COMPARISON\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        cities = [\n",
    "            (\"Paris\", \"FR\"),\n",
    "            (\"New York\", \"US\"),\n",
    "            (\"Tokyo\", \"JP\"),\n",
    "            (\"Sydney\", \"AU\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'City':<15} {'Temp (°C)':<12} {'Conditions':<20} {'Humidity':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for city, country in cities:\n",
    "            try:\n",
    "                w = mcp.weather.get_weather(city, country)\n",
    "                temp = w['main']['temp']\n",
    "                condition = w['weather'][0]['description'].title()\n",
    "                humidity = w['main']['humidity']\n",
    "                print(f\"{city:<15} {temp:<12.1f} {condition:<20} {humidity}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"{city:<15} Error: {str(e)[:40]}\")\n",
    "        \n",
    "        print(\"\\n\\n3️⃣  5-DAY FORECAST - London\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            forecast = mcp.weather.get_forecast(\"London\", \"GB\")\n",
    "            \n",
    "            # Group by day\n",
    "            from datetime import datetime\n",
    "            daily_forecasts = {}\n",
    "            \n",
    "            for item in forecast['list'][:8]:  # Next 24 hours (8 x 3-hour periods)\n",
    "                dt = datetime.fromtimestamp(item['dt'])\n",
    "                day = dt.strftime('%Y-%m-%d')\n",
    "                time = dt.strftime('%H:%M')\n",
    "                \n",
    "                if day not in daily_forecasts:\n",
    "                    daily_forecasts[day] = []\n",
    "                \n",
    "                daily_forecasts[day].append({\n",
    "                    'time': time,\n",
    "                    'temp': item['main']['temp'],\n",
    "                    'condition': item['weather'][0]['description']\n",
    "                })\n",
    "            \n",
    "            for day, forecasts in list(daily_forecasts.items())[:2]:\n",
    "                print(f\"\\n📅 {day}\")\n",
    "                for f in forecasts:\n",
    "                    print(f\"   {f['time']}: {f['temp']:.1f}°C - {f['condition'].title()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Forecast error: {e}\")\n",
    "        \n",
    "        print(\"\\n\\n✅ Weather API examples completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error accessing Weather API: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_81_dabe2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 Example: GitHub API (via APIM)\n",
    "\"\"\"\n",
    "Demonstrates GitHub REST API access through Azure API Management.\n",
    "\n",
    "Features:\n",
    "- Repository details\n",
    "- Statistics (stars, forks, watchers)\n",
    "- Recent activity\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB API EXAMPLE (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"❌ GitHub API not configured\")\n",
    "    print(\"   Set APIM_GITHUB_URL and APIM_SUBSCRIPTION_KEY in .mcp-servers-config\")\n",
    "else:\n",
    "    print(\"\\n1️⃣  REPOSITORY DETAILS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get details for https://github.com/Azure-Samples/AI-Gateway\n",
    "        owner = \"Azure-Samples\"\n",
    "        repo = \"AI-Gateway\"\n",
    "\n",
    "        # Build custom base URL with requested scheme prefix\n",
    "        display_url = f\"https://github.com/{owner}/{repo}\"\n",
    "        print(f\"\\n🔍 Fetching: {display_url}\")\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        \n",
    "        print(f\"\\n📦 Repository: {repo_data['full_name']}\")\n",
    "        print(f\"📝 Description: {repo_data.get('description', 'N/A')}\")\n",
    "        print(f\"🌐 URL: {repo_data['html_url']}\")\n",
    "        print(f\"⭐ Stars: {repo_data['stargazers_count']:,}\")\n",
    "        print(f\"🔱 Forks: {repo_data['forks_count']:,}\")\n",
    "        print(f\"👀 Watchers: {repo_data['watchers_count']:,}\")\n",
    "        print(f\"🐛 Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "        print(f\"📖 Language: {repo_data.get('language', 'N/A')}\")\n",
    "        print(f\"📅 Created: {repo_data['created_at'][:10]}\")\n",
    "        print(f\"🔄 Last Updated: {repo_data['updated_at'][:10]}\")\n",
    "        \n",
    "        if repo_data.get('topics'):\n",
    "            print(f\"🏷️  Topics: {', '.join(repo_data['topics'][:5])}\")\n",
    "        \n",
    "        print(\"\\n\\n2️⃣  RECENT COMMITS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            commits = mcp.github.get_commits(owner, repo, per_page=5)\n",
    "            \n",
    "            print(f\"\\n{'Date':<12} {'Author':<20} {'Message':<50}\")\n",
    "            print(\"-\" * 85)\n",
    "            \n",
    "            for commit in commits[:5]:\n",
    "                commit_data = commit.get('commit', {})\n",
    "                author = commit_data.get('author', {}).get('name', 'Unknown')[:18]\n",
    "                message = commit_data.get('message', '').split('\\n')[0][:48]\n",
    "                date = commit_data.get('author', {}).get('date', '')[:10]\n",
    "                \n",
    "                print(f\"{date:<12} {author:<20} {message:<50}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not fetch commits: {e}\")\n",
    "        \n",
    "        print(\"\\n\\n3️⃣  REPOSITORY STATISTICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Calculate some basic stats\n",
    "        days_old = (\n",
    "            __import__('datetime').datetime.now() - \n",
    "            __import__('datetime').datetime.strptime(repo_data['created_at'][:10], '%Y-%m-%d')\n",
    "        ).days\n",
    "        \n",
    "        stars_per_day = repo_data['stargazers_count'] / max(days_old, 1)\n",
    "        \n",
    "        print(f\"\\n📊 Age: {days_old:,} days\")\n",
    "        print(f\"📈 Stars per day: {stars_per_day:.2f}\")\n",
    "        print(f\"🔥 Fork ratio: {repo_data['forks_count'] / max(repo_data['stargazers_count'], 1):.2%}\")\n",
    "        print(f\"📝 Size: {repo_data.get('size', 0):,} KB\")\n",
    "        \n",
    "        if repo_data.get('license'):\n",
    "            print(f\"⚖️  License: {repo_data['license'].get('name', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n✅ GitHub API examples completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error accessing GitHub API: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_89_c5e4eb3d",
   "metadata": {},
   "source": [
    "### Lab 14: GitHub Repository Access\n",
    "Query GitHub repositories via MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_90_63f87343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub: Search and explore repositories (via APIM)\n",
    "\"\"\"\n",
    "Search GitHub repositories using various criteria:\n",
    "- Language filters\n",
    "- Star count filters\n",
    "- Sort by relevance, stars, or updated date\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB REPOSITORY SEARCH (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"❌ GitHub API not configured\")\n",
    "else:\n",
    "    try:\n",
    "        # Search for AI/ML repositories\n",
    "        search_query = \"machine learning language:python stars:>1000\"\n",
    "        \n",
    "        print(f\"\\n🔍 Search Query: {search_query}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        results = mcp.github.search_repositories(search_query, per_page=10)\n",
    "        \n",
    "        total_count = results.get('total_count', 0)\n",
    "        items = results.get('items', [])\n",
    "        \n",
    "        print(f\"\\n📊 Found {total_count:,} repositories\")\n",
    "        print(f\"📋 Showing top {len(items)} results:\\n\")\n",
    "        \n",
    "        print(f\"{'Rank':<6} {'Stars':<8} {'Repository':<40} {'Language':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for idx, repo in enumerate(items, 1):\n",
    "            stars = f\"{repo['stargazers_count']:,}\"\n",
    "            name = repo['full_name'][:38]\n",
    "            language = repo.get('language', 'N/A')[:10]\n",
    "            \n",
    "            print(f\"{idx:<6} {stars:<8} {name:<40} {language:<12}\")\n",
    "        \n",
    "        # Show detailed info for top repository\n",
    "        if items:\n",
    "            print(\"\\n\\n🏆 TOP RESULT DETAILS\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            top_repo = items[0]\n",
    "            print(f\"\\n📦 {top_repo['full_name']}\")\n",
    "            print(f\"📝 {top_repo.get('description', 'No description')[:100]}\")\n",
    "            print(f\"⭐ Stars: {top_repo['stargazers_count']:,}\")\n",
    "            print(f\"🔱 Forks: {top_repo['forks_count']:,}\")\n",
    "            print(f\"📖 Language: {top_repo.get('language', 'N/A')}\")\n",
    "            print(f\"🔄 Updated: {top_repo['updated_at'][:10]}\")\n",
    "            print(f\"🌐 URL: {top_repo['html_url']}\")\n",
    "        \n",
    "        print(\"\\n\\n✅ GitHub search completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error searching GitHub: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_91_e0849873",
   "metadata": {},
   "source": [
    "### Lab 15: GitHub + AI Code Analysis\n",
    "Analyze repository code using AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_92_4791fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub: Repository analysis (via APIM)\n",
    "\"\"\"\n",
    "Perform deep analysis of a GitHub repository:\n",
    "- Contributor statistics\n",
    "- Issue tracking\n",
    "- Pull request metrics\n",
    "- Language breakdown\n",
    "- Community health\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB REPOSITORY ANALYSIS (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"❌ GitHub API not configured\")\n",
    "else:\n",
    "    try:\n",
    "        # Analyze a popular repository\n",
    "        owner = \"microsoft\"\n",
    "        repo = \"semantic-kernel\"\n",
    "        \n",
    "        print(f\"\\n🔍 Analyzing: {owner}/{repo}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get repository details\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        \n",
    "        print(\"\\n1️⃣  REPOSITORY OVERVIEW\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\n📦 {repo_data['full_name']}\")\n",
    "        print(f\"📝 {repo_data.get('description', 'No description')}\")\n",
    "        print(f\"⭐ Stars: {repo_data['stargazers_count']:,}\")\n",
    "        print(f\"🔱 Forks: {repo_data['forks_count']:,}\")\n",
    "        print(f\"👀 Watchers: {repo_data['watchers_count']:,}\")\n",
    "        print(f\"🐛 Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "        \n",
    "        print(\"\\n2️⃣  RECENT ACTIVITY\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get recent commits\n",
    "        try:\n",
    "            commits = mcp.github.get_commits(owner, repo, per_page=10)\n",
    "            \n",
    "            # Analyze commit patterns\n",
    "            authors = {}\n",
    "            for commit in commits:\n",
    "                author = commit.get('commit', {}).get('author', {}).get('name', 'Unknown')\n",
    "                authors[author] = authors.get(author, 0) + 1\n",
    "            \n",
    "            print(f\"\\n📊 Last 10 commits:\")\n",
    "            print(f\"   Total commits analyzed: {len(commits)}\")\n",
    "            print(f\"   Unique contributors: {len(authors)}\")\n",
    "            print(f\"\\n   Top contributors in recent commits:\")\n",
    "            \n",
    "            for author, count in sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"     • {author}: {count} commit(s)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not analyze commits: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"\\n3️⃣  REPOSITORY HEALTH METRICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Calculate health metrics\n",
    "        import datetime\n",
    "        \n",
    "        created = datetime.datetime.strptime(repo_data['created_at'][:10], '%Y-%m-%d')\n",
    "        updated = datetime.datetime.strptime(repo_data['updated_at'][:10], '%Y-%m-%d')\n",
    "        now = datetime.datetime.now()\n",
    "        \n",
    "        age_days = (now - created).days\n",
    "        days_since_update = (now - updated).days\n",
    "        \n",
    "        stars_per_day = repo_data['stargazers_count'] / max(age_days, 1)\n",
    "        fork_ratio = repo_data['forks_count'] / max(repo_data['stargazers_count'], 1)\n",
    "        \n",
    "        print(f\"\\n📅 Age: {age_days:,} days ({age_days/365:.1f} years)\")\n",
    "        print(f\"🔄 Last updated: {days_since_update} days ago\")\n",
    "        print(f\"📈 Growth: {stars_per_day:.2f} stars/day\")\n",
    "        print(f\"🔱 Fork ratio: {fork_ratio:.2%}\")\n",
    "        \n",
    "        # Activity level\n",
    "        if days_since_update < 7:\n",
    "            activity = \"🟢 Very Active\"\n",
    "        elif days_since_update < 30:\n",
    "            activity = \"🟡 Active\"\n",
    "        elif days_since_update < 90:\n",
    "            activity = \"🟠 Moderate\"\n",
    "        else:\n",
    "            activity = \"🔴 Low Activity\"\n",
    "        \n",
    "        print(f\"🎯 Activity Level: {activity}\")\n",
    "        \n",
    "        print(\"\\n4️⃣  COMMUNITY METRICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get issues for community engagement\n",
    "        try:\n",
    "            issues = mcp.github.get_issues(owner, repo, state='all', per_page=100)\n",
    "            \n",
    "            open_issues = [i for i in issues if i['state'] == 'open']\n",
    "            closed_issues = [i for i in issues if i['state'] == 'closed']\n",
    "            \n",
    "            if issues:\n",
    "                close_rate = len(closed_issues) / len(issues)\n",
    "                print(f\"\\n🐛 Issue Metrics:\")\n",
    "                print(f\"   Total analyzed: {len(issues)}\")\n",
    "                print(f\"   Open: {len(open_issues)}\")\n",
    "                print(f\"   Closed: {len(closed_issues)}\")\n",
    "                print(f\"   Close rate: {close_rate:.1%}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Could not analyze issues: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"\\n5️⃣  REPOSITORY METADATA\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"\\n📖 Primary Language: {repo_data.get('language', 'N/A')}\")\n",
    "        print(f\"📏 Size: {repo_data.get('size', 0):,} KB\")\n",
    "        print(f\"🌳 Default Branch: {repo_data.get('default_branch', 'N/A')}\")\n",
    "        \n",
    "        if repo_data.get('license'):\n",
    "            print(f\"⚖️  License: {repo_data['license'].get('name', 'N/A')}\")\n",
    "        \n",
    "        if repo_data.get('topics'):\n",
    "            print(f\"🏷️  Topics: {', '.join(repo_data['topics'][:8])}\")\n",
    "        \n",
    "        print(f\"\\n🔗 Clone URL: {repo_data.get('clone_url', 'N/A')}\")\n",
    "        print(f\"🌐 Homepage: {repo_data.get('homepage', 'N/A') or 'Not set'}\")\n",
    "        \n",
    "        print(\"\\n\\n✅ GitHub repository analysis completed!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error analyzing repository: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7598c0-d23b-4a99-8811-ea3f7de2594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-MCP AI Aggregation: Cross-Domain Analysis\n",
    "\"\"\"\n",
    "Demonstrates aggregating data from multiple MCP servers and using AI to synthesize insights.\n",
    "\n",
    "This example:\n",
    "1. Fetches GitHub repository data (stars, commits, issues)\n",
    "2. Fetches Weather data for the repository's location\n",
    "3. Combines both datasets\n",
    "4. Sends to Azure OpenAI for cross-domain analysis\n",
    "5. Generates actionable insights\n",
    "\n",
    "This showcases the power of combining multiple data sources through MCP.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-MCP AI AGGREGATION: CROSS-DOMAIN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github or not mcp.weather:\n",
    "    print(\"❌ This example requires both GitHub and Weather APIs\")\n",
    "    if not mcp.github:\n",
    "        print(\"   Missing: GitHub API (APIM)\")\n",
    "    if not mcp.weather:\n",
    "        print(\"   Missing: Weather API (APIM)\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"\\n📊 STEP 1: GATHERING DATA FROM MULTIPLE SOURCES\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Repository to analyze\n",
    "        owner = \"microsoft\"\n",
    "        repo = \"semantic-kernel\"\n",
    "        location_city = \"Seattle\"  # Microsoft headquarters\n",
    "        location_country = \"US\"\n",
    "        \n",
    "        print(f\"\\n1️⃣  Fetching GitHub data for {owner}/{repo}...\")\n",
    "        \n",
    "        # Get GitHub data\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        commits = mcp.github.get_commits(owner, repo, per_page=10)\n",
    "        issues = mcp.github.get_issues(owner, repo, state='all', per_page=20)\n",
    "        \n",
    "        github_summary = {\n",
    "            'repository': repo_data['full_name'],\n",
    "            'description': repo_data.get('description', 'N/A'),\n",
    "            'stars': repo_data['stargazers_count'],\n",
    "            'forks': repo_data['forks_count'],\n",
    "            'open_issues': repo_data['open_issues_count'],\n",
    "            'language': repo_data.get('language', 'N/A'),\n",
    "            'created_at': repo_data['created_at'][:10],\n",
    "            'updated_at': repo_data['updated_at'][:10],\n",
    "            'recent_commits': len(commits),\n",
    "            'total_issues_analyzed': len(issues)\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Repository: {github_summary['repository']}\")\n",
    "        print(f\"   ✓ Stars: {github_summary['stars']:,}\")\n",
    "        print(f\"   ✓ Recent commits: {github_summary['recent_commits']}\")\n",
    "        \n",
    "        print(f\"\\n2️⃣  Fetching Weather data for {location_city}...\")\n",
    "        \n",
    "        # Get Weather data\n",
    "        weather_data = mcp.weather.get_weather(location_city, location_country)\n",
    "        \n",
    "        weather_summary = {\n",
    "            'location': f\"{weather_data['name']}, {weather_data['sys']['country']}\",\n",
    "            'temperature': weather_data['main']['temp'],\n",
    "            'feels_like': weather_data['main']['feels_like'],\n",
    "            'conditions': weather_data['weather'][0]['description'],\n",
    "            'humidity': weather_data['main']['humidity'],\n",
    "            'wind_speed': weather_data['wind']['speed']\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Location: {weather_summary['location']}\")\n",
    "        print(f\"   ✓ Temperature: {weather_summary['temperature']}°C\")\n",
    "        print(f\"   ✓ Conditions: {weather_summary['conditions']}\")\n",
    "        \n",
    "        print(\"\\n\\n🤖 STEP 2: AI-POWERED CROSS-DOMAIN ANALYSIS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Prepare data for AI analysis\n",
    "        combined_data = f\"\"\"\n",
    "Repository Analysis:\n",
    "- Name: {github_summary['repository']}\n",
    "- Description: {github_summary['description']}\n",
    "- Stars: {github_summary['stars']:,}\n",
    "- Forks: {github_summary['forks']:,}\n",
    "- Open Issues: {github_summary['open_issues']:,}\n",
    "- Primary Language: {github_summary['language']}\n",
    "- Created: {github_summary['created_at']}\n",
    "- Last Updated: {github_summary['updated_at']}\n",
    "- Recent Activity: {github_summary['recent_commits']} commits in last batch\n",
    "\n",
    "Weather Context (Repository Location):\n",
    "- Location: {weather_summary['location']}\n",
    "- Current Temperature: {weather_summary['temperature']}°C (feels like {weather_summary['feels_like']}°C)\n",
    "- Conditions: {weather_summary['conditions']}\n",
    "- Humidity: {weather_summary['humidity']}%\n",
    "- Wind Speed: {weather_summary['wind_speed']} m/s\n",
    "\n",
    "Task: Analyze this data and provide:\n",
    "1. Repository health assessment\n",
    "2. Weather context relevance\n",
    "3. Any interesting correlations or insights\n",
    "4. Recommendations for the development team\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\n📤 Sending combined data to Azure OpenAI for analysis...\")\n",
    "        \n",
    "        # Note: This would normally call Azure OpenAI\n",
    "        # For demonstration, we'll show what would be sent\n",
    "        print(\"\\n📊 COMBINED DATA SUMMARY:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\nGitHub Metrics:\")\n",
    "        print(f\"  • Repository: {github_summary['repository']}\")\n",
    "        print(f\"  • Community: {github_summary['stars']:,} stars, {github_summary['forks']:,} forks\")\n",
    "        print(f\"  • Activity: {github_summary['recent_commits']} recent commits\")\n",
    "        print(f\"  • Health: {github_summary['open_issues']:,} open issues\")\n",
    "        \n",
    "        print(f\"\\nWeather Context:\")\n",
    "        print(f\"  • Location: {weather_summary['location']}\")\n",
    "        print(f\"  • Current: {weather_summary['conditions']}, {weather_summary['temperature']}°C\")\n",
    "        print(f\"  • Conditions: Humidity {weather_summary['humidity']}%, Wind {weather_summary['wind_speed']} m/s\")\n",
    "        \n",
    "        print(\"\\n\\n💡 SIMULATED AI INSIGHTS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"\"\"\n",
    "1. REPOSITORY HEALTH:\n",
    "   The repository shows strong community engagement with high star count\n",
    "   and active development (recent commits). The open issues indicate an\n",
    "   active user base providing feedback.\n",
    "\n",
    "2. WEATHER CONTEXT:\n",
    "   Current weather conditions in Seattle are favorable for development work.\n",
    "   Moderate temperatures and typical Pacific Northwest conditions.\n",
    "\n",
    "3. CROSS-DOMAIN INSIGHTS:\n",
    "   - Repository activity appears consistent regardless of weather\n",
    "   - Strong global community (not weather-dependent)\n",
    "   - Documentation and async work well-suited for variable weather\n",
    "\n",
    "4. RECOMMENDATIONS:\n",
    "   - Continue current development pace\n",
    "   - Consider timezone distribution of contributors\n",
    "   - Weather-independent workflow is well-established\n",
    "   - Focus on issue triage during inclement weather periods\n",
    "\"\"\")\n",
    "        \n",
    "        print(\"\\n✅ Multi-MCP AI Aggregation completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n📝 This example demonstrates:\")\n",
    "        print(\"   • Fetching data from multiple MCP sources (GitHub + Weather)\")\n",
    "        print(\"   • Combining datasets for richer context\")\n",
    "        print(\"   • Preparing data for AI analysis\")\n",
    "        print(\"   • Cross-domain insight generation\")\n",
    "        print(\"\\n💡 In production, this would call Azure OpenAI API for actual AI synthesis.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error in multi-MCP aggregation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Semantic Kernel & AutoGen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612aa1d-af9a-43fb-8362-0b1a6249b9e4",
   "metadata": {},
   "source": [
    "## SEMANTIC KERNEL & AUTOGEN\n",
    "\n",
    "**Purpose**: Systematically test different approaches to fix Semantic Kernel + MCP hanging\n",
    "\n",
    "**Status**: Testing in progress\n",
    "**Reference**: See MCP-Test/15-TESTING-TECHNIQUES.md for full documentation\n",
    "\n",
    "### Testing Phases:\n",
    "1. ✅ Baseline Tests (Techniques 1-3)\n",
    "2. 🔍 MCP Diagnostics (Techniques 4-6)\n",
    "3. 🔄 Alternative Frameworks (Techniques 7-8)\n",
    "4. ⚡ Optimization (Techniques 9-12)\n",
    "5. 🎯 Advanced (Techniques 13-15)\n",
    "\n",
    "**Instructions**: Run cells sequentially. Each cell logs results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30affb1-aeb3-433f-80f1-c1346afea0da",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 1: SK Plugin for Gateway-Routed Function Calling\n",
    "\n",
    "**Purpose**: SK Plugin for Gateway-Routed Function Calling\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577e929-9902-45cd-b19c-84abc4bc1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Plugin with Function Calling via APIM Gateway\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK plugin creation with kernel_function decorator\n",
    "- Automatic function calling with FunctionChoiceBehavior.Auto()\n",
    "- Routing SK chat completion through APIM gateway\n",
    "- Multi-step planning with automatic function invocation\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Function Calling Plugin via APIM Gateway\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create SK Plugin with Kernel Functions\n",
    "# ============================================================================\n",
    "\n",
    "class WorkshopPlugin:\n",
    "    \"\"\"Custom plugin for AI Gateway workshop demonstrations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get the current UTC time\")\n",
    "    def get_current_time(self) -> str:\n",
    "        \"\"\"Returns current UTC time in ISO format.\"\"\"\n",
    "        return datetime.utcnow().isoformat()\n",
    "\n",
    "    @kernel_function(description=\"Get weather information for a city\")\n",
    "    def get_weather(self, city: str) -> str:\n",
    "        \"\"\"\n",
    "        Get simulated weather for a city.\n",
    "\n",
    "        Args:\n",
    "            city: Name of the city\n",
    "        \"\"\"\n",
    "        # Simulated weather data\n",
    "        weather_data = {\n",
    "            \"seattle\": \"Rainy, 55°F (13°C)\",\n",
    "            \"san francisco\": \"Foggy, 62°F (17°C)\",\n",
    "            \"boston\": \"Cloudy, 48°F (9°C)\",\n",
    "            \"paris\": \"Partly cloudy, 15°C (59°F)\",\n",
    "        }\n",
    "        city_lower = city.lower()\n",
    "        return weather_data.get(city_lower, f\"Weather data unavailable for {city}\")\n",
    "\n",
    "    @kernel_function(description=\"Calculate the square of a number\")\n",
    "    def calculate_square(self, number: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate square of a number.\n",
    "\n",
    "        Args:\n",
    "            number: Number to square\n",
    "        \"\"\"\n",
    "        return number * number\n",
    "\n",
    "print(\"\\n✓ Workshop plugin created with 3 functions\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Configure Custom Azure OpenAI Client for APIM\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure gateway URL is available from existing notebook variables\n",
    "if 'apim_gateway_url' not in globals():\n",
    "    if 'APIM_GATEWAY_URL' in globals():\n",
    "        apim_gateway_url = APIM_GATEWAY_URL\n",
    "    elif 'step1_outputs' in globals():\n",
    "        apim_gateway_url = step1_outputs.get('apimGatewayUrl')\n",
    "    else:\n",
    "        raise RuntimeError(\"APIM gateway URL not found. Define APIM_GATEWAY_URL or step1_outputs['apimGatewayUrl'].\")\n",
    "\n",
    "# Derive subscription key if not already defined\n",
    "if 'subscription_key_both' not in globals():\n",
    "    if 'APIM_API_KEY' in globals():\n",
    "        subscription_key_both = APIM_API_KEY\n",
    "    elif 'subs' in globals() and isinstance(subs, list) and subs:\n",
    "        subscription_key_both = subs[0].get('key')\n",
    "    elif 'step1_outputs' in globals():\n",
    "        # Try to pull a key from apimSubscriptions array if present\n",
    "        subs_list = step1_outputs.get('apimSubscriptions', [])\n",
    "        subscription_key_both = next(\n",
    "            (s.get('primaryKey') or s.get('key') for s in subs_list if isinstance(s, dict)),\n",
    "            None\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"Unable to derive subscription key. Define subscription_key_both manually.\")\n",
    "    if not subscription_key_both:\n",
    "        raise RuntimeError(\"Derived subscription_key_both is empty. Provide a valid APIM subscription key.\")\n",
    "\n",
    "# Prepare headers if not already present\n",
    "if 'headers_both' not in globals():\n",
    "    headers_both = {\n",
    "        \"api-key\": subscription_key_both,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "# Create custom client pointing to APIM gateway (ensure correct inference path to avoid 404)\n",
    "# Normalize and append inference path (expected by APIM route rewrite)\n",
    "if 'inference_api_path' not in globals():\n",
    "    if 'INFERENCE_API_PATH' in globals():\n",
    "        inference_api_path = INFERENCE_API_PATH.strip('/')\n",
    "    elif 'step2_outputs' in globals():\n",
    "        inference_api_path = step2_outputs.get('inferenceAPIPath', 'inference').strip('/')\n",
    "    else:\n",
    "        inference_api_path = 'inference'\n",
    "\n",
    "# Ensure single trailing slash on base\n",
    "base_url = apim_gateway_url.rstrip('/') + '/'\n",
    "gateway_inference_endpoint = base_url + inference_api_path\n",
    "\n",
    "# Update/openai_endpoint variable (fix earlier missing slash issue)\n",
    "openai_endpoint = gateway_inference_endpoint\n",
    "\n",
    "custom_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=gateway_inference_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,  # From existing notebook variables\n",
    "    default_headers=headers_both    # From existing notebook variables\n",
    ")\n",
    "\n",
    "print(\"✓ Custom Azure OpenAI client configured for APIM gateway\")\n",
    "print(f\"  Base Gateway URL: {apim_gateway_url}\")\n",
    "print(f\"  Inference Endpoint: {gateway_inference_endpoint}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Initialize Semantic Kernel with Plugin\n",
    "# ============================================================================\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service with custom client\n",
    "chat_service = AzureChatCompletion(\n",
    "    service_id=\"apim_chat\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=custom_client,\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Add the workshop plugin\n",
    "kernel.add_plugin(\n",
    "    WorkshopPlugin(),\n",
    "    plugin_name=\"Workshop\"\n",
    ")\n",
    "\n",
    "print(\"✓ Semantic Kernel initialized\")\n",
    "print(\"  Service: Azure OpenAI via APIM\")\n",
    "print(\"  Plugin: WorkshopPlugin (3 functions)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Configure Auto Function Calling\n",
    "# ============================================================================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"apim_chat\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Enable automatic function calling\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"✓ Execution settings configured\")\n",
    "print(\"  Function calling: Automatic\")\n",
    "print(\"  Max tokens: 500\")\n",
    "print(\"  Temperature: 0.7\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Function Calling Examples\n",
    "# ============================================================================\n",
    "\n",
    "async def run_sk_function_calling():\n",
    "    \"\"\"Execute SK function calling examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Simple Function Call\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create chat history\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What time is it right now?\")\n",
    "\n",
    "    # Get response (SK will automatically call get_current_time function)\n",
    "    result = await chat_service.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What time is it right now?\")\n",
    "    print(f\"Assistant: {result}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Multi-Step Function Calling\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history2 = ChatHistory()\n",
    "    history2.add_user_message(\n",
    "        \"What's the weather in Seattle and what's the square of 12?\"\n",
    "    )\n",
    "\n",
    "    result2 = await chat_service.get_chat_message_content(\n",
    "        chat_history=history2,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What's the weather in Seattle and what's the square of 12?\")\n",
    "    print(f\"Assistant: {result2}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Complex Planning\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history3 = ChatHistory()\n",
    "    history3.add_user_message(\n",
    "        \"First tell me the current time, then check the weather in Paris, \"\n",
    "        \"and finally calculate the square of 7. Present all results.\"\n",
    "    )\n",
    "\n",
    "    result3 = await chat_service.get_chat_message_content(\n",
    "        chat_history=history3,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: First tell me the current time, then check the weather in Paris,\")\n",
    "    print(f\"      and finally calculate the square of 7. Present all results.\")\n",
    "    print(f\"Assistant: {result3}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FUNCTION CALLING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total examples executed: 3\")\n",
    "    print(f\"All calls routed through: {apim_gateway_url}\")\n",
    "    print(f\"Plugin used: WorkshopPlugin\")\n",
    "    print(f\"Functions available: get_current_time, get_weather, calculate_square\")\n",
    "\n",
    "# Run the async function\n",
    "await run_sk_function_calling()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ SK Plugin Function Calling Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. SK plugins encapsulate reusable functionality\")\n",
    "print(\"2. Auto function calling handles multi-step planning automatically\")\n",
    "print(\"3. All LLM calls route through APIM gateway\")\n",
    "print(\"4. No manual function call parsing required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fa11a-2d09-43ba-8e99-4bfa7af1b9ba",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 2: SK Streaming Chat with Function Calling\n",
    "\n",
    "**Purpose**: SK Streaming Chat with Function Calling\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6dd01-3392-4559-bb93-4ead03860231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Streaming Chat with Function Calling\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- Real-time streaming responses through APIM\n",
    "- Streaming with automatic function calling\n",
    "- Async iteration over response chunks\n",
    "- Progressive output rendering\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Streaming Chat with Function Calling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Setup Kernel (reuse from previous cell or create new)\n",
    "# ============================================================================\n",
    "\n",
    "# Simple plugin for streaming demo\n",
    "class StreamingDemoPlugin:\n",
    "    \"\"\"Plugin for streaming demonstrations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get information about a programming language\")\n",
    "    def get_language_info(self, language: str) -> str:\n",
    "        \"\"\"Get information about a programming language.\"\"\"\n",
    "        info = {\n",
    "            \"python\": \"Python is a high-level, interpreted language known for simplicity and readability. Created by Guido van Rossum in 1991.\",\n",
    "            \"javascript\": \"JavaScript is a dynamic, interpreted language primarily used for web development. Created by Brendan Eich in 1995.\",\n",
    "            \"csharp\": \"C# is a modern, object-oriented language developed by Microsoft. Released in 2000 as part of .NET Framework.\",\n",
    "            \"java\": \"Java is a class-based, object-oriented language designed to have minimal implementation dependencies. Released by Sun Microsystems in 1995.\",\n",
    "        }\n",
    "        return info.get(language.lower(), f\"Information not available for {language}\")\n",
    "\n",
    "    @kernel_function(description=\"Count words in a text\")\n",
    "    def count_words(self, text: str) -> int:\n",
    "        \"\"\"Count the number of words in text.\"\"\"\n",
    "        return len(text.split())\n",
    "\n",
    "# Create kernel with custom APIM client\n",
    "stream_kernel = Kernel()\n",
    "\n",
    "# Ensure we target the correct APIM API path (e.g. /inference) to avoid 404 NotFound\n",
    "# Prefer already provided openai_endpoint if available, else build from base + path_var.\n",
    "streaming_endpoint = (\n",
    "    openai_endpoint\n",
    "    if \"openai_endpoint\" in globals()\n",
    "    else f\"{apim_gateway_url.rstrip('/')}/{path_var}\"\n",
    ")\n",
    "\n",
    "print(f\"Configured streaming endpoint: {streaming_endpoint}\")\n",
    "\n",
    "custom_stream_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=streaming_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both,\n",
    ")\n",
    "\n",
    "stream_chat_service = AzureChatCompletion(\n",
    "    service_id=\"apim_stream\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=custom_stream_client,\n",
    ")\n",
    "\n",
    "stream_kernel.add_service(stream_chat_service)\n",
    "stream_kernel.add_plugin(StreamingDemoPlugin(), plugin_name=\"StreamingDemo\")\n",
    "\n",
    "print(\"✓ Streaming kernel configured\")\n",
    "print(f\"  Endpoint: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Configure Streaming Settings\n",
    "# ============================================================================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "stream_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"apim_stream\",\n",
    "    max_tokens=800,\n",
    "    temperature=0.8,\n",
    ")\n",
    "stream_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"✓ Streaming settings configured\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Streaming Examples\n",
    "# ============================================================================\n",
    "\n",
    "async def run_streaming_examples():\n",
    "    \"\"\"Execute streaming chat examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Basic Streaming Response\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"Tell me a short story about an AI learning to paint.\")\n",
    "\n",
    "    print(\"\\nUser: Tell me a short story about an AI learning to paint.\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    # Get streaming response\n",
    "    response_stream = stream_chat_service.get_streaming_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=stream_settings,\n",
    "        kernel=stream_kernel,\n",
    "    )\n",
    "\n",
    "    # Collect chunks for later use\n",
    "    chunks = []\n",
    "    async for chunk in response_stream:\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    print(\"\\n\")  # New line after streaming\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Streaming with Function Call\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history2 = ChatHistory()\n",
    "    history2.add_user_message(\n",
    "        \"Give me detailed information about Python and then explain why it's popular.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nUser: Give me detailed information about Python and then explain why it's popular.\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    response_stream2 = stream_chat_service.get_streaming_chat_message_content(\n",
    "        chat_history=history2,\n",
    "        settings=stream_settings,\n",
    "        kernel=stream_kernel,\n",
    "    )\n",
    "\n",
    "    chunks2 = []\n",
    "    async for chunk in response_stream2:\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            chunks2.append(chunk)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Interactive Streaming Conversation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Multi-turn conversation with streaming\n",
    "    conv_history = ChatHistory()\n",
    "\n",
    "    messages = [\n",
    "        \"What programming language should I learn first?\",\n",
    "        \"Tell me more about Python specifically.\",\n",
    "        \"How many words have you used in your last response?\"\n",
    "    ]\n",
    "\n",
    "    for msg in messages:\n",
    "        print(f\"\\nUser: {msg}\")\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        conv_history.add_user_message(msg)\n",
    "\n",
    "        stream_response = stream_chat_service.get_streaming_chat_message_content(\n",
    "            chat_history=conv_history,\n",
    "            settings=stream_settings,\n",
    "            kernel=stream_kernel,\n",
    "        )\n",
    "\n",
    "        full_response_chunks = []\n",
    "        async for chunk in stream_response:\n",
    "            if chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full_response_chunks.append(chunk)\n",
    "\n",
    "        # Combine chunks into full message for history\n",
    "        if full_response_chunks:\n",
    "            full_response = sum(full_response_chunks[1:], full_response_chunks[0])\n",
    "            conv_history.add_message(full_response)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STREAMING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Examples executed: 3\")\n",
    "    print(f\"Streaming endpoint: {apim_gateway_url}\")\n",
    "    print(f\"Function calling: Enabled (auto)\")\n",
    "    print(f\"Response mode: Real-time streaming\")\n",
    "\n",
    "# Run streaming examples\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ SK Streaming Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Streaming provides real-time response rendering\")\n",
    "print(\"2. Function calling works seamlessly with streaming\")\n",
    "print(\"3. Async iteration enables progressive output\")\n",
    "print(\"4. All streaming goes through APIM gateway\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210dde5-9a7c-4dba-85b0-b80b0b73a760",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 3: AutoGen Multi-Agent Conversation via APIM\n",
    "\n",
    "**Purpose**: AutoGen Multi-Agent Conversation via APIM\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087606e7-42e0-49c6-bb89-b8d852afe726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AutoGen: Multi-Agent Conversation via APIM Gateway\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- Multiple AutoGen agents with specialized roles\n",
    "- Agent-to-agent communication\n",
    "- Tool/function registration and execution\n",
    "- Routing all AutoGen LLM calls through APIM\n",
    "- Termination conditions and conversation flow\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Annotated, Literal\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AUTOGEN: Multi-Agent Conversation via APIM Gateway\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Configure AutoGen for APIM Gateway\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure deployment_name exists (fallback to a known model)\n",
    "if \"deployment_name\" not in globals() or not deployment_name:\n",
    "    deployment_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Build correct endpoint (APIM base + inference path)\n",
    "endpoint = openai_endpoint if \"openai_endpoint\" in globals() and openai_endpoint else (\n",
    "    apim_gateway_url.rstrip(\"/\") + \"/inference\"\n",
    ")\n",
    "\n",
    "# Build correct endpoint (APIM base + inference path)\n",
    "if \"openai_endpoint\" in globals() and openai_endpoint:\n",
    "    endpoint = openai_endpoint.rstrip(\"/\")\n",
    "else:\n",
    "    apim_base = apim_gateway_url if \"apim_gateway_url\" in globals() and apim_gateway_url else os.getenv(\"APIM_GATEWAY_URL\", \"\")\n",
    "    inference_path = inference_api_path if \"inference_api_path\" in globals() else os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "    endpoint = f\"{apim_base.rstrip('/')}/{inference_path.strip('/')}\"\n",
    "\n",
    "# Get API key\n",
    "api_key = subscription_key_both if \"subscription_key_both\" in globals() and subscription_key_both else (\n",
    "    apim_api_key if \"apim_api_key\" in globals() and apim_api_key else os.getenv(\"APIM_API_KEY\", \"\")\n",
    ")\n",
    "\n",
    "# Validate configuration\n",
    "if not endpoint or not api_key:\n",
    "    print(\"❌ Missing AutoGen configuration:\")\n",
    "    if not endpoint:\n",
    "        print(\"   - APIM endpoint not found (need APIM_GATEWAY_URL)\")\n",
    "    if not api_key:\n",
    "        print(\"   - API key not found (need APIM_API_KEY or subscription_key)\")\n",
    "    raise RuntimeError(\"Missing AutoGen configuration. Please ensure master-lab.env is loaded.\")\n",
    "\n",
    "# AutoGen configuration pointing to APIM\n",
    "autogen_config = {\n",
    "    \"model\": deployment_name,\n",
    "    \"api_type\": \"azure\",\n",
    "    \"api_key\": api_key,\n",
    "    \"base_url\": endpoint,\n",
    "    \"api_version\": \"2024-02-01\",\n",
    "}\n",
    "\n",
    "config_list = [autogen_config]\n",
    "\n",
    "print(\"✓ AutoGen configuration created\")\n",
    "print(f\"  Model: {deployment_name}\")\n",
    "print(f\"  Base URL: {endpoint}\")\n",
    "print(f\"  API Key: {'*' * 8}{api_key[-4:] if len(api_key) > 4 else '****'}\")\n",
    "\n",
    "print(\"✓ AutoGen configuration created\")\n",
    "print(f\"  Model: {deployment_name}\")\n",
    "print(f\"  Base URL: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Define Tools for Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Simple calculator tool\n",
    "Operator = Literal[\"+\", \"-\", \"*\", \"/\"]\n",
    "\n",
    "def calculator(a: float, b: float, operator: Annotated[Operator, \"operator\"]) -> float:\n",
    "    \"\"\"\n",
    "    Perform basic arithmetic operations.\n",
    "\n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "        operator: Operation to perform (+, -, *, /)\n",
    "\n",
    "    Returns:\n",
    "        Result of the calculation\n",
    "    \"\"\"\n",
    "    if operator == \"+\":\n",
    "        return a + b\n",
    "    elif operator == \"-\":\n",
    "        return a - b\n",
    "    elif operator == \"*\":\n",
    "        return a * b\n",
    "    elif operator == \"/\":\n",
    "        if b == 0:\n",
    "            return float('inf')  # Handle division by zero\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid operator: {operator}\")\n",
    "\n",
    "print(\"✓ Calculator tool defined\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create Specialized Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Agent 1: Analyst (suggests approaches)\n",
    "analyst_agent = ConversableAgent(\n",
    "    name=\"Analyst\",\n",
    "    system_message=(\n",
    "        \"You are a data analyst. Your role is to analyze problems and suggest \"\n",
    "        \"approaches using available tools. When calculations are needed, clearly \"\n",
    "        \"state what needs to be calculated. Return 'TERMINATE' when the task is complete.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "# Agent 2: Calculator (executes calculations)\n",
    "calculator_agent = ConversableAgent(\n",
    "    name=\"Calculator\",\n",
    "    system_message=(\n",
    "        \"You are a calculator agent. You execute mathematical calculations accurately. \"\n",
    "        \"Use the calculator tool for all computations.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list, \"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "# Agent 3: User Proxy (manages execution and termination)\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"UserProxy\",\n",
    "    llm_config=False,  # No LLM for proxy\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None\n",
    "                                   and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"✓ Three agents created:\")\n",
    "print(\"  1. Analyst - Problem analysis and planning\")\n",
    "print(\"  2. Calculator - Execution of calculations\")\n",
    "print(\"  3. UserProxy - Tool execution and flow control\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Register Tools with Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Register calculator tool\n",
    "analyst_agent.register_for_llm(\n",
    "    name=\"calculator\",\n",
    "    description=\"A calculator that performs basic arithmetic\"\n",
    ")(calculator)\n",
    "\n",
    "calculator_agent.register_for_llm(\n",
    "    name=\"calculator\",\n",
    "    description=\"A calculator that performs basic arithmetic\"\n",
    ")(calculator)\n",
    "\n",
    "user_proxy.register_for_execution(name=\"calculator\")(calculator)\n",
    "\n",
    "print(\"✓ Calculator tool registered with all agents\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Multi-Agent Conversations\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: Simple Calculation Task\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response1 = user_proxy.initiate_chat(\n",
    "    analyst_agent,\n",
    "    message=\"Calculate (15 + 27) * 3 and then subtract 50. What's the final result?\",\n",
    "    max_turns=10\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Example 1 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: Complex Multi-Step Problem\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response2 = user_proxy.initiate_chat(\n",
    "    analyst_agent,\n",
    "    message=(\n",
    "        \"A company has quarterly revenues of $125,000, $138,000, $142,000, and $155,000. \"\n",
    "        \"Calculate the total annual revenue and then the average quarterly revenue.\"\n",
    "    ),\n",
    "    max_turns=10\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Example 2 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: Agent Collaboration Pattern\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# More complex scenario requiring agent collaboration\n",
    "response3 = user_proxy.initiate_chat(\n",
    "    analyst_agent,\n",
    "    message=(\n",
    "        \"If a product costs $89.99 and there's a 15% discount, what's the final price? \"\n",
    "        \"Then, if I buy 7 units at the discounted price, what's my total cost?\"\n",
    "    ),\n",
    "    max_turns=15\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Example 3 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-AGENT CONVERSATION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total examples: 3\")\n",
    "print(f\"Agents involved: Analyst, Calculator, UserProxy\")\n",
    "print(f\"Tool calls: Calculator function\")\n",
    "print(f\"All LLM calls routed through: {apim_gateway_url}\")\n",
    "print(f\"Model used: {deployment_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ AutoGen Multi-Agent Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. AutoGen enables multi-agent collaboration patterns\")\n",
    "print(\"2. Agents can have specialized roles and tools\")\n",
    "print(\"3. Tool registration separates LLM decision from execution\")\n",
    "print(\"4. All agent LLM calls route through APIM gateway\")\n",
    "print(\"5. Termination conditions control conversation flow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f574827-187e-4099-8ab3-2435e94958c7",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 4: SK Agent with Custom Azure OpenAI Client\n",
    "\n",
    "**Purpose**: SK Agent with Custom Azure OpenAI Client\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003954bc-9f75-44a6-826d-4c87b1158c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: ChatCompletionAgent with APIM Routing\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK ChatCompletionAgent with custom Azure OpenAI client\n",
    "- Multi-turn conversation with thread management\n",
    "- Agent streaming capabilities\n",
    "- Integration with existing APIM infrastructure\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt, KernelArguments\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: ChatCompletionAgent with APIM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create Kernel with Custom Client\n",
    "# ============================================================================\n",
    "\n",
    "agent_kernel = Kernel()\n",
    "\n",
    "# Custom client for APIM\n",
    "agent_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Add chat completion service\n",
    "agent_chat_service = AzureChatCompletion(\n",
    "    service_id=\"agent_service\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=agent_client,\n",
    ")\n",
    "agent_kernel.add_service(agent_chat_service)\n",
    "\n",
    "print(\"✓ Agent kernel created\")\n",
    "print(f\"  Service: Azure OpenAI via APIM\")\n",
    "print(f\"  Endpoint: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Add Plugin Function to Agent\n",
    "# ============================================================================\n",
    "\n",
    "# Add a simple prompt-based function\n",
    "documentation_function = agent_kernel.add_function(\n",
    "    plugin_name=\"DocsHelper\",\n",
    "    function=KernelFunctionFromPrompt(\n",
    "        function_name=\"explain_concept\",\n",
    "        prompt=\"\"\"You are a technical documentation expert.\n",
    "\n",
    "Explain the following concept clearly and concisely:\n",
    "\n",
    "Concept: {{$concept}}\n",
    "\n",
    "Provide:\n",
    "1. Brief definition\n",
    "2. Key characteristics\n",
    "3. Common use cases\n",
    "4. A simple example\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✓ Documentation helper function added to kernel\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Configure Agent Settings\n",
    "# ============================================================================\n",
    "\n",
    "agent_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"agent_service\",\n",
    "    max_tokens=600,\n",
    "    temperature=0.7,\n",
    ")\n",
    "agent_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"✓ Agent execution settings configured\")\n",
    "print(\"  Function calling: Auto\")\n",
    "print(\"  Max tokens: 600\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create ChatCompletionAgent\n",
    "# ============================================================================\n",
    "\n",
    "workshop_agent = ChatCompletionAgent(\n",
    "    kernel=agent_kernel,\n",
    "    name=\"WorkshopAssistant\",\n",
    "    instructions=(\n",
    "        \"You are an AI assistant for an Azure AI Gateway workshop. \"\n",
    "        \"Help users understand AI Gateway concepts, API Management, \"\n",
    "        \"and Azure OpenAI integration. Be concise and practical. \"\n",
    "        \"Use available functions to provide detailed explanations when needed.\"\n",
    "    ),\n",
    "    arguments=KernelArguments(settings=agent_settings),\n",
    ")\n",
    "\n",
    "print(\"✓ ChatCompletionAgent created\")\n",
    "print(f\"  Name: {workshop_agent.name}\")\n",
    "print(\"  Instructions: Workshop assistance\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Agent Conversations\n",
    "# ============================================================================\n",
    "\n",
    "async def run_agent_examples():\n",
    "    \"\"\"Execute agent conversation examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Simple Agent Interaction\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create new thread (handle SK version differences)\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread = workshop_agent.new_thread()\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"ChatCompletionAgent has no thread creation method (create_thread/new_thread). \"\n",
    "            \"Update semantic_kernel package or remove thread usage.\"\n",
    "        )\n",
    "\n",
    "    # First interaction\n",
    "    result1 = await workshop_agent.run(\n",
    "        \"What is Azure API Management?\",\n",
    "        thread=thread\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What is Azure API Management?\")\n",
    "    print(f\"Agent: {result1.text}\\n\")\n",
    "\n",
    "    # Second interaction (agent remembers context)\n",
    "    result2 = await workshop_agent.run(\n",
    "        \"How does it help with AI Gateway patterns?\",\n",
    "        thread=thread\n",
    "    )\n",
    "\n",
    "    print(f\"User: How does it help with AI Gateway patterns?\")\n",
    "    print(f\"Agent: {result2.text}\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Agent with Function Calling\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread2 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread2 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread2 = thread  # Fallback: reuse existing thread\n",
    "\n",
    "    result3 = await workshop_agent.run(\n",
    "        \"Explain the concept of 'semantic kernel' in detail\",\n",
    "        thread=thread2\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: Explain the concept of 'semantic kernel' in detail\")\n",
    "    print(f\"Agent: {result3.text}\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Streaming Agent Response\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread3 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread3 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread3 = thread  # Fallback\n",
    "\n",
    "    print(\"\\nUser: Explain the benefits of using an AI Gateway for enterprise deployments\")\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "\n",
    "    # Stream the response\n",
    "    async for chunk in workshop_agent.run_stream(\n",
    "        \"Explain the benefits of using an AI Gateway for enterprise deployments\",\n",
    "        thread=thread3\n",
    "    ):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: Multi-Turn Technical Discussion\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread4 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread4 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread4 = thread  # Fallback\n",
    "\n",
    "    questions = [\n",
    "        \"What is function calling in LLMs?\",\n",
    "        \"How does Semantic Kernel implement function calling?\",\n",
    "        \"What's the difference between manual and auto function invocation?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        result = await workshop_agent.run(question, thread=thread4)\n",
    "        print(f\"\\nUser: {question}\")\n",
    "        print(f\"Agent: {result.text[:200]}...\")  # Truncate for readability\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGENT CONVERSATION STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total examples: 4\")\n",
    "    print(f\"Agent: WorkshopAssistant\")\n",
    "    print(f\"Threads created: 4\")\n",
    "    print(f\"Total interactions: 8+\")\n",
    "    print(f\"All routed through: {apim_gateway_url}\")\n",
    "    print(f\"Streaming enabled: Yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b39ce5-2f96-4f8b-8a95-8f9a1d4964b5",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 5: SK Vector Search with Gateway-Routed Embeddings\n",
    "\n",
    "**Purpose**: SK Vector Search with Gateway-Routed Embeddings\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac5d80-4adb-460a-9e07-c5f700a1f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Vector Search with APIM-Routed Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK in-memory vector store for quick demos\n",
    "- Embedding generation through APIM gateway\n",
    "- Vector search for RAG pattern\n",
    "- SK search functions for retrieval\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding, AzureChatCompletion\n",
    "from openai import AsyncAzureOpenAI  # Removed unused InMemoryVectorStore import (was causing ImportError)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Vector Search with Gateway Embeddings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Configure Kernel with Embedding Service\n",
    "# ============================================================================\n",
    "\n",
    "memory_kernel = Kernel()\n",
    "\n",
    "# Ensure lowercase gateway variable is available (some cells define APIM_GATEWAY_URL only)\n",
    "if \"apim_gateway_url\" not in globals() and \"APIM_GATEWAY_URL\" in globals():\n",
    "    apim_gateway_url = APIM_GATEWAY_URL\n",
    "\n",
    "# Custom client for embeddings through APIM\n",
    "embedding_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Note: You'll need an embedding deployment in your Azure OpenAI.\n",
    "# Attempt a list of possible embedding deployment names, first that works is used.\n",
    "candidate_embedding_deployments = [\n",
    "    \"text-embedding-3-small\",\n",
    "    \"text-embedding-3-large\",\n",
    "    \"text-embedding-ada-002\"\n",
    "]\n",
    "\n",
    "embedding_service = None\n",
    "embedding_deployment = None\n",
    "embeddings_available = False\n",
    "\n",
    "for dep_name in candidate_embedding_deployments:\n",
    "    try:\n",
    "        test_service = AzureTextEmbedding(\n",
    "            service_id=\"apim_embeddings\",\n",
    "            deployment_name=dep_name,\n",
    "            async_client=embedding_client,\n",
    "        )\n",
    "        _ = asyncio.get_event_loop().run_until_complete(\n",
    "            test_service.generate_embeddings([\"ping\"])\n",
    "        )\n",
    "        embedding_service = test_service\n",
    "        embedding_deployment = dep_name\n",
    "        memory_kernel.add_service(embedding_service)\n",
    "        embeddings_available = True\n",
    "        print(f\"✓ Embedding service configured\")\n",
    "        print(f\"  Deployment detected: {embedding_deployment}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not embeddings_available:\n",
    "    print(\"⚠ No embedding deployment found. Using simulated embeddings.\")\n",
    "memory_chat_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Attempt to auto-detect a valid chat deployment to avoid 404 errors.\n",
    "candidate_chat_deployments = []\n",
    "# Prefer any provided requested_models variable\n",
    "if \"requested_models\" in globals() and isinstance(requested_models, list):\n",
    "    candidate_chat_deployments.extend(requested_models)\n",
    "# Common fallbacks\n",
    "candidate_chat_deployments.extend([\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-35-turbo\",\n",
    "])\n",
    "\n",
    "# Deduplicate while preserving order\n",
    "seen = set()\n",
    "candidate_chat_deployments = [m for m in candidate_chat_deployments if not (m in seen or seen.add(m))]\n",
    "\n",
    "chat_service_available = False\n",
    "chat_deployment_name = None\n",
    "\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "for dep in candidate_chat_deployments:\n",
    "    try:\n",
    "        test_service = AzureChatCompletion(\n",
    "            service_id=\"memory_chat\",\n",
    "            deployment_name=dep,\n",
    "            async_client=memory_chat_client,\n",
    "        )\n",
    "        # Minimal probe\n",
    "        history = ChatHistory()\n",
    "        history.add_user_message(\"ping\")\n",
    "        settings = AzureChatPromptExecutionSettings(\n",
    "            service_id=\"memory_chat\",\n",
    "            max_tokens=5,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        _ = asyncio.get_event_loop().run_until_complete(\n",
    "            test_service.get_chat_message_content(\n",
    "                chat_history=history,\n",
    "                settings=settings,\n",
    "                kernel=memory_kernel,\n",
    "            )\n",
    "        )\n",
    "        memory_chat_service = test_service\n",
    "        memory_kernel.add_service(memory_chat_service)\n",
    "        chat_service_available = True\n",
    "        chat_deployment_name = dep\n",
    "        print(f\"✓ Chat service configured (deployment: {chat_deployment_name})\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not chat_service_available:\n",
    "    print(\"⚠ No valid chat deployment found. Will use simulated responses.\")\n",
    "    chat_deployment_name = \"simulated-chat\"\n",
    "else:\n",
    "    # Add only if real chat service exists\n",
    "    memory_kernel.add_service(memory_chat_service)\n",
    "\n",
    "print(\"✓ Chat service added for RAG pattern (mode: \" + (\"real\" if chat_service_available else \"simulated\") + \")\")\n",
    "# ============================================================================\n",
    "# Step 2: Create Sample Knowledge Base\n",
    "# ============================================================================\n",
    "\n",
    "# Knowledge base documents\n",
    "knowledge_base = {\n",
    "    \"apim_basics\": \"\"\"\n",
    "Azure API Management (APIM) is a fully managed service that lets you publish, secure,\n",
    "transform, maintain, and monitor APIs. It provides a consistent interface and governance\n",
    "layer over backend services.\n",
    "\"\"\",\n",
    "    \"ai_gateway\": \"\"\"\n",
    "An AI Gateway uses API Management to front multiple AI model endpoints (e.g. region/sku variants).\n",
    "It centralizes auth, rate limiting, observability, routing, and policy enforcement (e.g. content safety).\n",
    "\"\"\",\n",
    "    \"semantic_kernel\": \"\"\"\n",
    "Semantic Kernel is an SDK that composes AI services (Azure OpenAI, OpenAI, Hugging Face, etc.)\n",
    "with traditional code via plugins, planners, and memory abstractions to build AI-centric workflows.\n",
    "\"\"\",\n",
    "    \"function_calling\": \"\"\"\n",
    "Function calling allows an LLM to decide when to invoke backend functions (tools) by emitting\n",
    "structured calls. The host intercepts the call, executes the function, supplies the result back\n",
    "to the model, enabling tool-augmented reasoning and retrieval.\n",
    "\"\"\"\n",
    "}\n",
    "# Strict (no simulated) embedding creation\n",
    "async def create_vector_memory():\n",
    "    # Provide a graceful fallback to simulated embeddings when none are available.\n",
    "    if not embeddings_available or embedding_service is None:\n",
    "        print(\"\\n⚠ Using simulated embeddings (deterministic hash-based vectors)\")\n",
    "        dim = 256  # Fallback dimension\n",
    "        def embed_text(text: str, dim: int = 256):\n",
    "            h = hashlib.sha256(text.encode(\"utf-8\")).digest()\n",
    "            seed = int.from_bytes(h[:8], \"big\")\n",
    "            rng = np.random.default_rng(seed)\n",
    "            vec = rng.normal(0, 1, dim)\n",
    "            vec /= np.linalg.norm(vec)\n",
    "            return vec.tolist()\n",
    "        vectors = {}\n",
    "        for key, text in knowledge_base.items():\n",
    "            vec = embed_text(text)\n",
    "            vectors[key] = vec\n",
    "            print(f\"  ✓ {key}: {len(vec)} dims (simulated)\")\n",
    "        global embedding_deployment\n",
    "        if embedding_deployment is None:\n",
    "            embedding_deployment = \"simulated-embeddings\"\n",
    "        return vectors\n",
    "\n",
    "    print(\"\\n🔄 Generating embeddings through APIM gateway...\")\n",
    "    vectors = {}\n",
    "    for key, text in knowledge_base.items():\n",
    "        emb = await embedding_service.generate_embeddings([text])\n",
    "        vec = emb[0]\n",
    "        vectors[key] = vec\n",
    "        print(f\"  ✓ {key}: {len(vec)} dims (real)\")\n",
    "    return vectors\n",
    "\n",
    "# Vector search utilities\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "# Unified search (supports real & simulated embeddings)\n",
    "async def search_knowledge_base(query: str, top_k: int = 2):\n",
    "    print(f\"\\n🔄 Searching for: '{query}'\")\n",
    "    if embeddings_available and embedding_service is not None:\n",
    "        q_emb = await embedding_service.generate_embeddings([query])\n",
    "        q_vec = np.array(q_emb[0])\n",
    "        q_vec /= np.linalg.norm(q_vec)\n",
    "    else:\n",
    "        # Simulated deterministic embedding (same method as fallback vectors)\n",
    "        h = hashlib.sha256(query.encode(\"utf-8\")).digest()\n",
    "        seed = int.from_bytes(h[:8], \"big\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "        dim = len(next(iter(vectors.values()))) if vectors else 256\n",
    "        q_vec = rng.normal(0, 1, dim)\n",
    "        q_vec /= np.linalg.norm(q_vec)\n",
    "\n",
    "    sims = []\n",
    "    for key, vec in vectors.items():\n",
    "        sims.append((key, knowledge_base[key], cosine_similarity(q_vec, vec)))\n",
    "    sims.sort(key=lambda x: x[2], reverse=True)\n",
    "    return sims[:top_k]\n",
    "\n",
    "vectors = await create_vector_memory()\n",
    "print(\"✓ Vector embeddings created\")\n",
    "print(f\"  Total vectors: {len(vectors)}\")\n",
    "\n",
    "async def run_rag_examples():\n",
    "    \"\"\"Execute RAG examples using vector search and chat completion.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Single Query RAG\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Use existing 'question' variable if available, else fallback\n",
    "    user_question = question if 'question' in globals() else \"What is Semantic Kernel?\"\n",
    "    results = await search_knowledge_base(user_question, top_k=2)\n",
    "    print(f\"\\nQuery: {user_question}\")\n",
    "    print(\"🔄 Searching knowledge base...\")\n",
    "    print(f\"  Found {len(results)} relevant documents\")\n",
    "\n",
    "    # Build RAG prompt from retrieved context\n",
    "    context_blocks = []\n",
    "    for key, text, score in results:\n",
    "        context_blocks.append(f\"[{key}] (score={score:.4f})\\n{text.strip()}\")\n",
    "    rag_context = \"\\n\\n\".join(context_blocks)\n",
    "    rag_prompt = (\n",
    "        f\"Use the following context to answer the question.\\n\\n\"\n",
    "        f\"{rag_context}\\n\\nQuestion: {user_question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n🔄 Generating answer with retrieved context...\")\n",
    "\n",
    "    from semantic_kernel.contents import ChatHistory\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(rag_prompt)\n",
    "\n",
    "    rag_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=\"memory_chat\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Safe chat completion (fallback to simulated answer if unavailable or 404)\n",
    "    answer = None\n",
    "    if chat_service_available and 'memory_chat_service' in globals():\n",
    "        try:\n",
    "            answer = await memory_chat_service.get_chat_message_content(\n",
    "                chat_history=history,\n",
    "                settings=rag_settings,\n",
    "                kernel=memory_kernel,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Chat completion failed ({type(e).__name__}). Using simulated answer.\")\n",
    "    if answer is None:\n",
    "        answer = \"(Simulated answer)\\n\" + \" \".join(\n",
    "            [block.splitlines()[1][:120] + \"...\" for block in context_blocks]\n",
    "        )\n",
    "\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Multi-Query RAG (Top-1 Match)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Re-use same history; attempt second call only if service is valid\n",
    "    if chat_service_available and 'memory_chat_service' in globals():\n",
    "        try:\n",
    "            answer2 = await memory_chat_service.get_chat_message_content(\n",
    "                chat_history=history,\n",
    "                settings=rag_settings,\n",
    "                kernel=memory_kernel,\n",
    "            )\n",
    "        except Exception:\n",
    "            answer2 = \"(Simulated follow-up answer)\"\n",
    "    else:\n",
    "        answer2 = \"(Simulated follow-up answer)\"\n",
    "    print(f\"\\nAnswer: {answer2}\")\n",
    "\n",
    "    # Guard for 'queries' variable\n",
    "    queries_list = queries if 'queries' in globals() else [user_question]\n",
    "    for q in queries_list:\n",
    "        top = await search_knowledge_base(q, top_k=1)\n",
    "        print(f\"\\nQuery: {q}\")\n",
    "        if top:\n",
    "            key, text, score = top[0]\n",
    "            print(f\"  Best match: {key} (score: {score:.4f})\")\n",
    "            print(f\"  Snippet: {text.strip()[:80]}...\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VECTOR SEARCH STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    dims = len(next(iter(vectors.values()))) if vectors else 0\n",
    "    print(f\"Knowledge base size: {len(knowledge_base)} documents\")\n",
    "    print(f\"Vector dimensions: {dims}\")\n",
    "    print(f\"Search method: Cosine similarity\")\n",
    "    print(f\"Embeddings routed through: {apim_gateway_url}\")\n",
    "    print(f\"Chat completions routed through: {apim_gateway_url}\")\n",
    "\n",
    "# Run RAG examples\n",
    "await run_rag_examples()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ SK Vector Search Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Vector embeddings enable semantic search\")\n",
    "print(\"2. RAG combines retrieval with generation\")\n",
    "print(\"3. All embedding calls route through APIM\")\n",
    "print(\"4. In-memory stores work for quick prototypes\")\n",
    "print(\"5. Production would use Azure AI Search or Cosmos DB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd2498-b875-4e22-b5bf-489f7636d3c8",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 6: SK + AutoGen Hybrid Orchestration\n",
    "\n",
    "**Purpose**: SK + AutoGen Hybrid Orchestration\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58453bba-e69b-4101-8129-8c8902143743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Hybrid: Semantic Kernel Plugins + AutoGen Orchestration\n",
    "# ============================================================================\n",
    "# FIXED 2025-11-18: Corrected endpoint URL construction to prevent 404 errors\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK plugins as tools for AutoGen agents\n",
    "- Multi-agent orchestration with SK capabilities\n",
    "- Combining SK function calling with AutoGen decision making\n",
    "- Complex workflow coordination\n",
    "- All LLM calls through APIM gateway\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from typing import Annotated, Dict, Any\n",
    "from datetime import datetime\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from openai import AsyncAzureOpenAI\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID: Semantic Kernel + AutoGen Orchestration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create SK Plugin with Business Logic\n",
    "# ============================================================================\n",
    "\n",
    "class EnterprisePlugin:\n",
    "    \"\"\"SK Plugin for enterprise business operations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get customer information by ID\")\n",
    "    def get_customer_info(self, customer_id: str) -> str:\n",
    "        \"\"\"Retrieve customer information.\"\"\"\n",
    "        customers = {\n",
    "            \"C001\": \"Customer: Acme Corp, Tier: Gold, Balance: $50,000\",\n",
    "            \"C002\": \"Customer: Contoso Ltd, Tier: Silver, Balance: $25,000\",\n",
    "            \"C003\": \"Customer: Fabrikam Inc, Tier: Platinum, Balance: $100,000\",\n",
    "        }\n",
    "        return customers.get(customer_id, \"Customer not found\")\n",
    "\n",
    "    @kernel_function(description=\"Calculate discount based on customer tier\")\n",
    "    def calculate_discount(self, tier: str, amount: float) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate discount for a customer tier.\"\"\"\n",
    "        discount_rates = {\n",
    "            \"platinum\": 0.20,\n",
    "            \"gold\": 0.15,\n",
    "            \"silver\": 0.10,\n",
    "            \"bronze\": 0.05,\n",
    "        }\n",
    "        rate = discount_rates.get(tier.lower(), 0.0)\n",
    "        discount = amount * rate\n",
    "        final_price = amount - discount\n",
    "\n",
    "        return {\n",
    "            \"tier\": tier,\n",
    "            \"original_amount\": amount,\n",
    "            \"discount_rate\": rate,\n",
    "            \"discount_amount\": discount,\n",
    "            \"final_price\": final_price\n",
    "        }\n",
    "\n",
    "    @kernel_function(description=\"Process order and return order ID\")\n",
    "    def process_order(self, customer_id: str, amount: float) -> str:\n",
    "        \"\"\"Process a customer order.\"\"\"\n",
    "        order_id = f\"ORD-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return f\"Order {order_id} processed for customer {customer_id}, amount: ${amount:.2f}\"\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1.5: Configure Endpoints (FIXED for proper APIM routing)\n",
    "# ============================================================================\n",
    "\n",
    "# Get variables\n",
    "gateway_url = globals().get('apim_gateway_url', os.getenv('APIM_GATEWAY_URL', ''))\n",
    "api_key = globals().get('apim_api_key', os.getenv('APIM_API_KEY', ''))\n",
    "model_deployment = globals().get('deployment_name', 'gpt-4o-mini')\n",
    "inference_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "\n",
    "if not gateway_url or not api_key:\n",
    "    print(\"❌ Missing APIM configuration. Ensure apim_gateway_url and apim_api_key are set.\")\n",
    "    print(\"   Run Cell 32 (APIM Variable Definitions) first.\")\n",
    "    raise ValueError(\"APIM configuration required\")\n",
    "\n",
    "# FIXED: Proper URL construction for Azure OpenAI via APIM\n",
    "# Azure OpenAI client expects: https://<gateway>/<api-path>\n",
    "# The client will append /openai/deployments/<model>/... automatically\n",
    "# So we should NOT manually add /openai here\n",
    "\n",
    "# Normalize gateway URL (remove trailing slash)\n",
    "gateway_base = gateway_url.rstrip('/')\n",
    "\n",
    "# For AsyncAzureOpenAI (SK), the azure_endpoint should include the inference path\n",
    "# but NOT /openai (the SDK adds that)\n",
    "sk_endpoint = f\"{gateway_base}/{inference_path.strip('/')}\"\n",
    "\n",
    "# For AutoGen, same logic - just gateway + inference path\n",
    "autogen_base_url = f\"{gateway_base}/{inference_path.strip('/')}\"\n",
    "\n",
    "print(f\"[config] Gateway Base: {gateway_base}\")\n",
    "print(f\"[config] Inference Path: {inference_path}\")\n",
    "print(f\"[config] SK Endpoint: {sk_endpoint}\")\n",
    "print(f\"[config] AutoGen Base URL: {autogen_base_url}\")\n",
    "print(f\"[config] Model: {model_deployment}\")\n",
    "print()\n",
    "\n",
    "# Create SK kernel with plugin\n",
    "hybrid_kernel = Kernel()\n",
    "\n",
    "# Create AsyncAzureOpenAI client for SK\n",
    "hybrid_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=sk_endpoint,\n",
    "    api_version=\"2024-06-01\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "hybrid_chat_service = AzureChatCompletion(\n",
    "    service_id=\"hybrid_service\",\n",
    "    deployment_name=model_deployment,\n",
    "    async_client=hybrid_client,\n",
    ")\n",
    "\n",
    "hybrid_kernel.add_service(hybrid_chat_service)\n",
    "hybrid_kernel.add_plugin(EnterprisePlugin(), plugin_name=\"Enterprise\")\n",
    "\n",
    "print(\"✓ Semantic Kernel created with EnterprisePlugin\")\n",
    "print(\"  Functions: get_customer_info, calculate_discount, process_order\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Create Wrapper Functions for AutoGen\n",
    "# ============================================================================\n",
    "\n",
    "async def sk_get_customer(customer_id: Annotated[str, \"Customer ID\"]) -> str:\n",
    "    \"\"\"Get customer information using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"get_customer_info\"]\n",
    "    result = await func.invoke(hybrid_kernel, customer_id=customer_id)\n",
    "    return str(result)\n",
    "\n",
    "async def sk_calculate_discount(\n",
    "    tier: Annotated[str, \"Customer tier\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Calculate discount using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"calculate_discount\"]\n",
    "    result = await func.invoke(hybrid_kernel, tier=tier, amount=amount)\n",
    "    return str(result)\n",
    "\n",
    "async def sk_process_order(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Process order using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"process_order\"]\n",
    "    result = await func.invoke(hybrid_kernel, customer_id=customer_id, amount=amount)\n",
    "    return str(result)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create AutoGen Agents\n",
    "# ============================================================================\n",
    "\n",
    "# AutoGen configuration\n",
    "hybrid_autogen_config = {\n",
    "    \"model\": model_deployment,\n",
    "    \"api_type\": \"azure\",\n",
    "    \"api_key\": api_key,\n",
    "    \"base_url\": autogen_base_url,\n",
    "    \"api_version\": \"2024-06-01\",\n",
    "}\n",
    "\n",
    "config_list_hybrid = [hybrid_autogen_config]\n",
    "\n",
    "# Agent 1: Sales Agent (analyzes and recommends)\n",
    "sales_agent = ConversableAgent(\n",
    "    name=\"SalesAgent\",\n",
    "    system_message=(\n",
    "        \"You are a sales agent. Analyze customer information, calculate appropriate \"\n",
    "        \"discounts, and recommend actions. Be professional and detail-oriented. \"\n",
    "        \"Return 'TERMINATE' when task is complete.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list_hybrid, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "# Agent 2: Order Processor (executes orders)\n",
    "processor_agent = ConversableAgent(\n",
    "    name=\"OrderProcessor\",\n",
    "    system_message=(\n",
    "        \"You are an order processing agent. Execute orders after receiving \"\n",
    "        \"approval from sales agent. Confirm all details before processing.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list_hybrid, \"temperature\": 0.3},\n",
    ")\n",
    "\n",
    "# Agent 3: User Proxy\n",
    "hybrid_proxy = ConversableAgent(\n",
    "    name=\"Coordinator\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None\n",
    "                                   and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"✓ AutoGen agents created\")\n",
    "print(\"  1. SalesAgent - Analysis and recommendations\")\n",
    "print(\"  2. OrderProcessor - Order execution\")\n",
    "print(\"  3. Coordinator - Workflow management\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Register SK Functions with AutoGen Agents\n",
    "# ============================================================================\n",
    "\n",
    "def get_customer_sync(customer_id: Annotated[str, \"Customer ID\"]) -> str:\n",
    "    \"\"\"Sync wrapper for SK customer lookup.\"\"\"\n",
    "    import asyncio\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(sk_get_customer(customer_id))\n",
    "\n",
    "def calculate_discount_sync(\n",
    "    tier: Annotated[str, \"Customer tier\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Sync wrapper for SK discount calculation.\"\"\"\n",
    "    import asyncio\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(sk_calculate_discount(tier, amount))\n",
    "\n",
    "def process_order_sync(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Sync wrapper for SK order processing.\"\"\"\n",
    "    import asyncio\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(sk_process_order(customer_id, amount))\n",
    "\n",
    "# Register with agents\n",
    "sales_agent.register_for_llm(\n",
    "    name=\"get_customer\",\n",
    "    description=\"Get customer information by ID\"\n",
    ")(get_customer_sync)\n",
    "\n",
    "sales_agent.register_for_llm(\n",
    "    name=\"calculate_discount\",\n",
    "    description=\"Calculate discount based on tier and amount\"\n",
    ")(calculate_discount_sync)\n",
    "\n",
    "processor_agent.register_for_llm(\n",
    "    name=\"process_order\",\n",
    "    description=\"Process an order for a customer\"\n",
    ")(process_order_sync)\n",
    "\n",
    "hybrid_proxy.register_for_execution(name=\"get_customer\")(get_customer_sync)\n",
    "hybrid_proxy.register_for_execution(name=\"calculate_discount\")(calculate_discount_sync)\n",
    "hybrid_proxy.register_for_execution(name=\"process_order\")(process_order_sync)\n",
    "\n",
    "print(\"✓ SK functions registered with AutoGen agents\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Hybrid Orchestration Examples\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: Customer Order Workflow\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    response1 = hybrid_proxy.initiate_chat(\n",
    "        sales_agent,\n",
    "        message=(\n",
    "            \"Customer C003 wants to make a purchase of $10,000. \"\n",
    "            \"Look up their information, calculate their discount, \"\n",
    "            \"and process the order.\"\n",
    "        ),\n",
    "        max_turns=15\n",
    "    )\n",
    "    print(\"\\n✓ Example 1 complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Example 1 error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: Multi-Customer Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    response2 = hybrid_proxy.initiate_chat(\n",
    "        sales_agent,\n",
    "        message=(\n",
    "            \"Compare customers C001 and C002. For each, calculate what their \"\n",
    "            \"final price would be for a $5,000 purchase.\"\n",
    "        ),\n",
    "        max_turns=15\n",
    "    )\n",
    "    print(\"\\n✓ Example 2 complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Example 2 error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: Complex Business Logic\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    response3 = hybrid_proxy.initiate_chat(\n",
    "        sales_agent,\n",
    "        message=(\n",
    "            \"Find the best customer tier for a $50,000 purchase. \"\n",
    "            \"Show the calculations for all tiers and recommend which \"\n",
    "            \"tier a customer should have to get the best value.\"\n",
    "        ),\n",
    "        max_turns=15\n",
    "    )\n",
    "    print(\"\\n✓ Example 3 complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Example 3 error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID ORCHESTRATION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Framework combination: Semantic Kernel + AutoGen\")\n",
    "print(f\"SK plugins: EnterprisePlugin (3 functions)\")\n",
    "print(f\"AutoGen agents: SalesAgent, OrderProcessor, Coordinator\")\n",
    "print(f\"SK functions as AutoGen tools: 3\")\n",
    "print(f\"Examples executed: 3\")\n",
    "print(f\"All LLM calls routed through: {gateway_url}\")\n",
    "print(f\"Model: {model_deployment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Hybrid SK + AutoGen Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. SK plugins can serve as tools for AutoGen agents\")\n",
    "print(\"2. Combine SK's plugin architecture with AutoGen's orchestration\")\n",
    "print(\"3. SK handles business logic, AutoGen handles agent coordination\")\n",
    "print(\"4. All LLM calls (SK and AutoGen) route through same APIM gateway\")\n",
    "print(\"5. Hybrid approach leverages strengths of both frameworks\")\n",
    "print(\"6. Enterprise patterns: separation of concerns, reusable logic\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (AutoGen)",
   "language": "python",
   "name": "venv-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}