#!/usr/bin/env python3
"""
Add Message Storing and Vector Searching Labs to master-ai-gateway-fix-MCP-clean.ipynb
Inserts new labs after Lab 09 (Semantic Caching)
"""
import json
import sys
from pathlib import Path

def find_semantic_caching_end(cells):
    """Find the end of Lab 09 (Semantic Caching)"""
    for i, cell in enumerate(cells):
        if cell.get('cell_type') == 'markdown':
            source = ''.join(cell.get('source', []))
            if 'Lab 09: Semantic Caching' in source and '##' in source:
                # Look ahead for the next lab marker
                for j in range(i+1, len(cells)):
                    if cells[j].get('cell_type') == 'markdown':
                        next_source = ''.join(cells[j].get('source', []))
                        if 'Lab 10' in next_source or 'Lab 11' in next_source:
                            return j
                        # If we find another major section, return there
                        if next_source.strip().startswith('## '):
                            return j
    return None

def create_message_storing_cells():
    """Create cells for Message Storing lab"""

    header_cell = {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "<a id='lab10'></a>\n",
            "## Lab 10: Message Storing with Cosmos DB\n",
            "\n",
            "**Objective**: Capture and store AI conversation data in Cosmos DB for analytics and auditing.\n",
            "\n",
            "### What is Message Storing?\n",
            "\n",
            "This lab demonstrates how to store AI conversation details (prompts, completions, token counts) in Azure Cosmos DB. This enables:\n",
            "\n",
            "1. **Conversation History**: Track all AI interactions over time\n",
            "2. **Token Usage Analytics**: Monitor costs and usage patterns\n",
            "3. **Compliance & Auditing**: Maintain records for regulatory requirements\n",
            "4. **Quality Monitoring**: Analyze response quality and user satisfaction\n",
            "\n",
            "### Architecture\n",
            "\n",
            "```\n",
            "Client ‚Üí APIM Gateway ‚Üí Azure OpenAI\n",
            "            ‚Üì\n",
            "     [emit-metric policy]\n",
            "            ‚Üì\n",
            "     Azure Monitor Logs\n",
            "            ‚Üì\n",
            "     Diagnostic Settings\n",
            "            ‚Üì\n",
            "     Event Hub ‚Üí Stream Analytics\n",
            "            ‚Üì\n",
            "     Cosmos DB (messages container)\n",
            "```\n",
            "\n",
            "### Resources Used (Already Deployed)\n",
            "\n",
            "‚úÖ **Cosmos DB**: For storing conversation data\n",
            "- Account: `{COSMOS_ACCOUNT_NAME}`\n",
            "- Database: `llmdb` (will be created)\n",
            "- Container: `messages` (will be created)\n",
            "\n",
            "‚úÖ **APIM Service**: For routing and logging\n",
            "\n",
            "‚úÖ **Azure Monitor**: For capturing metrics (built-in)\n",
            "\n",
            "---"
        ]
    }

    setup_cosmos_cell = {
        "cell_type": "code",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Lab 10: Message Storing - Step 1: Setup Cosmos DB Database and Container\n",
            "\n",
            "# Load environment from master-lab.env\n",
            "import os\n",
            "from pathlib import Path\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "env_file = Path('master-lab.env')\n",
            "if env_file.exists():\n",
            "    load_dotenv(env_file)\n",
            "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
            "\n",
            "# Get Cosmos DB configuration\n",
            "cosmos_endpoint = os.environ.get('COSMOS_ENDPOINT')\n",
            "cosmos_key = os.environ.get('COSMOS_KEY')\n",
            "cosmos_account = os.environ.get('COSMOS_ACCOUNT_NAME')\n",
            "\n",
            "if not all([cosmos_endpoint, cosmos_key]):\n",
            "    print(\"[ERROR] Missing Cosmos DB environment variables\")\n",
            "    print(f\"COSMOS_ENDPOINT: {cosmos_endpoint}\")\n",
            "    print(f\"COSMOS_KEY: {'***' if cosmos_key else None}\")\n",
            "else:\n",
            "    print(f\"\\n[*] Step 1: Setting up Cosmos DB for message storage...\")\n",
            "    print(f\"    Cosmos Account: {cosmos_account}\")\n",
            "    print(f\"    Endpoint: {cosmos_endpoint}\")\n",
            "    \n",
            "    try:\n",
            "        from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
            "        \n",
            "        # Create Cosmos client\n",
            "        client = CosmosClient(cosmos_endpoint, cosmos_key)\n",
            "        \n",
            "        # Database configuration\n",
            "        database_name = 'llmdb'\n",
            "        container_name = 'messages'\n",
            "        \n",
            "        # Create database if it doesn't exist\n",
            "        try:\n",
            "            database = client.create_database(id=database_name)\n",
            "            print(f\"\\n‚úÖ Database '{database_name}' created\")\n",
            "        except exceptions.CosmosResourceExistsError:\n",
            "            database = client.get_database_client(database_name)\n",
            "            print(f\"\\n‚úÖ Database '{database_name}' already exists\")\n",
            "        \n",
            "        # Create container if it doesn't exist\n",
            "        try:\n",
            "            container = database.create_container(\n",
            "                id=container_name,\n",
            "                partition_key=PartitionKey(path=\"/conversationId\"),\n",
            "                offer_throughput=400\n",
            "            )\n",
            "            print(f\"‚úÖ Container '{container_name}' created\")\n",
            "            print(f\"   Partition Key: /conversationId\")\n",
            "            print(f\"   Throughput: 400 RU/s\")\n",
            "        except exceptions.CosmosResourceExistsError:\n",
            "            container = database.get_container_client(container_name)\n",
            "            print(f\"‚úÖ Container '{container_name}' already exists\")\n",
            "        \n",
            "        print(f\"\\n[OK] Step 1 Complete - Cosmos DB ready for message storage\")\n",
            "        \n",
            "        # Store for later use\n",
            "        cosmos_database_name = database_name\n",
            "        cosmos_container_name = container_name\n",
            "        \n",
            "    except ImportError:\n",
            "        print(\"\\n‚ö†Ô∏è  azure-cosmos package not installed\")\n",
            "        print(\"   Run: pip install azure-cosmos\")\n",
            "    except Exception as e:\n",
            "        print(f\"\\n‚ùå Error setting up Cosmos DB: {e}\")\n"
        ]
    }

    test_cell = {
        "cell_type": "code",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Lab 10: Message Storing - Step 2: Generate Sample Conversations\n",
            "\n",
            "import os\n",
            "from pathlib import Path\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "env_file = Path('master-lab.env')\n",
            "if env_file.exists():\n",
            "    load_dotenv(env_file)\n",
            "\n",
            "from openai import AzureOpenAI\n",
            "import time\n",
            "import uuid\n",
            "from datetime import datetime\n",
            "\n",
            "# Get configuration\n",
            "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
            "apim_api_key = os.environ.get('APIM_API_KEY')\n",
            "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
            "cosmos_endpoint = os.environ.get('COSMOS_ENDPOINT')\n",
            "cosmos_key = os.environ.get('COSMOS_KEY')\n",
            "\n",
            "print(\"\\n[*] Step 2: Generating sample conversations and storing in Cosmos DB...\")\n",
            "print(f\"    Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
            "print(f\"    Model: gpt-4o-mini\")\n",
            "\n",
            "# Initialize OpenAI client\n",
            "client = AzureOpenAI(\n",
            "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
            "    api_key=\"dummy\",\n",
            "    api_version=\"2024-08-01-preview\"\n",
            ")\n",
            "\n",
            "# Initialize Cosmos client\n",
            "try:\n",
            "    from azure.cosmos import CosmosClient\n",
            "    \n",
            "    cosmos_client = CosmosClient(cosmos_endpoint, cosmos_key)\n",
            "    database = cosmos_client.get_database_client('llmdb')\n",
            "    container = database.get_container_client('messages')\n",
            "    \n",
            "    # Sample conversations\n",
            "    conversations = [\n",
            "        \"What is Azure API Management?\",\n",
            "        \"Explain semantic caching in simple terms\",\n",
            "        \"How do I optimize AI costs?\",\n",
            "        \"What are the benefits of using APIM with Azure OpenAI?\",\n",
            "        \"Tell me about vector databases\"\n",
            "    ]\n",
            "    \n",
            "    conversation_id = str(uuid.uuid4())\n",
            "    stored_messages = []\n",
            "    \n",
            "    print(f\"\\n{'='*80}\")\n",
            "    print(\"üí¨ GENERATING CONVERSATIONS\")\n",
            "    print(f\"{'='*80}\")\n",
            "    print(f\"Conversation ID: {conversation_id}\")\n",
            "    \n",
            "    for i, question in enumerate(conversations, 1):\n",
            "        print(f\"\\n‚ñ∂Ô∏è  Message {i}/{len(conversations)}: {question}\")\n",
            "        \n",
            "        start_time = time.time()\n",
            "        try:\n",
            "            response = client.chat.completions.create(\n",
            "                model=\"gpt-4o-mini\",\n",
            "                messages=[\n",
            "                    {\"role\": \"system\", \"content\": \"You are a helpful Azure expert.\"},\n",
            "                    {\"role\": \"user\", \"content\": question}\n",
            "                ],\n",
            "                max_tokens=100,\n",
            "                extra_headers={'api-key': apim_api_key}\n",
            "            )\n",
            "            \n",
            "            response_time = time.time() - start_time\n",
            "            answer = response.choices[0].message.content\n",
            "            \n",
            "            # Create message document\n",
            "            message_doc = {\n",
            "                'id': str(uuid.uuid4()),\n",
            "                'conversationId': conversation_id,\n",
            "                'timestamp': datetime.utcnow().isoformat(),\n",
            "                'model': 'gpt-4o-mini',\n",
            "                'prompt': question,\n",
            "                'completion': answer,\n",
            "                'promptTokens': response.usage.prompt_tokens,\n",
            "                'completionTokens': response.usage.completion_tokens,\n",
            "                'totalTokens': response.usage.total_tokens,\n",
            "                'responseTime': round(response_time, 3)\n",
            "            }\n",
            "            \n",
            "            # Store in Cosmos DB\n",
            "            container.create_item(body=message_doc)\n",
            "            stored_messages.append(message_doc)\n",
            "            \n",
            "            print(f\"   üí¨ Response: {answer[:80]}...\")\n",
            "            print(f\"   üìä Tokens: {response.usage.total_tokens} ({response.usage.prompt_tokens} + {response.usage.completion_tokens})\")\n",
            "            print(f\"   ‚è±Ô∏è  Time: {response_time:.3f}s\")\n",
            "            print(f\"   ‚úÖ Stored in Cosmos DB\")\n",
            "            \n",
            "            time.sleep(0.5)  # Rate limiting\n",
            "            \n",
            "        except Exception as e:\n",
            "            print(f\"   ‚ùå Error: {str(e)[:100]}\")\n",
            "    \n",
            "    print(f\"\\n{'='*80}\")\n",
            "    print(\"üìä CONVERSATION SUMMARY\")\n",
            "    print(f\"{'='*80}\")\n",
            "    print(f\"Total Messages: {len(stored_messages)}\")\n",
            "    print(f\"Conversation ID: {conversation_id}\")\n",
            "    \n",
            "    if stored_messages:\n",
            "        total_tokens = sum(m['totalTokens'] for m in stored_messages)\n",
            "        avg_response_time = sum(m['responseTime'] for m in stored_messages) / len(stored_messages)\n",
            "        print(f\"Total Tokens: {total_tokens}\")\n",
            "        print(f\"Avg Response Time: {avg_response_time:.3f}s\")\n",
            "    \n",
            "    print(f\"\\n[OK] Step 2 Complete - Conversations stored in Cosmos DB\")\n",
            "    \n",
            "except ImportError:\n",
            "    print(\"\\n‚ö†Ô∏è  azure-cosmos package not installed\")\n",
            "    print(\"   Run: pip install azure-cosmos\")\n",
            "except Exception as e:\n",
            "    print(f\"\\n‚ùå Error: {e}\")\n"
        ]
    }

    query_cell = {
        "cell_type": "code",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Lab 10: Message Storing - Step 3: Query and Analyze Stored Messages\n",
            "\n",
            "import os\n",
            "from pathlib import Path\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "env_file = Path('master-lab.env')\n",
            "if env_file.exists():\n",
            "    load_dotenv(env_file)\n",
            "\n",
            "cosmos_endpoint = os.environ.get('COSMOS_ENDPOINT')\n",
            "cosmos_key = os.environ.get('COSMOS_KEY')\n",
            "\n",
            "print(\"\\n[*] Step 3: Querying stored messages from Cosmos DB...\")\n",
            "\n",
            "try:\n",
            "    from azure.cosmos import CosmosClient\n",
            "    import pandas as pd\n",
            "    \n",
            "    # Connect to Cosmos DB\n",
            "    cosmos_client = CosmosClient(cosmos_endpoint, cosmos_key)\n",
            "    database = cosmos_client.get_database_client('llmdb')\n",
            "    container = database.get_container_client('messages')\n",
            "    \n",
            "    # Query all messages\n",
            "    query = \"\"\"\n",
            "        SELECT \n",
            "            c.timestamp,\n",
            "            c.conversationId,\n",
            "            c.model,\n",
            "            c.prompt,\n",
            "            c.promptTokens,\n",
            "            c.completionTokens,\n",
            "            c.totalTokens,\n",
            "            c.responseTime\n",
            "        FROM c\n",
            "        ORDER BY c.timestamp DESC\n",
            "    \"\"\"\n",
            "    \n",
            "    items = list(container.query_items(\n",
            "        query=query,\n",
            "        enable_cross_partition_query=True\n",
            "    ))\n",
            "    \n",
            "    print(f\"\\n‚úÖ Found {len(items)} messages in Cosmos DB\")\n",
            "    \n",
            "    if items:\n",
            "        # Display as DataFrame\n",
            "        df = pd.DataFrame(items)\n",
            "        \n",
            "        print(f\"\\n{'='*80}\")\n",
            "        print(\"üìä MESSAGE ANALYTICS\")\n",
            "        print(f\"{'='*80}\")\n",
            "        \n",
            "        # Summary statistics\n",
            "        print(f\"\\nüìà Token Usage:\")\n",
            "        print(f\"   Total Tokens: {df['totalTokens'].sum()}\")\n",
            "        print(f\"   Avg Tokens per Message: {df['totalTokens'].mean():.1f}\")\n",
            "        print(f\"   Max Tokens: {df['totalTokens'].max()}\")\n",
            "        print(f\"   Min Tokens: {df['totalTokens'].min()}\")\n",
            "        \n",
            "        print(f\"\\n‚è±Ô∏è  Response Times:\")\n",
            "        print(f\"   Avg Response Time: {df['responseTime'].mean():.3f}s\")\n",
            "        print(f\"   Fastest: {df['responseTime'].min():.3f}s\")\n",
            "        print(f\"   Slowest: {df['responseTime'].max():.3f}s\")\n",
            "        \n",
            "        print(f\"\\nüí¨ Conversations:\")\n",
            "        print(f\"   Unique Conversations: {df['conversationId'].nunique()}\")\n",
            "        print(f\"   Total Messages: {len(df)}\")\n",
            "        \n",
            "        # Display recent messages\n",
            "        print(f\"\\nüìù Recent Messages:\")\n",
            "        print(df[['timestamp', 'prompt', 'totalTokens', 'responseTime']].head(10).to_string(index=False))\n",
            "        \n",
            "        # Token distribution by conversation\n",
            "        if 'conversationId' in df.columns:\n",
            "            print(f\"\\nüí∞ Token Usage by Conversation:\")\n",
            "            conv_summary = df.groupby('conversationId').agg({\n",
            "                'totalTokens': 'sum',\n",
            "                'prompt': 'count'\n",
            "            }).rename(columns={'prompt': 'messageCount'})\n",
            "            print(conv_summary.to_string())\n",
            "        \n",
            "        print(f\"\\n[OK] Step 3 Complete - Message analytics displayed\")\n",
            "    else:\n",
            "        print(\"\\n‚ö†Ô∏è  No messages found in database\")\n",
            "        print(\"   Run Step 2 to generate sample conversations\")\n",
            "        \n",
            "except ImportError as e:\n",
            "    print(f\"\\n‚ö†Ô∏è  Missing package: {e}\")\n",
            "    print(\"   Run: pip install azure-cosmos pandas\")\n",
            "except Exception as e:\n",
            "    print(f\"\\n‚ùå Error querying Cosmos DB: {e}\")\n",
            "\n",
            "print(\"\\n\" + \"=\"*80)\n",
            "print(\"üéâ LAB 10 COMPLETE: MESSAGE STORING\")\n",
            "print(\"=\"*80)\n",
            "print(\"\\nWhat you learned:\")\n",
            "print(\"‚úÖ How to set up Cosmos DB for storing AI conversations\")\n",
            "print(\"‚úÖ How to capture prompts, completions, and token counts\")\n",
            "print(\"‚úÖ How to query and analyze stored conversation data\")\n",
            "print(\"‚úÖ How to track usage patterns and costs\")\n",
            "print(\"\\nKey Benefits:\")\n",
            "print(\"üìä Analytics: Understand usage patterns and trends\")\n",
            "print(\"üí∞ Cost Tracking: Monitor token usage and costs\")\n",
            "print(\"üîç Auditing: Maintain complete conversation history\")\n",
            "print(\"üìà Insights: Analyze response quality and performance\")\n"
        ]
    }

    return [header_cell, setup_cosmos_cell, test_cell, query_cell]

def create_vector_searching_cells():
    """Create cells for Vector Searching lab"""

    header_cell = {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "<a id='lab11'></a>\n",
            "## Lab 11: Vector Searching with RAG Pattern\n",
            "\n",
            "**Objective**: Implement Retrieval Augmented Generation (RAG) using Azure AI Search and embeddings.\n",
            "\n",
            "### What is RAG (Retrieval Augmented Generation)?\n",
            "\n",
            "RAG combines the power of vector search with LLMs to provide accurate, context-aware responses based on your own data:\n",
            "\n",
            "1. **Vector Search**: Convert documents and queries to embeddings (vectors)\n",
            "2. **Similarity Matching**: Find documents similar to the query\n",
            "3. **Context Injection**: Add retrieved documents to the LLM prompt\n",
            "4. **Augmented Response**: LLM generates answers using your data\n",
            "\n",
            "### Architecture\n",
            "\n",
            "```\n",
            "User Query\n",
            "    ‚Üì\n",
            "[Convert to Embedding] ‚Üê text-embedding-3-small\n",
            "    ‚Üì\n",
            "[Vector Search in Azure AI Search] ‚Üí Find similar documents\n",
            "    ‚Üì\n",
            "[Inject into Prompt]\n",
            "    ‚Üì\n",
            "[Send to GPT-4o-mini] ‚Üê Generate answer\n",
            "    ‚Üì\n",
            "Response with citations\n",
            "```\n",
            "\n",
            "### Resources Used (Already Deployed)\n",
            "\n",
            "‚úÖ **Azure AI Search**: For vector storage and similarity search\n",
            "- Service: `{SEARCH_SERVICE_NAME}`\n",
            "- Index: `movies` (will be created)\n",
            "\n",
            "‚úÖ **Embedding Model**: `text-embedding-3-small`\n",
            "- For vectorizing queries and documents\n",
            "\n",
            "‚úÖ **GPT-4o-mini**: For generating responses\n",
            "\n",
            "‚úÖ **APIM Gateway**: For routing all requests\n",
            "\n",
            "---"
        ]
    }

    setup_search_cell = {
        "cell_type": "code",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Lab 11: Vector Searching - Step 1: Create Search Index and Load Movie Data\n",
            "\n",
            "# Load environment from master-lab.env\n",
            "import os\n",
            "from pathlib import Path\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "env_file = Path('master-lab.env')\n",
            "if env_file.exists():\n",
            "    load_dotenv(env_file)\n",
            "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
            "\n",
            "# Get configuration\n",
            "search_endpoint = os.environ.get('SEARCH_ENDPOINT')\n",
            "search_key = os.environ.get('SEARCH_ADMIN_KEY')\n",
            "search_service_name = os.environ.get('SEARCH_SERVICE_NAME')\n",
            "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
            "apim_api_key = os.environ.get('APIM_API_KEY')\n",
            "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
            "\n",
            "print(f\"\\n[*] Step 1: Setting up Azure AI Search for vector searching...\")\n",
            "print(f\"    Search Service: {search_service_name}\")\n",
            "print(f\"    Search Endpoint: {search_endpoint}\")\n",
            "print(f\"    Index Name: movies-rag\")\n",
            "\n",
            "try:\n",
            "    from azure.search.documents import SearchClient\n",
            "    from azure.search.documents.indexes import SearchIndexClient\n",
            "    from azure.search.documents.indexes.models import (\n",
            "        SearchIndex,\n",
            "        SimpleField,\n",
            "        SearchableField,\n",
            "        SearchField,\n",
            "        SearchFieldDataType,\n",
            "        VectorSearch,\n",
            "        VectorSearchProfile,\n",
            "        HnswAlgorithmConfiguration\n",
            "    )\n",
            "    from azure.core.credentials import AzureKeyCredential\n",
            "    from openai import AzureOpenAI\n",
            "    \n",
            "    # Initialize Search Index Client\n",
            "    index_client = SearchIndexClient(\n",
            "        endpoint=search_endpoint,\n",
            "        credential=AzureKeyCredential(search_key)\n",
            "    )\n",
            "    \n",
            "    # Define index schema\n",
            "    index_name = \"movies-rag\"\n",
            "    \n",
            "    fields = [\n",
            "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
            "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
            "        SearchableField(name=\"genre\", type=SearchFieldDataType.String),\n",
            "        SearchableField(name=\"overview\", type=SearchFieldDataType.String),\n",
            "        SearchField(\n",
            "            name=\"overview_vector\",\n",
            "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
            "            searchable=True,\n",
            "            vector_search_dimensions=1536,\n",
            "            vector_search_profile_name=\"movies-vector-profile\"\n",
            "        ),\n",
            "    ]\n",
            "    \n",
            "    # Vector search configuration\n",
            "    vector_search = VectorSearch(\n",
            "        profiles=[VectorSearchProfile(\n",
            "            name=\"movies-vector-profile\",\n",
            "            algorithm_configuration_name=\"movies-hnsw-config\"\n",
            "        )],\n",
            "        algorithms=[HnswAlgorithmConfiguration(\n",
            "            name=\"movies-hnsw-config\"\n",
            "        )],\n",
            "    )\n",
            "    \n",
            "    # Create or update index\n",
            "    index = SearchIndex(\n",
            "        name=index_name,\n",
            "        fields=fields,\n",
            "        vector_search=vector_search\n",
            "    )\n",
            "    \n",
            "    result = index_client.create_or_update_index(index)\n",
            "    print(f\"\\n‚úÖ Search index '{result.name}' created/updated\")\n",
            "    \n",
            "    # Sample movie data\n",
            "    movies = [\n",
            "        {\"id\": \"1\", \"title\": \"The Avengers\", \"genre\": \"Action, Sci-Fi\", \"overview\": \"Earth's mightiest heroes must come together and learn to fight as a team if they are going to stop the mischievous Loki and his alien army from enslaving humanity.\"},\n",
            "        {\"id\": \"2\", \"title\": \"The Dark Knight\", \"genre\": \"Action, Crime, Drama\", \"overview\": \"When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.\"},\n",
            "        {\"id\": \"3\", \"title\": \"Inception\", \"genre\": \"Action, Sci-Fi, Thriller\", \"overview\": \"A thief who steals corporate secrets through the use of dream-sharing technology is given the inverse task of planting an idea into the mind of a C.E.O.\"},\n",
            "        {\"id\": \"4\", \"title\": \"Interstellar\", \"genre\": \"Adventure, Drama, Sci-Fi\", \"overview\": \"A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\"},\n",
            "        {\"id\": \"5\", \"title\": \"The Matrix\", \"genre\": \"Action, Sci-Fi\", \"overview\": \"A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers.\"},\n",
            "        {\"id\": \"6\", \"title\": \"Pulp Fiction\", \"genre\": \"Crime, Drama\", \"overview\": \"The lives of two mob hitmen, a boxer, a gangster and his wife intertwine in four tales of violence and redemption.\"},\n",
            "        {\"id\": \"7\", \"title\": \"Forrest Gump\", \"genre\": \"Drama, Romance\", \"overview\": \"The presidencies of Kennedy and Johnson, the Vietnam War, and other historical events unfold from the perspective of an Alabama man with an IQ of 75.\"},\n",
            "        {\"id\": \"8\", \"title\": \"The Shawshank Redemption\", \"genre\": \"Drama\", \"overview\": \"Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.\"},\n",
            "    ]\n",
            "    \n",
            "    # Initialize OpenAI client for embeddings\n",
            "    openai_client = AzureOpenAI(\n",
            "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
            "        api_key=\"dummy\",\n",
            "        api_version=\"2024-08-01-preview\"\n",
            "    )\n",
            "    \n",
            "    print(f\"\\n[*] Generating embeddings for {len(movies)} movies...\")\n",
            "    \n",
            "    # Add embeddings to movies\n",
            "    for movie in movies:\n",
            "        # Generate embedding for overview\n",
            "        response = openai_client.embeddings.create(\n",
            "            model=\"text-embedding-3-small\",\n",
            "            input=movie['overview'],\n",
            "            extra_headers={'api-key': apim_api_key}\n",
            "        )\n",
            "        movie['overview_vector'] = response.data[0].embedding\n",
            "        print(f\"   ‚úÖ {movie['title']} - embedding generated\")\n",
            "    \n",
            "    # Upload documents to search index\n",
            "    search_client = SearchClient(\n",
            "        endpoint=search_endpoint,\n",
            "        index_name=index_name,\n",
            "        credential=AzureKeyCredential(search_key)\n",
            "    )\n",
            "    \n",
            "    result = search_client.upload_documents(documents=movies)\n",
            "    print(f\"\\n‚úÖ Uploaded {len(movies)} movies to search index\")\n",
            "    print(f\"\\n[OK] Step 1 Complete - Search index ready with vector embeddings\")\n",
            "    \n",
            "except ImportError as e:\n",
            "    print(f\"\\n‚ö†Ô∏è  Missing package: {e}\")\n",
            "    print(\"   Run: pip install azure-search-documents openai\")\n",
            "except Exception as e:\n",
            "    print(f\"\\n‚ùå Error: {e}\")\n"
        ]
    }

    rag_test_cell = {
        "cell_type": "code",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Lab 11: Vector Searching - Step 2: Implement RAG Pattern\n",
            "\n",
            "import os\n",
            "from pathlib import Path\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "env_file = Path('master-lab.env')\n",
            "if env_file.exists():\n",
            "    load_dotenv(env_file)\n",
            "\n",
            "# Get configuration\n",
            "search_endpoint = os.environ.get('SEARCH_ENDPOINT')\n",
            "search_key = os.environ.get('SEARCH_ADMIN_KEY')\n",
            "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
            "apim_api_key = os.environ.get('APIM_API_KEY')\n",
            "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
            "\n",
            "print(\"\\n[*] Step 2: Testing RAG Pattern with Vector Search...\")\n",
            "\n",
            "try:\n",
            "    from azure.search.documents import SearchClient\n",
            "    from azure.search.documents.models import VectorizedQuery\n",
            "    from azure.core.credentials import AzureKeyCredential\n",
            "    from openai import AzureOpenAI\n",
            "    \n",
            "    # Initialize clients\n",
            "    openai_client = AzureOpenAI(\n",
            "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
            "        api_key=\"dummy\",\n",
            "        api_version=\"2024-08-01-preview\"\n",
            "    )\n",
            "    \n",
            "    search_client = SearchClient(\n",
            "        endpoint=search_endpoint,\n",
            "        index_name=\"movies-rag\",\n",
            "        credential=AzureKeyCredential(search_key)\n",
            "    )\n",
            "    \n",
            "    # Test queries\n",
            "    queries = [\n",
            "        \"What are the best superhero movies?\",\n",
            "        \"Tell me about sci-fi movies involving space travel\",\n",
            "        \"What movies feature virtual reality or dream manipulation?\"\n",
            "    ]\n",
            "    \n",
            "    print(f\"\\n{'='*80}\")\n",
            "    print(\"üîç RAG PATTERN DEMONSTRATION\")\n",
            "    print(f\"{'='*80}\")\n",
            "    \n",
            "    for query in queries:\n",
            "        print(f\"\\n{'‚îÄ'*80}\")\n",
            "        print(f\"‚ùì Query: {query}\")\n",
            "        print(f\"{'‚îÄ'*80}\")\n",
            "        \n",
            "        # Step 1: Convert query to embedding\n",
            "        print(\"\\n[1] Converting query to embedding...\")\n",
            "        embedding_response = openai_client.embeddings.create(\n",
            "            model=\"text-embedding-3-small\",\n",
            "            input=query,\n",
            "            extra_headers={'api-key': apim_api_key}\n",
            "        )\n",
            "        query_vector = embedding_response.data[0].embedding\n",
            "        print(f\"    ‚úÖ Query vectorized ({len(query_vector)} dimensions)\")\n",
            "        \n",
            "        # Step 2: Vector search\n",
            "        print(\"\\n[2] Searching for similar movies...\")\n",
            "        vector_query = VectorizedQuery(\n",
            "            vector=query_vector,\n",
            "            k_nearest_neighbors=3,\n",
            "            fields=\"overview_vector\"\n",
            "        )\n",
            "        \n",
            "        results = list(search_client.search(\n",
            "            search_text=None,\n",
            "            vector_queries=[vector_query],\n",
            "            select=[\"title\", \"genre\", \"overview\"],\n",
            "            top=3\n",
            "        ))\n",
            "        \n",
            "        print(f\"    ‚úÖ Found {len(results)} relevant movies:\")\n",
            "        for i, result in enumerate(results, 1):\n",
            "            print(f\"       {i}. {result['title']} ({result['genre']})\")\n",
            "        \n",
            "        # Step 3: Build context from search results\n",
            "        context = \"\\n\\n\".join([\n",
            "            f\"Movie: {r['title']}\\nGenre: {r['genre']}\\nOverview: {r['overview']}\"\n",
            "            for r in results\n",
            "        ])\n",
            "        \n",
            "        # Step 4: Generate augmented response\n",
            "        print(\"\\n[3] Generating AI response with context...\")\n",
            "        \n",
            "        prompt = f\"\"\"Based on the following movie information, answer the user's question.\n",
            "Only use the provided movie data. If the data doesn't contain relevant information, say so.\n",
            "\n",
            "Movie Data:\n",
            "{context}\n",
            "\n",
            "User Question: {query}\n",
            "\n",
            "Provide a detailed answer with movie titles and why they match the query.\"\"\"\n",
            "        \n",
            "        response = openai_client.chat.completions.create(\n",
            "            model=\"gpt-4o-mini\",\n",
            "            messages=[\n",
            "                {\"role\": \"system\", \"content\": \"You are a helpful movie recommendation assistant.\"},\n",
            "                {\"role\": \"user\", \"content\": prompt}\n",
            "            ],\n",
            "            max_tokens=200,\n",
            "            extra_headers={'api-key': apim_api_key}\n",
            "        )\n",
            "        \n",
            "        answer = response.choices[0].message.content\n",
            "        \n",
            "        print(f\"\\nüí° AI Response:\")\n",
            "        print(f\"   {answer}\")\n",
            "        print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")\n",
            "    \n",
            "    print(f\"\\n{'='*80}\")\n",
            "    print(\"\\n[OK] Step 2 Complete - RAG pattern demonstrated successfully\")\n",
            "    \n",
            "except ImportError as e:\n",
            "    print(f\"\\n‚ö†Ô∏è  Missing package: {e}\")\n",
            "    print(\"   Run: pip install azure-search-documents openai\")\n",
            "except Exception as e:\n",
            "    print(f\"\\n‚ùå Error: {e}\")\n",
            "\n",
            "print(\"\\n\" + \"=\"*80)\n",
            "print(\"üéâ LAB 11 COMPLETE: VECTOR SEARCHING WITH RAG\")\n",
            "print(\"=\"*80)\n",
            "print(\"\\nWhat you learned:\")\n",
            "print(\"‚úÖ How to create vector embeddings from text\")\n",
            "print(\"‚úÖ How to store and index vectors in Azure AI Search\")\n",
            "print(\"‚úÖ How to perform vector similarity search\")\n",
            "print(\"‚úÖ How to implement RAG pattern for accurate responses\")\n",
            "print(\"\\nKey Benefits:\")\n",
            "print(\"üéØ Accuracy: Responses grounded in your own data\")\n",
            "print(\"üìö Context: LLM has access to relevant information\")\n",
            "print(\"üîç Semantic Search: Find similar content, not just keywords\")\n",
            "print(\"üí° Intelligence: Combine search with generative AI\")\n"
        ]
    }

    return [header_cell, setup_search_cell, rag_test_cell]

def main():
    notebook_path = Path('master-ai-gateway-fix-MCP-clean.ipynb')

    if not notebook_path.exists():
        print(f"ERROR: Notebook not found: {notebook_path}")
        return 1

    print(f"Reading notebook: {notebook_path}")

    with open(notebook_path, 'r', encoding='utf-8') as f:
        notebook = json.load(f)

    cells = notebook.get('cells', [])
    print(f"Total cells in notebook: {len(cells)}")

    # Find insertion point (after Semantic Caching lab)
    insertion_point = find_semantic_caching_end(cells)

    if insertion_point is None:
        print("ERROR: Could not find Lab 09 (Semantic Caching) end position")
        return 1

    print(f"Found insertion point at cell index: {insertion_point}")

    # Create new cells for both labs
    message_cells = create_message_storing_cells()
    vector_cells = create_vector_searching_cells()

    all_new_cells = message_cells + vector_cells

    print(f"Created {len(message_cells)} cells for Message Storing lab")
    print(f"Created {len(vector_cells)} cells for Vector Searching lab")
    print(f"Total new cells: {len(all_new_cells)}")

    # Insert new cells
    notebook['cells'] = cells[:insertion_point] + all_new_cells + cells[insertion_point:]

    print(f"New total cells: {len(notebook['cells'])}")

    # Backup original
    backup_path = notebook_path.with_suffix('.ipynb.backup-before-message-vector-labs')
    print(f"Creating backup: {backup_path}")

    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(json.load(open(notebook_path, 'r', encoding='utf-8')), f, indent=2)

    # Write updated notebook
    print(f"Writing updated notebook: {notebook_path}")

    with open(notebook_path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=2, ensure_ascii=False)

    print("\\n" + "="*80)
    print("‚úÖ SUCCESS: Message Storing and Vector Searching Labs added!")
    print("="*80)
    print(f"\\nLabs inserted after Lab 09 (Semantic Caching)")
    print(f"Total cells added: {len(all_new_cells)}")
    print(f"\\nBackup created: {backup_path}")
    print(f"\\nNew lab structure:")
    print(f"  - Lab 08: Model Routing")
    print(f"  - Lab 09: Semantic Caching")
    print(f"  - Lab 10: Message Storing (NEW)")
    print(f"  - Lab 11: Vector Searching with RAG (NEW)")
    print(f"  - Lab 12: AI Foundry SDK (previously Lab 10)")
    print(f"  - Lab 13: AI Foundry DeepSeek (previously Lab 11)")

    return 0

if __name__ == '__main__':
    sys.exit(main())
