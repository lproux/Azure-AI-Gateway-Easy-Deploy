<policies>
    <inbound>
        <base />
        <!-- Check API Key (your existing policy) -->
        <check-header name="api-key" failed-check-httpcode="401" failed-check-error-message="Missing or invalid API key" />

        <!-- NEW: Semantic Cache Lookup - Check Redis for similar prompts -->
        <azure-openai-semantic-cache-lookup
            score-threshold="0.8"
            embeddings-backend-id="embeddings-backend"
            embeddings-backend-auth="system-assigned" />

        <!-- Route to backend pool (your existing policy) -->
        <set-backend-service backend-id="inference-backend-pool" />
    </inbound>
    <backend>
        <!-- Retry logic for rate limits and service unavailable (your existing policy) -->
        <retry count="2" interval="0" first-fast-retry="true" condition="@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)">
            <forward-request buffer-request-body="true" />
        </retry>
    </backend>
    <outbound>
        <base />
        <!-- NEW: Store response in Redis cache for 2 minutes -->
        <azure-openai-semantic-cache-store duration="120" />
    </outbound>
    <on-error>
        <base />
        <!-- Error handling for 503 (your existing policy) -->
        <choose>
            <when condition="@(context.Response.StatusCode == 503)">
                <return-response>
                    <set-status code="503" reason="Service Unavailable" />
                    <set-header name="Content-Type" exists-action="override">
                        <value>application/json</value>
                    </set-header>
                    <set-body>{"error": {"code": "ServiceUnavailable", "message": "Service temporarily unavailable"}}</set-body>
                </return-response>
            </when>
        </choose>
    </on-error>
</policies>
