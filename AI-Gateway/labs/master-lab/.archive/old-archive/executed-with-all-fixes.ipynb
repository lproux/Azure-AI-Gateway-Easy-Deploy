{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_0_dcf63404",
   "metadata": {},
   "source": [
    "# Master AI Gateway Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell_1_eea3122c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:55.765516Z",
     "iopub.status.busy": "2025-11-17T18:36:55.765274Z",
     "iopub.status.idle": "2025-11-17T18:36:55.773383Z",
     "shell.execute_reply": "2025-11-17T18:36:55.772845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRun these cells (-1.x) in order before using legacy sections.\\nOrder:\\n  (-1.1) Env Loader\\n  (-1.2) Dependencies Install\\n  (-1.3) Azure CLI & Service Principal\\n  (-1.4) Endpoint Normalizer\\n  (upcoming) (-1.5) Deployment Helpers\\n  (upcoming) (-1.6) Unified Deployment Orchestrator\\n  (upcoming) (-1.7) Unified Policy Application\\n  (upcoming) (-1.8) Unified MCP Initialization\\nLegacy cells retained below for reference.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (-1.0) Section -1: Consolidated Provisioning & Initialization\n",
    "\"\"\"\n",
    "Run these cells (-1.x) in order before using legacy sections.\n",
    "Order:\n",
    "  (-1.1) Env Loader\n",
    "  (-1.2) Dependencies Install\n",
    "  (-1.3) Azure CLI & Service Principal\n",
    "  (-1.4) Endpoint Normalizer\n",
    "  (upcoming) (-1.5) Deployment Helpers\n",
    "  (upcoming) (-1.6) Unified Deployment Orchestrator\n",
    "  (upcoming) (-1.7) Unified Policy Application\n",
    "  (upcoming) (-1.8) Unified MCP Initialization\n",
    "Legacy cells retained below for reference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell_2_d100dc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:55.802917Z",
     "iopub.status.busy": "2025-11-17T18:36:55.802724Z",
     "iopub.status.idle": "2025-11-17T18:36:55.875353Z",
     "shell.execute_reply": "2025-11-17T18:36:55.874668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] ‚úÖ Derived APIM_SERVICE = apim-pavavy6pu5hpa\n",
      "[env] ‚úÖ Using default API_ID = inference-api\n",
      "[env] ‚úÖ BICEP_DIR = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "[env] ‚úÖ Loaded 49 environment variables\n",
      "[env] ‚úÖ Configuration: lab-master-lab @ norwayeast\n",
      "[env] ‚úÖ APIM Gateway: https://apim-pavavy6pu5hpa.azure-api.net...\n"
     ]
    }
   ],
   "source": [
    "# (-1.1) Consolidated Environment Loader (Enhanced)\n",
    "\"\"\"\n",
    "Single source of truth for environment configuration.\n",
    "Enhancements:\n",
    "- Auto-creates master-lab.env if missing\n",
    "- Loads and validates environment variables\n",
    "- Derives APIM_SERVICE from APIM_GATEWAY_URL if missing\n",
    "- Sets BICEP_DIR for deployment files\n",
    "- Provides NotebookConfig dataclass for structured access\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import re, os\n",
    "\n",
    "ENV_FILE = Path('master-lab.env')\n",
    "TEMPLATE = \"\"\"# master-lab.env (auto-generated template)\n",
    "SUBSCRIPTION_ID=\n",
    "RESOURCE_GROUP=\n",
    "LOCATION=uksouth\n",
    "APIM_GATEWAY_URL=\n",
    "APIM_SERVICE=\n",
    "API_ID=inference-api\n",
    "INFERENCE_API_PATH=/inference\n",
    "OPENAI_ENDPOINT=\n",
    "MODEL_SKU=gpt-4o-mini\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class NotebookConfig:\n",
    "    \"\"\"Structured configuration object\"\"\"\n",
    "    subscription_id: str = \"\"\n",
    "    resource_group: str = \"\"\n",
    "    location: str = \"uksouth\"\n",
    "    apim_gateway_url: str = \"\"\n",
    "    apim_service: str = \"\"\n",
    "    api_id: str = \"inference-api\"\n",
    "    inference_api_path: str = \"/inference\"\n",
    "    openai_endpoint: Optional[str] = None\n",
    "    model_sku: str = \"gpt-4o-mini\"\n",
    "\n",
    "def ensure_env():\n",
    "    \"\"\"Load environment file, create if missing\"\"\"\n",
    "    if not ENV_FILE.exists():\n",
    "        ENV_FILE.write_text(TEMPLATE, encoding='utf-8')\n",
    "        print(f\"[env] Created {ENV_FILE} - PLEASE FILL IN VALUES\")\n",
    "        return {}\n",
    "\n",
    "    env = {}\n",
    "    for line in ENV_FILE.read_text(encoding='utf-8').splitlines():\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            key, value = key.strip(), value.strip()\n",
    "            if value:  # Only set non-empty values\n",
    "                env[key] = value\n",
    "                os.environ[key] = value\n",
    "\n",
    "    # Auto-derive APIM_SERVICE if missing\n",
    "    if 'APIM_SERVICE' not in env and 'APIM_GATEWAY_URL' in env:\n",
    "        match = re.search(r'//([^.]+)', env['APIM_GATEWAY_URL'])\n",
    "        if match:\n",
    "            env['APIM_SERVICE'] = match.group(1)\n",
    "            os.environ['APIM_SERVICE'] = env['APIM_SERVICE']\n",
    "            print(f\"[env] ‚úÖ Derived APIM_SERVICE = {env['APIM_SERVICE']}\")\n",
    "\n",
    "    # Set default API_ID if missing\n",
    "    if 'API_ID' not in env:\n",
    "        env['API_ID'] = 'inference-api'\n",
    "        os.environ['API_ID'] = env['API_ID']\n",
    "        print(f\"[env] ‚úÖ Using default API_ID = {env['API_ID']}\")\n",
    "\n",
    "    return env\n",
    "\n",
    "# Load environment\n",
    "ENV = ensure_env()\n",
    "\n",
    "# Create config object for structured access\n",
    "config = NotebookConfig(\n",
    "    subscription_id=ENV.get('SUBSCRIPTION_ID', ''),\n",
    "    resource_group=ENV.get('RESOURCE_GROUP', ''),\n",
    "    location=ENV.get('LOCATION', 'uksouth'),\n",
    "    apim_gateway_url=ENV.get('APIM_GATEWAY_URL', ''),\n",
    "    apim_service=ENV.get('APIM_SERVICE', ''),\n",
    "    api_id=ENV.get('API_ID', 'inference-api'),\n",
    "    inference_api_path=ENV.get('INFERENCE_API_PATH', '/inference'),\n",
    "    openai_endpoint=ENV.get('OPENAI_ENDPOINT'),\n",
    "    model_sku=ENV.get('MODEL_SKU', 'gpt-4o-mini')\n",
    ")\n",
    "\n",
    "# Set BICEP_DIR for deployment files\n",
    "# HAVE TO CAHNGE IN FINAL COMMIT\n",
    "BICEP_DIR = Path(\"archive/scripts\")\n",
    "if not BICEP_DIR.exists():\n",
    "    print(f\"[env] ‚ö†Ô∏è  BICEP_DIR not found: {BICEP_DIR.resolve()}\")\n",
    "    BICEP_DIR = Path(\".\")  # Fallback\n",
    "else:\n",
    "    print(f\"[env] ‚úÖ BICEP_DIR = {BICEP_DIR.resolve()}\")\n",
    "\n",
    "os.environ['BICEP_DIR'] = str(BICEP_DIR.resolve())\n",
    "\n",
    "# Summary\n",
    "print(f\"[env] ‚úÖ Loaded {len(ENV)} environment variables\")\n",
    "print(f\"[env] ‚úÖ Configuration: {config.resource_group} @ {config.location}\")\n",
    "if config.apim_gateway_url:\n",
    "    print(f\"[env] ‚úÖ APIM Gateway: {config.apim_gateway_url[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell_3_41f69468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:55.877105Z",
     "iopub.status.busy": "2025-11-17T18:36:55.876954Z",
     "iopub.status.idle": "2025-11-17T18:36:56.423104Z",
     "shell.execute_reply": "2025-11-17T18:36:56.422589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deps] /usr/bin/python3 -m pip install -r requirements.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[deps] ‚ö†Ô∏è pip exit 1 stderr: \u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m√ó\u001b[0m This environment is externally managed\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xy\n"
     ]
    }
   ],
   "source": [
    "# (-1.2) Dependencies Install (Consolidated)\n",
    "import sys, subprocess, pathlib, shlex\n",
    "REQ_FILE = pathlib.Path('requirements.txt')\n",
    "if REQ_FILE.exists():\n",
    "    cmd=[sys.executable,'-m','pip','install','-r',str(REQ_FILE)]\n",
    "    print('[deps]',' '.join(shlex.quote(c) for c in cmd))\n",
    "    r=subprocess.run(cmd,capture_output=True,text=True)\n",
    "    print(r.stdout[:800])\n",
    "    if r.returncode==0: print('[deps] ‚úÖ complete')\n",
    "    else: print('[deps] ‚ö†Ô∏è pip exit',r.returncode,'stderr:',r.stderr[:200])\n",
    "else:\n",
    "    print('[deps] requirements.txt missing; skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell_4_820d7759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:56.424833Z",
     "iopub.status.busy": "2025-11-17T18:36:56.424636Z",
     "iopub.status.idle": "2025-11-17T18:36:58.243887Z",
     "shell.execute_reply": "2025-11-17T18:36:58.242993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az resolved: /usr/bin/az (reason=ranked selection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az version: azure-cli                         2.78.0 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] Active subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[azure] Loading existing credentials file\n",
      "  SUBSCRIPTION_ID=d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  AZURE_CLIENT_ID=4a5d0f1a-578e-479a-8ba9-05770ae9ce6b\n",
      "  AZURE_TENANT_ID=2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "  AZURE_CLIENT_SECRET=***\n"
     ]
    }
   ],
   "source": [
    "# (-1.3) Azure CLI & Service Principal Setup (Consolidated v2)\n",
    "import json, os, shutil, subprocess, sys, time\n",
    "from pathlib import Path\n",
    "AZ_CREDS_FILE=Path('.azure-credentials.env')\n",
    "\n",
    "OS_RELEASE = {}\n",
    "try:\n",
    "    if Path('/etc/os-release').exists():\n",
    "        for line in Path('/etc/os-release').read_text().splitlines():\n",
    "            if '=' in line:\n",
    "                k,v=line.split('=',1)\n",
    "                OS_RELEASE[k]=v.strip().strip('\"')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ARCH_LINUX = OS_RELEASE.get('ID') == 'arch'\n",
    "CODESPACES = bool(os.environ.get('CODESPACES')) or bool(os.environ.get('CODESPACE_NAME'))\n",
    "\n",
    "def resolve_az_cli():\n",
    "    # 1. Explicit override\n",
    "    override=os.environ.get('AZURE_CLI_PATH')\n",
    "    if override and Path(override).exists():\n",
    "        return override, 'env AZURE_CLI_PATH'\n",
    "    candidates = []\n",
    "    # which-based\n",
    "    for name in ['az','az.cmd','az.exe']:\n",
    "        p=shutil.which(name)\n",
    "        if p: candidates.append(p)\n",
    "    # Common Linux / macOS locations\n",
    "    candidates += [\n",
    "        '/usr/bin/az', '/usr/local/bin/az', '/snap/bin/az', '/opt/homebrew/bin/az'\n",
    "    ]\n",
    "    # Codespaces typical path (if pip user install)\n",
    "    if CODESPACES:\n",
    "        candidates.append(str(Path.home()/'.local/bin/az'))\n",
    "    # Windows typical install locations\n",
    "    candidates += [\n",
    "        'C:/Program Files (x86)/Microsoft SDKs/Azure/CLI2/wbin/az.cmd',\n",
    "        'C:/Program Files/Microsoft SDKs/Azure/CLI2/wbin/az.cmd'\n",
    "    ]\n",
    "    # Home azure-cli shim\n",
    "    home_cli = Path.home()/'.azure-cli/az'\n",
    "    candidates.append(str(home_cli))\n",
    "    # Remove non-existing\n",
    "    existing=[c for c in candidates if c and Path(c).exists()]\n",
    "    if not existing:\n",
    "        # Last-resort: if a pip install put az inside .venv Scripts\n",
    "        venv_az = Path(sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "        if venv_az.exists():\n",
    "            return str(venv_az), 'venv fallback'\n",
    "        return None, 'not found'\n",
    "    # Rank: prefer system-level (exclude .venv & Scripts) then shortest path\n",
    "    def rank(p):\n",
    "        p_low=p.lower()\n",
    "        penalty = 1000 if ('.venv' in p_low or 'scripts' in p_low) else 0\n",
    "        return penalty, len(p)\n",
    "    existing.sort(key=rank)\n",
    "    chosen=existing[0]\n",
    "    return chosen, 'ranked selection'\n",
    "\n",
    "az_cli, reason = resolve_az_cli()\n",
    "print(f'[azure] az resolved: {az_cli or \"NOT FOUND\"} (reason={reason})')\n",
    "if not az_cli:\n",
    "    if ARCH_LINUX:\n",
    "        print('[azure] Arch Linux detected. Install Azure CLI: sudo pacman -S azure-cli')\n",
    "    else:\n",
    "        print('[azure] Install Azure CLI: https://learn.microsoft.com/cli/azure/install-azure-cli')\n",
    "    raise SystemExit('Azure CLI not found.')\n",
    "\n",
    "os.environ['AZ_CLI']=az_cli\n",
    "# Quick version check with short timeout\n",
    "try:\n",
    "    ver=subprocess.run([az_cli,'--version'],capture_output=True,text=True,timeout=4)\n",
    "    if ver.returncode==0:\n",
    "        first_line=ver.stdout.splitlines()[0] if ver.stdout else ''\n",
    "        print('[azure] az version:', first_line)\n",
    "    else:\n",
    "        print('[azure] az --version exit', ver.returncode)\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('[azure] WARN: az version check timed out (continuing)')\n",
    "except Exception as e:\n",
    "    print('[azure] WARN: az version check error:', e)\n",
    "\n",
    "# Subscription discovery (robust with timeout retries)\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')  # existing env takes precedence\n",
    "sub_proc = None\n",
    "if not subscription_id:\n",
    "    attempts = 2\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        try:\n",
    "            timeout_sec = 8 if attempt == 1 else 20  # longer second attempt\n",
    "            sub_proc = subprocess.run(\n",
    "                [az_cli, 'account', 'show', '--output', 'json'],\n",
    "                capture_output=True, text=True, timeout=timeout_sec\n",
    "            )\n",
    "            if sub_proc.returncode == 0:\n",
    "                try:\n",
    "                    sub = json.loads(sub_proc.stdout)\n",
    "                    subscription_id = sub.get('id')\n",
    "                    print('[azure] Active subscription:', subscription_id)\n",
    "                    if subscription_id:\n",
    "                        os.environ.setdefault('SUBSCRIPTION_ID', subscription_id)\n",
    "                except Exception as e:\n",
    "                    print('[azure] Parse error account show:', e)\n",
    "                break\n",
    "            else:\n",
    "                print(f'[azure] account show failed (rc={sub_proc.returncode}): {sub_proc.stderr[:200]}')\n",
    "                break  # non-timeout failure; do not retry\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f'[azure] account show timed out (attempt {attempt}/{attempts}, timeout={timeout_sec}s)')\n",
    "            if attempt < attempts:\n",
    "                time.sleep(retry_delay_sec)  # use existing retry delay variable\n",
    "            else:\n",
    "                print('[azure] ERROR: account show timed out; skipping subscription discovery')\n",
    "else:\n",
    "    print('[azure] Using existing SUBSCRIPTION_ID from environment:', subscription_id)\n",
    "\n",
    "# Ensure Service Principal\n",
    "sp_env_keys=['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID']\n",
    "creds_present=all(os.environ.get(k) for k in sp_env_keys)\n",
    "if creds_present:\n",
    "    print('[azure] SP credentials already present; skipping creation')\n",
    "elif AZ_CREDS_FILE.exists():\n",
    "    print('[azure] Loading existing credentials file')\n",
    "    for line in AZ_CREDS_FILE.read_text().splitlines():\n",
    "        if line.strip() and '=' in line:\n",
    "            k,v=line.split('=',1); os.environ.setdefault(k.strip(),v.strip())\n",
    "else:\n",
    "    if not os.environ.get('SUBSCRIPTION_ID'):\n",
    "        print('[azure] Cannot create SP: missing SUBSCRIPTION_ID')\n",
    "    else:\n",
    "        print('[azure] Creating new service principal (Contributor)')\n",
    "        sp_cmd=[az_cli,'ad','sp','create-for-rbac','--name','ai-gateway-sp','--role','Contributor','--scopes',f\"/subscriptions/{os.environ.get('SUBSCRIPTION_ID','')}\",\"--sdk-auth\"]\n",
    "        r=subprocess.run(sp_cmd,capture_output=True,text=True,timeout=40)\n",
    "        if r.returncode!=0:\n",
    "            print('[azure] SP creation failed:', r.stderr[:300])\n",
    "        else:\n",
    "            data=json.loads(r.stdout)\n",
    "            mapping={'clientId':'AZURE_CLIENT_ID','clientSecret':'AZURE_CLIENT_SECRET','tenantId':'AZURE_TENANT_ID','subscriptionId':'SUBSCRIPTION_ID'}\n",
    "            for src,dst in mapping.items():\n",
    "                if src in data:\n",
    "                    os.environ[dst]=data[src]\n",
    "            lines=[f'{k}={os.environ[k]}' for k in mapping.values() if k in os.environ]\n",
    "            AZ_CREDS_FILE.write_text('\\n'.join(lines))\n",
    "            print('[azure] SP created & credentials saved (.azure-credentials.env)')\n",
    "\n",
    "# Masked summary\n",
    "for k in ['SUBSCRIPTION_ID','AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET']:\n",
    "    v=os.environ.get(k)\n",
    "    if not v: continue\n",
    "    masked='***' if 'SECRET' in k else v\n",
    "    print(f'  {k}={masked}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell_4b_msal_helper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:58.247142Z",
     "iopub.status.busy": "2025-11-17T18:36:58.246640Z",
     "iopub.status.idle": "2025-11-17T18:36:58.253481Z",
     "shell.execute_reply": "2025-11-17T18:36:58.252743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[msal] MSAL cache flush helpers loaded\n",
      "[msal] Available functions: flush_msal_cache(), az_with_msal_retry()\n"
     ]
    }
   ],
   "source": [
    "# (-1.3b) MSAL Cache Flush Helper\n",
    "\"\"\"Helper function to flush MSAL cache when Azure CLI encounters MSAL corruption.\n",
    "\n",
    "The MSAL error 'Can't get attribute NormalizedResponse' indicates cache corruption.\n",
    "This helper safely clears the MSAL cache and retries Azure CLI operations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def flush_msal_cache():\n",
    "    \"\"\"Flush MSAL cache directories to resolve cache corruption.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if cache was flushed successfully\n",
    "    \"\"\"\n",
    "    msal_cache_dirs = [\n",
    "        Path.home() / '.azure' / 'msal_token_cache.bin',\n",
    "        Path.home() / '.azure' / 'msal_token_cache.json',\n",
    "        Path.home() / '.azure' / 'msal_http_cache',\n",
    "        Path.home() / '.azure' / 'service_principal_entries.bin',\n",
    "    ]\n",
    "    \n",
    "    flushed = []\n",
    "    for cache_path in msal_cache_dirs:\n",
    "        try:\n",
    "            if cache_path.exists():\n",
    "                if cache_path.is_file():\n",
    "                    cache_path.unlink()\n",
    "                    flushed.append(str(cache_path))\n",
    "                elif cache_path.is_dir():\n",
    "                    shutil.rmtree(cache_path)\n",
    "                    flushed.append(str(cache_path))\n",
    "        except Exception as e:\n",
    "            print(f'[msal] Warning: Could not remove {cache_path}: {e}')\n",
    "    \n",
    "    if flushed:\n",
    "        print(f'[msal] Flushed {len(flushed)} cache entries')\n",
    "        return True\n",
    "    else:\n",
    "        print('[msal] No cache entries found to flush')\n",
    "        return False\n",
    "\n",
    "def az_with_msal_retry(az_cli, command_args, **kwargs):\n",
    "    \"\"\"Execute Azure CLI command with automatic MSAL cache flush on error.\n",
    "    \n",
    "    Args:\n",
    "        az_cli: Path to az CLI executable\n",
    "        command_args: List of command arguments (e.g., ['account', 'show'])\n",
    "        **kwargs: Additional arguments for subprocess.run()\n",
    "    \n",
    "    Returns:\n",
    "        subprocess.CompletedProcess: Result of the command\n",
    "    \"\"\"\n",
    "    # Ensure capture_output and text are set\n",
    "    kwargs.setdefault('capture_output', True)\n",
    "    kwargs.setdefault('text', True)\n",
    "    kwargs.setdefault('timeout', 30)\n",
    "    \n",
    "    # First attempt\n",
    "    result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "    \n",
    "    # Check for MSAL error\n",
    "    if result.returncode != 0 and 'NormalizedResponse' in result.stderr:\n",
    "        print('[msal] MSAL cache corruption detected, flushing cache...')\n",
    "        flush_msal_cache()\n",
    "        \n",
    "        # Re-login if needed\n",
    "        print('[msal] Re-authenticating...')\n",
    "        login_result = subprocess.run(\n",
    "            [az_cli, 'login'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if login_result.returncode == 0:\n",
    "            print('[msal] Re-authentication successful, retrying command...')\n",
    "            # Retry the original command\n",
    "            result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "        else:\n",
    "            print(f'[msal] Re-authentication failed: {login_result.stderr[:200]}')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('[msal] MSAL cache flush helpers loaded')\n",
    "print('[msal] Available functions: flush_msal_cache(), az_with_msal_retry()')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell_5_c9ea7412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:58.255789Z",
     "iopub.status.busy": "2025-11-17T18:36:58.255582Z",
     "iopub.status.idle": "2025-11-17T18:36:58.270007Z",
     "shell.execute_reply": "2025-11-17T18:36:58.269062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[endpoint] Existing OPENAI_ENDPOINT found; using as-is\n",
      "[endpoint] OPENAI_ENDPOINT = https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "[endpoint] Derived convenience vars: OPENAI_API_BASE, OPENAI_MODELS_URL\n"
     ]
    }
   ],
   "source": [
    "# (-1.4) Endpoint Normalizer & Derived Variables\n",
    "\"\"\"\n",
    "Derives OPENAI_ENDPOINT and related derived variables if missing.\n",
    "Logic priority:\n",
    "1. Use explicit OPENAI_ENDPOINT if set (leave unchanged).\n",
    "2. Else if APIM_GATEWAY_URL + INFERENCE_API_PATH present -> compose.\n",
    "3. Else attempt Foundry style endpoints (AZURE_OPENAI_ENDPOINT, AI_FOUNDRY_ENDPOINT).\n",
    "Persist back to master-lab.env if value was newly derived.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os, re\n",
    "env_path=Path('master-lab.env')\n",
    "text=env_path.read_text() if env_path.exists() else ''\n",
    "get=lambda k: os.environ.get(k) or re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE).group(1).strip() if re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE) else ''\n",
    "openai_endpoint=get('OPENAI_ENDPOINT')\n",
    "modified=False\n",
    "if openai_endpoint:\n",
    "    print('[endpoint] Existing OPENAI_ENDPOINT found; using as-is')\n",
    "else:\n",
    "    apim=get('APIM_GATEWAY_URL')\n",
    "    path_var=get('INFERENCE_API_PATH') or '/inference'\n",
    "    if apim:\n",
    "        openai_endpoint=apim.rstrip('/')+path_var\n",
    "        print('[endpoint] Derived from APIM_GATEWAY_URL + INFERENCE_API_PATH')\n",
    "        modified=True\n",
    "    else:\n",
    "        fallback=get('AZURE_OPENAI_ENDPOINT') or get('AI_FOUNDRY_ENDPOINT')\n",
    "        if fallback:\n",
    "            openai_endpoint=fallback.rstrip('/')\n",
    "            print('[endpoint] Derived from Foundry/Azure fallback endpoint')\n",
    "            modified=True\n",
    "        else:\n",
    "            print('[endpoint] Unable to derive endpoint; please set OPENAI_ENDPOINT manually in master-lab.env')\n",
    "if openai_endpoint:\n",
    "    os.environ['OPENAI_ENDPOINT']=openai_endpoint\n",
    "    print('[endpoint] OPENAI_ENDPOINT =', openai_endpoint)\n",
    "    if modified and env_path.exists():\n",
    "        # update file\n",
    "        lines=[]\n",
    "        found=False\n",
    "        for line in text.splitlines():\n",
    "            if line.startswith('OPENAI_ENDPOINT='):\n",
    "                lines.append(f'OPENAI_ENDPOINT={openai_endpoint}')\n",
    "                found=True\n",
    "            else:\n",
    "                lines.append(line)\n",
    "        if not found:\n",
    "            lines.append(f'OPENAI_ENDPOINT={openai_endpoint}')\n",
    "        env_path.write_text('\\n'.join(lines))\n",
    "        print('[endpoint] Persisted derived endpoint to master-lab.env')\n",
    "# Convenience derived variables (could be referenced later)\n",
    "os.environ.setdefault('OPENAI_API_BASE', openai_endpoint)\n",
    "os.environ.setdefault('OPENAI_MODELS_URL', openai_endpoint.rstrip('/') + '/models')\n",
    "print('[endpoint] Derived convenience vars: OPENAI_API_BASE, OPENAI_MODELS_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell_6_6bbec029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:58.272071Z",
     "iopub.status.busy": "2025-11-17T18:36:58.271880Z",
     "iopub.status.idle": "2025-11-17T18:36:59.546182Z",
     "shell.execute_reply": "2025-11-17T18:36:59.544448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[az] version: azure-cli                         2.78.0 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[az] account: ME-MngEnvMCAP592090-lproux-1 d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n"
     ]
    }
   ],
   "source": [
    "# (-1.5) Unified az() Helper & Login Check\n",
    "\"\"\"Provides a cached az CLI executor with:\n",
    "- Path reuse via AZ_CLI env (expects (-1.3) run first)\n",
    "- Automatic login prompt if account show fails and no service principal creds\n",
    "- Timeout controls & JSON parsing convenience\n",
    "Usage:\n",
    "    ok, data = az('account show', json_out=True)\n",
    "    ok, text = az('apim list --resource-group X')\n",
    "\"\"\"\n",
    "import os, subprocess, json, shlex\n",
    "from pathlib import Path\n",
    "AZ_CLI = os.environ.get('AZ_CLI') or os.environ.get('AZURE_CLI_PATH')\n",
    "_cached_version=None\n",
    "\n",
    "def az(cmd:str, json_out:bool=False, timeout:int=25, login_if_needed:bool=True):\n",
    "    global _cached_version\n",
    "    if not AZ_CLI:\n",
    "        return False, 'AZ_CLI not set; run (-1.3) first.'\n",
    "    parts=[AZ_CLI]+shlex.split(cmd)\n",
    "    try:\n",
    "        proc=subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, f'timeout after {timeout}s: {cmd}'\n",
    "    if proc.returncode!=0:\n",
    "        stderr=proc.stderr.strip()\n",
    "        if login_if_needed and 'az login' in stderr.lower():\n",
    "            # If SP creds exist, attempt non-interactive login; else instruct.\n",
    "            sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET'])\n",
    "            if sp_ok:\n",
    "                sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} \"\n",
    "                        f\"-p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "                print('[az] Attempting SP login ...')\n",
    "                lp=subprocess.run([AZ_CLI]+shlex.split(sp_cmd),capture_output=True,text=True,timeout=40)\n",
    "                if lp.returncode==0:\n",
    "                    print('[az] SP login successful; retrying command')\n",
    "                    return az(cmd,json_out=json_out,timeout=timeout,login_if_needed=False)\n",
    "                else:\n",
    "                    print('[az] SP login failed:', lp.stderr[:180])\n",
    "            else:\n",
    "                print('[az] Interactive login required: run \"az login\" in a terminal.')\n",
    "        return False, stderr or proc.stdout\n",
    "    out=proc.stdout\n",
    "    if json_out:\n",
    "        try:\n",
    "            return True, json.loads(out or '{}')\n",
    "        except Exception as e:\n",
    "            return False, f'json parse error: {e}\\nRaw: {out[:200]}'\n",
    "    return True, out\n",
    "\n",
    "# Cache version lazily\n",
    "if not _cached_version:\n",
    "    ok, ver = az('--version', json_out=False, timeout=5, login_if_needed=False)\n",
    "    if ok:\n",
    "        _cached_version=ver.splitlines()[0] if ver else ''\n",
    "        print('[az] version:', _cached_version)\n",
    "    else:\n",
    "        print('[az] version check skipped:', ver[:120])\n",
    "\n",
    "# Quick account context (suppresses login if SP already authenticated)\n",
    "ok, acct = az('account show', json_out=True, timeout=10)\n",
    "if ok:\n",
    "    print('[az] account:', acct.get('name'), acct.get('id'))\n",
    "else:\n",
    "    print('[az] account show issue:', acct[:160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell_7_778421b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:59.550149Z",
     "iopub.status.busy": "2025-11-17T18:36:59.549818Z",
     "iopub.status.idle": "2025-11-17T18:36:59.558888Z",
     "shell.execute_reply": "2025-11-17T18:36:59.557811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shim] AzureOpenAI shim ready.\n",
      "[deploy] helpers ready\n"
     ]
    }
   ],
   "source": [
    "# (-1.6) Deployment Helpers (Consolidated)\n",
    "\"\"\"Utilities for ARM/Bicep deployments via az CLI.\n",
    "Depends on az() from (-1.5).\n",
    "Functions:\n",
    "  compile_bicep(bicep_path) -> str json_template_path\n",
    "  deploy_template(rg, name, template_file, params: dict) -> (ok, result_json)\n",
    "  get_deployment_outputs(rg, name) -> dict outputs or {}\n",
    "  ensure_deployment(rg, name, template, params, skip_if_exists=True)\n",
    "\"\"\"\n",
    "import os, json, tempfile, pathlib, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "def compile_bicep(bicep_path:str):\n",
    "    b=Path(bicep_path)\n",
    "    if not b.exists():\n",
    "        raise FileNotFoundError(f'Bicep file not found: {bicep_path}')\n",
    "    out_json = b.with_suffix('.json')\n",
    "    ok, res = az(f'bicep build --file {shlex.quote(str(b))}')\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Failed bicep build: {res}')\n",
    "    if not out_json.exists():\n",
    "        raise RuntimeError(f'Expected compiled template missing: {out_json}')\n",
    "    print('[deploy] compiled', bicep_path, '->', out_json)\n",
    "    return str(out_json)\n",
    "\n",
    "def deploy_template(rg:str, name:str, template_file:str, params:dict):\n",
    "    param_args=[]\n",
    "    for k,v in params.items():\n",
    "        if isinstance(v, (dict,list)):\n",
    "            # Write complex params to temp file\n",
    "            tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "            tmp.write_text(json.dumps({\"value\": v}, indent=2))\n",
    "            param_args.append(f'{k}=@{tmp}')\n",
    "        else:\n",
    "            param_args.append(f'{k}={json.dumps(v)}')\n",
    "    params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "    cmd=f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}'\n",
    "    print('[deploy] running:', cmd)\n",
    "    ok, res = az(cmd, json_out=True, timeout=600)\n",
    "    return ok, res\n",
    "\n",
    "def get_deployment_outputs(rg:str, name:str):\n",
    "    ok,res = az(f'deployment group show --resource-group {rg} --name {name}', json_out=True)\n",
    "    if not ok:\n",
    "        print('[deploy] show failed:', res[:140])\n",
    "        return {}\n",
    "    outputs = res.get('properties',{}).get('outputs',{})\n",
    "    simplified={k: v.get('value') for k,v in outputs.items()} if isinstance(outputs, dict) else {}\n",
    "    print('[deploy] outputs keys:', ', '.join(simplified.keys()))\n",
    "    return simplified\n",
    "\n",
    "def check_deployment_exists(rg:str, name:str):\n",
    "    ok,res=az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=15)\n",
    "    return ok and res.get('name')==name\n",
    "\n",
    "def ensure_deployment(rg:str, name:str, bicep_file:str, params:dict, skip_if_exists:bool=True):\n",
    "    if skip_if_exists and check_deployment_exists(rg,name):\n",
    "        print('[deploy] existing deployment found:', name)\n",
    "        return get_deployment_outputs(rg,name)\n",
    "    template=compile_bicep(bicep_file) if bicep_file.endswith('.bicep') else bicep_file\n",
    "    ok,res=deploy_template(rg,name,template,params)\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Deployment {name} failed: {res}')\n",
    "    return get_deployment_outputs(rg,name)\n",
    "\n",
    "# AzureOpenAI Compatibility Import Shim\n",
    "# Some cells use: from openai import AzureOpenAI\n",
    "# Provide a unified accessor that can adapt if future SDK reorganizes paths.\n",
    "\n",
    "def get_azure_openai_client(**kwargs):\n",
    "    try:\n",
    "        from openai import AzureOpenAI  # standard location\n",
    "        return AzureOpenAI(**kwargs)\n",
    "    except ImportError as ex:\n",
    "        raise ImportError(\"AzureOpenAI class not found; ensure openai>=2.2,<3 installed.\") from ex\n",
    "\n",
    "print('[shim] AzureOpenAI shim ready.')\n",
    "\n",
    "print('[deploy] helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell_8_a9abfe41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:59.561732Z",
     "iopub.status.busy": "2025-11-17T18:36:59.561399Z",
     "iopub.status.idle": "2025-11-17T18:36:59.571281Z",
     "shell.execute_reply": "2025-11-17T18:36:59.570370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] apply_policies(policies) ready with auto-discovery (using Azure REST API)\n"
     ]
    }
   ],
   "source": [
    "# (-1.7) Unified Policy Application with Auto-Discovery\n",
    "\n",
    "\"\"\"Applies one or more API Management policies to the target API using Azure REST API.\n",
    "\n",
    "Provide policies as a list of (policy_name, policy_xml_string).\n",
    "\n",
    "Automatically discovers the API ID if not set in environment.\n",
    "Creates policy payloads and invokes az rest to apply them.\n",
    "\n",
    "Requires ENV values: RESOURCE_GROUP, APIM_SERVICE (service name)\n",
    "Optional: API_ID (will be auto-discovered if not provided)\n",
    "\n",
    "Note: Uses Azure REST API because 'az apim api policy' command is not available in all CLI versions.\n",
    "\"\"\"\n",
    "\n",
    "import os, json as json_module, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_POLICY_ENV=['RESOURCE_GROUP','APIM_SERVICE']\n",
    "\n",
    "missing=[k for k in REQUIRED_POLICY_ENV if not os.environ.get(k)]\n",
    "\n",
    "if missing:\n",
    "    print('[policy] Missing env vars; set:', ', '.join(missing))\n",
    "else:\n",
    "    def discover_api_id():\n",
    "        \"\"\"Discover the API ID from APIM instance.\"\"\"\n",
    "        service = os.environ['APIM_SERVICE']\n",
    "        rg = os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get subscription ID\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print('[policy] Failed to get subscription ID')\n",
    "            return None\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "\n",
    "        # List APIs using REST API\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{service}/apis?api-version=2022-08-01')\n",
    "\n",
    "        print('[policy] Discovering APIs in APIM instance...')\n",
    "        ok, result = az(f'rest --method get --url \"{url}\"', json_out=True, timeout=60)\n",
    "\n",
    "        if not ok or not result:\n",
    "            print('[policy] Failed to list APIs')\n",
    "            return None\n",
    "\n",
    "        apis = result.get('value', [])\n",
    "\n",
    "        if not apis:\n",
    "            print('[policy] ERROR: No APIs found in APIM instance')\n",
    "            print('[policy] HINT: You may need to deploy the infrastructure first')\n",
    "            return None\n",
    "\n",
    "        # Prefer APIs with 'openai' in the name\n",
    "        openai_apis = [api for api in apis if 'openai' in api.get('name', '').lower()]\n",
    "\n",
    "        if openai_apis:\n",
    "            api_id = openai_apis[0]['name']\n",
    "            print(f'[policy] Found OpenAI API: {api_id}')\n",
    "        else:\n",
    "            api_id = apis[0]['name']\n",
    "            print(f'[policy] Using first available API: {api_id}')\n",
    "\n",
    "        return api_id\n",
    "\n",
    "    def apply_policies(policies):\n",
    "        service=os.environ['APIM_SERVICE']\n",
    "        rg=os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get or discover API_ID\n",
    "        api_id = os.environ.get('API_ID')\n",
    "\n",
    "        if not api_id:\n",
    "            print('[policy] API_ID not set in environment, discovering...')\n",
    "            api_id = discover_api_id()\n",
    "\n",
    "            if not api_id:\n",
    "                print('[policy] ERROR: Could not discover API ID')\n",
    "                print('[policy] HINT: Set API_ID environment variable or deploy infrastructure')\n",
    "                return\n",
    "\n",
    "            # Save for future use\n",
    "            os.environ['API_ID'] = api_id\n",
    "            print(f'[policy] Saved API_ID to environment: {api_id}')\n",
    "\n",
    "        # Get subscription ID\n",
    "        print('[policy] Getting subscription ID...')\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print(f'[policy] Failed to get subscription ID: {sub_result}')\n",
    "            return\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "        print(f'[policy] Subscription ID: {subscription_id}')\n",
    "        print(f'[policy] Using API ID: {api_id}')\n",
    "\n",
    "        for name, xml in policies:\n",
    "            xml = xml.strip()\n",
    "\n",
    "            # Azure REST API endpoint for APIM policy\n",
    "            url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "                   f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "                   f'/service/{service}/apis/{api_id}/policies/policy?api-version=2022-08-01')\n",
    "\n",
    "            # Policy payload in Azure format\n",
    "            policy_payload = {\n",
    "                \"properties\": {\n",
    "                    \"value\": xml,\n",
    "                    \"format\": \"xml\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write JSON payload to temp file (Windows-friendly)\n",
    "            payload_file = Path(tempfile.gettempdir()) / f'apim-{name}-payload.json'\n",
    "            with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "                json_module.dump(policy_payload, f, indent=2)\n",
    "\n",
    "            print(f'[policy] Applying {name} via REST API...')\n",
    "\n",
    "            # Use az rest command with @file syntax for body\n",
    "            cmd = f'rest --method put --url \"{url}\" --body @\"{payload_file}\" --headers \"Content-Type=application/json\"'\n",
    "\n",
    "            ok, res = az(cmd, json_out=False, timeout=120)\n",
    "\n",
    "            # Clean up temp file\n",
    "            try:\n",
    "                payload_file.unlink()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if ok:\n",
    "                print(f'[policy] {name} applied successfully')\n",
    "            else:\n",
    "                error_msg = str(res)[:400] if res else 'Unknown error'\n",
    "                print(f'[policy] {name} failed: {error_msg}')\n",
    "\n",
    "    print('[policy] apply_policies(policies) ready with auto-discovery (using Azure REST API)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell_9_9200941f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:59.574180Z",
     "iopub.status.busy": "2025-11-17T18:36:59.573925Z",
     "iopub.status.idle": "2025-11-17T18:36:59.793961Z",
     "shell.execute_reply": "2025-11-17T18:36:59.793076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing MCP Client with 4 Data Sources...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MCP Client initialized successfully!\n",
      "üìä Available: 4/4 data sources\n",
      "\n",
      "üì° Data Sources:\n",
      "  1. Excel Analytics MCP\n",
      "     URL: http://excel-mcp-master.eastus.azurecontainer.io:8000\n",
      "     Type: Direct MCP Protocol\n",
      "     Capabilities: Analytics, charts, calculations\n",
      "\n",
      "  2. Research Documents MCP\n",
      "     URL: http://docs-mcp-master.eastus.azurecontainer.io:8000\n",
      "     Type: Direct MCP Protocol\n",
      "     Capabilities: Document search, retrieval, comparison\n",
      "\n",
      "  3. GitHub REST API (via APIM)\n",
      "     URL: https://apim-pavavy6pu5hpa.azure-api.net/github\n",
      "     Type: APIM-Routed REST API\n",
      "     Capabilities: Repo search, code analysis, issues\n",
      "\n",
      "  4. OpenWeather API (via APIM)\n",
      "     URL: https://apim-pavavy6pu5hpa.azure-api.net/weather\n",
      "     Type: APIM-Routed REST API\n",
      "     Capabilities: Real-time weather, forecasts\n",
      "\n",
      "üí° Configuration loaded from .mcp-servers-config\n",
      "   Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\n"
     ]
    }
   ],
   "source": [
    "# (-1.8) Unified MCP Initialization (Updated for 4 Data Sources)\n",
    "\"\"\"Initializes MCP servers and APIM-routed APIs.\n",
    "\n",
    "Available Data Sources:\n",
    "  1. Excel MCP (direct) - Analytics, charts, data processing\n",
    "  2. Docs MCP (direct) - Document search, retrieval\n",
    "  3. GitHub API (APIM) - Code repos, search\n",
    "  4. Weather API (APIM) - Real-time weather data\n",
    "\n",
    "Reads configuration from .mcp-servers-config file.\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "# Check if already initialized\n",
    "if 'mcp' in globals() and hasattr(mcp, 'excel'):\n",
    "    print(\"‚ö†Ô∏è  MCP Client already initialized. Skipping re-initialization.\")\n",
    "    print()\n",
    "    print(\"Available Data Sources:\")\n",
    "    if mcp.excel:\n",
    "        print(f\"  ‚úì Excel MCP: {mcp.excel.server_url}\")\n",
    "    if mcp.docs:\n",
    "        print(f\"  ‚úì Docs MCP: {mcp.docs.server_url}\")\n",
    "    if mcp.github:\n",
    "        url = getattr(mcp.github, 'base_url', 'configured')\n",
    "        print(f\"  ‚úì GitHub API (APIM): {url}\")\n",
    "    if mcp.weather:\n",
    "        url = getattr(mcp.weather, 'base_url', 'configured')\n",
    "        print(f\"  ‚úì Weather API (APIM): {url}\")\n",
    "else:\n",
    "    print(\"üîÑ Initializing MCP Client with 4 Data Sources...\")\n",
    "    print()\n",
    "    try:\n",
    "        mcp = MCPClient()\n",
    "\n",
    "        # Count available sources\n",
    "        available = []\n",
    "        if mcp.excel:\n",
    "            available.append(\"Excel MCP\")\n",
    "        if mcp.docs:\n",
    "            available.append(\"Docs MCP\")\n",
    "        if mcp.github:\n",
    "            available.append(\"GitHub API\")\n",
    "        if mcp.weather:\n",
    "            available.append(\"Weather API\")\n",
    "\n",
    "        print(f\"‚úÖ MCP Client initialized successfully!\")\n",
    "        print(f\"üìä Available: {len(available)}/4 data sources\")\n",
    "        print()\n",
    "        print(f\"üì° Data Sources:\")\n",
    "\n",
    "        if mcp.excel:\n",
    "            print(f\"  1. Excel Analytics MCP\")\n",
    "            print(f\"     URL: {mcp.excel.server_url}\")\n",
    "            print(f\"     Type: Direct MCP Protocol\")\n",
    "            print(f\"     Capabilities: Analytics, charts, calculations\")\n",
    "            print()\n",
    "\n",
    "        if mcp.docs:\n",
    "            print(f\"  2. Research Documents MCP\")\n",
    "            print(f\"     URL: {mcp.docs.server_url}\")\n",
    "            print(f\"     Type: Direct MCP Protocol\")\n",
    "            print(f\"     Capabilities: Document search, retrieval, comparison\")\n",
    "            print()\n",
    "\n",
    "        if mcp.github:\n",
    "            url = getattr(mcp.github, 'base_url', 'configured')\n",
    "            print(f\"  3. GitHub REST API (via APIM)\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     Type: APIM-Routed REST API\")\n",
    "            print(f\"     Capabilities: Repo search, code analysis, issues\")\n",
    "            print()\n",
    "\n",
    "        if mcp.weather:\n",
    "            url = getattr(mcp.weather, 'base_url', 'configured')\n",
    "            print(f\"  4. OpenWeather API (via APIM)\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     Type: APIM-Routed REST API\")\n",
    "            print(f\"     Capabilities: Real-time weather, forecasts\")\n",
    "            print()\n",
    "\n",
    "        if len(available) < 4:\n",
    "            print(\"‚ö†Ô∏è  Some data sources not configured:\")\n",
    "            if not mcp.excel:\n",
    "                print(\"  - Excel MCP: Set EXCEL_MCP_URL\")\n",
    "            if not mcp.docs:\n",
    "                print(\"  - Docs MCP: Set DOCS_MCP_URL\")\n",
    "            if not mcp.github:\n",
    "                print(\"  - GitHub API: Set APIM_GITHUB_URL + APIM_SUBSCRIPTION_KEY\")\n",
    "            if not mcp.weather:\n",
    "                print(\"  - Weather API: Set APIM_WEATHER_URL + OPENWEATHER_API_KEY\")\n",
    "            print()\n",
    "\n",
    "        print(f\"üí° Configuration loaded from .mcp-servers-config\")\n",
    "        print(f\"   Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize MCP Client: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "## For backward compatibility\n",
    "#MCP_SERVERS = {}\n",
    "#if mcp.excel:\n",
    "#    MCP_SERVERS['excel'] = mcp.excel\n",
    "#if mcp.docs:\n",
    "#    MCP_SERVERS['docs'] = mcp.docs\n",
    "#if mcp.github:\n",
    "#    MCP_SERVERS['github'] = mcp.github\n",
    "#if mcp.weather:\n",
    "#    MCP_SERVERS['weather'] = mcp.weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell_10_78abcae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:36:59.796317Z",
     "iopub.status.busy": "2025-11-17T18:36:59.796121Z",
     "iopub.status.idle": "2025-11-17T18:37:01.477520Z",
     "shell.execute_reply": "2025-11-17T18:37:01.476774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureOps] SDK import error; fallback to CLI deployments: No module named 'azure.mgmt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureOps] CLI: /usr/bin/az\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureOps] login status: OK\n",
      "[AzureOps] version: azure-cli                         2.78.0 *\n",
      "[AzureOps] strategy: cli\n"
     ]
    }
   ],
   "source": [
    "# (-1.9) Unified AzureOps Wrapper (Enhanced SDK Strategy)\n",
    "\"\"\"High-level Azure operations wrapper consolidating:\n",
    "- CLI resolution & version\n",
    "- Service principal / interactive login fallback\n",
    "- Generic az() invocation (JSON/text)\n",
    "- Resource group ensure (CLI or SDK)\n",
    "- Bicep compile (CLI) + group deployment (CLI or SDK)\n",
    "- AI Foundry model deployments (SDK)\n",
    "- APIM policy fragments + API policy apply (with rollback)\n",
    "- Deployment outputs retrieval & simplification\n",
    "- MCP server health probing\n",
    "\n",
    "Strategy:\n",
    "    AzureOps(strategy='sdk' | 'cli')  # default 'sdk' to favor richer status & long-running handling.\n",
    "\n",
    "Example:\n",
    "    AZ_OPS = AzureOps(strategy='sdk')\n",
    "    AZ_OPS.ensure_login()\n",
    "    AZ_OPS.ensure_resource_group(rg, location)\n",
    "    tpl = AZ_OPS.compile_bicep('deploy-01-core.bicep')\n",
    "    ok, res = AZ_OPS.deploy_group(rg,'core',tpl, params={})\n",
    "    outputs = AZ_OPS.get_deployment_outputs(rg,'core')\n",
    "    AZ_OPS.ensure_policy_fragment(rg, service, 'semanticCacheFragment', xml)\n",
    "    AZ_OPS.apply_api_policy_with_fragments(rg, service, api_id, ['semanticCacheFragment','contentSafetyFragment'])\n",
    "\n",
    "NOTE: Legacy helper cells remain for reference; prefer AzureOps going forward.\n",
    "\"\"\"\n",
    "import os, shutil, subprocess, json, time, shlex, tempfile, sys, socket\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Optional Azure SDK imports (defer errors until used)\n",
    "try:\n",
    "    from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "    from azure.mgmt.resource import ResourceManagementClient\n",
    "    from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "    from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "except Exception as _sdk_err:\n",
    "    _AZURE_SDK_IMPORT_ERROR = _sdk_err\n",
    "else:\n",
    "    _AZURE_SDK_IMPORT_ERROR = None\n",
    "\n",
    "class DeploymentError(Exception):\n",
    "    pass\n",
    "class PolicyError(Exception):\n",
    "    pass\n",
    "class ModelDeploymentError(Exception):\n",
    "    pass\n",
    "\n",
    "class AzureOps:\n",
    "    def __init__(self, strategy: str = 'sdk'):\n",
    "        self.strategy = strategy.lower()\n",
    "        if self.strategy not in {'sdk','cli'}:\n",
    "            self.strategy = 'sdk'\n",
    "        self.az_cli = None\n",
    "        self.version = None\n",
    "        self.subscription_id = os.environ.get('SUBSCRIPTION_ID') or os.environ.get('AZURE_SUBSCRIPTION_ID') or ''\n",
    "        self.credential = None\n",
    "        self.resource_client: Optional[ResourceManagementClient] = None\n",
    "        self.cog_client: Optional[CognitiveServicesManagementClient] = None\n",
    "        self._resolve_cli()\n",
    "        self._init_credentials_if_possible()\n",
    "        self._cache_version()\n",
    "\n",
    "    # ---------- CLI RESOLUTION ----------\n",
    "    def _resolve_cli(self):\n",
    "        override = os.environ.get('AZURE_CLI_PATH')\n",
    "        if override and Path(override).exists():\n",
    "            self.az_cli = override\n",
    "        else:\n",
    "            candidates = []\n",
    "            for name in ['az','az.cmd','az.exe']:\n",
    "                p = shutil.which(name)\n",
    "                if p: candidates.append(p)\n",
    "            candidates += [ '/usr/bin/az','/usr/local/bin/az', str(Path.home()/'.local/bin/az'), str(Path.home()/'.azure-cli/az') ]\n",
    "            existing = [c for c in candidates if c and Path(c).exists()]\n",
    "            if not existing:\n",
    "                venv = Path(os.environ.get('VIRTUAL_ENV','') or sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "                if venv.exists(): existing=[str(venv)]\n",
    "            if existing:\n",
    "                def rank(p):\n",
    "                    pl=p.lower(); penalty=1000 if '.venv' in pl or 'scripts' in pl else 0\n",
    "                    return penalty, len(p)\n",
    "                existing.sort(key=rank)\n",
    "                self.az_cli = existing[0]\n",
    "            else:\n",
    "                self.az_cli = 'az'\n",
    "        os.environ['AZ_CLI'] = self.az_cli\n",
    "\n",
    "    # ---------- GENERIC az() INVOCATION ----------\n",
    "    def _run(self, parts, timeout=30):\n",
    "        try:\n",
    "            return subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            class Dummy: returncode=1; stdout=''; stderr=f'timeout>{timeout}s'\n",
    "            return Dummy()\n",
    "\n",
    "    def az(self, cmd: str, json_out=False, timeout=30, login_retry=True) -> Tuple[bool, str | Dict]:\n",
    "        parts=[self.az_cli]+shlex.split(cmd)\n",
    "        proc=self._run(parts,timeout)\n",
    "        if proc.returncode!=0:\n",
    "            stderr=proc.stderr.strip()\n",
    "            if login_retry and 'az login' in stderr.lower():\n",
    "                if self.ensure_login(silent=True):\n",
    "                    return self.az(cmd,json_out=json_out,timeout=timeout,login_retry=False)\n",
    "            return False, stderr or proc.stdout\n",
    "        out=proc.stdout\n",
    "        if json_out:\n",
    "            try:\n",
    "                return True, json.loads(out or '{}')\n",
    "            except Exception as e:\n",
    "                return False, f'json parse error: {e}\\n{out[:200]}'\n",
    "        return True, out\n",
    "\n",
    "    def _cache_version(self):\n",
    "        ok, ver = self.az('--version', json_out=False, timeout=6, login_retry=False)\n",
    "        if ok:\n",
    "            self.version = ver.splitlines()[0] if ver else ''\n",
    "\n",
    "    # ---------- AUTHENTICATION ----------\n",
    "    def _init_credentials_if_possible(self):\n",
    "        # Service Principal first\n",
    "        sp_keys = ['AZURE_TENANT_ID','AZURE_CLIENT_ID','AZURE_CLIENT_SECRET']\n",
    "        if all(os.environ.get(k) for k in sp_keys):\n",
    "            try:\n",
    "                self.credential = ClientSecretCredential(\n",
    "                    tenant_id=os.environ['AZURE_TENANT_ID'],\n",
    "                    client_id=os.environ['AZURE_CLIENT_ID'],\n",
    "                    client_secret=os.environ['AZURE_CLIENT_SECRET']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] SP credential init failed:', e)\n",
    "                self.credential=None\n",
    "        if self.credential is None:\n",
    "            try:\n",
    "                self.credential = AzureCliCredential()\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] AzureCliCredential failed (defer login):', e)\n",
    "                self.credential=None\n",
    "        # Resource client if SDK chosen\n",
    "        if self.strategy=='sdk' and self.credential and self.subscription_id:\n",
    "            if _AZURE_SDK_IMPORT_ERROR:\n",
    "                print('[AzureOps] SDK import error; fallback to CLI deployments:', _AZURE_SDK_IMPORT_ERROR)\n",
    "                self.strategy='cli'\n",
    "                return\n",
    "            try:\n",
    "                self.resource_client = ResourceManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] ResourceManagementClient init failed:', e)\n",
    "                self.resource_client=None\n",
    "            try:\n",
    "                self.cog_client = CognitiveServicesManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] CognitiveServicesManagementClient init failed:', e)\n",
    "                self.cog_client=None\n",
    "\n",
    "    def ensure_login(self, silent=False):\n",
    "        ok,_ = self.az('account show', json_out=True, login_retry=False, timeout=8)\n",
    "        if ok:\n",
    "            acct_id = _.get('id') if isinstance(_,dict) else None\n",
    "            if acct_id and not self.subscription_id:\n",
    "                self.subscription_id = acct_id\n",
    "            return True\n",
    "        # Attempt SP non-interactive if creds exist\n",
    "        sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID'])\n",
    "        if sp_ok:\n",
    "            sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} -p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "            proc=self._run([self.az_cli]+shlex.split(sp_cmd),timeout=40)\n",
    "            if proc.returncode==0:\n",
    "                if not silent: print('[AzureOps] SP login successful')\n",
    "                return True\n",
    "            else:\n",
    "                if not silent: print('[AzureOps] SP login failed:', proc.stderr[:160])\n",
    "        if not silent:\n",
    "            print('[AzureOps] Interactive login required: run \"az login\" in terminal')\n",
    "        return False\n",
    "\n",
    "    # ---------- RESOURCE GROUP ----------\n",
    "    def ensure_resource_group(self, rg: str, location: str) -> bool:\n",
    "        if self.strategy=='sdk' and self.resource_client:\n",
    "            try:\n",
    "                self.resource_client.resource_groups.create_or_update(rg, {'location': location})\n",
    "                print('[AzureOps] RG ensured (sdk):', rg)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] RG ensure failed (sdk):', e)\n",
    "        # CLI fallback\n",
    "        ok,res=self.az(f'group exists --name {rg}', json_out=False)\n",
    "        exists = ok and res.strip()=='true'\n",
    "        if exists:\n",
    "            print('[AzureOps] RG exists:', rg); return True\n",
    "        ok,_=self.az(f'group create --name {rg} --location {location}', json_out=True, timeout=120)\n",
    "        print('[AzureOps] RG created' if ok else '[AzureOps] RG create failed')\n",
    "        return ok\n",
    "\n",
    "    # ---------- BICEP COMPILE ----------\n",
    "    def compile_bicep(self, path: str) -> str:\n",
    "        b=Path(path); out=b.with_suffix('.json')\n",
    "        ok,res=self.az(f'bicep build --file {shlex.quote(str(b))}', json_out=False)\n",
    "        if not ok or not out.exists():\n",
    "            raise DeploymentError(f'Bicep compile failed: {res}')\n",
    "        print('[AzureOps] compiled', path, '->', out)\n",
    "        return str(out)\n",
    "\n",
    "    # ---------- DEPLOYMENT (CLI OR SDK) ----------\n",
    "    def _deploy_group_cli(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        param_args=[]\n",
    "        for k,v in params.items():\n",
    "            if isinstance(v,(dict,list)):\n",
    "                tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "                tmp.write_text(json.dumps({\"value\":v}))\n",
    "                param_args.append(f'{k}=@{tmp}')\n",
    "            else:\n",
    "                param_args.append(f'{k}={json.dumps(v)}')\n",
    "        params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "        cmd=(f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}')\n",
    "        print('[AzureOps] deploy(cli):', cmd)\n",
    "        ok,res=self.az(cmd,json_out=True,timeout=timeout)\n",
    "        return ok,res\n",
    "\n",
    "    def _deploy_group_sdk(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if not self.resource_client:\n",
    "            print('[AzureOps] SDK resource_client missing; fallback to CLI')\n",
    "            return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "        template = json.loads(Path(template_file).read_text(encoding='utf-8'))\n",
    "        # Convert params to ARM expected {k:{\"value\":v}}\n",
    "        arm_params={k:{'value':v} for k,v in params.items()}\n",
    "        properties={'mode':'Incremental','template':template,'parameters':arm_params}\n",
    "        print('[AzureOps] deploy(sdk):', name)\n",
    "        poller = self.resource_client.deployments.begin_create_or_update(rg,name,{'properties':properties})\n",
    "        start=time.time();\n",
    "        while not poller.done():\n",
    "            time.sleep(30)\n",
    "            elapsed=int(time.time()-start)\n",
    "            if elapsed%120<30:  # periodic status\n",
    "                print(f'  [AzureOps] deploying... {elapsed}s')\n",
    "        result=poller.result()\n",
    "        state=getattr(result.properties,'provisioning_state',None)\n",
    "        ok = state=='Succeeded'\n",
    "        if ok:\n",
    "            print('[AzureOps] deployment succeeded:', name)\n",
    "        else:\n",
    "            print('[AzureOps] deployment state:', state)\n",
    "        return ok, {'properties':{'outputs': getattr(result.properties,'outputs',{})}}\n",
    "\n",
    "    def deploy_group(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if self.strategy=='sdk':\n",
    "            return self._deploy_group_sdk(rg,name,template_file,params,timeout)\n",
    "        return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "\n",
    "    def get_deployment_outputs(self, rg: str, name: str) -> Dict[str,str]:\n",
    "        # Attempt CLI first for uniformity\n",
    "        ok,res=self.az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=60)\n",
    "        if ok and isinstance(res,dict):\n",
    "            outputs=res.get('properties',{}).get('outputs',{})\n",
    "            return {k:v.get('value') for k,v in outputs.items()} if isinstance(outputs,dict) else {}\n",
    "        # SDK fallback if available\n",
    "        if self.resource_client:\n",
    "            try:\n",
    "                dep=self.resource_client.deployments.get(rg,name)\n",
    "                outs=getattr(dep.properties,'outputs',{})\n",
    "                return {k:v.get('value') for k,v in outs.items()} if isinstance(outs,dict) else {}\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] outputs retrieval failed (sdk):', e)\n",
    "        return {}\n",
    "\n",
    "    # ---------- MODEL DEPLOYMENTS (AI Foundry) ----------\n",
    "    def deploy_models_via_sdk(self, rg: str, foundries: List[dict], models_config: Dict[str,List[dict]]):\n",
    "        if not self.cog_client:\n",
    "            print('[AzureOps] Cognitive Services client not initialized; skipping model deployments')\n",
    "            return {'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        existing_accounts={acc.name:acc for acc in self.cog_client.accounts.list_by_resource_group(rg)}\n",
    "        results={'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        # Ensure accounts\n",
    "        for f in foundries:\n",
    "            name=f['name']; location=f['location']\n",
    "            if name in existing_accounts:\n",
    "                print(f'  [AzureOps] foundry exists: {name}')\n",
    "            else:\n",
    "                print(f'  [AzureOps] creating foundry: {name}')\n",
    "                try:\n",
    "                    account_params=Account(location=location, sku=CogSku(name='S0'), kind='AIServices', properties={'customSubDomainName':name.lower(),'publicNetworkAccess':'Enabled','allowProjectManagement':True}, identity={'type':'SystemAssigned'})\n",
    "                    poll=self.cog_client.accounts.begin_create(rg,name,account_params)\n",
    "                    poll.result(timeout=600)\n",
    "                    print(f'    [AzureOps] created {name}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [AzureOps] create failed {name}: {e}'); continue\n",
    "        # Deploy models\n",
    "        for f in foundries:\n",
    "            name=f['name']; short=name.split('-')[0]\n",
    "            models=models_config.get(short,[])\n",
    "            print(f'  [AzureOps] models for {name}: {len(models)}')\n",
    "            for m in models:\n",
    "                mname=m['name']\n",
    "                try:\n",
    "                    # Exists check\n",
    "                    try:\n",
    "                        existing=self.cog_client.deployments.get(rg,name,mname)\n",
    "                        if existing.properties.provisioning_state=='Succeeded':\n",
    "                            print(f'    [skip] {mname} already')\n",
    "                            results['skipped'].append(f'{short}/{mname}')\n",
    "                            continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    dep_params=Deployment(sku=CogSku(name=m['sku'],capacity=m['capacity']), properties=DeploymentProperties(model=DeploymentModel(format=m['format'],name=m['name'],version=m['version'])))\n",
    "                    poll=self.cog_client.deployments.begin_create_or_update(rg,name,mname,dep_params)\n",
    "                    poll.result(timeout=900)\n",
    "                    print(f'    [ok] {mname}')\n",
    "                    results['succeeded'].append(f'{short}/{mname}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [fail] {mname}: {e}')\n",
    "                    results['failed'].append({'model':f'{short}/{mname}','error':str(e)})\n",
    "        return results\n",
    "\n",
    "    # ---------- POLICY FRAGMENTS & API POLICY ----------\n",
    "    def ensure_policy_fragment(self, rg: str, service: str, fragment_name: str, xml_policy: str):\n",
    "        body={\"properties\":{\"format\":\"xml\",\"value\":xml_policy.strip()}}\n",
    "        url=(f'https://management.azure.com/subscriptions/{self.subscription_id}/resourceGroups/{rg}/providers/Microsoft.ApiManagement/service/{service}/policyFragments/{fragment_name}?api-version=2023-03-01-preview')\n",
    "        body_json=json.dumps(body)\n",
    "        ok,res=self.az(f\"rest --method put --url {shlex.quote(url)} --body {shlex.quote(body_json)} --headers Content-Type=application/json\", json_out=True, timeout=120)\n",
    "        if ok:\n",
    "            print(f'[AzureOps] fragment ensured: {fragment_name}')\n",
    "        else:\n",
    "            print(f'[AzureOps] fragment failed {fragment_name}: {str(res)[:160]}')\n",
    "        return ok\n",
    "\n",
    "    def backup_api_policy(self, rg: str, service: str, api_id: str):\n",
    "        ok,res=self.az(f'apim api policy show --resource-group {rg} --service-name {service} --api-id {api_id}', json_out=False, timeout=60)\n",
    "        if not ok:\n",
    "            print('[AzureOps] no existing policy (show failed)'); return None\n",
    "        backup_dir=Path('.apim-policy-backups'); backup_dir.mkdir(exist_ok=True)\n",
    "        ts=time.strftime('%Y%m%d-%H%M%S')\n",
    "        file=backup_dir/f'{api_id}-{ts}.xml'\n",
    "        file.write_text(res)\n",
    "        print('[AzureOps] policy backed up:', file)\n",
    "        return str(file)\n",
    "\n",
    "    def apply_api_policy_with_fragments(self, rg: str, service: str, api_id: str, fragments: List[str], extra_inbound: str=''):\n",
    "        self.backup_api_policy(rg,service,api_id)\n",
    "        fragment_tags='\\n'.join(f'        <fragment ref=\"{f}\" />' for f in fragments)\n",
    "        inbound = f\"<inbound>\\n        <base />\\n{fragment_tags}\\n{extra_inbound}\\n    </inbound>\".rstrip()\n",
    "        policy_xml=f\"<policies>\\n{inbound}\\n    <backend><base /></backend>\\n    <outbound><base /></outbound>\\n    <on-error><base /></on-error>\\n</policies>\"\n",
    "        tmp=Path(tempfile.gettempdir())/f'apim-{api_id}-policy.xml'\n",
    "        tmp.write_text(policy_xml)\n",
    "        ok,res=self.az(f'apim api policy create --resource-group {rg} --service-name {service} --api-id {api_id} --xml-path {tmp}', json_out=False, timeout=180)\n",
    "        if not ok:\n",
    "            raise PolicyError(f'Policy apply failed: {res}')\n",
    "        print('[AzureOps] API policy applied with fragments:', fragments)\n",
    "        return True\n",
    "\n",
    "    # ---------- MCP HEALTH ----------\n",
    "    def mcp_health(self, servers: Dict[str,object]) -> Dict[str,Dict[str,str]]:\n",
    "        summary={}\n",
    "        for name,client in servers.items():\n",
    "            url=getattr(client,'server_url',None) or getattr(client,'url',None) or ''\n",
    "            status='unknown'; latency_ms='-'\n",
    "            if url.startswith('http'):  # basic TCP connect\n",
    "                try:\n",
    "                    host=url.split('//',1)[1].split('/',1)[0].split(':')[0]\n",
    "                    port=443 if url.startswith('https') else (int(url.split(':')[2].split('/')[0]) if ':' in url[8:] else 80)\n",
    "                    s=socket.socket(); s.settimeout(3); start=time.time(); s.connect((host,port)); s.close(); latency_ms=int((time.time()-start)*1000); status='ok'\n",
    "                except Exception:\n",
    "                    status='unreachable'\n",
    "            summary[name]={'url':url,'status':status,'latency_ms':latency_ms}\n",
    "        return summary\n",
    "\n",
    "# Instantiate global wrapper (prefer sdk)\n",
    "AZ_OPS = AzureOps(strategy=os.environ.get('AZ_OPS_STRATEGY','sdk'))\n",
    "print('[AzureOps] CLI:', AZ_OPS.az_cli)\n",
    "az_ok = AZ_OPS.ensure_login(silent=True)\n",
    "print('[AzureOps] login status:', 'OK' if az_ok else 'AUTH REQUIRED')\n",
    "if AZ_OPS.version: print('[AzureOps] version:', AZ_OPS.version)\n",
    "print('[AzureOps] strategy:', AZ_OPS.strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_11_c260b025",
   "metadata": {},
   "source": [
    "# Section 0 : Consolidated Provisioning & Initialization\n",
    "\n",
    "This section provides an optimized, minimal set of cells to run the entire lab setup end-to-end.\n",
    "Run these in order, then skip legacy duplicates below. Original cells are retained for reference.\n",
    "\n",
    "Order:\n",
    "1. Env Loader & Masked Summary\n",
    "2. Dependency Installation\n",
    "3. Azure Auth + CLI + Service Principal\n",
    "4. Deployment Helpers (compile, deploy, utilities)\n",
    "5. Main 4-Step Deployment\n",
    "6. Generate master-lab.env\n",
    "7. Endpoint Normalizer (OPENAI + Inference)\n",
    "8. Unified Policy Application (Semantic Cache + Content Safety + others)\n",
    "9. Unified MCP Initialization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell_12_086d2df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.479768Z",
     "iopub.status.busy": "2025-11-17T18:37:01.479393Z",
     "iopub.status.idle": "2025-11-17T18:37:01.644309Z",
     "shell.execute_reply": "2025-11-17T18:37:01.643450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] Summary (masked=True)\n",
      "\n",
      "[apim]\n",
      "  APIM_API_ID = inference-api\n",
      "  APIM_API_KEY = b64e*************************cb0\n",
      "  APIM_GATEWAY_URL = https://apim-pavavy6pu5hpa.azure-api.net\n",
      "  APIM_NAME = apimmcpwksp321028\n",
      "  APIM_SERVICE = apim-pavavy6pu5hpa\n",
      "  APIM_SERVICE_ID = /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resourceGroups/lab-master-lab/providers/Microsoft.ApiManagement/service/apim-pavavy6pu5hpa\n",
      "  APIM_SERVICE_NAME = apim-pavavy6pu5hpa\n",
      "  INFERENCE_API_PATH = inference\n",
      "  LOCATION = norwayeast\n",
      "  RESOURCE_GROUP = lab-master-lab\n",
      "\n",
      "[redis]\n",
      "  REDIS_HOST = redis-pavavy6pu5hpa.uksouth.redis.azure.net\n",
      "  REDIS_KEY = MOEW*************************************J0=\n",
      "  REDIS_PORT = 10000\n",
      "\n",
      "[search]\n",
      "  SEARCH_ADMIN_KEY = B5dq*********************************************SgB\n",
      "  SEARCH_ENDPOINT = https://search-pavavy6pu5hpa.search.windows.net\n",
      "  SEARCH_SERVICE_NAME = search-pavavy6pu5hpa\n",
      "\n",
      "[cosmos]\n",
      "  COSMOS_ACCOUNT_NAME = cosmos-pavavy6pu5hpa\n",
      "  COSMOS_ENDPOINT = https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "  COSMOS_KEY = KL11*********************************************************************************w==\n",
      "\n",
      "[content_safety]\n",
      "  CONTENT_SAFETY_ENDPOINT = https://contentsafety-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  CONTENT_SAFETY_KEY = 5ZvG*****************************************************************************6p1\n",
      "\n",
      "[models]\n",
      "  DEPLOYMENT_PREFIX = master-lab\n",
      "  MODEL_DALL_E_3_ENDPOINT_R1 = https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_DALL_E_3_KEY_R1 = 62ef*************************5f9\n",
      "  MODEL_GPT_4O_ENDPOINT_R1 = https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_GPT_4O_KEY_R1 = 62ef*************************5f9\n",
      "  MODEL_GPT_4O_MINI_ENDPOINT_R1 = https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_GPT_4O_MINI_ENDPOINT_R2 = https://foundry2-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_GPT_4O_MINI_ENDPOINT_R3 = https://foundry3-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_GPT_4O_MINI_KEY_R1 = 62ef*************************5f9\n",
      "  MODEL_GPT_4O_MINI_KEY_R2 = 19cf*************************f37\n",
      "  MODEL_GPT_4O_MINI_KEY_R3 = b7ec*************************6cb\n",
      "  MODEL_GPT_4_1_NANO_ENDPOINT_R1 = https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_GPT_4_1_NANO_KEY_R1 = 62ef*************************5f9\n",
      "  MODEL_TEXT_EMBEDDING_3_LARGE_ENDPOINT_R1 = https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_TEXT_EMBEDDING_3_LARGE_KEY_R1 = 62ef*************************5f9\n",
      "  MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1 = https://foundry1-pavavy6pu5hpa.openai.azure.com/\n",
      "  MODEL_TEXT_EMBEDDING_3_SMALL_KEY_R1 = 62ef*************************5f9\n",
      "  OPENAI_MODELS_URL = https://apim-pavavy6pu5hpa.azure-api.netinference/models\n",
      "\n",
      "[other]\n",
      "  ACR_LOGIN_SERVER = acrmcpwksp321028.azurecr.io\n",
      "  ACR_NAME = acrmcpwksp321028\n",
      "  API_ID = inference-api\n",
      "  AZURE_CLIENT_ID = 4a5d0f1a-578e-479a-8ba9-05770ae9ce6b\n",
      "  AZURE_CLIENT_SECRET = lXV8*********************************aIr\n",
      "  AZURE_SUBSCRIPTION_ID = d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  AZURE_TENANT_ID = 2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "  AZ_CLI = /usr/bin/az\n",
      "  BICEP_DIR = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "  CLAUDECODE = 1\n",
      "  CLAUDE_CODE_ENTRYPOINT = cli\n",
      "  CLICOLOR = 1\n",
      "  CLICOLOR_FORCE = 1\n",
      "  CONTAINER_APP_ENV_ID = /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resourceGroups/lab-master-lab/providers/Microsoft.App/managedEnvironments/cae-pavavy6pu5hpa\n",
      "  CONTAINER_REGISTRY = acrpavavy6pu5hpa.azurecr.io\n",
      "  COREPACK_ENABLE_AUTO_PIN = 0\n",
      "  DBUS_SESSION_BUS_ADDRESS = unix:path=/run/user/1000/bus\n",
      "  DISPLAY = :0\n",
      "  FORCE_COLOR = 1\n",
      "  FOUNDRY_PROJECT_ENDPOINT = <empty>\n",
      "  GIT_EDITOR = true\n",
      "  GIT_PAGER = cat\n",
      "  HOME = /home/lproux\n",
      "  HOMEBREW_CELLAR = /home/linuxbrew/.linuxbrew/Cellar\n",
      "  HOMEBREW_PREFIX = /home/linuxbrew/.linuxbrew\n",
      "  HOMEBREW_REPOSITORY = /home/linuxbrew/.linuxbrew/Homebrew\n",
      "  HOSTTYPE = x86_64\n",
      "  INFOPATH = /home/linuxbrew/.linuxbrew/share/info:\n",
      "  JPY_PARENT_PID = 202675\n",
      "  LANG = C.UTF-8\n",
      "  LB_ENABLED = true\n",
      "  LB_GPT4O_MINI_ENDPOINTS = https://foundry1-pavavy6pu5hpa.openai.azure.com/,https://foundry2-pavavy6pu5hpa.openai.azure.com/,https://foundry3-pavavy6pu5hpa.openai.azure.com/\n",
      "  LB_REGIONS = uksouth,eastus,norwayeast\n",
      "  LESSCLOSE = /usr/bin/lesspipe %s %s\n",
      "  LESSOPEN = | /usr/bin/lesspipe %s\n",
      "  LOGNAME = lproux\n",
      "  LS_COLORS = rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=00:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.avif=01;35:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:*~=00;90:*#=00;90:*.bak=00;90:*.crdownload=00;90:*.dpkg-dist=00;90:*.dpkg-new=00;90:*.dpkg-old=00;90:*.dpkg-tmp=00;90:*.old=00;90:*.orig=00;90:*.part=00;90:*.rej=00;90:*.rpmnew=00;90:*.rpmorig=00;90:*.rpmsave=00;90:*.swp=00;90:*.tmp=00;90:*.ucf-dist=00;90:*.ucf-new=00;90:*.ucf-old=00;90:\n",
      "  MCP_SERVER_GITHUB_URL = https://mcp-github-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_MS_LEARN_URL = https://mcp-ms-learn-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_PLACE_ORDER_URL = https://mcp-place-order-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_PRODUCT_CATALOG_URL = https://mcp-product-catalog-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_WEATHER_URL = https://mcp-weather-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MOTD_SHOWN = update-motd\n",
      "  MPLBACKEND = module://matplotlib_inline.backend_inline\n",
      "  NAME = SurfacIT\n",
      "  NoDefaultCurrentDirectoryInExePath = 1\n",
      "  OLDPWD = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "  OPENAI_API_BASE = https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "  OPENAI_ENDPOINT = https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "  OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE = delta\n",
      "  PAGER = cat\n",
      "  PATH = /home/lproux/.local/bin:/home/lproux/bin:/home/lproux/.local/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Users/lproux/AppData/Roaming/Code/User/globalStorage/github.copilot-chat/debugCommand:/mnt/c/Users/lproux/AppData/Roaming/Code/User/globalStorage/github.copilot-chat/copilotCli:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/nodejs/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/GitHub CLI/:/mnt/c/Users/lproux/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code Insiders/bin:/mnt/c/Users/lproux/AppData/Roaming/npm:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/lproux/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/lproux/.local/bin:/mnt/c/Users/lproux/.local/bin:/mnt/c/Users/lproux/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code Insiders/bin:/mnt/c/Users/lproux/AppData/Roaming/npm:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/lproux/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/lproux/.vscode/extensions/ms-python.debugpy-2025.16.0-win32-x64/bundled/scripts/noConfigScripts:/snap/bin\n",
      "  PULSE_SERVER = unix:/mnt/wslg/PulseServer\n",
      "  PWD = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "  PYDEVD_USE_FRAME_EVAL = NO\n",
      "  RANDOM_SUFFIX = 24774\n",
      "  SHELL = /bin/bash\n",
      "  SHLVL = 2\n",
      "  STORAGE_ACCOUNT = stmcpwksp321028\n",
      "  SUBSCRIPTION_ID = d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  TERM = xterm-color\n",
      "  USER = lproux\n",
      "  WAYLAND_DISPLAY = wayland-0\n",
      "  WSL2_GUI_APPS_ENABLED = 1\n",
      "  WSLENV = <empty>\n",
      "  WSL_DISTRO_NAME = Ubuntu\n",
      "  WSL_INTEROP = /run/WSL/45825_interop\n",
      "  XDG_DATA_DIRS = /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "  XDG_RUNTIME_DIR = /run/user/1000/\n",
      "  _ = /usr/bin/timeout\n",
      "\n",
      "[load-balancing] Model Pools & Region Mapping\n",
      "  (no pools or region map defined)\n",
      "\n",
      "[env] Loader ready. Use ENV.get('KEY') in subsequent cells.\n"
     ]
    }
   ],
   "source": [
    "# === Unified Environment Loader & Load Balancing Overview ===\n",
    "\"\"\"\n",
    "This cell provides a single source of truth for configuration:\n",
    "- Auto-creates `master-lab.env` if missing (non-secret template placeholders).\n",
    "- Loads key=value pairs (duplicates allowed) and merges with current process env.\n",
    "- Masks sensitive values when displaying (KEY, SECRET, TOKEN, PASSWORD, API_KEY substrings).\n",
    "- Ensures `.gitignore` patterns include env files (both global and lab-specific).\n",
    "- Displays load balancing pools and region mapping across models.\n",
    "\n",
    "Duplication Policy: Allowed. Later cells still using os.getenv will continue working;\n",
    "new code should prefer ENV.get(\"NAME\").\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "ENV_FILE = Path(\"master-lab.env\")\n",
    "\n",
    "# Template includes both model names and deployment names (user may fill in later)\n",
    "_DEFAULT_ENV_TEMPLATE = \"\"\"# master-lab.env - autogenerated template (fill real credentials)\n",
    "# Lines beginning with # are comments. Duplicates permitted; last occurrence wins.\n",
    "\n",
    "# ===========================================\n",
    "# Core Azure API Management / Resource settings\n",
    "# ===========================================\n",
    "APIM_GATEWAY_URL=\n",
    "APIM_API_KEY=\n",
    "APIM_SERVICE_NAME=\n",
    "APIM_API_ID=\n",
    "RESOURCE_GROUP=\n",
    "LOCATION=\n",
    "INFERENCE_API_PATH=inference\n",
    "\n",
    "# ===========================================\n",
    "# AI Model Deployments (Multi-Region)\n",
    "# ===========================================\n",
    "\n",
    "# GPT-4o-mini (Load balanced across 3 regions)\n",
    "MODEL_GPT_4O_MINI_ENDPOINT_R1=\n",
    "MODEL_GPT_4O_MINI_KEY_R1=\n",
    "MODEL_GPT_4O_MINI_ENDPOINT_R2=\n",
    "MODEL_GPT_4O_MINI_KEY_R2=\n",
    "MODEL_GPT_4O_MINI_ENDPOINT_R3=\n",
    "MODEL_GPT_4O_MINI_KEY_R3=\n",
    "\n",
    "# GPT-4o (UK South only)\n",
    "MODEL_GPT_4O_ENDPOINT_R1=\n",
    "MODEL_GPT_4O_KEY_R1=\n",
    "\n",
    "# Text Embeddings - Small (UK South only)\n",
    "MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1=\n",
    "MODEL_TEXT_EMBEDDING_3_SMALL_KEY_R1=\n",
    "\n",
    "# Text Embeddings - Large (UK South only)\n",
    "MODEL_TEXT_EMBEDDING_3_LARGE_ENDPOINT_R1=\n",
    "MODEL_TEXT_EMBEDDING_3_LARGE_KEY_R1=\n",
    "\n",
    "# DALL-E 3 Image Generation (UK South only)\n",
    "MODEL_DALL_E_3_ENDPOINT_R1=\n",
    "MODEL_DALL_E_3_KEY_R1=\n",
    "\n",
    "# GPT-4.1 Nano (UK South only)\n",
    "MODEL_GPT_4_1_NANO_ENDPOINT_R1=\n",
    "MODEL_GPT_4_1_NANO_KEY_R1=\n",
    "\n",
    "# Load Balancing Configuration\n",
    "LB_REGIONS=uksouth,eastus,norwayeast\n",
    "LB_GPT4O_MINI_ENDPOINTS=\n",
    "LB_ENABLED=true\n",
    "\n",
    "# ===========================================\n",
    "# Supporting Services\n",
    "# ===========================================\n",
    "\n",
    "# Redis (Semantic Caching)\n",
    "REDIS_HOST=\n",
    "REDIS_PORT=10000\n",
    "REDIS_KEY=\n",
    "\n",
    "# Azure Cognitive Search\n",
    "SEARCH_SERVICE_NAME=\n",
    "SEARCH_ENDPOINT=\n",
    "SEARCH_ADMIN_KEY=\n",
    "\n",
    "# Cosmos DB\n",
    "COSMOS_ACCOUNT_NAME=\n",
    "COSMOS_ENDPOINT=\n",
    "COSMOS_KEY=\n",
    "\n",
    "# Content Safety\n",
    "CONTENT_SAFETY_ENDPOINT=\n",
    "CONTENT_SAFETY_KEY=\n",
    "\n",
    "# ===========================================\n",
    "# MCP Servers\n",
    "# ===========================================\n",
    "MCP_SERVER_WEATHER_URL=\n",
    "MCP_SERVER_GITHUB_URL=\n",
    "MCP_SERVER_PRODUCT_CATALOG_URL=\n",
    "MCP_SERVER_PLACE_ORDER_URL=\n",
    "MCP_SERVER_MS_LEARN_URL=\n",
    "\"\"\".strip() + \"\\n\"\n",
    "\n",
    "_SENSITIVE_SUBSTRINGS = [\"KEY\", \"SECRET\", \"TOKEN\", \"PASSWORD\", \"API_KEY\"]\n",
    "\n",
    "def create_env_file_if_missing(path: Path = ENV_FILE):\n",
    "    if not path.exists():\n",
    "        path.write_text(_DEFAULT_ENV_TEMPLATE, encoding=\"utf-8\")\n",
    "        print(f\"[env] Created missing env file: {path}\")\n",
    "\n",
    "def parse_env_lines(lines: Iterable[str]) -> Dict[str, str]:\n",
    "    data: Dict[str, str] = {}\n",
    "    for raw in lines:\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"): # comment / empty\n",
    "            continue\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\"=\", 1)\n",
    "        data[k.strip()] = v.strip()\n",
    "    return data\n",
    "\n",
    "def load_env(path: Path = ENV_FILE) -> Dict[str, str]:\n",
    "    create_env_file_if_missing(path)\n",
    "    file_text = path.read_text(encoding=\"utf-8\")\n",
    "    data = parse_env_lines(file_text.splitlines())\n",
    "    # Merge in process env (does not overwrite file values)\n",
    "    for k, v in os.environ.items():\n",
    "        if k not in data:\n",
    "            data[k] = v\n",
    "    return data\n",
    "\n",
    "def mask_value(v: str, keep_start: int = 4, keep_end: int = 3) -> str:\n",
    "    if v is None or v == \"\":\n",
    "        return \"<empty>\"\n",
    "    if len(v) <= keep_start + keep_end + 2:\n",
    "        return v  # too short to mask meaningfully\n",
    "    return v[:keep_start] + \"*\" * (len(v) - keep_start - keep_end) + v[-keep_end:]\n",
    "\n",
    "def is_sensitive(key: str) -> bool:\n",
    "    u = key.upper()\n",
    "    return any(sub in u for sub in _SENSITIVE_SUBSTRINGS)\n",
    "\n",
    "def ensure_gitignore_patterns():\n",
    "    patterns_to_add = [\"*.env\", \"master-lab.env\"]\n",
    "    # Walk a few ancestor levels to update existing .gitignore files\n",
    "    checked = []\n",
    "    for base in [Path(\".\"), Path(\"..\"), Path(\"../..\"), Path(\"../../..\")]:\n",
    "        gi = base / \".gitignore\"\n",
    "        if gi.exists():\n",
    "            try:\n",
    "                lines = gi.read_text(encoding=\"utf-8\").splitlines()\n",
    "            except Exception:\n",
    "                continue\n",
    "            changed = False\n",
    "            for p in patterns_to_add:\n",
    "                if not any(line.strip() == p for line in lines):\n",
    "                    lines.append(p)\n",
    "                    changed = True\n",
    "            if changed:\n",
    "                gi.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "                print(f\"[env] Updated gitignore: {gi} (added env patterns)\")\n",
    "            checked.append(str(gi))\n",
    "    if not checked:\n",
    "        print(\"[env] No .gitignore files found in scanned paths (env patterns not globally verified).\")\n",
    "\n",
    "def categorize_keys(env: Dict[str, str]) -> Dict[str, Dict[str, str]]:\n",
    "    categories = {\n",
    "        \"apim\": {},\n",
    "        \"redis\": {},\n",
    "        \"search\": {},\n",
    "        \"cosmos\": {},\n",
    "        \"content_safety\": {},\n",
    "        \"models\": {},\n",
    "        \"other\": {},\n",
    "    }\n",
    "    for k, v in env.items():\n",
    "        ku = k.upper()\n",
    "        if ku.startswith(\"APIM\") or ku in {\"RESOURCE_GROUP\", \"LOCATION\", \"INFERENCE_API_PATH\"}:\n",
    "            categories[\"apim\"][k] = v\n",
    "        elif ku.startswith(\"REDIS\"):\n",
    "            categories[\"redis\"][k] = v\n",
    "        elif ku.startswith(\"SEARCH\"):\n",
    "            categories[\"search\"][k] = v\n",
    "        elif ku.startswith(\"COSMOS\"):\n",
    "            categories[\"cosmos\"][k] = v\n",
    "        elif ku.startswith(\"CONTENT_SAFETY\"):\n",
    "            categories[\"content_safety\"][k] = v\n",
    "        elif \"MODEL\" in ku or \"DEPLOYMENT\" in ku or ku.endswith(\"_POOL\"):\n",
    "            categories[\"models\"][k] = v\n",
    "        else:\n",
    "            categories[\"other\"][k] = v\n",
    "    return categories\n",
    "\n",
    "def parse_region_map(env: Dict[str, str]) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    raw = env.get(\"MODEL_REGION_MAP\", \"\")\n",
    "    for part in raw.split(','):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if ':' in part:\n",
    "            model, region = part.split(':', 1)\n",
    "            mapping[model.strip()] = region.strip()\n",
    "    return mapping\n",
    "\n",
    "def show_load_balancing(env: Dict[str, str]):\n",
    "    print(\"\\n[load-balancing] Model Pools & Region Mapping\")\n",
    "    region_map = parse_region_map(env)\n",
    "    pools = [k for k in env.keys() if k.endswith(\"_POOL\")]\n",
    "    if not pools and not region_map:\n",
    "        print(\"  (no pools or region map defined)\")\n",
    "        return\n",
    "    for pk in pools:\n",
    "        models = [m.strip() for m in env.get(pk, \"\").split('|') if m.strip()]\n",
    "        print(f\"  Pool {pk}: {len(models)} model(s)\")\n",
    "        for m in models:\n",
    "            reg = region_map.get(m, \"<no-region>\")\n",
    "            print(f\"    - {m} @ {reg}\")\n",
    "    if region_map:\n",
    "        print(\"\\n  Region Map (all models):\")\n",
    "        for m, r in region_map.items():\n",
    "            print(f\"    {m}: {r}\")\n",
    "\n",
    "def list_env(env: Dict[str, str], mask: bool = True):\n",
    "    cats = categorize_keys(env)\n",
    "    print(\"[env] Summary (masked=\" + str(mask) + \")\")\n",
    "    for cname, items in cats.items():\n",
    "        if not items:\n",
    "            continue\n",
    "        print(f\"\\n[{cname}]\")\n",
    "        for k, v in sorted(items.items()):\n",
    "            display_v = mask_value(v) if (mask and is_sensitive(k)) else (v if v else \"<empty>\")\n",
    "            print(f\"  {k} = {display_v}\")\n",
    "\n",
    "# Execute setup\n",
    "ENV = load_env()\n",
    "ensure_gitignore_patterns()\n",
    "list_env(ENV, mask=True)\n",
    "show_load_balancing(ENV)\n",
    "print(\"\\n[env] Loader ready. Use ENV.get('KEY') in subsequent cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell_13_fad9bf81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.647430Z",
     "iopub.status.busy": "2025-11-17T18:37:01.647051Z",
     "iopub.status.idle": "2025-11-17T18:37:01.653875Z",
     "shell.execute_reply": "2025-11-17T18:37:01.653057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deps] requirements.txt missing at c:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab/requirements.txt; skip.\n"
     ]
    }
   ],
   "source": [
    "# Unified Dependencies Install (replaces older dependency cell)\n",
    "import os, sys, subprocess, pathlib, shlex\n",
    "LAB_ROOT = pathlib.Path(r\"c:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab\")\n",
    "REQ_FILE = LAB_ROOT / \"requirements.txt\"\n",
    "if REQ_FILE.exists():\n",
    "    print(f\"[deps] Installing from {REQ_FILE} (idempotent)\")\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(REQ_FILE)]\n",
    "    print(\"[deps] Command:\", \" \".join(shlex.quote(c) for c in cmd))\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"[deps][stderr]\", result.stderr[:400])\n",
    "        if result.returncode == 0:\n",
    "            print(\"[deps] ‚úÖ Requirements installed / already satisfied.\")\n",
    "        else:\n",
    "            print(f\"[deps] ‚ö†Ô∏è pip exited with code {result.returncode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[deps] ‚ùå Installation error: {e}\")\n",
    "else:\n",
    "    print(f\"[deps] requirements.txt missing at {REQ_FILE}; skip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_14_da5ac2e2",
   "metadata": {},
   "source": [
    "## ‚úÖ Optimized Execution Order (Cells 1‚Äì25 Refactor)\n",
    "\n",
    "Recommended run sequence for clean provisioning & testing:\n",
    "1. Environment Loader (already executed) ‚Äì establishes `ENV` and masking.\n",
    "2. Dependencies Install (new unified cell) ‚Äì ensures Python packages present.\n",
    "3. Azure Auth & CLI Setup ‚Äì resolves `az`, creates Service Principal if missing, sets subscription/rg/location.\n",
    "4. Deployment Helper Functions ‚Äì (original helper cell kept) defines utility functions.\n",
    "5. Main Deployment (4 steps) ‚Äì provisions core, AI Foundry, supporting services, MCP servers.\n",
    "6. Generate `master-lab.env` ‚Äì writes consolidated outputs.\n",
    "7. OPENAI Endpoint/Inference Path Normalizer ‚Äì derives `OPENAI_ENDPOINT` if missing.\n",
    "8. Unified APIM Policy Application ‚Äì applies content-safety + semantic caching policies post-deployment.\n",
    "9. Unified MCP Initialization ‚Äì initializes all deployed MCP servers once.\n",
    "10. Import Libraries ‚Äì (original imports cell) after environment & deployment.\n",
    "\n",
    "Deprecated cells replaced by stubs:\n",
    "- Old semantic caching policy cell\n",
    "- Redundant Azure CLI resolution cells\n",
    "- Duplicate MCP initialization cells (2 vs 5 servers)\n",
    "- Legacy `load_dotenv` environment loader\n",
    "- Separate Service Principal creation & config cells\n",
    "\n",
    "Rationale:\n",
    "- Prevent policy application before backend/API exist.\n",
    "- Single Azure CLI resolution reduces timeouts & path drift.\n",
    "- One MCP client avoids partial initialization confusion.\n",
    "- Centralized environment variable evolution (adds derived `OPENAI_ENDPOINT`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_16_59b5974e",
   "metadata": {},
   "source": [
    "# DEPLOY AND CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_18_9925d274",
   "metadata": {},
   "source": [
    "### Environment Standardization\n",
    "\n",
    "The notebook now **always loads** `master-lab.env` (and intentionally ignores a legacy `.env` if present). This ensures consistency across all mid-range cells (50‚Äì90) and later diagnostics.\n",
    "\n",
    "Key points:\n",
    "- Precedence: `master-lab.env` > previously loaded `.env`.\n",
    "- If `python-dotenv` isn't installed, a manual parser is used.\n",
    "- A legacy `.env` file is detected but not sourced (informational notice only).\n",
    "- Downstream MCP initialization and Azure deployment cells rely on values sourced here‚Äîre-run this cell first after any env changes.\n",
    "\n",
    "If servers still show unreachable statuses:\n",
    "1. Confirm URL entries in `master-lab.env` match those in `.mcp-servers-config` (config overrides env inside the improved MCP cell).\n",
    "2. Check for network/firewall restrictions (timeouts vs connection errors distinguished in diagnostics).\n",
    "3. For non-HTTP package/stdio servers, ensure local installation or runtime adapter before expecting probe success.\n",
    "\n",
    "Proceed to run the improved MCP diagnostics cell at the bottom, then re-run cells 50‚Äì90."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_19_7f8d96e1",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Run this first to install all dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_20_360bb0f9",
   "metadata": {},
   "source": [
    "<a id='init'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell_21_579ba0a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.656210Z",
     "iopub.status.busy": "2025-11-17T18:37:01.656011Z",
     "iopub.status.idle": "2025-11-17T18:37:01.669507Z",
     "shell.execute_reply": "2025-11-17T18:37:01.668581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Azure CLI resolved: /usr/bin/az\n",
      "[OK] API_ID from environment: inference-api\n"
     ]
    }
   ],
   "source": [
    "# APIM policy apply helper (patched Azure CLI resolution with autodiscovery)\n",
    "import shutil, subprocess, os, sys, textwrap, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "AZ_CANDIDATES = [\n",
    "    shutil.which(\"az\"),\n",
    "    str(Path(sys.prefix) / \"bin\" / \"az\"),\n",
    "]\n",
    "\n",
    "AZ_CANDIDATES += [c for c in [os.getenv(\"AZURE_CLI_PATH\"), os.getenv(\"AZ_PATH\")] if c]\n",
    "az_cli = next((c for c in AZ_CANDIDATES if c and Path(c).exists()), None)\n",
    "\n",
    "if not az_cli:\n",
    "    raise SystemExit(\"[FATAL] Azure CLI 'az' not found. Install it before continuing.\")\n",
    "\n",
    "print(f\"[INFO] Azure CLI resolved: {az_cli}\")\n",
    "\n",
    "# Ensure ENV is defined\n",
    "ENV = os.environ\n",
    "RESOURCE_GROUP = ENV.get(\"RESOURCE_GROUP\") or os.getenv(\"RESOURCE_GROUP\") or \"lab-master-lab\"\n",
    "APIM_SERVICE = ENV.get(\"APIM_SERVICE_NAME\") or os.getenv(\"APIM_SERVICE_NAME\") or \"apim-pavavy6pu5hpa\"\n",
    "\n",
    "# Autodiscover API_ID from APIM service\n",
    "def autodiscover_api_id():\n",
    "    \"\"\"Auto-discover the inference API ID from APIM service.\"\"\"\n",
    "    try:\n",
    "        # Get subscription ID\n",
    "        subscription_id_local = globals().get(\"subscription_id\")\n",
    "        if not subscription_id_local:\n",
    "            result_sub = subprocess.run([az_cli, \"account\", \"show\"], capture_output=True, text=True, timeout=30)\n",
    "            if result_sub.returncode != 0:\n",
    "                return None\n",
    "            import json as json_module\n",
    "            sub_info = json_module.loads(result_sub.stdout)\n",
    "            subscription_id_local = sub_info.get(\"id\")\n",
    "        \n",
    "        if not subscription_id_local:\n",
    "            return None\n",
    "        \n",
    "        # Query APIM for APIs\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id_local}'\n",
    "               f'/resourceGroups/{RESOURCE_GROUP}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{APIM_SERVICE}/apis?api-version=2022-08-01')\n",
    "        \n",
    "        result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                               capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return None\n",
    "        \n",
    "        import json as json_module\n",
    "        apis_data = json_module.loads(result.stdout)\n",
    "        apis = apis_data.get('value', [])\n",
    "        \n",
    "        # Find inference API\n",
    "        for api in apis:\n",
    "            api_id = api.get('name', '')\n",
    "            api_props = api.get('properties', {})\n",
    "            api_name = api_props.get('displayName', '').lower()\n",
    "            api_path = api_props.get('path', '').lower()\n",
    "            \n",
    "            if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                return api_id\n",
    "        \n",
    "        # Fallback to inference-api if exists\n",
    "        for api in apis:\n",
    "            if api.get('name') == 'inference-api':\n",
    "                return 'inference-api'\n",
    "        \n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Try to autodiscover, fallback to env or default\n",
    "API_ID = ENV.get(\"APIM_API_ID\") or os.getenv(\"APIM_API_ID\")\n",
    "\n",
    "if not API_ID:\n",
    "    print(\"[*] Auto-discovering API_ID from APIM service...\")\n",
    "    discovered_api_id = autodiscover_api_id()\n",
    "    if discovered_api_id:\n",
    "        API_ID = discovered_api_id\n",
    "        os.environ['APIM_API_ID'] = API_ID\n",
    "        print(f\"[OK] API_ID auto-discovered: {API_ID}\")\n",
    "    else:\n",
    "        # Fallback to default\n",
    "        API_ID = \"inference-api\"\n",
    "        os.environ['APIM_API_ID'] = API_ID\n",
    "        print(f\"[!] Could not auto-discover API_ID, using default: {API_ID}\")\n",
    "else:\n",
    "    print(f\"[OK] API_ID from environment: {API_ID}\")\n",
    "\n",
    "policy_xml = \"\"\"<policies>\n",
    "  <inbound>\n",
    "    <base />\n",
    "    <set-header name=\"X-Policy-Applied\" exists-action=\"override\">\n",
    "      <value>content-safety</value>\n",
    "    </set-header>\n",
    "  </inbound>\n",
    "  <backend>\n",
    "    <base />\n",
    "  </backend>\n",
    "  <outbound>\n",
    "    <base />\n",
    "  </outbound>\n",
    "  <on-error>\n",
    "    <base />\n",
    "  </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "def apply_policy(xml_str: str, label: str):\n",
    "    \"\"\"Apply APIM policy using Azure REST API.\"\"\"\n",
    "    import json as json_module\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Prefer existing subscription_id if already defined in notebook\n",
    "    subscription_id_local = globals().get(\"subscription_id\")\n",
    "    if not subscription_id_local:\n",
    "        result_sub = subprocess.run([az_cli, \"account\", \"show\"], capture_output=True, text=True, timeout=30)\n",
    "        if result_sub.returncode != 0:\n",
    "            print(f\"[ERROR] Failed to get subscription ID\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            sub_info = json_module.loads(result_sub.stdout)\n",
    "            subscription_id_local = sub_info[\"id\"]\n",
    "        except Exception:\n",
    "            print(f\"[ERROR] Could not parse subscription info\")\n",
    "            return\n",
    "\n",
    "    # Azure REST API endpoint for APIM policy\n",
    "    url = (f'https://management.azure.com/subscriptions/{subscription_id_local}'\n",
    "           f'/resourceGroups/{RESOURCE_GROUP}/providers/Microsoft.ApiManagement'\n",
    "           f'/service/{APIM_SERVICE}/apis/{API_ID}/policies/policy?api-version=2022-08-01')\n",
    "    # Policy payload in Azure format\n",
    "    policy_payload = {\n",
    "        \"properties\": {\n",
    "            \"value\": xml_str.strip(),\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write JSON payload to temp file\n",
    "    payload_file = Path(tempfile.gettempdir()) / f'apim-{label}-payload.json'\n",
    "    with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "        json_module.dump(policy_payload, f, indent=2)\n",
    "\n",
    "    # Build az rest command\n",
    "    cmd = [az_cli, \"rest\", \"--method\", \"put\", \"--url\", url, \"--body\", f\"@{payload_file}\", \"--headers\", \"Content-Type=application/json\"]\n",
    "    print(f\"[*] Applying {label} policy via REST API\")\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"[ERROR] {label} policy timed out after 120 seconds\")\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {label} policy failed: {e}\")\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "    else:\n",
    "        # Clean up temp file\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if result.returncode == 0:\n",
    "            print(f\"[SUCCESS] {label} policy applied\")\n",
    "        else:\n",
    "            print(f\"[ERROR] {label} policy failed rc={result.returncode}\\nSTDERR: {result.stderr[:400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_23_343c991d",
   "metadata": {},
   "source": [
    "### Load Environment Variables from Deployment Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_25_8b3b6457",
   "metadata": {},
   "source": [
    "### Master Lab Configuration\n",
    "\n",
    "Set deployment configuration for all 4 deployment steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell_26_13f05f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.671976Z",
     "iopub.status.busy": "2025-11-17T18:37:01.671763Z",
     "iopub.status.idle": "2025-11-17T18:37:01.676294Z",
     "shell.execute_reply": "2025-11-17T18:37:01.675583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Configuration set\n",
      "  Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  Resource Group: lab-master-lab\n",
      "  Location: uksouth\n",
      "  Deployment Prefix: master-lab\n"
     ]
    }
   ],
   "source": [
    "# Master Lab Configuration\n",
    "\n",
    "# IMPORTANT: Set your Azure subscription ID\n",
    "# Get this from: Azure Portal > Subscriptions > Copy Subscription ID\n",
    "subscription_id = 'd334f2cd-3efd-494e-9fd3-2470b1a13e4c'  # Replace with your subscription ID\n",
    "\n",
    "deployment_name_prefix = 'master-lab'\n",
    "resource_group_name = 'lab-master-lab'\n",
    "location = 'uksouth'\n",
    "\n",
    "# Deployment names for each step\n",
    "deployment_step1 = f'{deployment_name_prefix}-01-core'\n",
    "deployment_step2 = f'{deployment_name_prefix}-02-ai-foundry'\n",
    "deployment_step3 = f'{deployment_name_prefix}-03-supporting'\n",
    "deployment_step4 = f'{deployment_name_prefix}-04-mcp'\n",
    "\n",
    "print('[OK] Configuration set')\n",
    "print(f'  Subscription ID: {subscription_id}')\n",
    "print(f'  Resource Group: {resource_group_name}')\n",
    "print(f'  Location: {location}')\n",
    "print(f'  Deployment Prefix: {deployment_name_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_27_9aa17b08",
   "metadata": {},
   "source": [
    "### Deployment Helper Functions\n",
    "\n",
    "Azure SDK functions for deployment management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell_28_1ecfb480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.679011Z",
     "iopub.status.busy": "2025-11-17T18:37:01.678661Z",
     "iopub.status.idle": "2025-11-17T18:37:01.835949Z",
     "shell.execute_reply": "2025-11-17T18:37:01.835308Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.mgmt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmgmt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResourceManagementClient\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientSecretCredential, AzureCliCredential\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[*] Initializing Azure authentication...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure.mgmt'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "\n",
    "print('[*] Initializing Azure authentication...')\n",
    "print()\n",
    "\n",
    "# Try to load Service Principal credentials from .azure-credentials.env\n",
    "credentials_file = '.azure-credentials.env'\n",
    "credential = None\n",
    "\n",
    "if os.path.exists(credentials_file):\n",
    "    print(f'[*] Found {credentials_file}, using Service Principal authentication')\n",
    "    load_dotenv(credentials_file)\n",
    "\n",
    "    tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "    if tenant_id and client_id and client_secret:\n",
    "        try:\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id,\n",
    "                client_id=client_id,\n",
    "                client_secret=client_secret\n",
    "            )\n",
    "            print('[OK] Service Principal credentials loaded')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to create Service Principal credential: {e}')\n",
    "            credential = None\n",
    "    else:\n",
    "        print('[ERROR] Missing credentials in .azure-credentials.env')\n",
    "        print('[INFO] Required: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET')\n",
    "else:\n",
    "    print(f'[*] {credentials_file} not found')\n",
    "    print('[INFO] Run: create_service_principal.ps1 to create Service Principal')\n",
    "\n",
    "# Fallback to Azure CLI credential if Service Principal not available\n",
    "if credential is None:\n",
    "    print('[*] Falling back to Azure CLI authentication...')\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        print('[OK] Using Azure CLI credentials')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Azure CLI authentication failed: {e}')\n",
    "        print()\n",
    "        print('[ERROR] Authentication failed. Options:')\n",
    "        print('  1. Create Service Principal: run create_service_principal.ps1')\n",
    "        print('  2. Clear Azure CLI cache and re-login:')\n",
    "        print('     - Delete: %USERPROFILE%\\\\.azure')\n",
    "        print('     - Run: az login')\n",
    "        raise Exception('Authentication failed')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify subscription ID from config\n",
    "if not subscription_id or len(subscription_id) < 10:\n",
    "    raise Exception('Please set your subscription_id in Cell 11')\n",
    "\n",
    "print(f'[OK] Using Subscription ID: {subscription_id}')\n",
    "\n",
    "# Create Resource Management Client\n",
    "print('[*] Creating Azure Resource Management client...')\n",
    "try:\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "    # Test connection by listing resource groups\n",
    "    list(resource_client.resource_groups.list())\n",
    "    print('[OK] Azure SDK initialized and connection verified')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to initialize Resource Management client: {e}')\n",
    "    print()\n",
    "    print('[INFO] If you see MSAL or cache errors, try clearing Azure CLI cache:')\n",
    "    print('       rd /s /q \"%USERPROFILE%\\\\.azure\"')\n",
    "    print('       az login')\n",
    "    raise e\n",
    "\n",
    "print()\n",
    "\n",
    "def compile_bicep(bicep_file):\n",
    "    \"\"\"Compile Bicep to JSON - SIMPLIFIED: Just use existing JSON files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path if string\n",
    "    bicep_path = Path(bicep_file)\n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    print(f'[*] Looking for template: {json_path.name}...')\n",
    "    \n",
    "    # Check if JSON exists\n",
    "    if json_path.exists():\n",
    "        print(f'[OK] Found existing template: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    \n",
    "    # JSON doesn't exist - this is an error since we don't want to compile\n",
    "    print(f'[ERROR] Template not found: {json_path}')\n",
    "    print(f'[INFO] Expected location: {json_path.absolute()}')\n",
    "    print(f'[INFO] Please ensure Bicep templates are pre-compiled')\n",
    "    print(f'[INFO] Or set BICEP_DIR to the correct scripts directory')\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_resource_group_exists(rg_name):\n",
    "    \"\"\"Check if resource group exists\"\"\"\n",
    "    try:\n",
    "        resource_client.resource_groups.get(rg_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_deployment_exists(rg_name, deployment_name):\n",
    "    \"\"\"Check if deployment exists and succeeded - Fixed to return boolean only\"\"\"\n",
    "    try:\n",
    "        deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "        return deployment.properties.provisioning_state == 'Succeeded'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_template(rg_name, deployment_name, template_file, parameters_dict):\n",
    "    \"\"\"Deploy ARM template using Azure SDK\"\"\"\n",
    "    print(f'[*] Deploying {deployment_name}...')\n",
    "\n",
    "    # Read template\n",
    "    with open(template_file, 'r', encoding='utf-8') as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # Prepare deployment properties\n",
    "    deployment_properties = {\n",
    "        'mode': 'Incremental',\n",
    "        'template': template,\n",
    "        'parameters': parameters_dict\n",
    "    }\n",
    "\n",
    "    # Start deployment\n",
    "    print('[*] Starting deployment...')\n",
    "    deployment_async = resource_client.deployments.begin_create_or_update(\n",
    "        rg_name,\n",
    "        deployment_name,\n",
    "        {'properties': deployment_properties}\n",
    "    )\n",
    "\n",
    "    # Poll deployment status\n",
    "    print('[*] Deployment in progress. Polling status...')\n",
    "    start_time = time.time()\n",
    "    last_update = start_time\n",
    "\n",
    "    while not deployment_async.done():\n",
    "        time.sleep(30)\n",
    "        elapsed = time.time() - start_time\n",
    "        if time.time() - last_update >= 60:\n",
    "            mins = int(elapsed / 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f'[*] Still deploying... {mins}m {secs}s elapsed')\n",
    "            last_update = time.time()\n",
    "\n",
    "    # Get result\n",
    "    deployment_result = deployment_async.result()\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    if deployment_result.properties.provisioning_state == 'Succeeded':\n",
    "        print(f'[OK] Deployment succeeded in {mins}m {secs}s')\n",
    "        return True, deployment_result\n",
    "    else:\n",
    "        print(f'[ERROR] Deployment failed: {deployment_result.properties.provisioning_state}')\n",
    "        if deployment_result.properties.error:\n",
    "            print(f'[ERROR] Error: {deployment_result.properties.error.message}')\n",
    "        return False, deployment_result\n",
    "\n",
    "def get_deployment_outputs(rg_name, deployment_name):\n",
    "    \"\"\"Get deployment outputs\"\"\"\n",
    "    deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "    if deployment.properties.outputs:\n",
    "        return {k: v['value'] for k, v in deployment.properties.outputs.items()}\n",
    "    return {}\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_29_a7330fb3",
   "metadata": {},
   "source": [
    "### Main Deployment - All 4 Steps\n",
    "\n",
    "Deploys all infrastructure in sequence:\n",
    "1. Core (APIM, Log Analytics, App Insights) - ~10 min\n",
    "2. AI Foundry (3 hubs + 14 models) - ~15 min\n",
    "3. Supporting Services (Redis, Search, Cosmos, Content Safety) - ~10 min\n",
    "4. MCP Servers (Container Apps + 7 servers) - ~5 min\n",
    "\n",
    "**Total time: ~40 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell_30_1a0a2a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.838182Z",
     "iopub.status.busy": "2025-11-17T18:37:01.838021Z",
     "iopub.status.idle": "2025-11-17T18:37:01.921159Z",
     "shell.execute_reply": "2025-11-17T18:37:01.919907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)\n",
      "======================================================================\n",
      "\n",
      "[*] Step 0: Ensuring resource group exists...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_resource_group_exists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Ensure resource group exists\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[*] Step 0: Ensuring resource group exists...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_resource_group_exists\u001b[49m(resource_group_name):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[*] Creating resource group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_group_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m     resource_client.resource_groups.create_or_update(\n\u001b[32m     20\u001b[39m         resource_group_name,\n\u001b[32m     21\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m: location}\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'check_resource_group_exists' is not defined"
     ]
    }
   ],
   "source": [
    "print('=' * 70)\n",
    "# Load BICEP_DIR (set by Cell 3)\n",
    "BICEP_DIR = Path(os.getenv('BICEP_DIR', 'archive/scripts'))\n",
    "if not BICEP_DIR.exists():\n",
    "    print(f\"[deploy] ‚ö†Ô∏è  BICEP_DIR not found: {BICEP_DIR}\")\n",
    "    print(f\"[deploy] Looking in current directory instead\")\n",
    "    BICEP_DIR = Path(\".\")\n",
    "\n",
    "print('MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Ensure resource group exists\n",
    "print('[*] Step 0: Ensuring resource group exists...')\n",
    "if not check_resource_group_exists(resource_group_name):\n",
    "    print(f'[*] Creating resource group: {resource_group_name}')\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {'location': location}\n",
    "    )\n",
    "    print('[OK] Resource group created')\n",
    "else:\n",
    "    print('[OK] Resource group already exists')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: CORE INFRASTRUCTURE (Bicep - as before)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 1: CORE INFRASTRUCTURE')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Log Analytics, App Insights, API Management')\n",
    "print('[*] Estimated time: ~10 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step1 = 'master-lab-01-core'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step1):\n",
    "    print('[OK] Step 1 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 1 not found. Deploying...')\n",
    "\n",
    "    # Compile and deploy\n",
    "    # Fix: original compile_bicep used Path.replace(old, new) causing TypeError.\n",
    "    # Provide safe wrapper using Path.with_suffix('.json').\n",
    "    # Added resilient az CLI discovery & FileNotFoundError handling.\n",
    "    # Enhanced: auto-install bicep if missing; richer diagnostics; fallback to direct bicep use if JSON not produced.\n",
    "    def compile_bicep_safe(bicep_path: Path):\n",
    "        \"\"\"SIMPLIFIED: Just use existing JSON files - no compilation\"\"\"\n",
    "        if not bicep_path.exists():\n",
    "            print(f'[ERROR] Bicep file not found: {bicep_path}')\n",
    "            return None\n",
    "        \n",
    "        json_path = bicep_path.with_suffix('.json')\n",
    "        \n",
    "        if json_path.exists():\n",
    "            print(f'[OK] Using existing template: {json_path.name}')\n",
    "            return str(json_path)\n",
    "        \n",
    "        print(f'[ERROR] JSON template not found: {json_path}')\n",
    "        print(f'[INFO] Expected at: {json_path.absolute()}')\n",
    "        return None\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-01-core.bicep')\n",
    "\n",
    "    # Load parameters\n",
    "    with open(BICEP_DIR / 'params-01-core.json') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # Extract only the 'parameters' section from ARM parameter file\n",
    "    params_dict = params.get('parameters', {})\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step1, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 1 deployment failed')\n",
    "\n",
    "    print('[OK] Step 1 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# Get Step 1 outputs (with fallback to saved file)\n",
    "step1_outputs = None\n",
    "try:\n",
    "    step1_outputs = get_deployment_outputs(resource_group_name, deployment_step1)\n",
    "    print('[OK] Step 1 outputs retrieved from deployment')\n",
    "except Exception as e:\n",
    "    print(f'[WARN] Failed to retrieve Step 1 outputs from deployment: {str(e)}')\n",
    "    # Try loading from saved file\n",
    "    step1_output_file = BICEP_DIR / 'step1-outputs.json'\n",
    "    if step1_output_file.exists():\n",
    "        try:\n",
    "            with open(step1_output_file) as f:\n",
    "                step1_outputs = json.load(f)\n",
    "            print(f'[OK] Step 1 outputs loaded from {step1_output_file.name}')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Failed to load from file: {str(e2)}')\n",
    "    \n",
    "if not step1_outputs:\n",
    "    print('[ERROR] Cannot proceed without Step 1 outputs')\n",
    "    print('[INFO] Please ensure Step 1 deployment completed or step1-outputs.json exists')\n",
    "    raise Exception('Cannot proceed without Step 1 outputs')\n",
    "\n",
    "print(f\"  - APIM Gateway: {step1_outputs.get('apimGatewayUrl', 'N/A')}\")\n",
    "print(f\"  - Log Analytics: {step1_outputs.get('logAnalyticsWorkspaceId', 'N/A')[:60]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: AI FOUNDRY (RESILIENT PYTHON APPROACH)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 2: AI FOUNDRY (RESILIENT DEPLOYMENT)')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: 3 Foundry hubs, 3 projects, AI models')\n",
    "print('[*] Estimated time: ~15 minutes')\n",
    "print()\n",
    "\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "\n",
    "cog_client = CognitiveServicesManagementClient(credential, subscription_id)\n",
    "\n",
    "# Configuration\n",
    "resource_suffix = 'pavavy6pu5hpa'  # Consistent suffix\n",
    "foundries = [\n",
    "    {'name': f'foundry1-{resource_suffix}', 'location': 'uksouth', 'project': 'master-lab-foundry1'},\n",
    "    {'name': f'foundry2-{resource_suffix}', 'location': 'eastus', 'project': 'master-lab-foundry2'},\n",
    "    {'name': f'foundry3-{resource_suffix}', 'location': 'norwayeast', 'project': 'master-lab-foundry3'}\n",
    "]\n",
    "\n",
    "models_config = {\n",
    "    'foundry1': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4o', 'format': 'OpenAI', 'version': '2024-08-06', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'text-embedding-3-small', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "        {'name': 'text-embedding-3-large', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "                {'name': 'dall-e-3', 'format': 'OpenAI', 'version': '3.0', 'sku': 'Standard', 'capacity': 1},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry2': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry3': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Phase 2a: Check/Create Foundry Hubs\n",
    "print('[*] Phase 2a: AI Foundry Hubs')\n",
    "existing_accounts = {acc.name: acc for acc in cog_client.accounts.list_by_resource_group(resource_group_name)}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    if foundry_name in existing_accounts:\n",
    "        print(f'  [OK] {foundry_name} already exists')\n",
    "    else:\n",
    "        print(f'  [*] Creating {foundry_name}...')\n",
    "        try:\n",
    "            account_params = Account(\n",
    "                location=foundry['location'],\n",
    "                sku=CogSku(name='S0'),\n",
    "                kind='AIServices',\n",
    "                properties={\n",
    "                    'customSubDomainName': foundry_name.lower(),\n",
    "                    'publicNetworkAccess': 'Enabled',\n",
    "                    'allowProjectManagement': True\n",
    "                },\n",
    "                identity={'type': 'SystemAssigned'}\n",
    "            )\n",
    "            poller = cog_client.accounts.begin_create(resource_group_name, foundry_name, account_params)\n",
    "            poller.result(timeout=300)\n",
    "            print(f'  [OK] {foundry_name} created')\n",
    "        except Exception as e:\n",
    "            print(f'  [ERROR] Failed: {str(e)[:100]}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Phase 2b: Deploy Models (Resilient)\n",
    "print('[*] Phase 2b: AI Models (Resilient)')\n",
    "deployment_results = {'succeeded': [], 'failed': [], 'skipped': []}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    short_name = foundry_name.split('-')[0]\n",
    "    models = models_config.get(short_name, [])\n",
    "\n",
    "    print(f'  [*] {foundry_name}: {len(models)} models')\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model['name']\n",
    "        try:\n",
    "            # Check if exists\n",
    "            existing = cog_client.deployments.get(resource_group_name, foundry_name, model_name)\n",
    "            if existing.properties.provisioning_state == 'Succeeded':\n",
    "                deployment_results['skipped'].append(f'{short_name}/{model_name}')\n",
    "                print(f'    [OK] {model_name} already deployed')\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            print(f'    [*] Deploying {model_name}...')\n",
    "            deployment_params = Deployment(\n",
    "                sku=CogSku(name=model['sku'], capacity=model['capacity']),\n",
    "                properties=DeploymentProperties(\n",
    "                    model=DeploymentModel(\n",
    "                        format=model['format'],\n",
    "                        name=model['name'],\n",
    "                        version=model['version']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            poller = cog_client.deployments.begin_create_or_update(\n",
    "                resource_group_name, foundry_name, model_name, deployment_params\n",
    "            )\n",
    "            poller.result(timeout=600)\n",
    "            deployment_results['succeeded'].append(f'{short_name}/{model_name}')\n",
    "            print(f'    [OK] {model_name} deployed')\n",
    "        except Exception as e:\n",
    "            deployment_results['failed'].append({'model': f'{short_name}/{model_name}', 'error': str(e)})\n",
    "            print(f'    [SKIP] {model_name} failed: {str(e)[:80]}')\n",
    "\n",
    "print()\n",
    "print(f'[OK] Models: {len(deployment_results[\"succeeded\"])} deployed, {len(deployment_results[\"skipped\"])} skipped, {len(deployment_results[\"failed\"])} failed')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Collect Foundry Deployment Outputs for Env File\n",
    "# ============================================================================\n",
    "print()\n",
    "print('[*] Collecting foundry deployment outputs for env file...')\n",
    "step2_outputs = {\n",
    "    'foundryProjectEndpoint': '',\n",
    "    'inferenceAPIPath': 'inference',\n",
    "    'foundries': []\n",
    "}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    try:\n",
    "        # Get account details\n",
    "        account = cog_client.accounts.get(resource_group_name, foundry_name)\n",
    "        \n",
    "        # Get primary key\n",
    "        keys = cog_client.accounts.list_keys(resource_group_name, foundry_name)\n",
    "        primary_key = keys.key1\n",
    "        \n",
    "        # Build endpoint\n",
    "        endpoint = f\"https://{foundry_name}.openai.azure.com/\"\n",
    "        \n",
    "        # Get deployed model names for this foundry\n",
    "        short_name = foundry_name.split('-')[0]\n",
    "        model_names = [m['name'] for m in models_config.get(short_name, [])]\n",
    "        \n",
    "        foundry_output = {\n",
    "            'name': foundry_name,\n",
    "            'location': foundry['location'],\n",
    "            'endpoint': endpoint,\n",
    "            'key': primary_key,\n",
    "            'models': model_names\n",
    "        }\n",
    "        \n",
    "        step2_outputs['foundries'].append(foundry_output)\n",
    "        print(f\"  [OK] Captured {foundry_name}: {len(model_names)} models\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Could not capture {foundry_name} outputs: {str(e)[:80]}\")\n",
    "\n",
    "print(f'[OK] Captured {len(step2_outputs[\"foundries\"])} foundry outputs')\n",
    "print()\n",
    "\n",
    "print('[*] Phase 2c: APIM Inference API')\n",
    "\n",
    "deployment_step2c = 'master-lab-02c-apim-api'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step2c):\n",
    "    print('[OK] APIM API already configured. Skipping...')\n",
    "else:\n",
    "    print('[*] Configuring APIM Inference API...')\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-02c-apim-api.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 2c')\n",
    "\n",
    "    params_dict = {\n",
    "        'apimLoggerId': {'value': step1_outputs['apimLoggerId']},\n",
    "        'appInsightsId': {'value': step1_outputs['appInsightsId']},\n",
    "        'appInsightsInstrumentationKey': {'value': step1_outputs['appInsightsInstrumentationKey']},\n",
    "        'inferenceAPIPath': {'value': 'inference'},\n",
    "        'inferenceAPIType': {'value': 'AzureOpenAI'}\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step2c, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 2c deployment failed')\n",
    "\n",
    "    print('[OK] APIM API configured')\n",
    "\n",
    "print('[OK] Step 2 complete')\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SUPPORTING SERVICES (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print()\n",
    "\n",
    "deployment_step3 = 'master-lab-03-supporting'\n",
    "if check_deployment_exists(resource_group_name, deployment_step3):\n",
    "    print('[OK] Step 3 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 3 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-03-supporting.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 3')\n",
    "\n",
    "    params_dict = {}\n",
    "    if os.path.exists(BICEP_DIR / 'params-03-supporting.json'):\n",
    "        with open(BICEP_DIR / 'params-03-supporting.json') as f:\n",
    "            params = json.load(f)\n",
    "        # Extract only the 'parameters' section from ARM parameter file\n",
    "        params_dict = params.get('parameters', {})\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step3, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 3 deployment failed')\n",
    "    print('[OK] Step 3 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    print('[OK] Step 3 outputs retrieved')\n",
    "except Exception:\n",
    "    step3_outputs = {}\n",
    "    print('[*] No Step 3 outputs available')\n",
    "# =============================================================================\n",
    "# STEP 4: MCP SERVERS (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 4: MCP SERVERS')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Container Apps + 5 MCP servers')\n",
    "print('[*] Estimated time: ~5 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step4 = 'master-lab-04-mcp'\n",
    "if check_deployment_exists(resource_group_name, deployment_step4):\n",
    "    print('[OK] Step 4 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 4 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-04-mcp.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 4')\n",
    "\n",
    "    params_dict = {\n",
    "        'logAnalyticsCustomerId': {'value': step1_outputs.get('logAnalyticsCustomerId', '')},\n",
    "        'logAnalyticsPrimarySharedKey': {'value': step1_outputs.get('logAnalyticsPrimarySharedKey', '')},\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step4, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 4 deployment failed')\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    print('[OK] Step 4 outputs retrieved')\n",
    "except Exception:\n",
    "    step4_outputs = {}\n",
    "    print('[*] No Step 4 outputs available')\n",
    "\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT COMPLETE\n",
    "# =============================================================================\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "total_mins = int(total_elapsed / 60)\n",
    "total_secs = int(total_elapsed % 60)\n",
    "\n",
    "print('=' * 70)\n",
    "print('DEPLOYMENT COMPLETE')\n",
    "print('=' * 70)\n",
    "print(f'[OK] Total time: {total_mins}m {total_secs}s')\n",
    "print()\n",
    "print('[OK] All 4 steps deployed successfully!')\n",
    "print('[OK] Next: Run Cell 18-19 to generate master-lab.env')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:01.924005Z",
     "iopub.status.busy": "2025-11-17T18:37:01.923794Z",
     "iopub.status.idle": "2025-11-17T18:37:04.741057Z",
     "shell.execute_reply": "2025-11-17T18:37:04.740163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BICEP STEP 2: AI Foundry Accounts (Infra-as-Code Option)\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Foundry Bicep deployment already exists ‚Äì skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deploy] outputs keys: foundryAccounts\n",
      "[OK] Foundry outputs retrieved\n",
      "\n",
      "[Foundry Accounts]\n",
      "  - foundry1-pavavy6pu5hpa @ uksouth -> https://foundry1-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  - foundry2-pavavy6pu5hpa @ eastus -> https://foundry2-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  - foundry3-pavavy6pu5hpa @ norwayeast -> https://foundry3-pavavy6pu5hpa.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- OPTIONAL BICEP-BASED STEP 2 (AI FOUNDRY ACCOUNTS) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BICEP STEP 2: AI Foundry Accounts (Infra-as-Code Option)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Fallbacks if prior cell not executed\n",
    "if 'resource_group_name' not in globals():\n",
    "    resource_group_name = os.getenv('RESOURCE_GROUP', 'lab-master-lab')\n",
    "if 'foundry_suffix' not in globals():\n",
    "    foundry_suffix = 'pavavy6pu5hpa'\n",
    "if 'BICEP_DIR' not in globals():\n",
    "    BICEP_DIR = Path(os.getenv('BICEP_DIR', 'archive/scripts'))\n",
    "\n",
    "# WSL path normalization (if running under /mnt and windows-style root was set)\n",
    "if 'LAB_ROOT' in globals():\n",
    "    try:\n",
    "        lr = str(LAB_ROOT)\n",
    "        if lr[1:3] == ':\\\\':  # windows drive\n",
    "            drive = lr[0].lower()\n",
    "            wsl_path = \"/mnt/\" + drive + \"/\" + lr[3:].replace(\"\\\\\", \"/\")\n",
    "            if not BICEP_DIR.exists():\n",
    "                alt = Path(wsl_path) / 'archive/scripts'\n",
    "                if alt.exists():\n",
    "                    BICEP_DIR = alt\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if 'compile_bicep_safe' not in globals():\n",
    "    def compile_bicep_safe(bicep_path):\n",
    "        b = Path(bicep_path)\n",
    "        if not b.exists():\n",
    "            print(f'[ERROR] Missing bicep: {b}')\n",
    "            return None\n",
    "        json_path = b.with_suffix('.json')\n",
    "        if json_path.exists():\n",
    "            print(f'[OK] Using precompiled: {json_path.name}')\n",
    "            return str(json_path)\n",
    "        if 'compile_bicep' in globals():\n",
    "            try:\n",
    "                print('[*] Precompiled JSON not found; fallback compile_bicep()')\n",
    "                return compile_bicep(str(b))\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] compile_bicep() failed: {e}')\n",
    "        print(f'[ERROR] No JSON + no fallback: {json_path}')\n",
    "        return None\n",
    "\n",
    "bicep_foundry_deployment = 'master-lab-02-foundry'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, bicep_foundry_deployment):\n",
    "    print('[OK] Foundry Bicep deployment already exists ‚Äì skipping.')\n",
    "else:\n",
    "    print('[*] Deploying foundry accounts via Bicep...')\n",
    "    template_candidate = BICEP_DIR / 'deploy-02-foundry.bicep'\n",
    "    template_file = compile_bicep_safe(template_candidate)\n",
    "    if not template_file:\n",
    "        print(f\"[WARN] Bicep template or precompiled JSON not found at: {template_candidate}\")\n",
    "        print(\"[WARN] Skipping Bicep deployment and relying on previously created Python-based foundry resources.\")\n",
    "    else:\n",
    "        params_dict = {\n",
    "            'resourceSuffix': {'value': foundry_suffix},\n",
    "            # Optional custom config example:\n",
    "            # 'foundryConfig': {'value': [\n",
    "            #     {'name': 'foundry1', 'location': 'uksouth'},\n",
    "            #     {'name': 'foundry2', 'location': 'eastus'},\n",
    "            #     {'name': 'foundry3', 'location': 'norwayeast'}\n",
    "            # ]}\n",
    "        }\n",
    "        success, _ = deploy_template(resource_group_name, bicep_foundry_deployment, template_file, params_dict)\n",
    "        if not success:\n",
    "            print('[WARN] Foundry Bicep deployment failed ‚Äì continuing without Bicep deployment.')\n",
    "        else:\n",
    "            print('[OK] Foundry accounts deployed via Bicep')\n",
    "\n",
    "# Outputs (graceful fallback to existing_accounts if Bicep outputs unavailable)\n",
    "try:\n",
    "    foundry_outputs = get_deployment_outputs(resource_group_name, bicep_foundry_deployment)\n",
    "    print('[OK] Foundry outputs retrieved')\n",
    "    accounts = foundry_outputs.get('foundryAccounts', [])\n",
    "    if isinstance(accounts, list):\n",
    "        print('\\n[Foundry Accounts]')\n",
    "        for a in accounts:\n",
    "            print(f\"  - {a.get('name')} @ {a.get('location')} -> {a.get('endpoint')}\")\n",
    "    else:\n",
    "        print('[WARN] foundryAccounts output missing or wrong type')\n",
    "except Exception as e:\n",
    "    print('[WARN] Could not retrieve foundry outputs:', str(e)[:160])\n",
    "    if 'existing_accounts' in globals() and existing_accounts:\n",
    "        print('[INFO] Falling back to existing_accounts already provisioned:')\n",
    "        for name, acct_obj in existing_accounts.items():\n",
    "            try:\n",
    "                loc = getattr(acct_obj, 'location', 'unknown')\n",
    "                endpoint = getattr(acct_obj.properties, 'endpoint', None) or getattr(acct_obj.properties, 'apiEndpoint', '')\n",
    "                print(f\"  - {name} @ {loc} -> {endpoint}\")\n",
    "            except Exception:\n",
    "                print(f\"  - {name}\")\n",
    "    else:\n",
    "        print('[INFO] No existing_accounts fallback available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_31_c1f724ac",
   "metadata": {},
   "source": [
    "### Generate .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell_32_9a09bb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:04.743876Z",
     "iopub.status.busy": "2025-11-17T18:37:04.743543Z",
     "iopub.status.idle": "2025-11-17T18:37:07.049641Z",
     "shell.execute_reply": "2025-11-17T18:37:07.048226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating master-lab.env...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deploy] outputs keys: contentSafetyEndpoint, contentSafetyKey, cosmosDbAccountName, cosmosDbEndpoint, cosmosDbKey, redisCacheHost, redisCacheKey, redisCachePort, searchServiceAdminKey, searchServiceEndpoint, searchServiceName\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deploy] outputs keys: containerAppEnvId, containerRegistryLoginServer, containerRegistryName, mcpServerUrls\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'step1_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m         step4_outputs = {}\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Get API key from APIM subscriptions (prefer step1 outputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m apim_subscriptions = step1_outputs.get(\u001b[33m'\u001b[39m\u001b[33mapimSubscriptions\u001b[39m\u001b[33m'\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mstep1_outputs\u001b[49m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m     33\u001b[39m api_key = apim_subscriptions[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m apim_subscriptions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Auto-discover APIM API_ID from deployed APIM service\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'step1_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('[*] Generating master-lab.env...')\n",
    "\n",
    "# Ensure step2_outputs, step3_outputs and step4_outputs exist (safe fallback to empty dicts)\n",
    "try:\n",
    "    step2_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step2_outputs = get_deployment_outputs(resource_group_name, deployment_step2c)\n",
    "    except Exception:\n",
    "        step2_outputs = {}\n",
    "\n",
    "try:\n",
    "    step3_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    except Exception:\n",
    "        step3_outputs = {}\n",
    "\n",
    "try:\n",
    "    step4_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    except Exception:\n",
    "        step4_outputs = {}\n",
    "\n",
    "# Get API key from APIM subscriptions (prefer step1 outputs)\n",
    "apim_subscriptions = step1_outputs.get('apimSubscriptions', []) if isinstance(step1_outputs, dict) else []\n",
    "api_key = apim_subscriptions[0]['key'] if apim_subscriptions else 'N/A'\n",
    "\n",
    "# Auto-discover APIM API_ID from deployed APIM service\n",
    "print('[*] Auto-discovering APIM_API_ID...')\n",
    "discovered_api_id = None\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    import json as json_module\n",
    "    import shutil\n",
    "    \n",
    "    # Get APIM service name from step1 outputs\n",
    "    apim_service_name = step1_outputs.get('apimServiceName', 'apim-pavavy6pu5hpa')\n",
    "    \n",
    "    # Find Azure CLI\n",
    "    az_cli = shutil.which(\"az\")\n",
    "    if az_cli and subscription_id:\n",
    "        # Query APIM for APIs\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{resource_group_name}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{apim_service_name}/apis?api-version=2022-08-01')\n",
    "        \n",
    "        result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                               capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            apis_data = json_module.loads(result.stdout)\n",
    "            apis = apis_data.get('value', [])\n",
    "            \n",
    "            # Find inference API\n",
    "            for api in apis:\n",
    "                api_id = api.get('name', '')\n",
    "                api_props = api.get('properties', {})\n",
    "                api_name = api_props.get('displayName', '').lower()\n",
    "                api_path = api_props.get('path', '').lower()\n",
    "                \n",
    "                if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                    discovered_api_id = api_id\n",
    "                    print(f'[OK] Auto-discovered APIM_API_ID: {discovered_api_id}')\n",
    "                    break\n",
    "            \n",
    "            if not discovered_api_id:\n",
    "                # Fallback to inference-api if exists\n",
    "                for api in apis:\n",
    "                    if api.get('name') == 'inference-api':\n",
    "                        discovered_api_id = 'inference-api'\n",
    "                        print(f'[OK] Found APIM_API_ID: {discovered_api_id}')\n",
    "                        break\n",
    "except Exception as e:\n",
    "    print(f'[!] Could not auto-discover APIM_API_ID: {e}')\n",
    "\n",
    "# Use discovered ID or fallback to default\n",
    "apim_api_id = discovered_api_id if discovered_api_id else 'inference-api'\n",
    "if not discovered_api_id:\n",
    "    print(f'[!] Using default APIM_API_ID: {apim_api_id}')\n",
    "\n",
    "# Set in environment for downstream use\n",
    "os.environ['APIM_API_ID'] = apim_api_id\n",
    "\n",
    "# Build .env content with grouped structure\n",
    "env_content = f\"\"\"# Master AI Gateway Lab - Deployment Outputs\n",
    "# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "# Resource Group: {resource_group_name}\n",
    "\n",
    "# ===========================================\n",
    "# APIM (API Management)\n",
    "# ===========================================\n",
    "APIM_GATEWAY_URL={step1_outputs.get('apimGatewayUrl', '')}\n",
    "APIM_SERVICE_ID={step1_outputs.get('apimServiceId', '')}\n",
    "APIM_SERVICE_NAME={step1_outputs.get('apimServiceName', '')}\n",
    "APIM_API_KEY={api_key}\n",
    "APIM_API_ID={apim_api_id}\n",
    "\n",
    "# ===========================================\n",
    "# AI Foundry\n",
    "# ===========================================\n",
    "FOUNDRY_PROJECT_ENDPOINT={step2_outputs.get('foundryProjectEndpoint', '')}\n",
    "INFERENCE_API_PATH={step2_outputs.get('inferenceAPIPath', 'inference')}\n",
    "\"\"\"\n",
    "\n",
    "# ===========================================\n",
    "# AI Models (Multi-Region Load Balancing)\n",
    "# ===========================================\n",
    "# Extract foundry deployment information from step2_outputs\n",
    "foundries_data = step2_outputs.get('foundries', []) if isinstance(step2_outputs, dict) else []\n",
    "\n",
    "# Region mapping for display\n",
    "region_names = {\n",
    "    'uksouth': 'UK South',\n",
    "    'eastus': 'East US',\n",
    "    'norwayeast': 'Norway East'\n",
    "}\n",
    "\n",
    "# Track endpoints for load balancing\n",
    "lb_endpoints = []\n",
    "lb_regions = []\n",
    "\n",
    "env_content += \"\\n# ===========================================\\n\"\n",
    "env_content += \"# AI Models (Multi-Region Load Balancing)\\n\"\n",
    "env_content += \"# ===========================================\\n\\n\"\n",
    "\n",
    "# Process each foundry (region)\n",
    "for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "    if not isinstance(foundry_info, dict):\n",
    "        continue\n",
    "\n",
    "    foundry_name = foundry_info.get('name', '')\n",
    "    location = foundry_info.get('location', '')\n",
    "    endpoint = foundry_info.get('endpoint', '')\n",
    "    key = foundry_info.get('key', '')\n",
    "    models = foundry_info.get('models', [])\n",
    "\n",
    "    # Add region to load balancing config\n",
    "    if location:\n",
    "        lb_regions.append(location)\n",
    "\n",
    "    # Add comment for region\n",
    "    region_display = region_names.get(location, location)\n",
    "    env_content += f\"# Region {idx} ({region_display}) - {foundry_name}\\n\"\n",
    "\n",
    "    # Process each model in this foundry\n",
    "    for model_name in models:\n",
    "        # Normalize model name for env var (replace hyphens with underscores, uppercase)\n",
    "        model_var = model_name.upper().replace('-', '_').replace('.', '_')\n",
    "\n",
    "        # Add endpoint and key for this model in this region\n",
    "        env_content += f\"MODEL_{model_var}_ENDPOINT_R{idx}={endpoint}\\n\"\n",
    "        env_content += f\"MODEL_{model_var}_KEY_R{idx}={key}\\n\"\n",
    "\n",
    "        # Track gpt-4o-mini endpoints for load balancing\n",
    "        if model_name == 'gpt-4o-mini' and endpoint:\n",
    "            lb_endpoints.append(endpoint)\n",
    "\n",
    "    env_content += \"\\n\"\n",
    "\n",
    "# Add load balancing configuration\n",
    "env_content += \"# Load Balancing Configuration\\n\"\n",
    "env_content += f\"LB_REGIONS={','.join(lb_regions)}\\n\"\n",
    "env_content += f\"LB_GPT4O_MINI_ENDPOINTS={','.join(lb_endpoints)}\\n\"\n",
    "env_content += f\"LB_ENABLED={'true' if len(lb_endpoints) > 1 else 'false'}\\n\"\n",
    "env_content += \"\\n\"\n",
    "\n",
    "# Continue with supporting services\n",
    "env_content += f\"\"\"# ===========================================\n",
    "# Supporting Services\n",
    "# ===========================================\n",
    "\n",
    "# Redis (Semantic Caching)\n",
    "REDIS_HOST={step3_outputs.get('redisCacheHost', '')}\n",
    "REDIS_PORT={step3_outputs.get('redisCachePort', 10000)}\n",
    "REDIS_KEY={step3_outputs.get('redisCacheKey', '')}\n",
    "\n",
    "# Azure Cognitive Search\n",
    "SEARCH_SERVICE_NAME={step3_outputs.get('searchServiceName', '')}\n",
    "SEARCH_ENDPOINT={step3_outputs.get('searchServiceEndpoint', '')}\n",
    "SEARCH_ADMIN_KEY={step3_outputs.get('searchServiceAdminKey', '')}\n",
    "\n",
    "# Cosmos DB\n",
    "COSMOS_ACCOUNT_NAME={step3_outputs.get('cosmosDbAccountName', '')}\n",
    "COSMOS_ENDPOINT={step3_outputs.get('cosmosDbEndpoint', '')}\n",
    "COSMOS_KEY={step3_outputs.get('cosmosDbKey', '')}\n",
    "\n",
    "# Content Safety\n",
    "CONTENT_SAFETY_ENDPOINT={step3_outputs.get('contentSafetyEndpoint', '')}\n",
    "CONTENT_SAFETY_KEY={step3_outputs.get('contentSafetyKey', '')}\n",
    "\n",
    "# ===========================================\n",
    "# MCP Servers\n",
    "# ===========================================\n",
    "CONTAINER_REGISTRY={step4_outputs.get('containerRegistryLoginServer', '')}\n",
    "CONTAINER_APP_ENV_ID={step4_outputs.get('containerAppEnvId', '')}\n",
    "\"\"\"\n",
    "\n",
    "# Add MCP server URLs (safe handling if not present)\n",
    "mcp_urls = step4_outputs.get('mcpServerUrls', []) if isinstance(step4_outputs, dict) else []\n",
    "for mcp_server in mcp_urls:  # FIXED: Changed from 'mcp' to 'mcp_server' to avoid overwriting global mcp variable\n",
    "    # Guard against missing fields\n",
    "    name = mcp_server.get('name') if isinstance(mcp_server, dict) else None\n",
    "    url = mcp_server.get('url') if isinstance(mcp_server, dict) else None\n",
    "    if name and url:\n",
    "        var_name = f\"MCP_SERVER_{name.upper().replace('-', '_')}_URL\"\n",
    "        env_content += f\"{var_name}={url}\\n\"\n",
    "\n",
    "env_content += f\"\"\"\n",
    "# ===========================================\n",
    "# Deployment Info\n",
    "# ===========================================\n",
    "RESOURCE_GROUP={resource_group_name}\n",
    "LOCATION={location}\n",
    "DEPLOYMENT_PREFIX={deployment_name_prefix}\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "env_file = 'master-lab.env'\n",
    "with open(env_file, 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(f'[OK] Created {env_file}')\n",
    "print(f'[OK] File location: {os.path.abspath(env_file)}')\n",
    "\n",
    "# Display summary of model deployments\n",
    "if foundries_data:\n",
    "    print()\n",
    "    print('[*] Model Deployment Summary:')\n",
    "    for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "        if isinstance(foundry_info, dict):\n",
    "            location = foundry_info.get('location', 'unknown')\n",
    "            models = foundry_info.get('models', [])\n",
    "            region_display = region_names.get(location, location)\n",
    "            print(f'  Region {idx} ({region_display}): {len(models)} models')\n",
    "            for model in models:\n",
    "                print(f'    - {model}')\n",
    "\n",
    "# Display load balancing info\n",
    "if len(lb_endpoints) > 1:\n",
    "    print()\n",
    "    print(f'[OK] Load Balancing: ENABLED ({len(lb_endpoints)} regions)')\n",
    "    print(f'[OK] LB Regions: {\", \".join(lb_regions)}')\n",
    "else:\n",
    "    print()\n",
    "    print('[!] Load Balancing: Disabled (requires 2+ regions with gpt-4o-mini)')\n",
    "\n",
    "print()\n",
    "print('[OK] You can now load this in all lab tests:')\n",
    "print('  from dotenv import load_dotenv')\n",
    "print('  load_dotenv(\"master-lab.env\")')\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('SETUP COMPLETE - ALL LABS READY')\n",
    "print('=' * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "apim_vars_definition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:07.052500Z",
     "iopub.status.busy": "2025-11-17T18:37:07.052221Z",
     "iopub.status.idle": "2025-11-17T18:37:07.057583Z",
     "shell.execute_reply": "2025-11-17T18:37:07.056906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[APIM & API Variables Defined]\n",
      "  apim_gateway_url: https://apim-pavavy6pu5hpa.azure-api.net...\n",
      "  apim_api_key: ****2cb0\n",
      "  inference_api_path: inference\n",
      "  inference_api_version: 2024-08-01-preview\n",
      "  deployment_name: gpt-4o-mini\n",
      "  api_key: ****2cb0\n"
     ]
    }
   ],
   "source": [
    "# APIM Variable Definitions (for cells that use lowercase names)\n",
    "# These map environment variables to lowercase snake_case for backwards compatibility\n",
    "\n",
    "import os\n",
    "\n",
    "# APIM Gateway URLs\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "apim_resource_gateway_url = apim_gateway_url  # Same as gateway URL\n",
    "apim_api_key = os.environ.get('APIM_API_KEY', '')\n",
    "\n",
    "# Azure OpenAI API Configuration\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "inference_api_version = '2024-08-01-preview'  # Azure OpenAI API version\n",
    "api_key = apim_api_key  # Alias for backward compatibility\n",
    "\n",
    "# Model deployment (default to gpt-4o-mini for cost efficiency)\n",
    "deployment_name = 'gpt-4o-mini'\n",
    "\n",
    "# Display for verification\n",
    "print('[APIM & API Variables Defined]')\n",
    "print(f'  apim_gateway_url: {apim_gateway_url[:50]}...' if apim_gateway_url else '  apim_gateway_url: NOT SET')\n",
    "print(f'  apim_api_key: ****{apim_api_key[-4:]}' if len(apim_api_key) > 4 else '  apim_api_key: NOT SET')\n",
    "print(f'  inference_api_path: {inference_api_path}')\n",
    "print(f'  inference_api_version: {inference_api_version}')\n",
    "print(f'  deployment_name: {deployment_name}')\n",
    "print(f'  api_key: ****{api_key[-4:]}' if len(str(api_key)) > 4 else '  api_key: NOT SET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_33_bbe53d04",
   "metadata": {},
   "source": [
    "# Master AI Gateway Lab - 25 Labs Consolidated\n",
    "\n",
    "**One deployment. All features. Fully expanded tests.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Initialization](#init)\n",
    "- [Workshop Routes](#routes)\n",
    "- [Lab 01: Zero to Production](#lab01)\n",
    "- [Lab 02: Backend Pool Load Balancing](#lab02)\n",
    "- [Lab 03: Built-in Logging](#lab03)\n",
    "- [Lab 04: Token Metrics Emitting](#lab04)\n",
    "- [Lab 05: Token Rate Limiting](#lab05)\n",
    "- [Lab 06: Access Controlling](#lab06)\n",
    "- [Lab 07: Content Safety](#lab07)\n",
    "- [Lab 08: Model Routing](#lab08)\n",
    "- [Lab 09: AI Foundry SDK](#lab09)\n",
    "- [Lab 10: AI Foundry DeepSeek](#lab10)\n",
    "- [Lab 11: Model Context Protocol](#lab11)\n",
    "- [Lab 12: MCP from API](#lab12)\n",
    "- [Lab 13: MCP Client Authorization](#lab13)\n",
    "- [Lab 14: MCP A2A Agents](#lab14)\n",
    "- [Lab 15: OpenAI Agents](#lab15)\n",
    "- [Lab 16: AI Agent Service](#lab16)\n",
    "- [Lab 17: Realtime MCP Agents](#lab17)\n",
    "- [Lab 18: Function Calling](#lab18)\n",
    "- [Lab 19: Semantic Caching](#lab19)\n",
    "- [Lab 20: Message Storing](#lab20)\n",
    "- [Lab 21: Vector Searching](#lab21)\n",
    "- [Lab 22: Image Generation](#lab22)\n",
    "- [Lab 23: Multi-Server Orchestration](#lab23)\n",
    "- [Lab 24: FinOps Framework](#lab24)\n",
    "- [Lab 25: Secure Responses API](#lab25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_34_8b15779b",
   "metadata": {},
   "source": [
    "The following labs (01-10) cover essential Azure API Management features for AI workloads:\n",
    "\n",
    "- **Lab 01:** Zero to Production - Foundation setup and basic chat completion\n",
    "- **Lab 02:** Backend Pool Load Balancing - Multi-region routing and failover\n",
    "- **Lab 03:** Built-in Logging - Observability with Log Analytics and App Insights\n",
    "- **Lab 04:** Token Metrics Emitting - Cost monitoring and capacity planning\n",
    "- **Lab 05:** Token Rate Limiting - Quota management and abuse prevention\n",
    "- **Lab 06:** Access Controlling - OAuth 2.0 and Entra ID authentication\n",
    "- **Lab 07:** Content Safety - Harmful content detection and filtering\n",
    "- **Lab 08:** Model Routing - Intelligent model selection by criteria\n",
    "- **Lab 09:** AI Foundry SDK - Advanced AI capabilities and model catalog\n",
    "- **Lab 10:** AI Foundry DeepSeek - Open-source reasoning model integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_35_0a0d7ce7",
   "metadata": {},
   "source": [
    "<a id='lab01'></a>\n",
    "\n",
    "## Lab 01: Zero to Production\n",
    "\n",
    "![flow](../../images/GPT-4o-inferencing.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Learn the fundamentals of deploying and testing Azure OpenAI through API Management, establishing the foundation for all advanced labs.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Basic Chat Completion:** Send prompts to GPT-4o-mini and receive AI-generated responses\n",
    "- **Streaming Responses:** Handle real-time streaming output for better user experience\n",
    "- **Request Patterns:** Understand the HTTP request/response cycle through APIM gateway\n",
    "- **API Key Management:** Secure API access using APIM subscription keys\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](../../zero-to-production/result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Basic chat completion returns valid responses\n",
    "- Streaming works correctly with incremental tokens\n",
    "- Multiple requests complete successfully\n",
    "- Response times are < 2 seconds for simple prompts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_36_1f195b0a",
   "metadata": {},
   "source": [
    "### Test 1: Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell_37_7bb1f71e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:07.059683Z",
     "iopub.status.busy": "2025-11-17T18:37:07.059491Z",
     "iopub.status.idle": "2025-11-17T18:37:08.095598Z",
     "shell.execute_reply": "2025-11-17T18:37:08.094821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAB 02: Token Metrics Configuration\n",
      "================================================================================\n",
      "\n",
      "[policy] Backend ID: inference-backend-pool\n",
      "[policy] Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[policy] Using API ID: inference-api\n",
      "[policy] Applying token-metrics via REST API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] Status: 200 - SUCCESS\n",
      "\n",
      "[OK] Policy application complete\n",
      "[INFO] Metrics will be available in Azure Monitor\n",
      "[NEXT] Run the cells below to test token metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LAB 02: Token Metrics (OpenAI API Monitoring)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 02: Token Metrics Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend ID: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Token metrics policy with API-KEY authentication\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "        <azure-openai-emit-token-metric namespace=\"openai\">\n",
    "            <dimension name=\"Subscription ID\" value=\"@(context.Subscription.Id)\" />\n",
    "            <dimension name=\"Client IP\" value=\"@(context.Request.IpAddress)\" />\n",
    "            <dimension name=\"API ID\" value=\"@(context.Api.Id)\" />\n",
    "            <dimension name=\"User ID\" value=\"@(context.Request.Headers.GetValueOrDefault(&quot;x-user-id&quot;, &quot;N/A&quot;))\" />\n",
    "        </azure-openai-emit-token-metric>\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying token-metrics via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Metrics will be available in Azure Monitor\")\n",
    "print(\"[NEXT] Run the cells below to test token metrics\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 01: Test 1 - Basic Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_39_0478a7f3",
   "metadata": {},
   "source": [
    "### Test 2: Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell_40_4d2ace70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:08.098070Z",
     "iopub.status.busy": "2025-11-17T18:37:08.097816Z",
     "iopub.status.idle": "2025-11-17T18:37:08.103724Z",
     "shell.execute_reply": "2025-11-17T18:37:08.103134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Testing streaming...\n",
      "[ERROR] Streaming exception: name 'client' is not defined\n",
      "[HINT] If this persists, verify the APIM operation allows streaming and the backend model supports it.\n"
     ]
    }
   ],
   "source": [
    "# Lab 01: Test 2 - Streaming Response (robust with fallback)\n",
    "\n",
    "print('[*] Testing streaming...')\n",
    "\n",
    "prompt_messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant. Stream numbers.'},\n",
    "    {'role': 'user', 'content': 'Count from 1 to 5'}\n",
    "]\n",
    "\n",
    "def stream_completion():\n",
    "    return client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompt_messages,\n",
    "        max_tokens=32,\n",
    "        temperature=0.2,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "def non_stream_completion():\n",
    "    return client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompt_messages,\n",
    "        max_tokens=32,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "try:\n",
    "    stream = stream_completion()\n",
    "    had_output = False\n",
    "    for chunk in stream:\n",
    "        try:\n",
    "            # Support both delta.content and delta with list of content parts\n",
    "            if chunk.choices:\n",
    "                delta = getattr(chunk.choices[0], 'delta', None)\n",
    "                if delta:\n",
    "                    piece = getattr(delta, 'content', None)\n",
    "                    if piece:\n",
    "                        print(piece, end='', flush=True)\n",
    "                        had_output = True\n",
    "        except Exception:\n",
    "            # Ignore malformed chunk pieces\n",
    "            pass\n",
    "    if not had_output:\n",
    "        print('[WARN] Stream yielded no incremental content; backend may not support streaming through APIM.')\n",
    "        raise RuntimeError('Empty stream')\n",
    "    print()  # newline after stream\n",
    "    print('[OK] Streaming works!')\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if '500' in msg or 'Internal server error' in msg:\n",
    "        print(f'[WARN] Streaming failed with backend 500: {msg[:140]}')\n",
    "        print('[INFO] Falling back to non-streaming completion (APIM policy or backend may not support streaming).')\n",
    "        try:\n",
    "            resp = non_stream_completion()\n",
    "            try:\n",
    "                full = resp.choices[0].message.content\n",
    "            except AttributeError:\n",
    "                full = resp.choices[0].message.get('content', '')\n",
    "            print(full)\n",
    "            print('[OK] Fallback non-streaming completion succeeded.')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Fallback non-streaming also failed: {e2}')\n",
    "            print('[HINT] Check APIM policies (buffering, rewrite), model deployment name, and gateway trace.')\n",
    "    else:\n",
    "        print(f'[ERROR] Streaming exception: {msg}')\n",
    "        print('[HINT] If this persists, verify the APIM operation allows streaming and the backend model supports it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_41_d7ea554a",
   "metadata": {},
   "source": [
    "### Test 3: Multiple Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_43_67b478de",
   "metadata": {},
   "source": [
    "<a id='lab02'></a>\n",
    "\n",
    "## Lab 02: Backend Pool Load Balancing\n",
    "\n",
    "![Backend Pool Load Balancing](../../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "üìñ [Workshop Guide](https://azure-samples.github.io/AI-Gateway/docs/azure-openai/dynamic-failover)\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Master multi-region load balancing with priority-based routing and automatic failover across Azure regions.\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Priority Routing:** Configure priority 1 (UK South) with fallback to priority 2 regions\n",
    "- **Round-Robin Distribution:** Balance traffic across Sweden Central and West Europe (50/50 weight)\n",
    "- **Automatic Retry:** APIM retries on HTTP 429 (rate limit) transparently\n",
    "- **Regional Headers:** Track which region served each request via `x-ms-region` header\n",
    "- **Performance Analysis:** Visualize response times and regional distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Backend Pool Configuration\n",
    "\n",
    "Azure API Management supports three load balancing strategies:\n",
    "\n",
    "<details>\n",
    "<summary><b>1. Round-Robin Distribution</b></summary>\n",
    "\n",
    "Distributes requests evenly across all backends with equal weight.\n",
    "\n",
    "**Configuration:**\n",
    "- All backends have the same priority level\n",
    "- Equal weight distribution (or default weights)\n",
    "- Requests rotate sequentially through backends\n",
    "\n",
    "**Use Case:** When all regions have equal capacity and you want even distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>2. Priority-Based Routing</b></summary>\n",
    "\n",
    "Lower priority values receive traffic first, with automatic failover to higher priority backends.\n",
    "\n",
    "**Example Configuration:**\n",
    "- **East US:** Priority 1 (primary)\n",
    "- **West US:** Priority 2 (fallback)\n",
    "- **Sweden Central:** Priority 3 (fallback)\n",
    "\n",
    "**Use Case:** When you have a preferred region for latency or cost reasons.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>3. Weighted Load Balancing</b></summary>\n",
    "\n",
    "Assigns different traffic proportions within the same priority level.\n",
    "\n",
    "**Example Configuration:**\n",
    "- **East US:** Priority 1, Weight 100\n",
    "- **West US:** Priority 2, Weight 50\n",
    "- **Sweden Central:** Priority 2, Weight 50\n",
    "\n",
    "When Priority 1 is unavailable, traffic splits 50/50 between Priority 2 backends.\n",
    "\n",
    "**Use Case:** When backends have different capacities or you want controlled traffic distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Circuit Breaker Configuration\n",
    "\n",
    "> **üí° Tip:** Each backend should have a circuit breaker rule to handle failures gracefully.\n",
    "\n",
    "**Recommended Settings:**\n",
    "- **Failure Count:** 1 (trip after single failure)\n",
    "- **Failure Interval:** 5 minutes\n",
    "- **Custom Range:** HTTP 429 (rate limit)\n",
    "- **Trip Duration:** 1 minute\n",
    "- **Retry-After Header:** Enabled\n",
    "\n",
    "This configuration ensures that when a backend hits its rate limit (HTTP 429), APIM automatically routes traffic to other backends for 1 minute.\n",
    "\n",
    "---\n",
    "\n",
    "### Monitoring Regional Distribution\n",
    "\n",
    "> **‚ö†Ô∏è Note:** The `x-ms-region` header in responses indicates which backend processed the request.\n",
    "\n",
    "This header allows you to:\n",
    "- Verify load distribution patterns\n",
    "- Monitor failover behavior\n",
    "- Analyze regional performance\n",
    "- Debug routing issues\n",
    "\n",
    "**Example Response Headers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell_44_f7e0fe5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:08.105401Z",
     "iopub.status.busy": "2025-11-17T18:37:08.105260Z",
     "iopub.status.idle": "2025-11-17T18:37:09.135683Z",
     "shell.execute_reply": "2025-11-17T18:37:09.134504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAB 03: Load Balancing Configuration\n",
      "================================================================================\n",
      "\n",
      "[policy] Backend Pool: inference-backend-pool\n",
      "[policy] Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[policy] Using API ID: inference-api\n",
      "[policy] Applying load-balancing via REST API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] Status: 200 - SUCCESS\n",
      "\n",
      "[OK] Policy application complete\n",
      "[INFO] Load balancing will distribute requests across backend pool\n",
      "[INFO] Retry logic will handle 429 (rate limit) and 503 (unavailable) errors\n",
      "[NEXT] Run load balancing tests in cells below\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LAB 03: Load Balancing with Retry Logic\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 03: Load Balancing Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend Pool: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Load balancing policy with API-KEY authentication and retry logic\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\"\n",
    "               condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n",
    "            <forward-request buffer-request-body=\"true\" />\n",
    "        </retry>\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "        <choose>\n",
    "            <when condition=\"@(context.Response.StatusCode == 503)\">\n",
    "                <return-response>\n",
    "                    <set-status code=\"503\" reason=\"Service Unavailable\" />\n",
    "                    <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "                        <value>application/json</value>\n",
    "                    </set-header>\n",
    "                    <set-body>{{\"error\": {{\"code\": \"ServiceUnavailable\", \"message\": \"Service temporarily unavailable\"}}}}</set-body>\n",
    "                </return-response>\n",
    "            </when>\n",
    "        </choose>\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying load-balancing via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Load balancing will distribute requests across backend pool\")\n",
    "print(\"[INFO] Retry logic will handle 429 (rate limit) and 503 (unavailable) errors\")\n",
    "print(\"[NEXT] Run load balancing tests in cells below\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.137993Z",
     "iopub.status.busy": "2025-11-17T18:37:09.137791Z",
     "iopub.status.idle": "2025-11-17T18:37:09.180599Z",
     "shell.execute_reply": "2025-11-17T18:37:09.179352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIX: Creating Backend Pool for Round-Robin Load Balancing (Preview API)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.mgmt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFIX: Creating Backend Pool for Round-Robin Load Balancing (Preview API)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmgmt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapimanagement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApiManagementClient\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmgmt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapimanagement\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendContract\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure.mgmt'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX: Create Backend Pool for Load Balancing (Preview API)\n",
    "# ============================================================================\n",
    "# Ensure backend pool uses API version >= 2023-05-01-preview and omits\n",
    "# unsupported properties (url/protocol) when type = 'Pool'.\n",
    "# If you still get validation errors complaining about API version,\n",
    "# double‚Äëcheck:\n",
    "#   1. Region of APIM service supports backend pools (feature rollout).\n",
    "#   2. API version string EXACTLY matches '2023-05-01-preview'.\n",
    "#   3. No stale variable pool_url from a prior run (restart kernel if needed).\n",
    "#   4. You removed old cell output using lower API version.\n",
    "#\n",
    "# Added: Verification of existing pool, conditional PUT only if absent, and\n",
    "# GET after creation. Debug prints trimmed for clarity.\n",
    "#\n",
    "# FIXED 2025-11-17: Changed priority/weight for true round-robin distribution:\n",
    "#   - All backends now have priority=1 (same priority = no priority-based routing)\n",
    "#   - All backends now have weight=1 (equal weight = even distribution)\n",
    "#   - Previous config had foundry1=P1W100, foundry2=P2W50, foundry3=P2W50\n",
    "#   - This caused 100% traffic to foundry1 (highest priority)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIX: Creating Backend Pool for Round-Robin Load Balancing (Preview API)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from azure.mgmt.apimanagement import ApiManagementClient\n",
    "from azure.mgmt.apimanagement.models import BackendContract\n",
    "import requests, json\n",
    "\n",
    "apim_client = ApiManagementClient(credential, subscription_id)\n",
    "\n",
    "resource_suffix = 'pavavy6pu5hpa'\n",
    "backends_config = [\n",
    "    {'id': 'foundry1', 'url': f'https://foundry1-{resource_suffix}.openai.azure.com/openai', 'location': 'uksouth', 'priority': 1, 'weight': 1},\n",
    "    {'id': 'foundry2', 'url': f'https://foundry2-{resource_suffix}.openai.azure.com/openai', 'location': 'eastus', 'priority': 1, 'weight': 1},\n",
    "    {'id': 'foundry3', 'url': f'https://foundry3-{resource_suffix}.openai.azure.com/openai', 'location': 'norwayeast', 'priority': 1, 'weight': 1},\n",
    "]\n",
    "\n",
    "print(\"[*] Step 1: Ensuring individual backends...\")\n",
    "backend_arm_ids = []\n",
    "for cfg in backends_config:\n",
    "    bid = cfg['id']\n",
    "    try:\n",
    "        apim_client.backend.get(resource_group, apim_service_name, bid)\n",
    "        print(f\"  [OK] Backend '{bid}' exists\")\n",
    "    except Exception:\n",
    "        print(f\"  [*] Creating backend '{bid}'...\")\n",
    "        backend = BackendContract(\n",
    "            url=cfg['url'],\n",
    "            protocol=\"http\",\n",
    "            description=f\"Azure OpenAI - {cfg['location']}\",\n",
    "            tls={\"validateCertificateChain\": True, \"validateCertificateName\": True}\n",
    "        )\n",
    "        try:\n",
    "            apim_client.backend.create_or_update(resource_group, apim_service_name, bid, backend)\n",
    "            print(f\"  [OK] Backend '{bid}' created\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Backend create failed '{bid}': {str(e)[:160]}\")\n",
    "            continue\n",
    "    backend_arm_ids.append({\n",
    "        'id': f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/backends/{bid}\",\n",
    "        'priority': cfg['priority'],\n",
    "        'weight': cfg['weight']\n",
    "    })\n",
    "\n",
    "print(\"\\n[*] Step 2: Ensuring backend POOL (preview)...\")\n",
    "POOL_API_VERSION = \"2023-05-01-preview\"\n",
    "pool_id = \"inference-backend-pool\"\n",
    "services = [{\"id\": b['id'], \"priority\": b['priority'], \"weight\": b['weight']} for b in backend_arm_ids]\n",
    "\n",
    "# Build URL with preview version (must match exactly)\n",
    "pool_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/\"\n",
    "    f\"Microsoft.ApiManagement/service/{apim_service_name}/backends/{pool_id}?api-version={POOL_API_VERSION}\"\n",
    ")\n",
    "\n",
    "# Check if pool already exists\n",
    "try:\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    existing_resp = requests.get(pool_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    pool_body = {\n",
    "        \"properties\": {\n",
    "            \"description\": \"Round-robin load balancer (equal priority=1, weight=1 for all backends)\",\n",
    "            \"type\": \"Pool\",\n",
    "            \"pool\": {\"services\": services}\n",
    "        }\n",
    "    }\n",
    "    if existing_resp.status_code == 200:\n",
    "        print(f\"  [OK] Pool '{pool_id}' exists - updating to round-robin configuration...\")\n",
    "    else:\n",
    "        print(f\"  [*] Pool '{pool_id}' not found (status {existing_resp.status_code}); creating...\")\n",
    "    \n",
    "    put_resp = requests.put(\n",
    "        pool_url,\n",
    "        headers={\"Authorization\": f\"Bearer {token.token}\", \"Content-Type\": \"application/json\"},\n",
    "        json=pool_body,\n",
    "        timeout=60\n",
    "    )\n",
    "    if put_resp.status_code in (200, 201):\n",
    "        print(f\"  [OK] Pool '{pool_id}' configured for round-robin (status {put_resp.status_code})\")\n",
    "    else:\n",
    "        print(f\"  [ERROR] Pool create/update failed: {put_resp.status_code}\")\n",
    "        try:\n",
    "            print(json.dumps(put_resp.json(), indent=2)[:1500])\n",
    "        except Exception:\n",
    "            print(put_resp.text[:1500])\n",
    "        if \"Backend Type and Pool properties\" in put_resp.text:\n",
    "            print(\"  [HINT] Preview feature may not be enabled in this region or API version mismatch.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Exception during pool ensure: {str(e)[:200]}\")\n",
    "\n",
    "# Final verification GET\n",
    "try:\n",
    "    verify = requests.get(pool_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    print(\"\\n[*] Verification GET status:\", verify.status_code)\n",
    "    if verify.status_code == 200:\n",
    "        data = verify.json()\n",
    "        services_out = (data.get('properties', {}).get('pool', {}) or {}).get('services', [])\n",
    "        print(f\"  [OK] Pool has {len(services_out)} services:\")\n",
    "        priorities = []\n",
    "        weights = []\n",
    "        for s in services_out:\n",
    "            name = s.get('id','').split('/')[-1]\n",
    "            priority = s.get('priority')\n",
    "            weight = s.get('weight')\n",
    "            priorities.append(priority)\n",
    "            weights.append(weight)\n",
    "            print(f\"    - {name}: priority={priority}, weight={weight}\")\n",
    "        \n",
    "        # Verify round-robin configuration\n",
    "        if len(set(priorities)) == 1 and len(set(weights)) == 1:\n",
    "            print(f\"  ‚úì ROUND-ROBIN CONFIRMED: all backends have priority={priorities[0]}, weight={weights[0]}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö† NOT ROUND-ROBIN: priorities={priorities}, weights={weights}\")\n",
    "    else:\n",
    "        print(\"  [WARN] Could not verify pool; status\", verify.status_code)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Verification failed: {str(e)[:160]}\")\n",
    "\n",
    "print(\"\\n[OK] Backend pool configuration complete.\")\n",
    "print(\"[INFO] Expected behavior: ~33% distribution across UK South, East US, Norway East\")\n",
    "print(\"[NEXT] Run Cell 47 to test load balancing distribution\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.183901Z",
     "iopub.status.busy": "2025-11-17T18:37:09.183642Z",
     "iopub.status.idle": "2025-11-17T18:37:09.658893Z",
     "shell.execute_reply": "2025-11-17T18:37:09.658114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LIST] status: 200\n",
      "[LIST] 4 backends returned (including pool if successful):\n",
      "  [BACKEND] foundry1: type=Standard\n",
      "  [BACKEND] foundry2: type=Standard\n",
      "  [BACKEND] foundry3: type=Standard\n",
      "  [POOL] inference-backend-pool: services=3\n"
     ]
    }
   ],
   "source": [
    "# Verification Helper (Optional): List all backends to confirm pool presence\n",
    "import requests, json\n",
    "POOL_API_VERSION = \"2023-05-01-preview\"\n",
    "list_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/\"\n",
    "    f\"Microsoft.ApiManagement/service/{apim_service_name}/backends?api-version={POOL_API_VERSION}\"\n",
    ")\n",
    "try:\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    r = requests.get(list_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    print(\"[LIST] status:\", r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        items = r.json().get('value', [])\n",
    "        print(f\"[LIST] {len(items)} backends returned (including pool if successful):\")\n",
    "        for it in items:\n",
    "            pid = it.get('name') or it.get('id','').split('/')[-1]\n",
    "            ptype = it.get('properties', {}).get('type', 'Standard')\n",
    "            if ptype == 'Pool':\n",
    "                services = (it.get('properties', {}).get('pool', {}) or {}).get('services', [])\n",
    "                print(f\"  [POOL] {pid}: services={len(services)}\")\n",
    "            else:\n",
    "                print(f\"  [BACKEND] {pid}: type={ptype}\")\n",
    "    else:\n",
    "        print(r.text[:800])\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Backend list failed:\", str(e)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_45_7d2cb75c",
   "metadata": {},
   "source": [
    "### Test 1: Load Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell_46_c665adef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.662268Z",
     "iopub.status.busy": "2025-11-17T18:37:09.661757Z",
     "iopub.status.idle": "2025-11-17T18:37:09.710347Z",
     "shell.execute_reply": "2025-11-17T18:37:09.709483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing load balancing across 3 regions...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'step1_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m backend_ids = []  \u001b[38;5;66;03m# Track which backend served each request\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Resolve required variables (avoid NameError)\u001b[39;00m\n\u001b[32m      7\u001b[39m apim_gateway_url = (\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     (step1_outputs.get(\u001b[33m'\u001b[39m\u001b[33mapimGatewayUrl\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mstep1_outputs\u001b[49m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m      9\u001b[39m     os.environ.get(\u001b[33m'\u001b[39m\u001b[33mAPIM_GATEWAY_URL\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m inference_api_path = (\n\u001b[32m     12\u001b[39m     (step2_outputs.get(\u001b[33m'\u001b[39m\u001b[33minferenceAPIPath\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step2_outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m     13\u001b[39m     os.environ.get(\u001b[33m'\u001b[39m\u001b[33mINFERENCE_API_PATH\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minference\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m apim_api_key = (\n\u001b[32m     16\u001b[39m     (step1_outputs.get(\u001b[33m'\u001b[39m\u001b[33mapimSubscriptions\u001b[39m\u001b[33m'\u001b[39m, [{}])[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step1_outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m     17\u001b[39m     os.environ.get(\u001b[33m'\u001b[39m\u001b[33mAPIM_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'step1_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "print('Testing load balancing across 3 regions...')\n",
    "responses = []\n",
    "regions = []  # Track which region processed each request\n",
    "backend_ids = []  # Track which backend served each request\n",
    "\n",
    "# Resolve required variables (avoid NameError)\n",
    "apim_gateway_url = (\n",
    "    (step1_outputs.get('apimGatewayUrl') if isinstance(step1_outputs, dict) else None) or\n",
    "    os.environ.get('APIM_GATEWAY_URL')\n",
    ")\n",
    "inference_api_path = (\n",
    "    (step2_outputs.get('inferenceAPIPath') if isinstance(step2_outputs, dict) else None) or\n",
    "    os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    ")\n",
    "apim_api_key = (\n",
    "    (step1_outputs.get('apimSubscriptions', [{}])[0].get('key') if isinstance(step1_outputs, dict) else None) or\n",
    "    os.environ.get('APIM_API_KEY')\n",
    ")\n",
    "api_version = os.environ.get('OPENAI_API_VERSION', '2024-06-01')\n",
    "\n",
    "missing = [n for n, v in {\n",
    "    'apim_gateway_url': apim_gateway_url,\n",
    "    'inference_api_path': inference_api_path,\n",
    "    'apim_api_key': apim_api_key,\n",
    "    'api_version': api_version\n",
    "}.items() if not v]\n",
    "\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing required variables: {', '.join(missing)}\")\n",
    "    print(\"[HINT] Ensure Cell 8 (.env generation) ran and load with: from dotenv import load_dotenv; load_dotenv('master-lab.env')\")\n",
    "    # Abort early to avoid further errors\n",
    "else:\n",
    "    # Use requests library to access HTTP headers (avoid duplicate import)\n",
    "    try:\n",
    "        requests\n",
    "    except NameError:\n",
    "        import requests\n",
    "\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "            url = f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}/openai/deployments/gpt-4o-mini/chat/completions\"\n",
    "            response = requests.post(\n",
    "                url=f\"{url}?api-version={api_version}\",\n",
    "                headers={\n",
    "                    \"api-key\": apim_api_key,\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": f\"Test {i+1}\"}],\n",
    "                    \"max_tokens\": 5\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            responses.append(elapsed)\n",
    "\n",
    "            region = response.headers.get('x-ms-region', 'Unknown')\n",
    "            backend_id = response.headers.get('x-ms-backend-id', 'Unknown')\n",
    "\n",
    "            regions.append(region)\n",
    "            backend_ids.append(backend_id)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Request {i+1}: {elapsed:.2f}s - Region: {region} - Backend: {backend_id}\")\n",
    "            else:\n",
    "                print(f\"Request {i+1}: {elapsed:.2f}s - HTTP {response.status_code} - Region: {region} - Backend: {backend_id}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] Request {i+1} failed: {e}\")\n",
    "            responses.append(0)\n",
    "            regions.append('Error')\n",
    "            backend_ids.append('Error')\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    avg_time = sum(responses) / len(responses) if responses else 0\n",
    "    print(f\"\\nAverage response time: {avg_time:.2f}s\")\n",
    "\n",
    "    from collections import Counter\n",
    "    region_counts = Counter(regions)\n",
    "    print(f\"\\nRegion Distribution:\")\n",
    "    for region, count in region_counts.items():\n",
    "        pct = (count / len(regions) * 100) if regions else 0\n",
    "        print(f\"  {region}: {count} requests ({pct:.1f}%)\")\n",
    "\n",
    "    unknown_count = region_counts.get('Unknown', 0)\n",
    "    if unknown_count == len(regions) and len(regions) > 0:\n",
    "        print('')\n",
    "        print('[INFO] All regions showing as \"Unknown\" - region headers may not be configured in APIM')\n",
    "        print('')\n",
    "        print('üìã TO ADD REGION HEADERS VIA APIM POLICY:')\n",
    "        print('   1. Azure Portal ‚Üí API Management ‚Üí APIs ‚Üí inference-api')\n",
    "        print('   2. Click \"All operations\" ‚Üí Outbound processing ‚Üí Add policy')\n",
    "        print('   3. Add this XML to <outbound> section:')\n",
    "        print('')\n",
    "        print('   <set-header name=\"x-ms-region\" exists-action=\"override\">')\n",
    "        print('       <value>@(context.Deployment.Region)</value>')\n",
    "        print('   </set-header>')\n",
    "        print('   <set-header name=\"x-ms-backend-id\" exists-action=\"override\">')\n",
    "        print('       <value>@(context.Request.MatchedParameters.GetValueOrDefault(\"backend-id\", \"unknown\"))</value>')\n",
    "        print('   </set-header>')\n",
    "        print('')\n",
    "        print('   4. Save the policy')\n",
    "        print('')\n",
    "        print('‚ÑπÔ∏è  Region detection is informational only - load balancing still works')\n",
    "        print('')\n",
    "\n",
    "# Fallback util if utils.print_ok not available\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Load balancing test complete!')\n",
    "else:\n",
    "    print('[OK] Load balancing test complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_47_c20a7ffc",
   "metadata": {},
   "source": [
    "### Test 2: Visualize Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell_48_b37f6034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.712697Z",
     "iopub.status.busy": "2025-11-17T18:37:09.712521Z",
     "iopub.status.idle": "2025-11-17T18:37:09.743727Z",
     "shell.execute_reply": "2025-11-17T18:37:09.743165Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Create DataFrame with response times and regions\n",
    "df = pd.DataFrame({\n",
    "    'Request': range(1, len(responses)+1),\n",
    "    'Time (s)': responses,\n",
    "    'Region': regions\n",
    "})\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Response times with region colors\n",
    "region_colors = {'Unknown': 'gray'}\n",
    "unique_regions = [r for r in set(regions) if r != 'Unknown']\n",
    "color_palette = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for idx, region in enumerate(unique_regions):\n",
    "    region_colors[region] = color_palette[idx % len(color_palette)]\n",
    "\n",
    "colors = [region_colors.get(r, 'gray') for r in regions]\n",
    "\n",
    "ax1.scatter(df['Request'], df['Time (s)'], c=colors, alpha=0.6, s=100)\n",
    "ax1.axhline(y=avg_time, color='r', linestyle='--', label=f'Average: {avg_time:.2f}s')\n",
    "ax1.set_xlabel('Request Number')\n",
    "ax1.set_ylabel('Response Time (s)')\n",
    "ax1.set_title('Load Balancing Response Times by Region')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Create custom legend for regions\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=region_colors[r], label=r) for r in set(regions)]\n",
    "ax1.legend(handles=legend_elements + [plt.Line2D([0], [0], color='r', linestyle='--', label=f'Avg: {avg_time:.2f}s')],\n",
    "          loc='upper right')\n",
    "\n",
    "# Plot 2: Region distribution bar chart\n",
    "region_counts = Counter(regions)\n",
    "regions_list = list(region_counts.keys())\n",
    "counts_list = list(region_counts.values())\n",
    "\n",
    "bars = ax2.bar(regions_list, counts_list, color=[region_colors.get(r, 'gray') for r in regions_list])\n",
    "ax2.set_xlabel('Region')\n",
    "ax2.set_ylabel('Number of Requests')\n",
    "ax2.set_title('Request Distribution Across Regions')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Replaced utils.print_ok (undefined) with a simple confirmation print\n",
    "print('Lab 02 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_49_35b632c0",
   "metadata": {},
   "source": [
    "Implement comprehensive observability using Azure Log Analytics and Application Insights for AI gateway monitoring.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Log Analytics Integration:** Automatic logging of all APIM requests and responses\n",
    "- **Application Insights:** Track performance metrics, failures, and dependencies\n",
    "- **Diagnostic Settings:** Configure what data to log and where to send it\n",
    "- **Query Language (KQL):** Write queries to analyze request patterns\n",
    "- **Dashboard Creation:** Build monitoring dashboards for AI gateway operations\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- All API requests logged to Log Analytics workspace\n",
    "- Application Insights captures latency metrics\n",
    "- KQL queries return request data successfully\n",
    "- Can trace individual requests end-to-end\n",
    "- Dashboards show real-time gateway health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_51_7bbce6e3",
   "metadata": {},
   "source": [
    "<a id='lab04'></a>\n",
    "\n",
    "## Lab 04: Token Metrics Emitting\n",
    "\n",
    "![flow](../../images/ai-gateway.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Track and emit token usage metrics for cost monitoring and capacity planning across all AI requests.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Token Counting:** Capture prompt tokens, completion tokens, and total tokens\n",
    "- **Custom Metrics:** Emit token metrics to Application Insights\n",
    "- **Cost Calculation:** Understand token-based pricing and cost attribution\n",
    "- **Usage Patterns:** Analyze token consumption trends over time\n",
    "- **Quota Management:** Track usage against allocated quotas\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](../../token-metrics-emitting/result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Token metrics logged for every request\n",
    "- Custom Application Insights metrics show token usage\n",
    "- Can query total tokens consumed per time period\n",
    "- Cost estimates available based on token pricing\n",
    "- Alerts configured for unusual token consumption\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell_52_d2e70a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.745941Z",
     "iopub.status.busy": "2025-11-17T18:37:09.745790Z",
     "iopub.status.idle": "2025-11-17T18:37:09.780000Z",
     "shell.execute_reply": "2025-11-17T18:37:09.779241Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step1_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m total_tokens = \u001b[32m0\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Resolve required endpoint pieces from previously loaded deployment outputs / env\u001b[39;00m\n\u001b[32m      5\u001b[39m apim_gateway_url = (\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     (step1_outputs.get(\u001b[33m'\u001b[39m\u001b[33mapimGatewayUrl\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mstep1_outputs\u001b[49m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mAPIM_GATEWAY_URL\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m inference_api_path = (\n\u001b[32m     10\u001b[39m     (step2_outputs.get(\u001b[33m'\u001b[39m\u001b[33minferenceAPIPath\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step2_outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mINFERENCE_API_PATH\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minference\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m apim_api_key = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'step1_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Lab 04 token usage aggregation (auto-initialize client if missing)\n",
    "total_tokens = 0\n",
    "\n",
    "# Resolve required endpoint pieces from previously loaded deployment outputs / env\n",
    "apim_gateway_url = (\n",
    "    (step1_outputs.get('apimGatewayUrl') if isinstance(step1_outputs, dict) else None)\n",
    "    or os.environ.get('APIM_GATEWAY_URL')\n",
    ")\n",
    "inference_api_path = (\n",
    "    (step2_outputs.get('inferenceAPIPath') if isinstance(step2_outputs, dict) else None)\n",
    "    or os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    ")\n",
    "apim_api_key = None\n",
    "if isinstance(step1_outputs, dict):\n",
    "    subs = step1_outputs.get('apimSubscriptions') or []\n",
    "    if subs and isinstance(subs[0], dict):\n",
    "        apim_api_key = subs[0].get('key')\n",
    "if not apim_api_key:\n",
    "    apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "\n",
    "api_version = os.environ.get('OPENAI_API_VERSION', '2024-06-01')\n",
    "\n",
    "missing = [n for n, v in {\n",
    "    'apim_gateway_url': apim_gateway_url,\n",
    "    'inference_api_path': inference_api_path,\n",
    "    'apim_api_key': apim_api_key\n",
    "}.items() if not v]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required values for client init: {', '.join(missing)}. \"\n",
    "                       f\"Ensure earlier environment/deployment cells have been run.\")\n",
    "\n",
    "# Initialize AzureOpenAI client only if not already present\n",
    "if 'client' not in globals():\n",
    "    try:\n",
    "        # Prefer shim if loaded\n",
    "        if 'get_azure_openai_client' in globals():\n",
    "            client = get_azure_openai_client(\n",
    "                azure_endpoint=f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}\",\n",
    "                api_key=apim_api_key,\n",
    "                api_version=api_version\n",
    "            )\n",
    "        else:\n",
    "            from openai import AzureOpenAI\n",
    "            client = AzureOpenAI(\n",
    "                azure_endpoint=f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}\",\n",
    "                api_key=apim_api_key,\n",
    "                api_version=api_version\n",
    "            )\n",
    "        print(\"[init] AzureOpenAI client initialized\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"[ERROR] openai package not found. Install dependencies first.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize AzureOpenAI client: {e}\")\n",
    "\n",
    "# Perform multiple requests and sum token usage\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{'role': 'user', 'content': 'Tell me about AI'}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Request {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Robust usage extraction (handles SDK variations)\n",
    "    tokens = 0\n",
    "    usage_obj = getattr(response, 'usage', None)\n",
    "    if usage_obj is not None:\n",
    "        # New SDK: usage fields may be attributes\n",
    "        tokens = getattr(usage_obj, 'total_tokens', None)\n",
    "        if tokens is None and isinstance(usage_obj, dict):\n",
    "            tokens = usage_obj.get('total_tokens')\n",
    "    if tokens is None:\n",
    "        # Fallback: sum prompt + completion if available\n",
    "        prompt_t = getattr(usage_obj, 'prompt_tokens', None) if usage_obj else None\n",
    "        completion_t = getattr(usage_obj, 'completion_tokens', None) if usage_obj else None\n",
    "        if isinstance(usage_obj, dict):\n",
    "            prompt_t = prompt_t or usage_obj.get('prompt_tokens')\n",
    "            completion_t = completion_t or usage_obj.get('completion_tokens')\n",
    "        if prompt_t is not None and completion_t is not None:\n",
    "            tokens = prompt_t + completion_t\n",
    "    if tokens is None:\n",
    "        tokens = 0  # default if usage unavailable\n",
    "\n",
    "    total_tokens += tokens\n",
    "    print(f\"Request {i+1}: {tokens} tokens\")\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")\n",
    "print(\"[OK] Lab 04 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_53_e6cddee5",
   "metadata": {},
   "source": [
    "<a id='lab05'></a>\n",
    "\n",
    "## Lab 05: API Gateway Policy Foundations\n",
    "\n",
    "> Establish core Azure API Management (APIM) policies before adding advanced access control (Lab 06). This lab focuses on baseline resilience, observability, and request hygiene.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Lay down essential APIM inbound/outbound policies to:\n",
    "- Normalize headers and enforce HTTPS\n",
    "- Add correlation IDs for tracing\n",
    "- Rate-limit abusive clients\n",
    "- Set caching directives where appropriate\n",
    "- Instrument responses for latency and status analytics\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Policy Composition:** How inbound/outbound sections work together\n",
    "- **Chaining Policies Safely:** Order considerations (validation ‚Üí transformation ‚Üí routing)\n",
    "- **Correlation & Logging:** Injecting IDs for distributed tracing\n",
    "- **Basic Throttling:** Using `rate-limit` and `quota` policies\n",
    "- **Response Shaping:** Adding custom headers for monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### Core Policy Anatomy\n",
    "\n",
    "APIM policies execute in an XML pipeline:\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <!-- Validation / Security -->\n",
    "  <!-- Transformation -->\n",
    "  <!-- Routing -->\n",
    "</inbound>\n",
    "<backend>\n",
    "  <!-- Optional backend-specific modifications -->\n",
    "</backend>\n",
    "<outbound>\n",
    "  <!-- Response shaping / telemetry -->\n",
    "</outbound>\n",
    "<on-error>\n",
    "  <!-- Fallback handling / structured errors -->\n",
    "</on-error>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Starter Inbound Policy Example\n",
    "\n",
    "<details><summary><b>Baseline Hardened Inbound</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <!-- Enforce HTTPS -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.OriginalUrl.Scheme != \\\"https\\\")\">\n",
    "      <return-response>\n",
    "        <set-status code=\"301\" reason=\"Moved Permanently\" />\n",
    "        <set-header name=\"Location\" exists-action=\"override\">\n",
    "          <value>@(context.Request.OriginalUrl.ToString().Replace(\"http://\",\"https://\"))</value>\n",
    "        </set-header>\n",
    "      </return-response>\n",
    "    </when>\n",
    "  </choose>\n",
    "\n",
    "  <!-- Correlation ID (generate if absent) -->\n",
    "  <set-variable name=\"corrId\" value=\"@(context.Request.Headers.GetValueOrDefault(\\\"x-correlation-id\\\", Guid.NewGuid().ToString()))\" />\n",
    "  <set-header name=\"x-correlation-id\" exists-action=\"override\">\n",
    "    <value>@(context.Variables.GetValueOrDefault(\"corrId\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <!-- Basic abuse protection -->\n",
    "  <rate-limit calls=\"100\" renewal-period=\"60\" />\n",
    "  <quota calls=\"1000\" renewal-period=\"3600\" />\n",
    "\n",
    "  <!-- Normalize User-Agent (example) -->\n",
    "  <set-header name=\"x-user-agent\" exists-action=\"override\">\n",
    "    <value>@(context.Request.Headers.GetValueOrDefault(\"User-Agent\",\"unknown\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <!-- Forward to backend -->\n",
    "  <base />\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Outbound Policy Enhancements\n",
    "\n",
    "<details><summary><b>Latency Instrumentation & Cache Guidance</b></summary>\n",
    "\n",
    "```xml\n",
    "<outbound>\n",
    "  <!-- Add processing time header -->\n",
    "  <set-header name=\"x-apim-elapsed-ms\" exists-action=\"override\">\n",
    "    <value>@((DateTime.UtcNow - context.Request.TimestampUtc).TotalMilliseconds.ToString(\"F0\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <!-- Simple cache hint for successful GETs -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Response.StatusCode == 200 && context.Operation?.Method == \\\"GET\\\")\">\n",
    "      <set-header name=\"Cache-Control\" exists-action=\"override\">\n",
    "        <value>public, max-age=60</value>\n",
    "      </set-header>\n",
    "    </when>\n",
    "  </choose>\n",
    "\n",
    "  <!-- Propagate correlation ID -->\n",
    "  <set-header name=\"x-correlation-id\" exists-action=\"override\">\n",
    "    <value>@(context.Request.Headers.GetValueOrDefault(\"x-correlation-id\",\"none\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <base />\n",
    "</outbound>\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Error Handling Pattern\n",
    "\n",
    "<details><summary><b>Structured on-error Block</b></summary>\n",
    "\n",
    "```xml\n",
    "<on-error>\n",
    "  <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "    <value>application/json</value>\n",
    "  </set-header>\n",
    "  <return-response>\n",
    "    <set-status code=\"500\" reason=\"Internal Server Error\" />\n",
    "    <set-body><![CDATA[{\n",
    "      \\\"error\\\": {\n",
    "        \\\"code\\\": \\\"InternalError\\\",\n",
    "        \\\"message\\\": \\\"An unexpected error occurred\\\",\n",
    "        \\\"correlationId\\\": \\\"@(context.Request.Headers.GetValueOrDefault(\\\"x-correlation-id\\\",\\\"none\\\"))\\\"\n",
    "      }\n",
    "    }]]></set-body>\n",
    "  </return-response>\n",
    "</on-error>\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Policy Ordering Tips\n",
    "\n",
    "| Stage | Purpose | Common Policies |\n",
    "|-------|---------|-----------------|\n",
    "| Early (Inbound) | Reject bad requests fast | `validate-content`, HTTPS redirect, auth |\n",
    "| Mid (Inbound) | Enrich & shape | header set, variables, rate/quotas |\n",
    "| Late (Inbound) | Routing | backend selection, rewrite-uri |\n",
    "| Early (Outbound) | Telemetry | timing headers, correlation propagation |\n",
    "| Mid (Outbound) | Optimization | caching hints, compression |\n",
    "| Late (Outbound) | Final shaping | remove/override headers |\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Checklist\n",
    "\n",
    "- ‚úÖ HTTPS enforced\n",
    "- ‚úÖ Correlation ID present\n",
    "- ‚úÖ Basic rate limit + quota applied\n",
    "- ‚úÖ Latency header added\n",
    "- ‚úÖ Consistent error shape on failures\n",
    "- ‚úÖ Cache hint on idempotent success responses\n",
    "\n",
    "---\n",
    "\n",
    "### Transition to Lab 06\n",
    "\n",
    "Next lab layers authentication and authorization (JWT validation, scopes, roles) on top of these foundational policies. Ensure baseline stability before adding access control logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell_54_bd2e7969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.782733Z",
     "iopub.status.busy": "2025-11-17T18:37:09.782429Z",
     "iopub.status.idle": "2025-11-17T18:37:09.785919Z",
     "shell.execute_reply": "2025-11-17T18:37:09.785367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC CELL SKIPPED (optional troubleshooting tool)\n",
      "================================================================================\n",
      "\n",
      "[INFO] This cell is only needed for debugging 500 errors\n",
      "[INFO] Your main labs (Cells 16, 38, 45) are working fine\n",
      "[INFO] Set SKIP_DIAGNOSTIC = False if you need to run diagnostics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL DIAGNOSTIC CELL - Can be skipped\n",
    "# ============================================================================\n",
    "# This cell is for troubleshooting 500 errors.\n",
    "# If everything is working, you can skip this cell.\n",
    "# ============================================================================\n",
    "\n",
    "SKIP_DIAGNOSTIC = True  # Set to False to run diagnostic\n",
    "\n",
    "if SKIP_DIAGNOSTIC:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIAGNOSTIC CELL SKIPPED (optional troubleshooting tool)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n[INFO] This cell is only needed for debugging 500 errors\")\n",
    "    print(\"[INFO] Your main labs (Cells 16, 38, 45) are working fine\")\n",
    "    print(\"[INFO] Set SKIP_DIAGNOSTIC = False if you need to run diagnostics\\n\")\n",
    "else:\n",
    "    # Original diagnostic code would go here\n",
    "    # (keeping the structure in case needed later)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"APIM DIAGNOSTIC - IDENTIFYING 500 ERROR ROOT CAUSE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n[INFO] Diagnostic tool disabled in this version\")\n",
    "    print(\"[INFO] Use Azure Portal or Azure CLI for advanced diagnostics\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_55_a4257ecc",
   "metadata": {},
   "source": [
    "<a id='lab06'></a>\n",
    "## Lab 06: Access Controlling\n",
    "\n",
    "![Access Controlling](../../images/access-controlling.gif)\n",
    "\n",
    "üìñ **Workshop Guide:** https://azure-samples.github.io/AI-Gateway/\n",
    "\n",
    "### Objective\n",
    "Secure AI Gateway endpoints using OAuth 2.0 and Microsoft Entra ID (formerly Azure AD) for enterprise authentication.\n",
    "\n",
    "### What You'll Learn\n",
    "- **OAuth 2.0 Flow:** Implement token-based authentication with Entra ID\n",
    "- **JWT Validation:** Validate JSON Web Tokens in APIM policies\n",
    "- **RBAC Integration:** Control access based on Azure roles and groups\n",
    "- **API Scopes:** Define granular permissions for different API operations\n",
    "- **Token Claims:** Extract user identity and roles from access tokens\n",
    "\n",
    "---\n",
    "### Understanding OAuth 2.0 with Microsoft Entra ID\n",
    "> üí° **Tip:** OAuth 2.0 provides token-based authentication without exposing credentials in each request.\n",
    "\n",
    "**Authentication Flow:**\n",
    "1. **User Login:** Client application redirects user to Entra ID login\n",
    "2. **Authentication:** User enters credentials and consents to permissions\n",
    "3. **Token Issuance:** Entra ID issues JWT access token\n",
    "4. **API Request:** Client includes token in `Authorization: Bearer <token>` header\n",
    "5. **Token Validation:** APIM validates token signature, expiration, and claims\n",
    "6. **Request Processing:** If valid, request forwarded to Azure OpenAI backend\n",
    "\n",
    "---\n",
    "### JWT Validation Policy\n",
    "Azure API Management uses the `validate-jwt` policy to secure endpoints.\n",
    "\n",
    "<details><summary><b>Basic JWT Validation Example</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt\n",
    "      header-name=\"Authorization\"\n",
    "      failed-validation-httpcode=\"401\"\n",
    "      failed-validation-error-message=\"Unauthorized. Access token is missing or invalid.\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <audiences>\n",
    "      <audience>api://your-api-client-id</audience>\n",
    "    </audiences>\n",
    "    <issuers>\n",
    "      <issuer>https://sts.windows.net/{tenant-id}/</issuer>\n",
    "    </issuers>\n",
    "    <required-claims>\n",
    "      <claim name=\"roles\" match=\"any\">\n",
    "        <value>AI.User</value>\n",
    "        <value>AI.Admin</value>\n",
    "      </claim>\n",
    "    </required-claims>\n",
    "  </validate-jwt>\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- `header-name`: HTTP header containing the JWT (typically `Authorization`)\n",
    "- `openid-config`: URL to Entra ID's OpenID Connect metadata\n",
    "- `audiences`: Valid `aud` claim values\n",
    "- `issuers`: Trusted token issuers\n",
    "- `required-claims`: Claims that must be present in the token\n",
    "</details>\n",
    "\n",
    "---\n",
    "### Microsoft Entra ID Integration\n",
    "> ‚ö†Ô∏è **Note:** You must register your application in Microsoft Entra ID before implementing OAuth 2.0.\n",
    "\n",
    "**Setup Steps:**\n",
    "1. **Register Application:** Azure Portal ‚Üí Entra ID ‚Üí App Registrations ‚Üí New registration (note Application (client) ID & Tenant ID)\n",
    "2. **Configure API Permissions:** Add permissions and define custom scopes (e.g., `AI.Read`, `AI.Write`); grant admin consent if required\n",
    "3. **Create App Roles:** Define roles in app manifest (e.g., `AI.User`, `AI.Admin`) and assign users/groups\n",
    "4. **Configure APIM:** Add `validate-jwt` policy, reference tenant & client IDs, map roles to operations\n",
    "\n",
    "---\n",
    "### Role-Based Access Control (RBAC)\n",
    "<details><summary><b>Policy Example: Role-Based Backend Routing</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt header-name=\"Authorization\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <audiences>\n",
    "      <audience>api://your-api-client-id</audience>\n",
    "    </audiences>\n",
    "  </validate-jwt>\n",
    "\n",
    "  <!-- Admin users get priority routing -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.Headers.GetValueOrDefault(\\\"Authorization\\\",\\\"\").AsJwt()?.Claims.GetValueOrDefault(\\\"roles\\\",\\\"\").Contains(\\\"AI.Admin\\\") == true)\">\n",
    "      <set-backend-service backend-id=\"openai-premium-backend\" />\n",
    "    </when>\n",
    "    <!-- Regular users get standard backend -->\n",
    "    <otherwise>\n",
    "      <set-backend-service backend-id=\"openai-standard-backend\" />\n",
    "    </otherwise>\n",
    "  </choose>\n",
    "</inbound>\n",
    "```\n",
    "This example routes admin users to a premium backend with higher quotas.\n",
    "</details>\n",
    "\n",
    "<details><summary><b>Policy Example: Scope-Based Operation Control</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt header-name=\"Authorization\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <required-claims>\n",
    "      <claim name=\"scp\" match=\"any\">\n",
    "        <value>AI.Read</value>\n",
    "        <value>AI.Write</value>\n",
    "      </claim>\n",
    "    </required-claims>\n",
    "  </validate-jwt>\n",
    "\n",
    "  <!-- Check if operation requires write permission -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.Method != \\\"GET\\\")\">\n",
    "      <validate-jwt header-name=\"Authorization\">\n",
    "        <required-claims>\n",
    "          <claim name=\"scp\" match=\"any\">\n",
    "            <value>AI.Write</value>\n",
    "          </claim>\n",
    "        </required-claims>\n",
    "      </validate-jwt>\n",
    "    </when>\n",
    "  </choose>\n",
    "</inbound>\n",
    "```\n",
    "This ensures only tokens with `AI.Write` scope can perform non-GET operations.\n",
    "</details>\n",
    "\n",
    "---\n",
    "### Token Claims and User Identity\n",
    "JWT tokens contain claims that provide user context.\n",
    "\n",
    "**Common Claims:**\n",
    "- `sub`: Subject (unique user identifier)\n",
    "- `name`: User's display name\n",
    "- `email`: User's email address\n",
    "- `roles`: User's assigned roles\n",
    "- `scp`: Delegated permissions (scopes)\n",
    "- `aud`: Audience (intended recipient)\n",
    "- `iss`: Issuer (token authority)\n",
    "- `exp`: Expiration timestamp\n",
    "\n",
    "**Extracting Claims in Policy:**\n",
    "```xml\n",
    "<set-header name=\"X-User-Email\" exists-action=\"override\">\n",
    "  <value>@(context.Request.Headers.GetValueOrDefault(\"Authorization\",\"\" ).AsJwt()?.Claims.GetValueOrDefault(\"email\", \"unknown\"))</value>\n",
    "</set-header>\n",
    "```\n",
    "\n",
    "---\n",
    "### Testing Access Control\n",
    "**Test Scenarios:**\n",
    "1. No Token ‚Üí 401 Unauthorized\n",
    "2. Invalid Token ‚Üí 401 Unauthorized\n",
    "3. Valid Token ‚Üí 200 OK\n",
    "4. Insufficient Permissions ‚Üí 403 Forbidden\n",
    "5. Token Expired ‚Üí 401 Unauthorized\n",
    "\n",
    "**Python Example with Azure Identity:**\n",
    "```python\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "\n",
    "# Acquire token from Azure Identity\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"api://your-api-client-id/.default\")\n",
    "\n",
    "# Use token with Azure OpenAI via APIM\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://your-apim.azure-api.net\",\n",
    "    api_key=token.token,  # JWT token used as API key\n",
    "    api_version=\"2024-02-15-preview\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "---\n",
    "### Security Best Practices\n",
    "> ‚úÖ **Checklist:**\n",
    "- Validate JWT signature using OpenID configuration\n",
    "- Check token expiration (`exp`)\n",
    "- Verify audience (`aud`) matches your API\n",
    "- Validate issuer (`iss`) is trusted\n",
    "- Enforce HTTPS only\n",
    "- Handle errors without leaking sensitive info\n",
    "- Log authentication failures\n",
    "- Rotate client secrets regularly\n",
    "- Apply least-privilege role assignments\n",
    "\n",
    "---\n",
    "### Expected Outcome\n",
    "**Success Criteria:**\n",
    "- Unauthenticated requests return 401\n",
    "- Valid Entra ID tokens grant access\n",
    "- JWT validation enforces signature & claims\n",
    "- Roles restrict privileged operations\n",
    "- Scope checks block unauthorized writes\n",
    "- Expired tokens rejected cleanly\n",
    "- Clear error messages guide remediation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_57_a90b96df",
   "metadata": {},
   "source": [
    "# Access Control Workshop\n",
    "\n",
    "The following cells demonstrate token acquisition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell_59_8f6ce564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:09.787759Z",
     "iopub.status.busy": "2025-11-17T18:37:09.787545Z",
     "iopub.status.idle": "2025-11-17T18:37:15.489250Z",
     "shell.execute_reply": "2025-11-17T18:37:15.488054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìù APPLY: JWT Only Policy (disable subscriptionRequired)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Current subscriptionRequired: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] ‚úì Disabled subscriptionRequired for 'inference-api'\n",
      "\n",
      "[3] Applying JWT policy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Policy Status: 200 - ‚úì SUCCESS\n",
      "\n",
      "‚úì JWT policy applied with multi-issuer support\n",
      "‚è≥ Waiting 3 seconds for propagation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ready for testing\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"üìù APPLY: JWT Only Policy (disable subscriptionRequired)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# STEP 1: Disable subscription requirement for pure JWT auth\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        api_config = response.json()\n",
    "        current_subscription_required = api_config.get('properties', {}).get('subscriptionRequired', False)\n",
    "        \n",
    "        print(f\"[1] Current subscriptionRequired: {current_subscription_required}\")\n",
    "        \n",
    "        if current_subscription_required:\n",
    "            # Disable subscription requirement\n",
    "            api_config['properties']['subscriptionRequired'] = False\n",
    "            \n",
    "            update_response = requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "            \n",
    "            if update_response.status_code in [200, 201]:\n",
    "                print(f\"[2] ‚úì Disabled subscriptionRequired for '{api_id}'\")\n",
    "            else:\n",
    "                print(f\"[2] ‚úó Failed: {update_response.status_code}\")\n",
    "        else:\n",
    "            print(f\"[2] ‚úì subscriptionRequired already disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {str(e)}\")\n",
    "\n",
    "# STEP 2: Apply JWT policy with v1.0 + v2.0 issuer support\n",
    "print(f\"\\n[3] Applying JWT policy...\")\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID\")\n",
    "else:\n",
    "    # JWT policy - CRITICAL: correct element order (openid-config, audiences, issuers)\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "    \n",
    "    try:\n",
    "        policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "        \n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "        \n",
    "        print(f\"[4] Policy Status: {response.status_code} - {'‚úì SUCCESS' if response.status_code in [200, 201] else '‚úó FAILED'}\")\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"\\n‚úì JWT policy applied with multi-issuer support\")\n",
    "            print(f\"‚è≥ Waiting 3 seconds for propagation...\")\n",
    "            time.sleep(3)\n",
    "            print(f\"‚úì Ready for testing\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6708c8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:37:15.492441Z",
     "iopub.status.busy": "2025-11-17T18:37:15.492147Z",
     "iopub.status.idle": "2025-11-17T18:38:19.152035Z",
     "shell.execute_reply": "2025-11-17T18:38:19.151412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Policy Applied: JWT Only\n",
      "Status: 200 - SUCCESS\n",
      "Note: Disabled subscriptionRequired for pure JWT authentication\n",
      "Waiting 60 seconds for policy to propagate...\n",
      "   60 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   59 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   58 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   57 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   56 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   55 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   54 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   53 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   52 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   51 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   49 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   48 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   42 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   37 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   36 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 seconds remaining...\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy propagation complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get management token and tenant ID\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run([az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "                       capture_output=True, text=True, timeout=10)\n",
    "tenant_id = result.stdout.strip()\n",
    "\n",
    "# Disable subscription requirement (allows pure JWT auth)\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "headers = {\"Authorization\": f\"Bearer {mgmt_token.token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(api_url, headers=headers, timeout=30)\n",
    "if response.status_code == 200:\n",
    "    api_config = response.json()\n",
    "    api_config['properties']['subscriptionRequired'] = False\n",
    "    requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "\n",
    "# Apply JWT-only policy (supports both v1.0 and v2.0 tokens)\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "            <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "            <audiences><audience>https://cognitiveservices.azure.com</audience></audiences>\n",
    "            <issuers>\n",
    "                <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "            </issuers>\n",
    "        </validate-jwt>\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend><base /></backend>\n",
    "    <outbound><base /></outbound>\n",
    "    <on-error><base /></on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "body = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "print(f\"üìù Policy Applied: JWT Only\")\n",
    "print(f\"Status: {response.status_code} - {'SUCCESS' if response.status_code in [200, 201] else 'FAILED'}\")\n",
    "print(f\"Note: Disabled subscriptionRequired for pure JWT authentication\")\n",
    "\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"Waiting 60 seconds for policy to propagate...\")\n",
    "    for i in range(60, 0, -1):\n",
    "        print(f\"   {i} seconds remaining...\", end='\\r')\n",
    "        time.sleep(1)\n",
    "    print(\"\\nPolicy propagation complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc65e15a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:38:19.154039Z",
     "iopub.status.busy": "2025-11-17T18:38:19.153797Z",
     "iopub.status.idle": "2025-11-17T18:39:21.819230Z",
     "shell.execute_reply": "2025-11-17T18:39:21.818420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìù APPLY: Dual Auth (JWT + API Key)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auth] Resolved tenant_id: 2b9d9f47-1fb6-400a-a438-39fe7d768649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Policy Applied: Dual Auth (JWT + API Key)\n",
      "Status: 200 - ‚úì SUCCESS\n",
      "Policy requires BOTH:\n",
      "  ‚Ä¢ Valid JWT token (Authorization header)\n",
      "  ‚Ä¢ Valid API key (api-key header)\n",
      "‚è≥ Waiting 60 seconds for policy to propagate...\n",
      "   60 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   59 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   58 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   57 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   56 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   55 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   54 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   53 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   52 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   51 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   49 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   48 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   42 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   37 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   36 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Policy propagation complete!\n",
      "üí° TIP: Run Cell 65 to test Dual Auth\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"üìù APPLY: Dual Auth (JWT + API Key)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID. Ensure az login completed.\")\n",
    "else:\n",
    "    print(f\"[auth] Resolved tenant_id: {tenant_id}\")\n",
    "\n",
    "    # Dual Auth policy - BOTH JWT validation AND API key check\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing API key\" />\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "\n",
    "    # Apply policy\n",
    "    try:\n",
    "        url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "        print(f\"üìù Policy Applied: Dual Auth (JWT + API Key)\")\n",
    "        print(f\"Status: {response.status_code} - {'‚úì SUCCESS' if response.status_code in [200, 201] else '‚úó FAILED'}\")\n",
    "\n",
    "        if response.status_code not in [200, 201]:\n",
    "            print(f\"Error: {response.text[:500]}\")\n",
    "        else:\n",
    "            print(\"Policy requires BOTH:\")\n",
    "            print(\"  ‚Ä¢ Valid JWT token (Authorization header)\")\n",
    "            print(\"  ‚Ä¢ Valid API key (api-key header)\")\n",
    "\n",
    "            print(\"‚è≥ Waiting 60 seconds for policy to propagate...\")\n",
    "            for i in range(60, 0, -1):\n",
    "                print(f\"   {i} seconds remaining...\", end='')\n",
    "                time.sleep(1)\n",
    "            print(\"‚úì Policy propagation complete!\")\n",
    "            print(\"üí° TIP: Run Cell 65 to test Dual Auth\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Policy application failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell_65_9ed0a378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:21.821195Z",
     "iopub.status.busy": "2025-11-17T18:39:21.821033Z",
     "iopub.status.idle": "2025-11-17T18:39:54.876810Z",
     "shell.execute_reply": "2025-11-17T18:39:54.876012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîÑ RESET: API-KEY Authentication (for remaining labs)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ‚úì Re-enabled subscriptionRequired for API-KEY authentication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Policy Reset: API-KEY Only\n",
      "    Status: 200 - ‚úì SUCCESS\n",
      "‚è≥ Waiting 30 seconds for policy to propagate...\n",
      "   30 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 seconds remaining..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Policy reset complete!\n",
      "üí° All remaining labs will use API-KEY authentication\n"
     ]
    }
   ],
   "source": [
    "import requests, os, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"üîÑ RESET: API-KEY Authentication (for remaining labs)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Re-enable subscription requirement (for API-KEY authentication)\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "response = requests.get(api_url, headers=headers, timeout=30)\n",
    "if response.status_code == 200:\n",
    "    api_config = response.json()\n",
    "    api_config['properties']['subscriptionRequired'] = True\n",
    "    requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "    print(\"[1] ‚úì Re-enabled subscriptionRequired for API-KEY authentication\")\n",
    "\n",
    "# Apply simple API-KEY only policy\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend><base /></backend>\n",
    "    <outbound><base /></outbound>\n",
    "    <on-error><base /></on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "body = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "print(f\"[2] Policy Reset: API-KEY Only\")\n",
    "print(f\"    Status: {response.status_code} - {'‚úì SUCCESS' if response.status_code in [200, 201] else '‚úó FAILED'}\")\n",
    "\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"‚è≥ Waiting 30 seconds for policy to propagate...\")\n",
    "    for i in range(30, 0, -1):\n",
    "        print(f\"   {i} seconds remaining...\", end='')\n",
    "        time.sleep(1)\n",
    "    print(\"‚úì Policy reset complete!\")\n",
    "    print(\"üí° All remaining labs will use API-KEY authentication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c43932",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| 401 Unauthorized | Wait 30-60 seconds for policy to propagate |\n",
    "| 500 Internal Server Error | Check backend health with Azure CLI |\n",
    "| Token not found | Run `az login` to authenticate |\n",
    "| Missing API Key | Verify `APIM_API_KEY` in environment variables |\n",
    "\n",
    "**Verify Resources:**\n",
    "\n",
    "```bash\n",
    "az apim api list --service-name $APIM_SERVICE_NAME --resource-group $RESOURCE_GROUP --output table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_68_6a12cc5e",
   "metadata": {},
   "source": [
    "<a id='lab07'></a>\n",
    "\n",
    "## Lab 07: Content Safety\n",
    "\n",
    "![flow](../../images/content-safety.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Azure AI Content Safety to automatically detect and block harmful, offensive, or inappropriate content in AI prompts and responses.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Content Safety Policy:** Apply the llm-content-safety policy to AI endpoints\n",
    "- **Harmful Content Detection:** Identify violence, hate speech, sexual content, and self-harm\n",
    "- **Severity Thresholds:** Configure sensitivity levels (low, medium, high)\n",
    "- **Automated Blocking:** Return HTTP 403 when harmful content detected\n",
    "- **Prompt Filtering:** Scan prompts before sending to backend LLM\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- Harmful prompts blocked with HTTP 403 Forbidden\n",
    "- Safe prompts processed normally\n",
    "- Content Safety policy correctly integrated with APIM\n",
    "- Severity thresholds can be adjusted\n",
    "- Detailed error messages explain why content was blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell_69_39cd0d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:54.878915Z",
     "iopub.status.busy": "2025-11-17T18:39:54.878751Z",
     "iopub.status.idle": "2025-11-17T18:39:55.712165Z",
     "shell.execute_reply": "2025-11-17T18:39:55.711518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Safe content request failed: No module named 'openai'\n",
      "Content blocked: No module named 'openai'\n",
      "[OK] Lab 07 Complete!\n"
     ]
    }
   ],
   "source": [
    "# Lab 07 Content Safety Test (adds JWT auth if required by current APIM policy)\n",
    "\n",
    "def _get_jwt_token():\n",
    "    # Reuse existing jwt_token if earlier cell created it\n",
    "    if 'jwt_token' in globals() and jwt_token:\n",
    "        return jwt_token\n",
    "    try:\n",
    "        cred = DefaultAzureCredential()\n",
    "        tok = cred.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return tok.token\n",
    "    except Exception as _e:\n",
    "        print(f'[auth] WARN: Unable to acquire JWT token ({_e}); proceeding without it.')\n",
    "        return None\n",
    "\n",
    "_jwt = _get_jwt_token()\n",
    "extra_headers = {}\n",
    "if _jwt:\n",
    "    extra_headers['Authorization'] = f'Bearer {_jwt}'\n",
    "\n",
    "def safe_call(messages):\n",
    "    \"\"\"Call helper that retries with JWT if first attempt 401 (in case policy just changed).\"\"\"\n",
    "    from openai import AuthenticationError\n",
    "    try:\n",
    "        return client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=messages,\n",
    "            max_tokens=20,\n",
    "            extra_headers=extra_headers if extra_headers else None\n",
    "        )\n",
    "    except AuthenticationError as e:\n",
    "        msg = str(e)\n",
    "        if '401' in msg and not extra_headers:\n",
    "            # Acquire token and retry once\n",
    "            print('[auth] 401 without JWT; attempting to fetch token and retry...')\n",
    "            new_jwt = _get_jwt_token()\n",
    "            if new_jwt:\n",
    "                return client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages,\n",
    "                    max_tokens=20,\n",
    "                    extra_headers={'Authorization': f'Bearer {new_jwt}'}\n",
    "                )\n",
    "        raise\n",
    "\n",
    "# Test with safe content\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'What is the weather like?'}])\n",
    "    print(f\"Safe content: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Safe content request failed: {e}')\n",
    "\n",
    "# Test with potentially harmful content (expect policy to possibly block)\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'How to harm someone?'}])\n",
    "    print('Content passed (policy did not block or returned sanitized answer)')\n",
    "except Exception as e:\n",
    "    # Could be a 403 from content safety or auth issue\n",
    "    print(f'Content blocked: {e}')\n",
    "\n",
    "# Fallback if utils not available\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Lab 07 Complete!')\n",
    "else:\n",
    "    print('[OK] Lab 07 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_70_caa353d3",
   "metadata": {},
   "source": [
    "<a id='lab08'></a>\n",
    "\n",
    "## Lab 08: Model Routing\n",
    "\n",
    "![flow](../../images/ai-gateway.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Implement intelligent request routing to automatically select the best AI model based on criteria like prompt complexity, cost, or performance requirements.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Conditional Routing:** Route to different models based on request properties\n",
    "- **Model Selection Logic:** Choose between GPT-4o, GPT-4o-mini, DeepSeek, etc.\n",
    "- **Cost Optimization:** Route simple queries to cheaper models automatically\n",
    "- **Performance Tuning:** Send complex queries to more capable models\n",
    "- **Header-Based Routing:** Allow clients to specify model preferences\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- Simple prompts routed to GPT-4o-mini (cost-effective)\n",
    "- Complex prompts routed to GPT-4o (high capability)\n",
    "- Custom headers can override default routing\n",
    "- Routing logic is transparent and logged\n",
    "- Cost savings measurable compared to always using premium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell_71_0250903e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:55.714066Z",
     "iopub.status.busy": "2025-11-17T18:39:55.713922Z",
     "iopub.status.idle": "2025-11-17T18:39:55.796528Z",
     "shell.execute_reply": "2025-11-17T18:39:55.795715Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Lab 08: Model Routing test (fixed for Dual Auth + invalid model + 401 handling)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AuthenticationError\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Ensure DefaultAzureCredential is available even if this cell runs before its import elsewhere.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "# Lab 08: Model Routing test (fixed for Dual Auth + invalid model + 401 handling)\n",
    "\n",
    "import os\n",
    "from openai import AuthenticationError\n",
    "\n",
    "# Ensure DefaultAzureCredential is available even if this cell runs before its import elsewhere.\n",
    "try:\n",
    "    DefaultAzureCredential  # type: ignore\n",
    "except NameError:\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Acquire JWT (audience: https://cognitiveservices.azure.com) ‚Äì may be required with APIM dual auth.\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "except Exception as e:\n",
    "    jwt_token = None\n",
    "    print(f\"[auth] WARN: Unable to acquire JWT token: {e}\")\n",
    "\n",
    "extra_headers = {}\n",
    "if jwt_token:\n",
    "    extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "# Only test models that are actually deployed. gpt-4.1-mini not deployed; skip automatically.\n",
    "requested_models = ['gpt-4o-mini', 'gpt-4.1-nano']  # FIXED: Changed to gpt-4.1-nano (deployed in cell 28)\n",
    "available_models = {'gpt-4o-mini', 'gpt-4o', 'gpt-4.1-nano', 'text-embedding-3-small', 'text-embedding-3-large', 'dall-e-3'}  # from Step 2 config\n",
    "models_to_test = [m for m in requested_models if m in available_models]\n",
    "\n",
    "if len(models_to_test) != len(requested_models):\n",
    "    missing = [m for m in requested_models if m not in models_to_test]\n",
    "    print(f\"[routing] Skipping unavailable models: {', '.join(missing)}\")\n",
    "\n",
    "# Guard if OpenAI client is not yet defined (e.g., cell ordering)\n",
    "if 'client' not in globals():\n",
    "    print(\"[WARN] OpenAI client 'client' not found; skipping model tests.\")\n",
    "    models_to_test = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"[*] Testing model: {model}\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "            max_tokens=10,\n",
    "            extra_headers=extra_headers if extra_headers else None\n",
    "        )\n",
    "        # Robust content extraction\n",
    "        content = \"\"\n",
    "        try:\n",
    "            content = response.choices[0].message.content\n",
    "        except AttributeError:\n",
    "            if hasattr(response.choices[0].message, 'get'):\n",
    "                content = response.choices[0].message.get('content', '')\n",
    "        print(f\"Model {model}: {content}\")\n",
    "    except AuthenticationError as e:\n",
    "        # Attempt one silent JWT refresh if first attempt lacked/invalid token\n",
    "        if not jwt_token:\n",
    "            print(f\"[auth] 401 without JWT; attempting token fetch & retry...\")\n",
    "            try:\n",
    "                credential = DefaultAzureCredential()\n",
    "                jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "                extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "                retry_resp = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "                    max_tokens=10,\n",
    "                    extra_headers=extra_headers\n",
    "                )\n",
    "                retry_content = \"\"\n",
    "                try:\n",
    "                    retry_content = retry_resp.choices[0].message.content\n",
    "                except AttributeError:\n",
    "                    if hasattr(retry_resp.choices[0].message, 'get'):\n",
    "                        retry_content = retry_resp.choices[0].message.get('content', '')\n",
    "                print(f\"Model {model} (retry): {retry_content}\")\n",
    "                continue\n",
    "            except Exception as e2:\n",
    "                print(f\"[ERROR] Retry after acquiring JWT failed: {e2}\")\n",
    "        print(f\"[ERROR] Auth failed for {model}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Request failed for {model}: {e}\")\n",
    "\n",
    "# Safe completion notification without NameError if utils is absent\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Lab 08 Complete!')\n",
    "else:\n",
    "    print('[OK] Lab 08 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_72_62fb5c72",
   "metadata": {},
   "source": [
    "<a id='lab09'></a>\n",
    "\n",
    "## Lab 09: AI Foundry SDK\n",
    "\n",
    "![flow](../../images/ai-foundry-sdk.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Azure AI Foundry SDK for advanced AI capabilities including model catalog, evaluations, and agent frameworks.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **AI Foundry Integration:** Connect to AI Foundry projects through APIM\n",
    "- **Model Catalog:** Access diverse AI models beyond Azure OpenAI\n",
    "- **Inference API:** Use unified inference API for multiple model types\n",
    "- **Agent Framework:** Build AI agents with tools and orchestration\n",
    "- **Evaluation Tools:** Assess model performance and quality\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- AI Foundry SDK successfully connects through APIM gateway\n",
    "- Can list available models in the catalog\n",
    "- Inference requests work for different model types\n",
    "- Agent framework tools execute correctly\n",
    "- Evaluation metrics collected and analyzed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_73_cc780c0e",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "‚ñ∂Ô∏è Click `Run All` to execute all steps sequentially, or execute them `Step by Step`...\n",
    "\n",
    "ChatCompletionsClient must use FULL deployment path:\n",
    "  {apim_gateway_url}/{inference_api_path}/openai/deployments/{deployment_name}\n",
    "\n",
    "Reuse imports already loaded in earlier cells (avoid re-import)\n",
    "Variables expected from earlier cells:\n",
    "  apim_gateway_url, inference_api_path, apim_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cell_75_ce629d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:55.798611Z",
     "iopub.status.busy": "2025-11-17T18:39:55.798455Z",
     "iopub.status.idle": "2025-11-17T18:39:57.251917Z",
     "shell.execute_reply": "2025-11-17T18:39:57.251396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Inference Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Acquired JWT token\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[WARN] Unable to acquire JWT token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[INFO] Will attempt call with API key only (may fail if JWT required)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionsClient\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcredentials\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureKeyCredential\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SystemMessage, UserMessage\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure.ai'"
     ]
    }
   ],
   "source": [
    "deployment_name = \"gpt-4o-mini\"\n",
    "\n",
    "missing_vars = [k for k, v in {\n",
    "    'apim_gateway_url': globals().get('apim_gateway_url'),\n",
    "    'inference_api_path': globals().get('inference_api_path'),\n",
    "    'apim_api_key': globals().get('apim_api_key')\n",
    "}.items() if not v]\n",
    "\n",
    "if missing_vars:\n",
    "    raise RuntimeError(f\"Missing required variables: {', '.join(missing_vars)}. Run the earlier env/config cells first.\")\n",
    "\n",
    "# Normalize endpoint (avoid double slashes)\n",
    "base = apim_gateway_url.rstrip('/')\n",
    "inference_path = inference_api_path.strip('/')\n",
    "\n",
    "inference_endpoint = f\"{base}/{inference_path}/openai/deployments/{deployment_name}\"\n",
    "print(f\"[OK] Inference Endpoint: {inference_endpoint}\")\n",
    "\n",
    "# Acquire JWT if current APIM policy enforces validate-jwt (dual auth or JWT-only)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "jwt_token = None\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Audience used in active APIM policies\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(\"[OK] Acquired JWT token\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Unable to acquire JWT token: {e}\")\n",
    "    print(\"[INFO] Will attempt call with API key only (may fail if JWT required)\")\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "inference_client = ChatCompletionsClient(\n",
    "    endpoint=inference_endpoint,\n",
    "    credential=AzureKeyCredential(apim_api_key)  # use correct API key variable\n",
    ")\n",
    "\n",
    "print(\"[OK] ChatCompletionsClient created successfully\\n\")\n",
    "\n",
    "# Prepare headers: dual auth requires both api-key (handled via AzureKeyCredential) and Authorization\n",
    "call_headers = {}\n",
    "if jwt_token:\n",
    "    call_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "print(\"[*] Testing chat completion with Azure AI Inference SDK...\")\n",
    "try:\n",
    "    response = inference_client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=\"You are helpful.\"),\n",
    "            UserMessage(content=\"What is Azure AI Foundry?\")\n",
    "        ],\n",
    "        headers=call_headers if call_headers else None  # azure-core style header injection\n",
    "    )\n",
    "    print(f\"[SUCCESS] Response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if \"Invalid JWT\" in msg or \"401\" in msg:\n",
    "        print(f\"[ERROR] Authentication failed: {msg}\")\n",
    "        print(\"[HINT] Active APIM policy likely requires a valid JWT. Ensure az login completed or managed identity available.\")\n",
    "        print(\"[HINT] Retry after confirming validate-jwt audiences match https://cognitiveservices.azure.com\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Request failed: {msg}\")\n",
    "        print(\"[HINT] Verify deployment name matches APIM backend path and policy didn't strip /openai segment.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Lab 09 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_77_7dd6b64b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section: MCP Fundamentals\n",
    "\n",
    "Learn MCP basics:\n",
    "- Client initialization\n",
    "- Calling MCP tools\n",
    "- Data retrieval\n",
    "\n",
    "## MCP Server Integration\n",
    "\"\"\"\n",
    "MCP servers are initialized in Cell 11 using MCPClient.\n",
    "\n",
    "The global 'mcp' object provides access to all configured data sources:\n",
    "  - mcp.excel    (Excel Analytics MCP - direct)\n",
    "  - mcp.docs     (Research Documents MCP - direct)\n",
    "  - mcp.github   (GitHub API via APIM)\n",
    "  - mcp.weather  (Weather API via APIM)\n",
    "\n",
    "All configuration is loaded from .mcp-servers-config file.\n",
    "No additional initialization needed in this cell.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MCP SERVER INTEGRATION - LAB 10\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"‚úì MCP Client initialized in Cell 11\")\n",
    "print()\n",
    "print(\"Available Data Sources:\")\n",
    "\n",
    "if 'mcp' in globals():\n",
    "    if hasattr(mcp, 'excel') and mcp.excel:\n",
    "        print(\"  ‚úì Excel MCP (direct)\")\n",
    "    if hasattr(mcp, 'docs') and mcp.docs:\n",
    "        print(\"  ‚úì Docs MCP (direct)\")\n",
    "    if hasattr(mcp, 'github') and mcp.github:\n",
    "        print(\"  ‚úì GitHub API (APIM)\")\n",
    "    if hasattr(mcp, 'weather') and mcp.weather:\n",
    "        print(\"  ‚úì Weather API (APIM)\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üí° Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  MCP not initialized. Please run Cell 11 first.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_78_2e777ad7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "MCP servers are initialized in Cell 11 using MCPClient.\n",
    "\n",
    "The global 'mcp' object provides access to all configured data sources:\n",
    "  - mcp.excel    (Excel Analytics MCP - direct)\n",
    "  - mcp.docs     (Research Documents MCP - direct)\n",
    "  - mcp.github   (GitHub API via APIM)\n",
    "  - mcp.weather  (Weather API via APIM)\n",
    "\n",
    "All configuration is loaded from .mcp-servers-config file.\n",
    "No additional initialization needed in this cell.\n",
    "\"\"\"\n",
    "---\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. AI application sends MCP request to APIM\n",
    "2. APIM validates OAuth token and enforces policies\n",
    "3. Request forwarded to MCP server\n",
    "4. MCP server executes tool and returns result\n",
    "5. APIM proxies response back to client\n",
    "6. AI model processes tool result and generates response\n",
    "\n",
    "---\n",
    "\n",
    "### Two MCP Connection Patterns\n",
    "\n",
    "**Important:** This lab uses HTTP-based MCP servers that communicate via POST requests to `/mcp/` endpoints.\n",
    "\n",
    "<details>\n",
    "<summary><b>Pattern 1: HTTP-Based MCP</b> (‚úÖ Used in this notebook)</summary>\n",
    "\n",
    "**How It Works:**\n",
    "- **Protocol:** HTTP POST requests\n",
    "- **Endpoint:** `{server_url}/mcp/`\n",
    "- **Format:** JSON-RPC 2.0\n",
    "- **Communication:** Request/response pattern\n",
    "\n",
    "**Advantages:**\n",
    "- Simple, reliable, works with standard HTTP clients\n",
    "- Easy to test with curl or Postman\n",
    "- Works through standard load balancers and API gateways\n",
    "- No special client libraries required\n",
    "- Firewall-friendly (standard HTTP/HTTPS)\n",
    "\n",
    "**Example Request:**\n",
    "```http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cell_80_5c80f06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:57.253997Z",
     "iopub.status.busy": "2025-11-17T18:39:57.253855Z",
     "iopub.status.idle": "2025-11-17T18:39:58.759884Z",
     "shell.execute_reply": "2025-11-17T18:39:58.759177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEATHER API EXAMPLE (via APIM)\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  CURRENT WEATHER - London\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Location: London, GB\n",
      "üå°Ô∏è  Temperature: 4.77¬∞C (feels like 3.04¬∞C)\n",
      "‚òÅÔ∏è  Conditions: Clear Sky\n",
      "üí® Wind: 2.06 m/s\n",
      "üíß Humidity: 80%\n",
      "üîΩ Pressure: 1025 hPa\n",
      "\n",
      "\n",
      "2Ô∏è‚É£  MULTI-CITY COMPARISON\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "City            Temp (¬∞C)    Conditions           Humidity  \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris           5.5          Clear Sky            81%\n",
      "New York        6.7          Scattered Clouds     42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo           13.8         Broken Clouds        53%\n",
      "Sydney          12.7         Clear Sky            64%\n",
      "\n",
      "\n",
      "3Ô∏è‚É£  5-DAY FORECAST - London\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ 2025-11-17\n",
      "   21:00: 4.8¬∞C - Clear Sky\n",
      "\n",
      "üìÖ 2025-11-18\n",
      "   00:00: 3.2¬∞C - Few Clouds\n",
      "   03:00: 1.5¬∞C - Clear Sky\n",
      "   06:00: 0.8¬∞C - Clear Sky\n",
      "   09:00: 3.2¬∞C - Scattered Clouds\n",
      "   12:00: 6.0¬∞C - Broken Clouds\n",
      "   15:00: 7.0¬∞C - Overcast Clouds\n",
      "   18:00: 6.3¬∞C - Broken Clouds\n",
      "\n",
      "\n",
      "‚úÖ Weather API examples completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab Example: Weather API (via APIM)\n",
    "\"\"\"\n",
    "Demonstrates Weather API access through Azure API Management.\n",
    "\n",
    "Features:\n",
    "- Current weather for a city\n",
    "- Multi-city comparison\n",
    "- 5-day forecast\n",
    "- Temperature, conditions, humidity\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WEATHER API EXAMPLE (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.weather:\n",
    "    print(\"‚ùå Weather API not configured\")\n",
    "    print(\"   Set APIM_WEATHER_URL and OPENWEATHER_API_KEY in .mcp-servers-config\")\n",
    "else:\n",
    "    print(\"\\n1Ô∏è‚É£  CURRENT WEATHER - London\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get weather for London\n",
    "        weather = mcp.weather.get_weather(\"London\", \"GB\")\n",
    "        \n",
    "        print(f\"\\nüìç Location: {weather['name']}, {weather['sys']['country']}\")\n",
    "        print(f\"üå°Ô∏è  Temperature: {weather['main']['temp']}¬∞C (feels like {weather['main']['feels_like']}¬∞C)\")\n",
    "        print(f\"‚òÅÔ∏è  Conditions: {weather['weather'][0]['description'].title()}\")\n",
    "        print(f\"üí® Wind: {weather['wind']['speed']} m/s\")\n",
    "        print(f\"üíß Humidity: {weather['main']['humidity']}%\")\n",
    "        print(f\"üîΩ Pressure: {weather['main']['pressure']} hPa\")\n",
    "        \n",
    "        print(\"\\n\\n2Ô∏è‚É£  MULTI-CITY COMPARISON\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        cities = [\n",
    "            (\"Paris\", \"FR\"),\n",
    "            (\"New York\", \"US\"),\n",
    "            (\"Tokyo\", \"JP\"),\n",
    "            (\"Sydney\", \"AU\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'City':<15} {'Temp (¬∞C)':<12} {'Conditions':<20} {'Humidity':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for city, country in cities:\n",
    "            try:\n",
    "                w = mcp.weather.get_weather(city, country)\n",
    "                temp = w['main']['temp']\n",
    "                condition = w['weather'][0]['description'].title()\n",
    "                humidity = w['main']['humidity']\n",
    "                print(f\"{city:<15} {temp:<12.1f} {condition:<20} {humidity}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"{city:<15} Error: {str(e)[:40]}\")\n",
    "        \n",
    "        print(\"\\n\\n3Ô∏è‚É£  5-DAY FORECAST - London\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            forecast = mcp.weather.get_forecast(\"London\", \"GB\")\n",
    "            \n",
    "            # Group by day\n",
    "            from datetime import datetime\n",
    "            daily_forecasts = {}\n",
    "            \n",
    "            for item in forecast['list'][:8]:  # Next 24 hours (8 x 3-hour periods)\n",
    "                dt = datetime.fromtimestamp(item['dt'])\n",
    "                day = dt.strftime('%Y-%m-%d')\n",
    "                time = dt.strftime('%H:%M')\n",
    "                \n",
    "                if day not in daily_forecasts:\n",
    "                    daily_forecasts[day] = []\n",
    "                \n",
    "                daily_forecasts[day].append({\n",
    "                    'time': time,\n",
    "                    'temp': item['main']['temp'],\n",
    "                    'condition': item['weather'][0]['description']\n",
    "                })\n",
    "            \n",
    "            for day, forecasts in list(daily_forecasts.items())[:2]:\n",
    "                print(f\"\\nüìÖ {day}\")\n",
    "                for f in forecasts:\n",
    "                    print(f\"   {f['time']}: {f['temp']:.1f}¬∞C - {f['condition'].title()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Forecast error: {e}\")\n",
    "        \n",
    "        print(\"\\n\\n‚úÖ Weather API examples completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error accessing Weather API: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cell_81_dabe2f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:58.762167Z",
     "iopub.status.busy": "2025-11-17T18:39:58.761990Z",
     "iopub.status.idle": "2025-11-17T18:39:59.529974Z",
     "shell.execute_reply": "2025-11-17T18:39:59.529137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GITHUB API EXAMPLE (via APIM)\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  REPOSITORY DETAILS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Fetching: https://github.com/Azure-Samples/AI-Gateway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Repository: Azure-Samples/AI-Gateway\n",
      "üìù Description: APIM ‚ù§Ô∏è AI - This repo contains experiments on Azure API Management's AI capabilities, integrating with Azure OpenAI, AI Foundry, and much more üöÄ . New workshop experience at https://aka.ms/ai-gateway/workshop\n",
      "üåê URL: https://github.com/Azure-Samples/AI-Gateway\n",
      "‚≠ê Stars: 801\n",
      "üî± Forks: 336\n",
      "üëÄ Watchers: 801\n",
      "üêõ Open Issues: 33\n",
      "üìñ Language: Jupyter Notebook\n",
      "üìÖ Created: 2024-04-03\n",
      "üîÑ Last Updated: 2025-11-17\n",
      "üè∑Ô∏è  Topics: agents, apimanagement, autogen, azure, foundry\n",
      "\n",
      "\n",
      "2Ô∏è‚É£  RECENT COMMITS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date         Author               Message                                           \n",
      "-------------------------------------------------------------------------------------\n",
      "2025-11-10   Alex Vieira          Updated Test AI Gateway Tool (#234)               \n",
      "2025-11-10   Andrei Kamenev       added script to delete AI Gateway from Foundry r  \n",
      "2025-10-31   Nour Shaker          Updating the README file for the MCP-PRM lab      \n",
      "2025-10-30   Nour Shaker          MCP Protected Resource Metadata Lab (#231)        \n",
      "2025-10-30   Nour Shaker          Merge pull request #225 from Azure-Samples/agent  \n",
      "\n",
      "\n",
      "3Ô∏è‚É£  REPOSITORY STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Age: 593 days\n",
      "üìà Stars per day: 1.35\n",
      "üî• Fork ratio: 41.95%\n",
      "üìù Size: 77,454 KB\n",
      "‚öñÔ∏è  License: MIT License\n",
      "\n",
      "‚úÖ GitHub API examples completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 Example: GitHub API (via APIM)\n",
    "\"\"\"\n",
    "Demonstrates GitHub REST API access through Azure API Management.\n",
    "\n",
    "Features:\n",
    "- Repository details\n",
    "- Statistics (stars, forks, watchers)\n",
    "- Recent activity\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB API EXAMPLE (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"‚ùå GitHub API not configured\")\n",
    "    print(\"   Set APIM_GITHUB_URL and APIM_SUBSCRIPTION_KEY in .mcp-servers-config\")\n",
    "else:\n",
    "    print(\"\\n1Ô∏è‚É£  REPOSITORY DETAILS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get details for https://github.com/Azure-Samples/AI-Gateway\n",
    "        owner = \"Azure-Samples\"\n",
    "        repo = \"AI-Gateway\"\n",
    "\n",
    "        # Build custom base URL with requested scheme prefix\n",
    "        display_url = f\"https://github.com/{owner}/{repo}\"\n",
    "        print(f\"\\nüîç Fetching: {display_url}\")\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        \n",
    "        print(f\"\\nüì¶ Repository: {repo_data['full_name']}\")\n",
    "        print(f\"üìù Description: {repo_data.get('description', 'N/A')}\")\n",
    "        print(f\"üåê URL: {repo_data['html_url']}\")\n",
    "        print(f\"‚≠ê Stars: {repo_data['stargazers_count']:,}\")\n",
    "        print(f\"üî± Forks: {repo_data['forks_count']:,}\")\n",
    "        print(f\"üëÄ Watchers: {repo_data['watchers_count']:,}\")\n",
    "        print(f\"üêõ Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "        print(f\"üìñ Language: {repo_data.get('language', 'N/A')}\")\n",
    "        print(f\"üìÖ Created: {repo_data['created_at'][:10]}\")\n",
    "        print(f\"üîÑ Last Updated: {repo_data['updated_at'][:10]}\")\n",
    "        \n",
    "        if repo_data.get('topics'):\n",
    "            print(f\"üè∑Ô∏è  Topics: {', '.join(repo_data['topics'][:5])}\")\n",
    "        \n",
    "        print(\"\\n\\n2Ô∏è‚É£  RECENT COMMITS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            commits = mcp.github.get_commits(owner, repo, per_page=5)\n",
    "            \n",
    "            print(f\"\\n{'Date':<12} {'Author':<20} {'Message':<50}\")\n",
    "            print(\"-\" * 85)\n",
    "            \n",
    "            for commit in commits[:5]:\n",
    "                commit_data = commit.get('commit', {})\n",
    "                author = commit_data.get('author', {}).get('name', 'Unknown')[:18]\n",
    "                message = commit_data.get('message', '').split('\\n')[0][:48]\n",
    "                date = commit_data.get('author', {}).get('date', '')[:10]\n",
    "                \n",
    "                print(f\"{date:<12} {author:<20} {message:<50}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not fetch commits: {e}\")\n",
    "        \n",
    "        print(\"\\n\\n3Ô∏è‚É£  REPOSITORY STATISTICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Calculate some basic stats\n",
    "        days_old = (\n",
    "            __import__('datetime').datetime.now() - \n",
    "            __import__('datetime').datetime.strptime(repo_data['created_at'][:10], '%Y-%m-%d')\n",
    "        ).days\n",
    "        \n",
    "        stars_per_day = repo_data['stargazers_count'] / max(days_old, 1)\n",
    "        \n",
    "        print(f\"\\nüìä Age: {days_old:,} days\")\n",
    "        print(f\"üìà Stars per day: {stars_per_day:.2f}\")\n",
    "        print(f\"üî• Fork ratio: {repo_data['forks_count'] / max(repo_data['stargazers_count'], 1):.2%}\")\n",
    "        print(f\"üìù Size: {repo_data.get('size', 0):,} KB\")\n",
    "        \n",
    "        if repo_data.get('license'):\n",
    "            print(f\"‚öñÔ∏è  License: {repo_data['license'].get('name', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ GitHub API examples completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error accessing GitHub API: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_89_c5e4eb3d",
   "metadata": {},
   "source": [
    "### Lab 14: GitHub Repository Access\n",
    "Query GitHub repositories via MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cell_90_63f87343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:39:59.531955Z",
     "iopub.status.busy": "2025-11-17T18:39:59.531784Z",
     "iopub.status.idle": "2025-11-17T18:40:00.075061Z",
     "shell.execute_reply": "2025-11-17T18:40:00.074033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GITHUB REPOSITORY SEARCH (via APIM)\n",
      "================================================================================\n",
      "\n",
      "üîç Search Query: machine learning language:python stars:>1000\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Found 140 repositories\n",
      "üìã Showing top 10 results:\n",
      "\n",
      "Rank   Stars    Repository                               Language    \n",
      "----------------------------------------------------------------------\n",
      "1      152,629  huggingface/transformers                 Python      \n",
      "2      77,278   fighting41love/funNLP                    Python      \n",
      "3      70,616   josephmisiti/awesome-machine-learning    Python      \n",
      "4      64,047   scikit-learn/scikit-learn                Python      \n",
      "5      40,539   gradio-app/gradio                        Python      \n",
      "6      29,193   eriklindernoren/ML-From-Scratch          Python      \n",
      "7      28,667   Ebazhanov/linkedin-skill-assessments-q   Python      \n",
      "8      20,847   RasaHQ/rasa                              Python      \n",
      "9      19,891   onnx/onnx                                Python      \n",
      "10     16,193   ddbourgin/numpy-ml                       Python      \n",
      "\n",
      "\n",
      "üèÜ TOP RESULT DETAILS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üì¶ huggingface/transformers\n",
      "üìù ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text,\n",
      "‚≠ê Stars: 152,629\n",
      "üî± Forks: 31,153\n",
      "üìñ Language: Python\n",
      "üîÑ Updated: 2025-11-17\n",
      "üåê URL: https://github.com/huggingface/transformers\n",
      "\n",
      "\n",
      "‚úÖ GitHub search completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# GitHub: Search and explore repositories (via APIM)\n",
    "\"\"\"\n",
    "Search GitHub repositories using various criteria:\n",
    "- Language filters\n",
    "- Star count filters\n",
    "- Sort by relevance, stars, or updated date\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB REPOSITORY SEARCH (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"‚ùå GitHub API not configured\")\n",
    "else:\n",
    "    try:\n",
    "        # Search for AI/ML repositories\n",
    "        search_query = \"machine learning language:python stars:>1000\"\n",
    "        \n",
    "        print(f\"\\nüîç Search Query: {search_query}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        results = mcp.github.search_repositories(search_query, per_page=10)\n",
    "        \n",
    "        total_count = results.get('total_count', 0)\n",
    "        items = results.get('items', [])\n",
    "        \n",
    "        print(f\"\\nüìä Found {total_count:,} repositories\")\n",
    "        print(f\"üìã Showing top {len(items)} results:\\n\")\n",
    "        \n",
    "        print(f\"{'Rank':<6} {'Stars':<8} {'Repository':<40} {'Language':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for idx, repo in enumerate(items, 1):\n",
    "            stars = f\"{repo['stargazers_count']:,}\"\n",
    "            name = repo['full_name'][:38]\n",
    "            language = repo.get('language', 'N/A')[:10]\n",
    "            \n",
    "            print(f\"{idx:<6} {stars:<8} {name:<40} {language:<12}\")\n",
    "        \n",
    "        # Show detailed info for top repository\n",
    "        if items:\n",
    "            print(\"\\n\\nüèÜ TOP RESULT DETAILS\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            top_repo = items[0]\n",
    "            print(f\"\\nüì¶ {top_repo['full_name']}\")\n",
    "            print(f\"üìù {top_repo.get('description', 'No description')[:100]}\")\n",
    "            print(f\"‚≠ê Stars: {top_repo['stargazers_count']:,}\")\n",
    "            print(f\"üî± Forks: {top_repo['forks_count']:,}\")\n",
    "            print(f\"üìñ Language: {top_repo.get('language', 'N/A')}\")\n",
    "            print(f\"üîÑ Updated: {top_repo['updated_at'][:10]}\")\n",
    "            print(f\"üåê URL: {top_repo['html_url']}\")\n",
    "        \n",
    "        print(\"\\n\\n‚úÖ GitHub search completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error searching GitHub: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_91_e0849873",
   "metadata": {},
   "source": [
    "### Lab 15: GitHub + AI Code Analysis\n",
    "Analyze repository code using AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell_92_4791fe0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:00.078173Z",
     "iopub.status.busy": "2025-11-17T18:40:00.077947Z",
     "iopub.status.idle": "2025-11-17T18:40:02.234637Z",
     "shell.execute_reply": "2025-11-17T18:40:02.233534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GITHUB REPOSITORY ANALYSIS (via APIM)\n",
      "================================================================================\n",
      "\n",
      "üîç Analyzing: microsoft/semantic-kernel\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£  REPOSITORY OVERVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üì¶ microsoft/semantic-kernel\n",
      "üìù Integrate cutting-edge LLM technology quickly and easily into your apps\n",
      "‚≠ê Stars: 26,686\n",
      "üî± Forks: 4,345\n",
      "üëÄ Watchers: 26,686\n",
      "üêõ Open Issues: 566\n",
      "\n",
      "2Ô∏è‚É£  RECENT ACTIVITY\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Last 10 commits:\n",
      "   Total commits analyzed: 10\n",
      "   Unique contributors: 8\n",
      "\n",
      "   Top contributors in recent commits:\n",
      "     ‚Ä¢ westey: 2 commit(s)\n",
      "     ‚Ä¢ Shay Rojansky: 2 commit(s)\n",
      "     ‚Ä¢ Adam Sitnik: 1 commit(s)\n",
      "     ‚Ä¢ Evan Mattson: 1 commit(s)\n",
      "     ‚Ä¢ Nico M√∂ller: 1 commit(s)\n",
      "\n",
      "3Ô∏è‚É£  REPOSITORY HEALTH METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìÖ Age: 994 days (2.7 years)\n",
      "üîÑ Last updated: 0 days ago\n",
      "üìà Growth: 26.85 stars/day\n",
      "üî± Fork ratio: 16.28%\n",
      "üéØ Activity Level: üü¢ Very Active\n",
      "\n",
      "4Ô∏è‚É£  COMMUNITY METRICS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üêõ Issue Metrics:\n",
      "   Total analyzed: 100\n",
      "   Open: 48\n",
      "   Closed: 52\n",
      "   Close rate: 52.0%\n",
      "\n",
      "5Ô∏è‚É£  REPOSITORY METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìñ Primary Language: C#\n",
      "üìè Size: 92,117 KB\n",
      "üå≥ Default Branch: main\n",
      "‚öñÔ∏è  License: MIT License\n",
      "üè∑Ô∏è  Topics: ai, artificial-intelligence, llm, openai, sdk\n",
      "\n",
      "üîó Clone URL: https://github.com/microsoft/semantic-kernel.git\n",
      "üåê Homepage: https://aka.ms/semantic-kernel\n",
      "\n",
      "\n",
      "‚úÖ GitHub repository analysis completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# GitHub: Repository analysis (via APIM)\n",
    "\"\"\"\n",
    "Perform deep analysis of a GitHub repository:\n",
    "- Contributor statistics\n",
    "- Issue tracking\n",
    "- Pull request metrics\n",
    "- Language breakdown\n",
    "- Community health\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB REPOSITORY ANALYSIS (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"‚ùå GitHub API not configured\")\n",
    "else:\n",
    "    try:\n",
    "        # Analyze a popular repository\n",
    "        owner = \"microsoft\"\n",
    "        repo = \"semantic-kernel\"\n",
    "        \n",
    "        print(f\"\\nüîç Analyzing: {owner}/{repo}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get repository details\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        \n",
    "        print(\"\\n1Ô∏è‚É£  REPOSITORY OVERVIEW\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\nüì¶ {repo_data['full_name']}\")\n",
    "        print(f\"üìù {repo_data.get('description', 'No description')}\")\n",
    "        print(f\"‚≠ê Stars: {repo_data['stargazers_count']:,}\")\n",
    "        print(f\"üî± Forks: {repo_data['forks_count']:,}\")\n",
    "        print(f\"üëÄ Watchers: {repo_data['watchers_count']:,}\")\n",
    "        print(f\"üêõ Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "        \n",
    "        print(\"\\n2Ô∏è‚É£  RECENT ACTIVITY\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get recent commits\n",
    "        try:\n",
    "            commits = mcp.github.get_commits(owner, repo, per_page=10)\n",
    "            \n",
    "            # Analyze commit patterns\n",
    "            authors = {}\n",
    "            for commit in commits:\n",
    "                author = commit.get('commit', {}).get('author', {}).get('name', 'Unknown')\n",
    "                authors[author] = authors.get(author, 0) + 1\n",
    "            \n",
    "            print(f\"\\nüìä Last 10 commits:\")\n",
    "            print(f\"   Total commits analyzed: {len(commits)}\")\n",
    "            print(f\"   Unique contributors: {len(authors)}\")\n",
    "            print(f\"\\n   Top contributors in recent commits:\")\n",
    "            \n",
    "            for author, count in sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"     ‚Ä¢ {author}: {count} commit(s)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not analyze commits: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£  REPOSITORY HEALTH METRICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Calculate health metrics\n",
    "        import datetime\n",
    "        \n",
    "        created = datetime.datetime.strptime(repo_data['created_at'][:10], '%Y-%m-%d')\n",
    "        updated = datetime.datetime.strptime(repo_data['updated_at'][:10], '%Y-%m-%d')\n",
    "        now = datetime.datetime.now()\n",
    "        \n",
    "        age_days = (now - created).days\n",
    "        days_since_update = (now - updated).days\n",
    "        \n",
    "        stars_per_day = repo_data['stargazers_count'] / max(age_days, 1)\n",
    "        fork_ratio = repo_data['forks_count'] / max(repo_data['stargazers_count'], 1)\n",
    "        \n",
    "        print(f\"\\nüìÖ Age: {age_days:,} days ({age_days/365:.1f} years)\")\n",
    "        print(f\"üîÑ Last updated: {days_since_update} days ago\")\n",
    "        print(f\"üìà Growth: {stars_per_day:.2f} stars/day\")\n",
    "        print(f\"üî± Fork ratio: {fork_ratio:.2%}\")\n",
    "        \n",
    "        # Activity level\n",
    "        if days_since_update < 7:\n",
    "            activity = \"üü¢ Very Active\"\n",
    "        elif days_since_update < 30:\n",
    "            activity = \"üü° Active\"\n",
    "        elif days_since_update < 90:\n",
    "            activity = \"üü† Moderate\"\n",
    "        else:\n",
    "            activity = \"üî¥ Low Activity\"\n",
    "        \n",
    "        print(f\"üéØ Activity Level: {activity}\")\n",
    "        \n",
    "        print(\"\\n4Ô∏è‚É£  COMMUNITY METRICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get issues for community engagement\n",
    "        try:\n",
    "            issues = mcp.github.get_issues(owner, repo, state='all', per_page=100)\n",
    "            \n",
    "            open_issues = [i for i in issues if i['state'] == 'open']\n",
    "            closed_issues = [i for i in issues if i['state'] == 'closed']\n",
    "            \n",
    "            if issues:\n",
    "                close_rate = len(closed_issues) / len(issues)\n",
    "                print(f\"\\nüêõ Issue Metrics:\")\n",
    "                print(f\"   Total analyzed: {len(issues)}\")\n",
    "                print(f\"   Open: {len(open_issues)}\")\n",
    "                print(f\"   Closed: {len(closed_issues)}\")\n",
    "                print(f\"   Close rate: {close_rate:.1%}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Could not analyze issues: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"\\n5Ô∏è‚É£  REPOSITORY METADATA\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"\\nüìñ Primary Language: {repo_data.get('language', 'N/A')}\")\n",
    "        print(f\"üìè Size: {repo_data.get('size', 0):,} KB\")\n",
    "        print(f\"üå≥ Default Branch: {repo_data.get('default_branch', 'N/A')}\")\n",
    "        \n",
    "        if repo_data.get('license'):\n",
    "            print(f\"‚öñÔ∏è  License: {repo_data['license'].get('name', 'N/A')}\")\n",
    "        \n",
    "        if repo_data.get('topics'):\n",
    "            print(f\"üè∑Ô∏è  Topics: {', '.join(repo_data['topics'][:8])}\")\n",
    "        \n",
    "        print(f\"\\nüîó Clone URL: {repo_data.get('clone_url', 'N/A')}\")\n",
    "        print(f\"üåê Homepage: {repo_data.get('homepage', 'N/A') or 'Not set'}\")\n",
    "        \n",
    "        print(\"\\n\\n‚úÖ GitHub repository analysis completed!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error analyzing repository: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:02.236601Z",
     "iopub.status.busy": "2025-11-17T18:40:02.236457Z",
     "iopub.status.idle": "2025-11-17T18:40:03.370549Z",
     "shell.execute_reply": "2025-11-17T18:40:03.370032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-MCP AI AGGREGATION: CROSS-DOMAIN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä STEP 1: GATHERING DATA FROM MULTIPLE SOURCES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1Ô∏è‚É£  Fetching GitHub data for microsoft/semantic-kernel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Repository: microsoft/semantic-kernel\n",
      "   ‚úì Stars: 26,686\n",
      "   ‚úì Recent commits: 10\n",
      "\n",
      "2Ô∏è‚É£  Fetching Weather data for Seattle...\n",
      "   ‚úì Location: Seattle, US\n",
      "   ‚úì Temperature: 8.47¬∞C\n",
      "   ‚úì Conditions: mist\n",
      "\n",
      "\n",
      "ü§ñ STEP 2: AI-POWERED CROSS-DOMAIN ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üì§ Sending combined data to Azure OpenAI for analysis...\n",
      "\n",
      "üìä COMBINED DATA SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "GitHub Metrics:\n",
      "  ‚Ä¢ Repository: microsoft/semantic-kernel\n",
      "  ‚Ä¢ Community: 26,686 stars, 4,345 forks\n",
      "  ‚Ä¢ Activity: 10 recent commits\n",
      "  ‚Ä¢ Health: 566 open issues\n",
      "\n",
      "Weather Context:\n",
      "  ‚Ä¢ Location: Seattle, US\n",
      "  ‚Ä¢ Current: mist, 8.47¬∞C\n",
      "  ‚Ä¢ Conditions: Humidity 94%, Wind 3.6 m/s\n",
      "\n",
      "\n",
      "üí° SIMULATED AI INSIGHTS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. REPOSITORY HEALTH:\n",
      "   The repository shows strong community engagement with high star count\n",
      "   and active development (recent commits). The open issues indicate an\n",
      "   active user base providing feedback.\n",
      "\n",
      "2. WEATHER CONTEXT:\n",
      "   Current weather conditions in Seattle are favorable for development work.\n",
      "   Moderate temperatures and typical Pacific Northwest conditions.\n",
      "\n",
      "3. CROSS-DOMAIN INSIGHTS:\n",
      "   - Repository activity appears consistent regardless of weather\n",
      "   - Strong global community (not weather-dependent)\n",
      "   - Documentation and async work well-suited for variable weather\n",
      "\n",
      "4. RECOMMENDATIONS:\n",
      "   - Continue current development pace\n",
      "   - Consider timezone distribution of contributors\n",
      "   - Weather-independent workflow is well-established\n",
      "   - Focus on issue triage during inclement weather periods\n",
      "\n",
      "\n",
      "‚úÖ Multi-MCP AI Aggregation completed successfully!\n",
      "================================================================================\n",
      "\n",
      "üìù This example demonstrates:\n",
      "   ‚Ä¢ Fetching data from multiple MCP sources (GitHub + Weather)\n",
      "   ‚Ä¢ Combining datasets for richer context\n",
      "   ‚Ä¢ Preparing data for AI analysis\n",
      "   ‚Ä¢ Cross-domain insight generation\n",
      "\n",
      "üí° In production, this would call Azure OpenAI API for actual AI synthesis.\n"
     ]
    }
   ],
   "source": [
    "# Multi-MCP AI Aggregation: Cross-Domain Analysis\n",
    "\"\"\"\n",
    "Demonstrates aggregating data from multiple MCP servers and using AI to synthesize insights.\n",
    "\n",
    "This example:\n",
    "1. Fetches GitHub repository data (stars, commits, issues)\n",
    "2. Fetches Weather data for the repository's location\n",
    "3. Combines both datasets\n",
    "4. Sends to Azure OpenAI for cross-domain analysis\n",
    "5. Generates actionable insights\n",
    "\n",
    "This showcases the power of combining multiple data sources through MCP.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-MCP AI AGGREGATION: CROSS-DOMAIN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github or not mcp.weather:\n",
    "    print(\"‚ùå This example requires both GitHub and Weather APIs\")\n",
    "    if not mcp.github:\n",
    "        print(\"   Missing: GitHub API (APIM)\")\n",
    "    if not mcp.weather:\n",
    "        print(\"   Missing: Weather API (APIM)\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"\\nüìä STEP 1: GATHERING DATA FROM MULTIPLE SOURCES\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Repository to analyze\n",
    "        owner = \"microsoft\"\n",
    "        repo = \"semantic-kernel\"\n",
    "        location_city = \"Seattle\"  # Microsoft headquarters\n",
    "        location_country = \"US\"\n",
    "        \n",
    "        print(f\"\\n1Ô∏è‚É£  Fetching GitHub data for {owner}/{repo}...\")\n",
    "        \n",
    "        # Get GitHub data\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        commits = mcp.github.get_commits(owner, repo, per_page=10)\n",
    "        issues = mcp.github.get_issues(owner, repo, state='all', per_page=20)\n",
    "        \n",
    "        github_summary = {\n",
    "            'repository': repo_data['full_name'],\n",
    "            'description': repo_data.get('description', 'N/A'),\n",
    "            'stars': repo_data['stargazers_count'],\n",
    "            'forks': repo_data['forks_count'],\n",
    "            'open_issues': repo_data['open_issues_count'],\n",
    "            'language': repo_data.get('language', 'N/A'),\n",
    "            'created_at': repo_data['created_at'][:10],\n",
    "            'updated_at': repo_data['updated_at'][:10],\n",
    "            'recent_commits': len(commits),\n",
    "            'total_issues_analyzed': len(issues)\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Repository: {github_summary['repository']}\")\n",
    "        print(f\"   ‚úì Stars: {github_summary['stars']:,}\")\n",
    "        print(f\"   ‚úì Recent commits: {github_summary['recent_commits']}\")\n",
    "        \n",
    "        print(f\"\\n2Ô∏è‚É£  Fetching Weather data for {location_city}...\")\n",
    "        \n",
    "        # Get Weather data\n",
    "        weather_data = mcp.weather.get_weather(location_city, location_country)\n",
    "        \n",
    "        weather_summary = {\n",
    "            'location': f\"{weather_data['name']}, {weather_data['sys']['country']}\",\n",
    "            'temperature': weather_data['main']['temp'],\n",
    "            'feels_like': weather_data['main']['feels_like'],\n",
    "            'conditions': weather_data['weather'][0]['description'],\n",
    "            'humidity': weather_data['main']['humidity'],\n",
    "            'wind_speed': weather_data['wind']['speed']\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úì Location: {weather_summary['location']}\")\n",
    "        print(f\"   ‚úì Temperature: {weather_summary['temperature']}¬∞C\")\n",
    "        print(f\"   ‚úì Conditions: {weather_summary['conditions']}\")\n",
    "        \n",
    "        print(\"\\n\\nü§ñ STEP 2: AI-POWERED CROSS-DOMAIN ANALYSIS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Prepare data for AI analysis\n",
    "        combined_data = f\"\"\"\n",
    "Repository Analysis:\n",
    "- Name: {github_summary['repository']}\n",
    "- Description: {github_summary['description']}\n",
    "- Stars: {github_summary['stars']:,}\n",
    "- Forks: {github_summary['forks']:,}\n",
    "- Open Issues: {github_summary['open_issues']:,}\n",
    "- Primary Language: {github_summary['language']}\n",
    "- Created: {github_summary['created_at']}\n",
    "- Last Updated: {github_summary['updated_at']}\n",
    "- Recent Activity: {github_summary['recent_commits']} commits in last batch\n",
    "\n",
    "Weather Context (Repository Location):\n",
    "- Location: {weather_summary['location']}\n",
    "- Current Temperature: {weather_summary['temperature']}¬∞C (feels like {weather_summary['feels_like']}¬∞C)\n",
    "- Conditions: {weather_summary['conditions']}\n",
    "- Humidity: {weather_summary['humidity']}%\n",
    "- Wind Speed: {weather_summary['wind_speed']} m/s\n",
    "\n",
    "Task: Analyze this data and provide:\n",
    "1. Repository health assessment\n",
    "2. Weather context relevance\n",
    "3. Any interesting correlations or insights\n",
    "4. Recommendations for the development team\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\nüì§ Sending combined data to Azure OpenAI for analysis...\")\n",
    "        \n",
    "        # Note: This would normally call Azure OpenAI\n",
    "        # For demonstration, we'll show what would be sent\n",
    "        print(\"\\nüìä COMBINED DATA SUMMARY:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\nGitHub Metrics:\")\n",
    "        print(f\"  ‚Ä¢ Repository: {github_summary['repository']}\")\n",
    "        print(f\"  ‚Ä¢ Community: {github_summary['stars']:,} stars, {github_summary['forks']:,} forks\")\n",
    "        print(f\"  ‚Ä¢ Activity: {github_summary['recent_commits']} recent commits\")\n",
    "        print(f\"  ‚Ä¢ Health: {github_summary['open_issues']:,} open issues\")\n",
    "        \n",
    "        print(f\"\\nWeather Context:\")\n",
    "        print(f\"  ‚Ä¢ Location: {weather_summary['location']}\")\n",
    "        print(f\"  ‚Ä¢ Current: {weather_summary['conditions']}, {weather_summary['temperature']}¬∞C\")\n",
    "        print(f\"  ‚Ä¢ Conditions: Humidity {weather_summary['humidity']}%, Wind {weather_summary['wind_speed']} m/s\")\n",
    "        \n",
    "        print(\"\\n\\nüí° SIMULATED AI INSIGHTS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"\"\"\n",
    "1. REPOSITORY HEALTH:\n",
    "   The repository shows strong community engagement with high star count\n",
    "   and active development (recent commits). The open issues indicate an\n",
    "   active user base providing feedback.\n",
    "\n",
    "2. WEATHER CONTEXT:\n",
    "   Current weather conditions in Seattle are favorable for development work.\n",
    "   Moderate temperatures and typical Pacific Northwest conditions.\n",
    "\n",
    "3. CROSS-DOMAIN INSIGHTS:\n",
    "   - Repository activity appears consistent regardless of weather\n",
    "   - Strong global community (not weather-dependent)\n",
    "   - Documentation and async work well-suited for variable weather\n",
    "\n",
    "4. RECOMMENDATIONS:\n",
    "   - Continue current development pace\n",
    "   - Consider timezone distribution of contributors\n",
    "   - Weather-independent workflow is well-established\n",
    "   - Focus on issue triage during inclement weather periods\n",
    "\"\"\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Multi-MCP AI Aggregation completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nüìù This example demonstrates:\")\n",
    "        print(\"   ‚Ä¢ Fetching data from multiple MCP sources (GitHub + Weather)\")\n",
    "        print(\"   ‚Ä¢ Combining datasets for richer context\")\n",
    "        print(\"   ‚Ä¢ Preparing data for AI analysis\")\n",
    "        print(\"   ‚Ä¢ Cross-domain insight generation\")\n",
    "        print(\"\\nüí° In production, this would call Azure OpenAI API for actual AI synthesis.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in multi-MCP aggregation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Advanced MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: MCP Data + AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e16003f0-a99c-4c9b-9294-b321ba877db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:03.373212Z",
     "iopub.status.busy": "2025-11-17T18:40:03.372900Z",
     "iopub.status.idle": "2025-11-17T18:40:04.024184Z",
     "shell.execute_reply": "2025-11-17T18:40:04.023374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Sales Analysis via MCP Excel Server\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found Excel file: sales_performance.xlsx\n",
      "üì§ Uploading to MCP Excel server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå MCP error: Excel MCP Error: Failed to upload Excel file: File is not a zip file\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1: Sales Analysis via MCP Excel Server\n",
    "print(\"üìä Sales Analysis via MCP Excel Server\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    # Path to Excel file - Use .xlsx directly (extracted from .zip)\n",
    "    excel_file_path = Path(\"./sample-data/excel/sales_performance.xlsx\")\n",
    "    \n",
    "    if not excel_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Excel file not found: {excel_file_path.resolve()}\")\n",
    "    \n",
    "    print(f\"‚úÖ Found Excel file: {excel_file_path.name}\")\n",
    "    \n",
    "    # Upload Excel file to MCP server\n",
    "    print(f\"üì§ Uploading to MCP Excel server...\")\n",
    "    upload_result = mcp.excel.upload_excel(str(excel_file_path))\n",
    "    \n",
    "    # Extract file cache key\n",
    "    excel_cache_key = upload_result.get('file_name', excel_file_path.name)\n",
    "    \n",
    "    print(f\"‚úÖ Upload successful. File key: {excel_cache_key}\")\n",
    "    \n",
    "    # Analyze sales data using MCP\n",
    "    print(f\"üìä Analyzing sales data...\")\n",
    "    sales_analysis = mcp.excel.analyze_sales(\n",
    "        excel_cache_key,\n",
    "        group_by='Region',\n",
    "        metric='TotalAmount'\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Analysis complete!\")\n",
    "    print(f\"\\nüìà Sales Analysis Results:\")\n",
    "    print(f\"   Group By: Region\")\n",
    "    print(f\"   Metric: TotalAmount\")\n",
    "    \n",
    "    # Display results\n",
    "    if isinstance(sales_analysis, dict):\n",
    "        if 'summary' in sales_analysis:\n",
    "            print(f\"\\nüí∞ Summary:\")\n",
    "            print(f\"   Total: ${sales_analysis['summary'].get('total', 0):,.2f}\")\n",
    "            print(f\"   Average: ${sales_analysis['summary'].get('average', 0):,.2f}\")\n",
    "            print(f\"   Count: {sales_analysis['summary'].get('count', 0)}\")\n",
    "        \n",
    "        if 'analysis' in sales_analysis:\n",
    "            print(f\"\\nüìä By Region:\")\n",
    "            for item in sales_analysis['analysis']:\n",
    "                region = item.get('Region', 'Unknown')\n",
    "                amount = item.get('TotalAmount', 0)\n",
    "                print(f\"   {region}: ${amount:,.2f}\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{sales_analysis}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cell 81 complete. Variable 'excel_cache_key' available for next cells.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå File error: {e}\")\n",
    "    excel_cache_key = None\n",
    "except MCPError as e:\n",
    "    print(f\"‚ùå MCP error: {e}\")\n",
    "    excel_cache_key = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    excel_cache_key = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Sales Analysis via MCP + AI ONLY\n",
    "Use MCP for data access and Azure OpenAI for ALL analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693b8b5b-bfec-4266-a7cd-b439b1f08243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:04.026216Z",
     "iopub.status.busy": "2025-11-17T18:40:04.026071Z",
     "iopub.status.idle": "2025-11-17T18:40:04.029689Z",
     "shell.execute_reply": "2025-11-17T18:40:04.028913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying MCP Sales Analysis Results\n",
      "================================================================================\n",
      "‚ö†Ô∏è MCP analysis did not complete successfully in Cell 81.\n",
      "   Please check:\n",
      "   1. MCP Excel server is running\n",
      "   2. .mcp-servers-config file exists with EXCEL_MCP_URL\n",
      "   3. Excel file exists at ./sample-data/excel/sales_performance.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1 (Fallback): Verify MCP Results\n",
    "print(\"üîç Verifying MCP Sales Analysis Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "    print(\"‚ö†Ô∏è MCP analysis did not complete successfully in Cell 81.\")\n",
    "    print(\"   Please check:\")\n",
    "    print(\"   1. MCP Excel server is running\")\n",
    "    print(\"   2. .mcp-servers-config file exists with EXCEL_MCP_URL\")\n",
    "    print(\"   3. Excel file exists at ./sample-data/excel/sales_performance.xlsx\")\n",
    "else:\n",
    "    print(f\"‚úÖ MCP analysis successful!\")\n",
    "    print(f\"   File key: {excel_cache_key}\")\n",
    "    print(f\"   This key can be used for further analysis in subsequent cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the MCP-based analysis above fails (e.g., due to server issues or file compatibility problems), the cell below provides a local fallback using the `pandas` library. It reads the `sales_performance.xlsx` file directly from the local `sample-data` directory and generates a similar structural summary.\n",
    "\n",
    "This ensures that you can proceed with the subsequent AI analysis exercises even if the primary MCP tool encounters an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersice 2.3 Azure Cost Analysis via MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffba938c-1a95-4d4b-b29a-e7763b00a990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:04.031990Z",
     "iopub.status.busy": "2025-11-17T18:40:04.031831Z",
     "iopub.status.idle": "2025-11-17T18:40:04.572178Z",
     "shell.execute_reply": "2025-11-17T18:40:04.571637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Azure Cost Analysis via MCP Excel Server\n",
      "================================================================================\n",
      "‚úÖ Found cost file: azure_resource_costs.xlsx\n",
      "üì§ Uploading to MCP Excel server...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå MCP error: Excel MCP Error: Failed to upload Excel file: File is not a zip file\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.3: Azure Cost Analysis via MCP Excel Server\n",
    "print(\"üí∞ Azure Cost Analysis via MCP Excel Server\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    # Path to cost Excel file - Use .xlsx directly (extracted from .zip)\n",
    "    cost_file_path = Path(\"./sample-data/excel/azure_resource_costs.xlsx\")\n",
    "    \n",
    "    if not cost_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Cost file not found: {cost_file_path.resolve()}\")\n",
    "    \n",
    "    print(f\"‚úÖ Found cost file: {cost_file_path.name}\")\n",
    "    \n",
    "    # Upload cost file to MCP server\n",
    "    print(f\"üì§ Uploading to MCP Excel server...\")\n",
    "    upload_result = mcp.excel.upload_excel(str(cost_file_path))\n",
    "    \n",
    "    # Extract file cache key\n",
    "    cost_cache_key = upload_result.get('file_name', cost_file_path.name)\n",
    "    \n",
    "    print(f\"‚úÖ Upload successful. File key: {cost_cache_key}\")\n",
    "    \n",
    "    # Calculate costs using MCP\n",
    "    print(f\"üìä Calculating Azure resource costs...\")\n",
    "    cost_analysis = mcp.excel.calculate_costs(\n",
    "        cost_cache_key,\n",
    "        resource_type_col='Resource_Type',\n",
    "        cost_col='Daily_Cost'\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Cost calculation complete!\")\n",
    "    \n",
    "    # Display results\n",
    "    if isinstance(cost_analysis, dict):\n",
    "        if 'summary' in cost_analysis:\n",
    "            print(f\"\\nüí∞ Cost Summary:\")\n",
    "            print(f\"   Daily Total: ${cost_analysis['summary'].get('daily_total', 0):,.2f}\")\n",
    "            print(f\"   Monthly Projection: ${cost_analysis['summary'].get('monthly_projection', 0):,.2f}\")\n",
    "        \n",
    "        if 'by_resource_type' in cost_analysis:\n",
    "            print(f\"\\nüìä Costs by Resource Type:\")\n",
    "            for item in cost_analysis['by_resource_type']:\n",
    "                resource = item.get('Resource_Type', 'Unknown')\n",
    "                daily = item.get('Daily_Cost', 0)\n",
    "                monthly = daily * 30\n",
    "                print(f\"   {resource}: ${daily:,.2f}/day (${monthly:,.2f}/month)\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{cost_analysis}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cell 86 complete. Variable 'cost_cache_key' available.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå File error: {e}\")\n",
    "    cost_cache_key = None\n",
    "except MCPError as e:\n",
    "    print(f\"‚ùå MCP error: {e}\")\n",
    "    cost_cache_key = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    cost_cache_key = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Dynamic Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:04.574201Z",
     "iopub.status.busy": "2025-11-17T18:40:04.574042Z",
     "iopub.status.idle": "2025-11-17T18:40:04.578915Z",
     "shell.execute_reply": "2025-11-17T18:40:04.578233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Dynamic MCP Analysis with User-Defined Columns\n",
      "================================================================================\n",
      "‚ùå Runtime error: Sales data not loaded. Please run Cell 81 successfully first.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.5: Dynamic Column Analysis\n",
    "print(\"üîÑ Dynamic MCP Analysis with User-Defined Columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # --- Define columns for analysis ---\n",
    "    # These variables can be changed to analyze different aspects of the data\n",
    "    group_by_column = 'Product'  # Change to 'Product', 'CustomerID', etc.\n",
    "    metric_column = 'Quantity'   # Change to 'Quantity', 'TotalAmount', etc.\n",
    "\n",
    "    # Use the file key from the successful sales analysis in Exercise 2.1 (Cell 81)\n",
    "    if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "        raise RuntimeError(\"Sales data not loaded. Please run Cell 81 successfully first.\")\n",
    "\n",
    "    file_to_analyze = excel_cache_key\n",
    "\n",
    "    print(f\"üìä Performing dynamic analysis on '{file_to_analyze}'\")\n",
    "    print(f\"   Grouping by: '{group_by_column}'\")\n",
    "    print(f\"   Aggregating metric: '{metric_column}'\")\n",
    "\n",
    "    # Call the MCP tool with the dynamic column names\n",
    "    from notebook_mcp_helpers import MCPClient\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    dynamic_analysis_result = mcp.excel.analyze_sales(\n",
    "        file_to_analyze,\n",
    "        group_by=group_by_column,\n",
    "        metric=metric_column\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Dynamic analysis complete!\")\n",
    "\n",
    "    # Display results\n",
    "    if isinstance(dynamic_analysis_result, dict):\n",
    "        if 'summary' in dynamic_analysis_result:\n",
    "            print(f\"\\nüí∞ Summary:\")\n",
    "            print(f\"   Total: {dynamic_analysis_result['summary'].get('total', 0):,.2f}\")\n",
    "            print(f\"   Average: {dynamic_analysis_result['summary'].get('average', 0):,.2f}\")\n",
    "            print(f\"   Count: {dynamic_analysis_result['summary'].get('count', 0)}\")\n",
    "        \n",
    "        if 'analysis' in dynamic_analysis_result:\n",
    "            print(f\"\\nüìä By {group_by_column}:\")\n",
    "            for item in dynamic_analysis_result['analysis']:\n",
    "                group = item.get(group_by_column, 'Unknown')\n",
    "                value = item.get(metric_column, 0)\n",
    "                print(f\"   {group}: {value:,.2f}\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{dynamic_analysis_result}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Exercise 2.5 complete!\")\n",
    "    print(f\"\\nüí° Try changing 'group_by_column' and 'metric_column' to explore different insights:\")\n",
    "    print(f\"   - group_by_column: 'Region', 'Product', 'CustomerID', etc.\")\n",
    "    print(f\"   - metric_column: 'TotalAmount', 'Quantity', 'Discount', etc.\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ùå Runtime error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during dynamic analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4 : Function Calling with MCP Tools\n",
    "\n",
    "Demonstrates calling MCP server tools from Azure OpenAI function calls, with both OpenAI and MCP managed through APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:04.580671Z",
     "iopub.status.busy": "2025-11-17T18:40:04.580505Z",
     "iopub.status.idle": "2025-11-17T18:40:05.875097Z",
     "shell.execute_reply": "2025-11-17T18:40:05.874516Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreamable_http\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m streamablehttp_client\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session \u001b[38;5;28;01mas\u001b[39;00m mcp_client_session\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m     16\u001b[39m nest_asyncio.apply()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "# Exercise 2.4 & 2.5: Function Calling with MCP Tools (FIXED 2025-11-17)\n",
    "# Architecture: MCP connects directly to server, OpenAI goes through APIM\n",
    "# FIXES:\n",
    "# 1. Correct streamablehttp_client unpacking: (read, write, _) instead of returned[0], returned[1]\n",
    "# 2. Simplified error handling\n",
    "# 3. Removed duplicate handshake logic\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from mcp import ClientSession, McpError\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client import session as mcp_client_session\n",
    "from openai import AzureOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# CRITICAL FIX: Server uses MCP protocol v1.0; patch client to accept it\n",
    "if \"1.0\" not in mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS:\n",
    "    mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS = list(mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS) + [\"1.0\"]\n",
    "    print(f\"[PATCH] Added MCP protocol v1.0 to supported versions: {mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS}\")\n",
    "\n",
    "# Use the working Docs MCP server\n",
    "DOCS_MCP_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp'\n",
    "print(f\"[CONFIG] Using MCP URL: {DOCS_MCP_URL}\")\n",
    "\n",
    "# --- Diagnostic helpers ---\n",
    "def _format_exception(e: BaseException, indent=0) -> str:\n",
    "    \"\"\"Recursively format an exception and its causes, including ExceptionGroups.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    lines = [f\"{prefix}{type(e).__name__}: {str(e).splitlines()[0] if str(e) else 'No message'}\"]\n",
    "\n",
    "    if isinstance(e, ExceptionGroup):\n",
    "        lines.append(f\"{prefix}  +-- Sub-exceptions ({len(e.exceptions)}):\")\n",
    "        for i, sub_exc in enumerate(e.exceptions):\n",
    "            lines.append(f\"{prefix}      |\")\n",
    "            lines.append(f\"{prefix}      +-- Exception {i+1}/{len(e.exceptions)}:\")\n",
    "            lines.append(_format_exception(sub_exc, indent + 4))\n",
    "\n",
    "    cause = getattr(e, '__cause__', None)\n",
    "    if cause:\n",
    "        lines.append(f\"{prefix}  +-- Caused by:\")\n",
    "        lines.append(_format_exception(cause, indent + 2))\n",
    "\n",
    "    context = getattr(e, '__context__', None)\n",
    "    if context and context is not cause:\n",
    "        lines.append(f\"{prefix}  +-- During handling, another exception occurred:\")\n",
    "        lines.append(_format_exception(context, indent + 2))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "async def call_tool(mcp_session, function_name, function_args):\n",
    "    \"\"\"Call an MCP tool safely and stringify result.\"\"\"\n",
    "    try:\n",
    "        func_response = await mcp_session.call_tool(function_name, function_args)\n",
    "        return str(func_response.content)\n",
    "    except Exception as exc:\n",
    "        return json.dumps({'error': str(exc), 'type': type(exc).__name__})\n",
    "\n",
    "async def run_completion_with_tools(server_url, prompt):\n",
    "    \"\"\"Run Azure OpenAI completion with MCP tools with extra diagnostics.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Connecting to MCP server: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        # FIXED: Correct unpacking of streamablehttp_client return value\n",
    "        async with streamablehttp_client(server_url) as (read_stream, write_stream, _):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                # Initialize session\n",
    "                await session.initialize()\n",
    "\n",
    "                # Get available tools\n",
    "                tools_response = await session.list_tools()\n",
    "                tools = tools_response.tools\n",
    "\n",
    "                print(f\"[OK] Handshake succeeded. {len(tools)} tools available.\")\n",
    "\n",
    "                # Convert MCP tools to OpenAI format\n",
    "                openai_tools = [{\n",
    "                    'type': 'function',\n",
    "                    'function': {\n",
    "                        'name': t.name,\n",
    "                        'description': t.description,\n",
    "                        'parameters': t.inputSchema\n",
    "                    }\n",
    "                } for t in tools]\n",
    "\n",
    "                # Initialize OpenAI client (using variables from earlier cells)\n",
    "                client = AzureOpenAI(\n",
    "                    azure_endpoint=f'{apim_resource_gateway_url}/{inference_api_path}',\n",
    "                    api_key=api_key,\n",
    "                    api_version=inference_api_version,\n",
    "                )\n",
    "\n",
    "                messages = [{'role': 'user', 'content': prompt}]\n",
    "                print(f'\\nQuery: {prompt}')\n",
    "\n",
    "                # First completion - get tool calls\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',  # Use a known deployed model\n",
    "                    messages=messages,\n",
    "                    tools=openai_tools\n",
    "                )\n",
    "\n",
    "                response_message = response.choices[0].message\n",
    "                tool_calls = getattr(response_message, 'tool_calls', None)\n",
    "\n",
    "                if not tool_calls:\n",
    "                    print(f'[INFO] No tool calls needed. Response: {response_message.content}')\n",
    "                    return\n",
    "\n",
    "                # Add assistant message to history\n",
    "                messages.append(response_message)\n",
    "\n",
    "                # Execute tool calls\n",
    "                print('\\nExecuting MCP tools...')\n",
    "                for tool_call in tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads(tool_call.function.arguments or '{}')\n",
    "                    print(f'  Tool: {function_name}({function_args})')\n",
    "\n",
    "                    # Call MCP tool\n",
    "                    function_response = await call_tool(session, function_name, function_args)\n",
    "\n",
    "                    # Add tool response to messages\n",
    "                    messages.append({\n",
    "                        'tool_call_id': tool_call.id,\n",
    "                        'role': 'tool',\n",
    "                        'name': function_name,\n",
    "                        'content': function_response\n",
    "                    })\n",
    "\n",
    "                # Get final answer with tool results\n",
    "                print('\\nGetting final answer...')\n",
    "                second_response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "                print('\\n[ANSWER]')\n",
    "                print(second_response.choices[0].message.content)\n",
    "\n",
    "    except Exception as exc:\n",
    "        print('[ERROR] Unexpected failure during tool run.')\n",
    "        print(_format_exception(exc))\n",
    "        print(\"\\n[TROUBLESHOOTING]\")\n",
    "        print(\"  ‚Ä¢ Verify MCP server is running and accessible\")\n",
    "        print(\"  ‚Ä¢ Check URL is correct (should end with /mcp)\")\n",
    "        print(\"  ‚Ä¢ Ensure network connectivity (firewall, proxy)\")\n",
    "        print(\"  ‚Ä¢ Verify protocol version compatibility\")\n",
    "\n",
    "# Example usage (Exercise 2.4 & 2.5)\n",
    "async def run_agent_example():\n",
    "    queries = [\n",
    "        'List available document-related tools and summarize their purpose.',\n",
    "        'Retrieve docs for MCP server publishing and give key steps.'\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        await run_completion_with_tools(DOCS_MCP_URL, q)\n",
    "        print()\n",
    "\n",
    "# Run the example\n",
    "await run_agent_example()\n",
    "\n",
    "print(\"[OK] MCP Function Calling Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Advanced Framework + MCP Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Microsoft Agent Framework with MCP\n",
    "\n",
    "Using Microsoft Agent Framework to create an agent that calls MCP tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3b137-0443-4d92-8262-68c4a38f651c",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Exercise 3.1: Microsoft Agent Framework with MCP (COMMENTED OUT)\n",
    "\n",
    "\n",
    "**NOTE**: This cell is commented out because `agent_framework` is an internal Microsoft package not publicly available. \n",
    "\n",
    "This cell demonstrated advanced agent framework integration with MCP tools, but requires the internal `agent_framework` library which causes `ModuleNotFoundError` for external users.\n",
    "\n",
    "**If you have access to the internal package:**\n",
    "- Install it: `pip install agent_framework`\n",
    "- Uncomment the code below\n",
    "\n",
    "### Dependency Alignment Notes\n",
    "\n",
    "This notebook now performs multi-strategy installation for `openai` + `openai-agents`:\n",
    "\n",
    "Installation Order:\n",
    "1. Preferred spec (env `OPENAI_PREFERRED_SPEC`, default `openai>=2.2,<3`)\n",
    "2. Fallback specs list (env `OPENAI_FALLBACK_SPECS`)\n",
    "3. Agent fallbacks (env `OPENAI_AGENTS_FALLBACK_VERSIONS`) combined with all openai specs.\n",
    "\n",
    "Why previous attempts failed:\n",
    "- The target spec may not exist (mirror lag / version not published).\n",
    "- `openai-agents==0.4.1` might require an earlier major of `openai`.\n",
    "- Network or index restrictions prevented download.\n",
    "\n",
    "Override Examples:\n",
    "\n",
    "```bash\n",
    "export OPENAI_PREFERRED_SPEC=\"openai==1.60.1\"\n",
    "export OPENAI_FALLBACK_SPECS=\"openai==1.54.0,openai==1.40.0\"\n",
    "export OPENAI_AGENTS_PREFERRED_VERSION=\"0.3.0\"\n",
    "export OPENAI_AGENTS_FALLBACK_VERSIONS=\"0.2.0\"\n",
    "```\n",
    "\n",
    "Dry Run (no installs):\n",
    "\n",
    "```bash\n",
    "export DRY_RUN=1\n",
    "```\n",
    "\n",
    "Then rerun the first dependency cell.\n",
    "\n",
    "If ALL attempts fail:\n",
    "- Check connectivity: `pip index versions openai`\n",
    "- Try manual: `python -m pip install openai==1.60.1 openai-agents==0.3.0`\n",
    "- Consider updating notebook logic if `openai-agents` is deprecated.\n",
    "\n",
    "**Original Code (Commented):**\n",
    "\n",
    "```python\n",
    "# Exercise 3.1: Microsoft Agent Framework with MCP\n",
    "# This cell uses the higher-level agent framework to achieve the same goal.\n",
    "# It abstracts away the manual tool calling loop.\n",
    "\n",
    "# Install missing dependency that raised ModuleNotFoundError\n",
    "%pip install agentframework\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from mcp.client import session as mcp_client_session\n",
    "\n",
    "# Attempt imports with graceful fallback if package name differs\n",
    "try:\n",
    "    from agent_framework._tools import HostedMCPTool\n",
    "    from agent_framework.chat_client import AzureOpenAIChatClient\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"[ERROR] agent_framework package not found after install attempt.\")\n",
    "    print(\"If this is a private/internal library, ensure it is added to PYTHONPATH or install the correct wheel.\")\n",
    "    print(\"Expected modules: agent_framework._tools, agent_framework.chat_client\")\n",
    "    raise\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loops within Jupyter (already imported earlier but safe)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add MCP protocol v1.0 support (idempotent)\n",
    "if \"1.0\" not in mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS:\n",
    "    mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS = list(mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS) + [\"1.0\"]\n",
    "    print(f\"[PATCH] MCP supported versions: {mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS}\")\n",
    "\n",
    "# MCP URL (validated in diagnostics cell earlier)\n",
    "DOCS_MCP_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp'\n",
    "\n",
    "# Do NOT overwrite global inference_api_path used by other cells; use a local override if framework needs blank\n",
    "framework_inference_path = \"\"\n",
    "\n",
    "# Resolve api_key and model/deployment safely\n",
    "api_key_local = globals().get('api_key') or globals().get('APIM_API_KEY')\n",
    "if not api_key_local:\n",
    "    print(\"[WARN] 'api_key' not found; HostedMCPTool may fail if server requires key.\")\n",
    "\n",
    "deployment_name = globals().get('deployment_name') or globals().get('model') or 'gpt-4.1'\n",
    "\n",
    "async def run_agent_async():\n",
    "    \"\"\"\n",
    "    Asynchronously runs the agent to get sales insights using the agent framework.\n",
    "    \"\"\"\n",
    "    tool = HostedMCPTool(\n",
    "        mcp_url=DOCS_MCP_URL,\n",
    "        api_key=api_key_local,\n",
    "    )\n",
    "\n",
    "    # Initialize chat client (assumes env vars already set in earlier cells)\n",
    "    client = AzureOpenAIChatClient()\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What were the total sales for the 'Contoso' region?\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = await client.get_chat_response(\n",
    "            conversation,\n",
    "            tools=[tool],\n",
    "            use_function_invocation=True,\n",
    "            stream=False,\n",
    "            model=deployment_name,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        print(response)\n",
    "    except Exception as ex:\n",
    "        print(f\"[ERROR] Agent framework execution failed: {ex}\")\n",
    "        raise\n",
    "\n",
    "# Run the asynchronous function\n",
    "asyncio.run(run_agent_async())\n",
    "```\n",
    "\n",
    "**Skip this cell** - The notebook continues with publicly available alternatives (Semantic Kernel, OpenAI Agents, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612aa1d-af9a-43fb-8362-0b1a6249b9e4",
   "metadata": {},
   "source": [
    "## SEMANTIC KERNEL & AUTOGEN\n",
    "\n",
    "**Purpose**: Systematically test different approaches to fix Semantic Kernel + MCP hanging\n",
    "\n",
    "**Status**: Testing in progress\n",
    "**Reference**: See MCP-Test/15-TESTING-TECHNIQUES.md for full documentation\n",
    "\n",
    "### Testing Phases:\n",
    "1. ‚úÖ Baseline Tests (Techniques 1-3)\n",
    "2. üîç MCP Diagnostics (Techniques 4-6)\n",
    "3. üîÑ Alternative Frameworks (Techniques 7-8)\n",
    "4. ‚ö° Optimization (Techniques 9-12)\n",
    "5. üéØ Advanced (Techniques 13-15)\n",
    "\n",
    "**Instructions**: Run cells sequentially. Each cell logs results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0449950-c726-41e2-8000-e8ca90bf4e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:05.876920Z",
     "iopub.status.busy": "2025-11-17T18:40:05.876749Z",
     "iopub.status.idle": "2025-11-17T18:40:05.883848Z",
     "shell.execute_reply": "2025-11-17T18:40:05.883174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNIQUE 1: Direct Azure OpenAI\n",
      "======================================================================\n",
      "Purpose: Verify Azure OpenAI works through APIM\n",
      "\n",
      "\n",
      "‚ùå Error: ModuleNotFoundError: No module named 'openai'\n",
      "‚è±Ô∏è  Time: 0.00s\n",
      "\n",
      "RESULT: ‚ùå FAILED\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_202699/2878203890.py\", line 15, in <module>\n",
      "    from openai import AzureOpenAI\n",
      "ModuleNotFoundError: No module named 'openai'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TECHNIQUE 1: Direct Azure OpenAI (Baseline - No SK, No MCP)\n",
    "# ========================================================================\n",
    "import time\n",
    "\n",
    "print(\"TECHNIQUE 1: Direct Azure OpenAI\")\n",
    "print(\"=\"*70)\n",
    "print(\"Purpose: Verify Azure OpenAI works through APIM\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "result = \"NOT RUN\"\n",
    "\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=api_key,\n",
    "        api_version=inference_api_version\n",
    "    )\n",
    "    \n",
    "    print(f\"Client created. Testing with simple prompt...\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Hello from APIM!' in exactly 5 words.\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚úÖ SUCCESS\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Response: {response.choices[0].message.content}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    print(f\"üéØ Tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = f\"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nRESULT: {result}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ba31a0d-924c-49ad-8cfb-84f1b9342a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:05.886259Z",
     "iopub.status.busy": "2025-11-17T18:40:05.886048Z",
     "iopub.status.idle": "2025-11-17T18:40:05.892362Z",
     "shell.execute_reply": "2025-11-17T18:40:05.891772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNIQUE 2: Semantic Kernel Without MCP\n",
      "======================================================================\n",
      "Purpose: Verify SK works with Azure OpenAI\n",
      "\n",
      "\n",
      "‚ùå Error: ModuleNotFoundError: No module named 'semantic_kernel'\n",
      "‚è±Ô∏è  Time: 0.00s\n",
      "\n",
      "RESULT: ‚ùå FAILED\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_202699/2882390118.py\", line 20, in <module>\n",
      "    from semantic_kernel import Kernel\n",
      "ModuleNotFoundError: No module named 'semantic_kernel'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TECHNIQUE 2: Semantic Kernel Without MCP (Baseline)\n",
    "# ========================================================================\n",
    "# FIXED 2025-11-17: Updated to latest Semantic Kernel API\n",
    "# Changes:\n",
    "# 1. Removed kernel=kernel parameter from get_chat_message_contents()\n",
    "# 2. Added proper execution settings object\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"TECHNIQUE 2: Semantic Kernel Without MCP\")\n",
    "print(\"=\"*70)\n",
    "print(\"Purpose: Verify SK works with Azure OpenAI\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "result = \"NOT RUN\"\n",
    "\n",
    "try:\n",
    "    from semantic_kernel import Kernel\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "    from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "    print(\"Creating Kernel...\")\n",
    "    kernel = Kernel()\n",
    "\n",
    "    print(\"Adding Azure OpenAI service...\")\n",
    "    service = AzureChatCompletion(\n",
    "        endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=api_key,\n",
    "        api_version=inference_api_version,\n",
    "        deployment_name=deployment_name\n",
    "    )\n",
    "\n",
    "    kernel.add_service(service)\n",
    "\n",
    "    print(\"Testing with simple prompt...\")\n",
    "\n",
    "    # Create chat history\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What is 2+2? Answer with just the number.\")\n",
    "\n",
    "    # Create execution settings\n",
    "    settings = AzureChatPromptExecutionSettings(max_tokens=10)\n",
    "\n",
    "    # Get response - FIXED: removed kernel=kernel, added settings parameter\n",
    "    response = await service.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=settings\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚úÖ SUCCESS\"\n",
    "\n",
    "    print(f\"\\n‚úÖ Response: {response.content}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "\n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = f\"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nRESULT: {result}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c4f6ecb-9fb9-40eb-89b8-e6a8cbf49428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:05.894372Z",
     "iopub.status.busy": "2025-11-17T18:40:05.894126Z",
     "iopub.status.idle": "2025-11-17T18:40:05.901229Z",
     "shell.execute_reply": "2025-11-17T18:40:05.900686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNIQUE 3: ChatCompletionAgent Without MCP\n",
      "======================================================================\n",
      "Purpose: Verify SK Agent works without MCP plugin\n",
      "\n",
      "\n",
      "‚ùå Error: ModuleNotFoundError: No module named 'semantic_kernel'\n",
      "‚è±Ô∏è  Time: 0.00s\n",
      "\n",
      "RESULT: ‚ùå FAILED\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_202699/994873958.py\", line 16, in <module>\n",
      "    from semantic_kernel.agents import ChatCompletionAgent\n",
      "ModuleNotFoundError: No module named 'semantic_kernel'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TECHNIQUE 3: ChatCompletionAgent Without MCP (Baseline)\n",
    "# ========================================================================\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "print(\"TECHNIQUE 3: ChatCompletionAgent Without MCP\")\n",
    "print(\"=\"*70)\n",
    "print(\"Purpose: Verify SK Agent works without MCP plugin\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "result = \"NOT RUN\"\n",
    "\n",
    "try:\n",
    "    from semantic_kernel.agents import ChatCompletionAgent\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "    \n",
    "    print(\"Creating agent without MCP plugin...\")\n",
    "    \n",
    "    agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(\n",
    "            endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "            api_key=api_key,\n",
    "            api_version=inference_api_version,\n",
    "            deployment_name=deployment_name\n",
    "        ),\n",
    "        name=\"TestAgent\",\n",
    "        instructions=\"You are a helpful assistant. Be concise.\"\n",
    "        # NO PLUGINS - this is key\n",
    "    )\n",
    "    \n",
    "    print(\"Agent created successfully!\")\n",
    "    print(\"Testing with simple query...\")\n",
    "    \n",
    "    # Test with timeout\n",
    "    response = await asyncio.wait_for(\n",
    "        agent.get_response(messages=\"What is the capital of France? One word answer.\"),\n",
    "        timeout=30.0\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚úÖ SUCCESS\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Response: {response}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    \n",
    "except asyncio.TimeoutError:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚ö†Ô∏è  TIMEOUT\"\n",
    "    print(f\"\\n‚ö†Ô∏è  Agent timed out after 30s\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = f\"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nRESULT: {result}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "983589c2-ff56-4dc1-a725-ef49158094e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:05.902777Z",
     "iopub.status.busy": "2025-11-17T18:40:05.902628Z",
     "iopub.status.idle": "2025-11-17T18:40:06.148596Z",
     "shell.execute_reply": "2025-11-17T18:40:06.147621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNIQUE 4: Manual MCP Connection Test\n",
      "======================================================================\n",
      "Purpose: Test MCP server connectivity with timeout\n",
      "Target: http://docs-mcp-master.eastus.azurecontainer.io:8000\n",
      "\n",
      "Attempting connection with 15s timeout...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ MCP Server responding\n",
      "üì° Status Code: 404\n",
      "‚è±Ô∏è  Time: 0.24s\n",
      "\n",
      "RESULT: ‚úÖ CONNECTED (Status: 404)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TECHNIQUE 4: Manual MCP Connection with Timeout (Diagnostic)\n",
    "# ========================================================================\n",
    "import time\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "print(\"TECHNIQUE 4: Manual MCP Connection Test\")\n",
    "print(\"=\"*70)\n",
    "print(\"Purpose: Test MCP server connectivity with timeout\")\n",
    "print(f\"Target: {mcp.docs.server_url if mcp and hasattr(mcp, 'docs') else 'http://docs-mcp-master.eastus.azurecontainer.io:8000'}\")\n",
    "print()\n",
    "\n",
    "DOCS_MCP_URL = mcp.docs.server_url if (mcp and hasattr(mcp, \"docs\")) else \"http://docs-mcp-master.eastus.azurecontainer.io:8000\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = \"NOT RUN\"\n",
    "\n",
    "async def test_mcp_connection(url, timeout=15):\n",
    "    try:\n",
    "        print(f\"Attempting connection with {timeout}s timeout...\")\n",
    "        async with asyncio.timeout(timeout):\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                # Try basic health check\n",
    "                response = await client.get(url, timeout=timeout)\n",
    "                return response.status_code\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"‚ö†Ô∏è  Connection timeout after {timeout}s\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection error: {type(e).__name__}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    status = await test_mcp_connection(DOCS_MCP_URL, timeout=15)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if status:\n",
    "        result = f\"‚úÖ CONNECTED (Status: {status})\"\n",
    "        print(f\"\\n‚úÖ MCP Server responding\")\n",
    "        print(f\"üì° Status Code: {status}\")\n",
    "    else:\n",
    "        result = \"‚ö†Ô∏è  TIMEOUT/ERROR\"\n",
    "        print(f\"\\n‚ö†Ô∏è  MCP Server not accessible within timeout\")\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = f\"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\nRESULT: {result}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd9095c3-d49c-4738-9c18-33698a2f0cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:06.150841Z",
     "iopub.status.busy": "2025-11-17T18:40:06.150623Z",
     "iopub.status.idle": "2025-11-17T18:40:06.160396Z",
     "shell.execute_reply": "2025-11-17T18:40:06.159704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNIQUE 8: Direct Function Calling\n",
      "======================================================================\n",
      "Purpose: Manual function calling without agent framework\n",
      "\n",
      "\n",
      "‚ùå Error: ModuleNotFoundError: No module named 'openai'\n",
      "‚è±Ô∏è  Time: 0.00s\n",
      "\n",
      "RESULT: ‚ùå FAILED\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_202699/3769081631.py\", line 16, in <module>\n",
      "    from openai import AzureOpenAI\n",
      "ModuleNotFoundError: No module named 'openai'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TECHNIQUE 8: Direct Function Calling (No Framework)\n",
    "# ========================================================================\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"TECHNIQUE 8: Direct Function Calling\")\n",
    "print(\"=\"*70)\n",
    "print(\"Purpose: Manual function calling without agent framework\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "result = \"NOT RUN\"\n",
    "\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    import httpx\n",
    "    \n",
    "    # Define tool manually\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get current weather for a location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Creating OpenAI client...\")\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=api_key,\n",
    "        api_version=inference_api_version\n",
    "    )\n",
    "    \n",
    "    print(\"Sending initial request with tool definition...\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather in London?\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    print(f\"\\nüì® Response type: {response_message.role}\")\n",
    "    \n",
    "    # Check if tool was called\n",
    "    if response_message.tool_calls:\n",
    "        print(f\"üîß Tool called: {response_message.tool_calls[0].function.name}\")\n",
    "        print(f\"üìù Arguments: {response_message.tool_calls[0].function.arguments}\")\n",
    "        \n",
    "        # Simulate tool execution (would call actual API here)\n",
    "        tool_result = {\"temperature\": \"15¬∞C\", \"condition\": \"Cloudy\"}\n",
    "        \n",
    "        # Add tool response\n",
    "        messages.append(response_message)\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": response_message.tool_calls[0].id,\n",
    "            \"name\": response_message.tool_calls[0].function.name,\n",
    "            \"content\": json.dumps(tool_result)\n",
    "        })\n",
    "        \n",
    "        # Get final response\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Final answer: {final_response.choices[0].message.content}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nüí¨ Direct answer: {response_message.content}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚úÖ SUCCESS\"\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = f\"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nRESULT: {result}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9d246b9-99c3-4123-8b33-330037b5ccdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:06.162633Z",
     "iopub.status.busy": "2025-11-17T18:40:06.162352Z",
     "iopub.status.idle": "2025-11-17T18:40:06.172247Z",
     "shell.execute_reply": "2025-11-17T18:40:06.171407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNIQUE 15: Hybrid Approach (RECOMMENDED)\n",
      "======================================================================\n",
      "Purpose: Use SK for chat, bypass MCP plugin with direct HTTP\n",
      "\n",
      "\n",
      "‚ùå Error: ModuleNotFoundError: No module named 'semantic_kernel'\n",
      "‚è±Ô∏è  Time: 0.00s\n",
      "\n",
      "RESULT: ‚ùå FAILED\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_202699/4188947008.py\", line 23, in <module>\n",
      "    from semantic_kernel import Kernel\n",
      "ModuleNotFoundError: No module named 'semantic_kernel'\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# TECHNIQUE 15: Hybrid Approach - SK Orchestration + Direct HTTP\n",
    "# ========================================================================\n",
    "# FIXED 2025-11-17: Updated to latest Semantic Kernel API\n",
    "# Changes:\n",
    "# 1. Removed arguments=kernel.arguments (doesn't exist)\n",
    "# 2. Added proper execution settings with function calling\n",
    "# 3. Updated API call to use settings instead of arguments\n",
    "\n",
    "import time\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "print(\"TECHNIQUE 15: Hybrid Approach (RECOMMENDED)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Purpose: Use SK for chat, bypass MCP plugin with direct HTTP\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "result = \"NOT RUN\"\n",
    "\n",
    "try:\n",
    "    from semantic_kernel import Kernel\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "    from semantic_kernel.functions import kernel_function\n",
    "    from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "    from semantic_kernel.contents import ChatHistory\n",
    "\n",
    "    print(\"Creating Semantic Kernel...\")\n",
    "\n",
    "    kernel = Kernel()\n",
    "\n",
    "    service = AzureChatCompletion(\n",
    "        endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=api_key,\n",
    "        api_version=inference_api_version,\n",
    "        deployment_name=deployment_name\n",
    "    )\n",
    "\n",
    "    kernel.add_service(service)\n",
    "\n",
    "    print(\"Defining custom tools (no MCP plugin)...\")\n",
    "\n",
    "    # Define tools as native Python functions\n",
    "    class WeatherTools:\n",
    "        @kernel_function(name=\"get_weather\", description=\"Get weather for a location\")\n",
    "        async def get_weather(self, location: str) -> str:\n",
    "            \"\"\"Get weather via direct APIM call (no MCP)\"\"\"\n",
    "            try:\n",
    "                async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "                    # Call weather API directly through APIM\n",
    "                    response = await client.get(\n",
    "                        f\"{apim_gateway_url}/weather/api/current\",\n",
    "                        params={\"location\": location},\n",
    "                        headers={\"api-key\": api_key}\n",
    "                    )\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        return f\"Weather in {location}: {data.get('description', 'N/A')}, {data.get('temperature', 'N/A')}¬∞C\"\n",
    "                    else:\n",
    "                        return f\"Weather data unavailable for {location}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error getting weather: {str(e)}\"\n",
    "\n",
    "    # Add plugin\n",
    "    weather_plugin = kernel.add_plugin(WeatherTools(), \"weather\")\n",
    "\n",
    "    print(\"Testing hybrid approach...\")\n",
    "    print(\"Query: 'What's the weather in Paris?'\")\n",
    "\n",
    "    # Use SK with custom tools\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What's the weather in Paris? Use the get_weather function.\")\n",
    "\n",
    "    # Create execution settings with function calling - FIXED\n",
    "    settings = AzureChatPromptExecutionSettings()\n",
    "    settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    # Get response with function calling - FIXED: removed arguments=kernel.arguments\n",
    "    response = await asyncio.wait_for(\n",
    "        service.get_chat_message_content(\n",
    "            chat_history=history,\n",
    "            settings=settings,\n",
    "            kernel=kernel  # Kernel is needed when using function calling\n",
    "        ),\n",
    "        timeout=30.0\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚úÖ SUCCESS\"\n",
    "\n",
    "    print(f\"\\n‚úÖ Response: {response.content}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    print(\"\\nüí° Hybrid approach: SK for orchestration, HTTP for tools\")\n",
    "\n",
    "except asyncio.TimeoutError:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = \"‚ö†Ô∏è  TIMEOUT\"\n",
    "    print(f\"\\n‚ö†Ô∏è  Timed out after 30s\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "\n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    result = f\"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed:.2f}s\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nRESULT: {result}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='autogen'></a>\n",
    "### Exercise 3.3 Execute an [AutoGen Agent using MCP Tools](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html) via Azure API Management\n",
    "\n",
    "#### AutoGen Framework with Azure OpenAI + MCP\n",
    "\n",
    "![AutoGen](https://microsoft.github.io/autogen/stable/img/autogen-light.svg)\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Integrate Microsoft's [AutoGen](https://microsoft.github.io/autogen/) agent framework with Azure OpenAI and MCP servers to create AI agents that can use tools via the Model Context Protocol.\n",
    "\n",
    "**Key Features:**\n",
    "- **AutoGen Agent Orchestration**: Use AssistantAgent with tool calling capabilities\n",
    "- **MCP Tool Integration**: Connect to MCP servers via SSE (Server-Sent Events)\n",
    "- **Azure OpenAI Integration**: Use AzureOpenAIChatCompletionClient for model access\n",
    "- **Streaming Responses**: Display agent conversations via Console UI\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Configure AutoGen agents with Azure OpenAI models\n",
    "- Connect MCP servers as tool providers\n",
    "- Stream agent responses for interactive UX\n",
    "- Use multiple MCP servers in agent workflows\n",
    "\n",
    "**Prerequisites:**\n",
    "- AutoGen packages: `autogen-agentchat`, `autogen-ext`\n",
    "- MCP servers deployed and accessible\n",
    "- Azure OpenAI endpoint configured in APIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_99_a3ce812f",
   "metadata": {},
   "source": [
    "### Semantic Cache Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cell_15_85f5150d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:06.174440Z",
     "iopub.status.busy": "2025-11-17T18:40:06.174214Z",
     "iopub.status.idle": "2025-11-17T18:40:06.178319Z",
     "shell.execute_reply": "2025-11-17T18:40:06.177576Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================# LAB 01: Semantic Caching with Azure Redis# ============================================================================# FIXED 2025-11-17: Enhanced validation and error handling# Changes:# 1. Added environment variable validation# 2. Added policy verification after application# 3. Better error messages and diagnostics# 4. Added backend validationprint(\"\\n\" + \"=\"*80)print(\"LAB 01: Semantic Caching Configuration\")print(\"=\"*80 + \"\\n\")import requestsfrom azure.identity import DefaultAzureCredentialimport osimport json as json_module# Configuration with validationrequired_vars = {    'SUBSCRIPTION_ID': os.environ.get('SUBSCRIPTION_ID'),    'RESOURCE_GROUP': os.environ.get('RESOURCE_GROUP'),    'APIM_SERVICE_NAME': os.environ.get('APIM_SERVICE_NAME')}# Validate required environment variablesmissing_vars = [k for k, v in required_vars.items() if not v]if missing_vars:    print(f\"[ERROR] Missing required environment variables: {', '.join(missing_vars)}\")    print(\"[INFO] Please ensure these are set in your environment\")    raise ValueError(f\"Missing environment variables: {missing_vars}\")subscription_id = required_vars['SUBSCRIPTION_ID']resource_group = required_vars['RESOURCE_GROUP']apim_service_name = required_vars['APIM_SERVICE_NAME']# Optional variables with defaultsbackend_id = os.environ.get('APIM_BACKEND_ID', 'inference-backend-pool')embeddings_backend_id = os.environ.get('EMBEDDINGS_BACKEND_ID', 'foundry1')api_id = os.environ.get('APIM_API_ID', 'inference-api')print(f\"[config] Backend ID: {backend_id}\")print(f\"[config] Embeddings Backend ID: {embeddings_backend_id}\")print(f\"[config] Subscription ID: {subscription_id[:8]}...\")print(f\"[config] Resource Group: {resource_group}\")print(f\"[config] APIM Service: {apim_service_name}\")print(f\"[config] API ID: {api_id}\")print()# Semantic caching policy with API-KEY authenticationpolicy_xml = f\"\"\"<policies>    <inbound>        <base />        <check-header name=\"api-key\" failed-check-httpcode=\"401\"                      failed-check-error-message=\"Missing or invalid API key\" />        <azure-openai-semantic-cache-lookup            score-threshold=\"0.8\"            embeddings-backend-id=\"{embeddings_backend_id}\"            embeddings-backend-auth=\"system-assigned\" />        <set-backend-service backend-id=\"{backend_id}\" />    </inbound>    <backend>        <base />    </backend>    <outbound>        <azure-openai-semantic-cache-store duration=\"120\" />        <base />    </outbound>    <on-error>        <base />    </on-error></policies>\"\"\"# Apply policy using direct REST APIprint(\"[policy] Applying semantic-cache policy via Azure Management API...\")try:    # Get Azure credentials    credential = DefaultAzureCredential()    token = credential.get_token(\"https://management.azure.com/.default\")    # Construct API Management URL    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"    headers = {        \"Authorization\": f\"Bearer {token.token}\",        \"Content-Type\": \"application/json\"    }    body = {        \"properties\": {            \"value\": policy_xml,            \"format\": \"xml\"        }    }    # Apply policy    response = requests.put(url, headers=headers, json=body, timeout=60)    if response.status_code in [200, 201]:        print(f\"[policy] ‚úÖ Status: {response.status_code} - Policy applied successfully\")        # ADDED: Verify policy was applied        print(\"[policy] Verifying policy application...\")        verify_response = requests.get(url, headers=headers, timeout=30)        if verify_response.status_code == 200:            # Handle UTF-8 BOM in response            try:                response_text = verify_response.text                if response_text.startswith('\\ufeff'):                    response_text = response_text[1:]  # Remove BOM                applied_policy = json_module.loads(response_text)            except json_module.JSONDecodeError as e:                print(f\"[policy] ‚ùå JSON decode error: {e}\")                print(f\"[policy]    Response text (first 200 chars): {response_text[:200]}\")                applied_policy = {}            if 'azure-openai-semantic-cache-lookup' in applied_policy.get('properties', {}).get('value', ''):                print(\"[policy] ‚úÖ Verification: Semantic caching policy confirmed active\")            else:                print(\"[policy] ‚ö†Ô∏è  Verification: Policy applied but semantic caching not found\")        else:            print(f\"[policy] ‚ö†Ô∏è  Verification failed: {verify_response.status_code}\")    elif response.status_code == 404:        print(f\"[policy] ‚ùå Status: 404 - API not found\")        print(f\"[policy] API ID '{api_id}' does not exist in APIM service '{apim_service_name}'\")        print(f\"[policy] Available APIs can be listed with Cell 21 (API discovery)\")    elif response.status_code == 401:        print(f\"[policy] ‚ùå Status: 401 - Authentication failed\")        print(f\"[policy] Ensure DefaultAzureCredential has access to the subscription\")    elif response.status_code == 400:        print(f\"[policy] ‚ùå Status: 400 - Bad request\")        print(f\"[policy] Error: {response.text[:500]}\")        print(f\"[policy] Check that backend IDs exist:\")        print(f\"[policy]   - Backend pool: {backend_id}\")        print(f\"[policy]   - Embeddings backend: {embeddings_backend_id}\")    else:        print(f\"[policy] ‚ùå Status: {response.status_code} - Failed\")        print(f\"[policy] Error: {response.text[:500]}\")except Exception as e:    print(f\"[policy] ‚ùå ERROR: {type(e).__name__}: {str(e)}\")    print(\"\\n[TROUBLESHOOTING]\")    print(\"  1. Ensure Azure credentials are configured (az login)\")    print(\"  2. Verify you have Contributor access to the APIM resource\")    print(\"  3. Check that APIM service name and resource group are correct\")    print(f\"  4. Verify API ID '{api_id}' exists (run Cell 21 for API discovery)\")    print(\"  5. Ensure embeddings backend exists in APIM\")print(\"\\n[INFO] Policy propagation takes ~30-60 seconds\")print(\"[NEXT] Run the test cells below to verify semantic caching behavior\")print(\"       - First request: Cache MISS (full latency)\")print(\"       - Second request: Cache HIT (near-instant response)\")print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cell_100_d388e9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:06.181067Z",
     "iopub.status.busy": "2025-11-17T18:40:06.180745Z",
     "iopub.status.idle": "2025-11-17T18:40:40.918890Z",
     "shell.execute_reply": "2025-11-17T18:40:40.918370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Semantic Caching Performance Test\n",
      "================================================================================\n",
      "üìç Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "ü§ñ Model: gpt-4o-mini\n",
      "üìä Running 20 requests with semantic caching...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  1: 2.234s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  2: 1.046s | Cache: UNKNOWN  | Question: Tell me about coffee preparation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  3: 0.896s | Cache: UNKNOWN  | Question: Coffee making tips?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  4: 1.757s | Cache: UNKNOWN  | Question: Coffee making tips?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  5: 1.496s | Cache: UNKNOWN  | Question: Coffee making tips?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  6: 1.121s | Cache: UNKNOWN  | Question: Coffee making tips?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  7: 0.879s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  8: 1.092s | Cache: UNKNOWN  | Question: Tell me about coffee preparation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request  9: 0.980s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 10: 1.201s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 11: 1.455s | Cache: UNKNOWN  | Question: How to make coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 12: 1.171s | Cache: UNKNOWN  | Question: Coffee making tips?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 13: 1.578s | Cache: UNKNOWN  | Question: Tell me about coffee preparation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 14: 0.715s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 15: 1.179s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 16: 1.140s | Cache: UNKNOWN  | Question: How to make coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 17: 1.023s | Cache: UNKNOWN  | Question: Tell me about coffee preparation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 18: 1.276s | Cache: UNKNOWN  | Question: Coffee making tips?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 19: 1.294s | Cache: UNKNOWN  | Question: What is the best way to brew coffee?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Request 20: 1.183s | Cache: UNKNOWN  | Question: Tell me about coffee preparation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä Results Summary:\n",
      "   Total Requests: 20\n",
      "   Cache Hits: 0 (0.0%)\n",
      "   Cache Misses: 20 (100.0%)\n",
      "   Average Time: 1.236s\n",
      "   Min Time: 0.715s\n",
      "   Max Time: 2.234s\n",
      "\n",
      "‚ö†Ô∏è  Matplotlib not available - skipping visualization\n",
      "\n",
      "‚úÖ Lab 19: Semantic Caching Test Complete!\n"
     ]
    }
   ],
   "source": [
    "# Lab 19: Semantic Caching Performance Test\n",
    "print(\"üîÑ Semantic Caching Performance Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import random\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "# Test questions with semantic similarity\n",
    "questions = [\n",
    "    'How to make coffee?',\n",
    "    'What is the best way to brew coffee?',\n",
    "    'Tell me about coffee preparation',\n",
    "    'Coffee making tips?'\n",
    "]\n",
    "\n",
    "times = []\n",
    "cache_hits = []\n",
    "cache_misses = []\n",
    "\n",
    "# Initialize client if missing\n",
    "if 'apim_gateway_url' not in globals() or 'apim_api_key' not in globals():\n",
    "    print(\"‚ùå Missing APIM configuration. Please ensure Cell 14 has been run.\")\n",
    "    print(\"   Required: apim_gateway_url, apim_api_key, inference_api_path\")\n",
    "else:\n",
    "    # Build endpoint URL\n",
    "    endpoint_base = apim_gateway_url.rstrip('/')\n",
    "    api_path = globals().get('inference_api_path', 'inference').strip('/')\n",
    "    deployment = 'gpt-4o-mini'\n",
    "    api_version = globals().get('api_version', '2024-06-01')\n",
    "    \n",
    "    url = f\"{endpoint_base}/{api_path}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': apim_api_key,\n",
    "        'Ocp-Apim-Subscription-Key': apim_api_key\n",
    "    }\n",
    "    \n",
    "    print(f\"üìç Endpoint: {endpoint_base}/{api_path}\")\n",
    "    print(f\"ü§ñ Model: {deployment}\")\n",
    "    print(f\"üìä Running 20 requests with semantic caching...\")\n",
    "    print()\n",
    "    \n",
    "    # Run 20 requests\n",
    "    for i in range(20):\n",
    "        question = random.choice(questions)\n",
    "        \n",
    "        payload = {\n",
    "            'messages': [{'role': 'user', 'content': question}],\n",
    "            'max_tokens': 50,\n",
    "            'temperature': 0.7\n",
    "        }\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = httpx.post(url, json=payload, headers=headers, timeout=30.0)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            # Check cache status from response headers\n",
    "            x_cache = response.headers.get('x-cache', 'UNKNOWN')\n",
    "            x_cache_hits = response.headers.get('x-cache-hits', '0')\n",
    "            \n",
    "            # Alternative cache header names used by different systems\n",
    "            cache_status = (\n",
    "                response.headers.get('x-cache') or\n",
    "                response.headers.get('X-Cache') or\n",
    "                response.headers.get('x-azure-cache') or\n",
    "                response.headers.get('CF-Cache-Status') or\n",
    "                'UNKNOWN'\n",
    "            )\n",
    "            \n",
    "            is_cached = cache_status.upper() in ['HIT', 'CACHED']\n",
    "            \n",
    "            times.append(elapsed)\n",
    "            \n",
    "            if is_cached:\n",
    "                cache_hits.append(i + 1)\n",
    "                status_icon = \"‚úÖ\"\n",
    "            else:\n",
    "                cache_misses.append(i + 1)\n",
    "                status_icon = \"üîÑ\"\n",
    "            \n",
    "            print(f\"{status_icon} Request {i+1:2d}: {elapsed:.3f}s | Cache: {cache_status:8s} | Question: {question[:40]}...\")\n",
    "            \n",
    "            # Check for errors\n",
    "            if response.status_code != 200:\n",
    "                print(f\"   ‚ö†Ô∏è  HTTP {response.status_code}: {response.text[:100]}\")\n",
    "            \n",
    "        except httpx.TimeoutException:\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "            cache_misses.append(i + 1)\n",
    "            print(f\"‚è±Ô∏è  Request {i+1:2d}: TIMEOUT after {elapsed:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "            cache_misses.append(i + 1)\n",
    "            print(f\"‚ùå Request {i+1:2d}: ERROR - {type(e).__name__}: {str(e)[:60]}\")\n",
    "        \n",
    "        time.sleep(0.5)  # Brief pause between requests\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Results Summary:\")\n",
    "    print(f\"   Total Requests: 20\")\n",
    "    print(f\"   Cache Hits: {len(cache_hits)} ({len(cache_hits)/20*100:.1f}%)\")\n",
    "    print(f\"   Cache Misses: {len(cache_misses)} ({len(cache_misses)/20*100:.1f}%)\")\n",
    "    print(f\"   Average Time: {sum(times)/len(times):.3f}s\")\n",
    "    print(f\"   Min Time: {min(times):.3f}s\")\n",
    "    print(f\"   Max Time: {max(times):.3f}s\")\n",
    "    \n",
    "    # Visualize results\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Request': range(1, len(times) + 1),\n",
    "            'Time (s)': times,\n",
    "            'Cached': ['Hit' if i in cache_hits else 'Miss' for i in range(1, len(times) + 1)]\n",
    "        })\n",
    "        \n",
    "        # Create bar chart with different colors for hits/misses\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        \n",
    "        colors = ['green' if i in cache_hits else 'blue' for i in range(1, len(times) + 1)]\n",
    "        ax.bar(df['Request'], df['Time (s)'], color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.axhline(y=df['Time (s)'].mean(), color='red', linestyle='--', label=f'Average: {df[\"Time (s)\"].mean():.3f}s')\n",
    "        ax.set_xlabel('Request Number')\n",
    "        ax.set_ylabel('Response Time (seconds)')\n",
    "        ax.set_title('Semantic Caching Performance\\n(Green=Cache Hit, Blue=Cache Miss)')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n‚ö†Ô∏è  Matplotlib not available - skipping visualization\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Visualization error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Lab 19: Semantic Caching Test Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_101_5380e749",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Model Context Protocol (MCP) Integration\n",
    "\n",
    "The following labs demonstrate integration with MCP servers for extended AI capabilities:\n",
    "\n",
    "- **Lab 11:** Weather MCP - Real-time weather data integration\n",
    "- **Lab 12:** Weather + AI Analysis - Combine weather data with AI insights\n",
    "- **Lab 14:** GitHub Repository Access - GitHub integration via MCP\n",
    "- **Lab 15:** GitHub + AI Code Analysis - AI-powered code analysis\n",
    "- **Lab 23:** Multi-Server Orchestration - Coordinate multiple MCP servers\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_102_0c38e64a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Advanced Features\n",
    "\n",
    "The following labs cover advanced capabilities:\n",
    "\n",
    "- **Lab 19:** Semantic Caching - Performance optimization with Redis\n",
    "- **Lab 22:** Image Generation - Multi-modal image generation with DALL-E\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_103_55fe7a99",
   "metadata": {},
   "source": [
    "<a id='lab22'></a>\n",
    "## Lab 22: Image Generation\n",
    "## üé® Image Generation and multi-modal analysis + Authentication using JWT\n",
    "![flow](../../images/image-gen.gif)\n",
    "\n",
    "DALL-E 3 and FLUX image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_104_a23fe370",
   "metadata": {},
   "source": [
    "### Test: Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cell_108_e274080d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:40.921585Z",
     "iopub.status.busy": "2025-11-17T18:40:40.921336Z",
     "iopub.status.idle": "2025-11-17T18:40:40.925923Z",
     "shell.execute_reply": "2025-11-17T18:40:40.925365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lab 22: Image Generation & Vision Analysis (Consolidated)print(\"üé® Image Generation & Vision Analysis via APIM Gateway\")print(\"=\" * 80)import osimport base64import jsonimport requestsfrom typing import Optionalfrom IPython.display import displayimport matplotlib.pyplot as pltimport iotry:    import PIL.Image as PILImageexcept ImportError:    print(\"‚ö†Ô∏è  PIL not available - installing...\")    %pip install Pillow    import PIL.Image as PILImage# =============================================================================# Configuration from Environment# =============================================================================# APIM Gateway Configurationapim_gateway_url = os.getenv(\"APIM_GATEWAY_URL\") or globals().get(\"apim_gateway_url\")apim_api_key = os.getenv(\"APIM_API_KEY\") or globals().get(\"apim_api_key\")inference_api_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")# Model ConfigurationIMAGE_MODEL = os.getenv(\"DALL_E_DEPLOYMENT\", \"dall-e-3\")VISION_MODEL = os.getenv(\"VISION_MODEL\", \"gpt-4o-mini\")IMAGE_API_VERSION = os.getenv(\"OPENAI_IMAGE_API_VERSION\", \"2024-08-01-preview\")CHAT_API_VERSION = os.getenv(\"OPENAI_CHAT_API_VERSION\", \"2024-06-01\")DEFAULT_SIZE = os.getenv(\"DALL_E_DEFAULT_SIZE\", \"1024x1024\")# Validate configurationif not apim_gateway_url or not apim_api_key:    raise RuntimeError(\"Missing APIM configuration. Please run Cell 14 to load environment.\")print(f\"‚úÖ APIM Gateway: {apim_gateway_url}\")print(f\"‚úÖ Image Model: {IMAGE_MODEL}\")print(f\"‚úÖ Vision Model: {VISION_MODEL}\")# =============================================================================# Image Generation Function# =============================================================================def generate_image(    prompt: str,    model: str = IMAGE_MODEL,    size: str = DEFAULT_SIZE,    quality: str = \"standard\") -> dict:    \"\"\"    Generate image using DALL-E through APIM Gateway        Args:        prompt: Text description of image to generate        model: Model deployment name (default: dall-e-3)        size: Image size (1024x1024, 1792x1024, or 1024x1792)        quality: Image quality (standard or hd)        Returns:        dict with 'url' or 'b64_json' and 'revised_prompt'    \"\"\"    # Try direct foundry endpoint first, fallback to APIM    dalle_endpoint = os.getenv(\"MODEL_DALL_E_3_ENDPOINT_R1\")    dalle_key_env = os.getenv(\"MODEL_DALL_E_3_KEY_R1\")        if dalle_endpoint and dalle_key_env:        endpoint = dalle_endpoint.rstrip('/')        endpoint_key = dalle_key_env        print(f\"   Using direct foundry endpoint (bypassing APIM)\")    else:        endpoint = f\"{apim_gateway_url.rstrip('/')}/{inference_api_path.strip('/')}\"        endpoint_key = apim_api_key        print(f\"   Using APIM gateway endpoint\")    url = f\"{endpoint}/openai/deployments/{model}/images/generations?api-version={IMAGE_API_VERSION}\"        headers = {        \"Content-Type\": \"application/json\",        \"api-key\": endpoint_key,        \"Ocp-Apim-Subscription-Key\": endpoint_key  # APIM subscription key header    }        payload = {        \"prompt\": prompt,        \"size\": size,        \"n\": 1,        \"quality\": quality,        \"response_format\": \"b64_json\"  # Get base64 for direct display    }        try:        response = requests.post(url, json=payload, headers=headers, timeout=60.0)        response.raise_for_status()                result = response.json()        image_data = result['data'][0]                return {            'success': True,            'b64_json': image_data.get('b64_json'),            'url': image_data.get('url'),            'revised_prompt': image_data.get('revised_prompt', prompt),            'model': model,            'size': size        }            except requests.exceptions.HTTPError as e:        error_detail = e.response.text if e.response else str(e)        print(f\"‚ùå HTTP Error {e.response.status_code if e.response else 'N/A'}: {error_detail}\")        return {'success': False, 'error': error_detail}            except Exception as e:        print(f\"‚ùå Error: {type(e).__name__}: {str(e)}\")        return {'success': False, 'error': str(e)}# =============================================================================# Vision Analysis Function# =============================================================================def analyze_image(    image_b64: str,    question: str = \"Describe this image in detail\",    model: str = VISION_MODEL) -> str:    \"\"\"    Analyze image using GPT-4 Vision through APIM Gateway        Args:        image_b64: Base64-encoded image data        question: Question about the image        model: Vision model deployment name        Returns:        str: Analysis result from vision model    \"\"\"    # Try direct foundry endpoint first, fallback to APIM    dalle_endpoint = os.getenv(\"MODEL_DALL_E_3_ENDPOINT_R1\")    dalle_key_env = os.getenv(\"MODEL_DALL_E_3_KEY_R1\")        if dalle_endpoint and dalle_key_env:        endpoint = dalle_endpoint.rstrip('/')        endpoint_key = dalle_key_env        print(f\"   Using direct foundry endpoint (bypassing APIM)\")    else:        endpoint = f\"{apim_gateway_url.rstrip('/')}/{inference_api_path.strip('/')}\"        endpoint_key = apim_api_key        print(f\"   Using APIM gateway endpoint\")    url = f\"{endpoint}/openai/deployments/{model}/chat/completions?api-version={CHAT_API_VERSION}\"        headers = {        \"Content-Type\": \"application/json\",        \"api-key\": endpoint_key,        \"Ocp-Apim-Subscription-Key\": endpoint_key  # APIM subscription key header    }        # Construct vision message    payload = {        \"messages\": [            {                \"role\": \"user\",                \"content\": [                    {\"type\": \"text\", \"text\": question},                    {                        \"type\": \"image_url\",                        \"image_url\": {                            \"url\": f\"data:image/png;base64,{image_b64}\"                        }                    }                ]            }        ],        \"max_tokens\": 500,        \"temperature\": 0.7    }        try:        response = requests.post(url, json=payload, headers=headers, timeout=60.0)        response.raise_for_status()                result = response.json()        return result['choices'][0]['message']['content']            except Exception as e:        print(f\"‚ùå Vision analysis error: {e}\")        return f\"Error: {str(e)}\"# =============================================================================# Helper Function to Display Image# =============================================================================def display_generated_image(result: dict):    \"\"\"Display generated image from result dict\"\"\"    if not result.get('success'):        print(f\"‚ùå Cannot display - generation failed\")        return        b64_data = result.get('b64_json')    if not b64_data:        print(f\"‚ùå No image data in result\")        return        try:        image_bytes = base64.b64decode(b64_data)        image = PILImage.open(io.BytesIO(image_bytes))                plt.figure(figsize=(10, 10))        plt.imshow(image)        plt.axis('off')        plt.title(f\"Generated: {result.get('revised_prompt', '')[:60]}...\")        plt.tight_layout()        plt.show()                print(f\"\\n‚úÖ Image size: {image.size}\")        print(f\"‚úÖ Revised prompt: {result.get('revised_prompt')}\")            except Exception as e:        print(f\"‚ùå Display error: {e}\")# =============================================================================# Demo: Generate and Analyze Image# =============================================================================print(\"\\nüé® Generating test image...\")print(\"-\" * 80)test_prompt = \"A futuristic Azure data center shaped like a cloud, with glowing blue lights and modern architecture\"result = generate_image(    prompt=test_prompt,    model=IMAGE_MODEL,    size=\"1024x1024\",    quality=\"standard\")if result.get('success'):    print(f\"\\n‚úÖ Image generated successfully!\")    print(f\"   Model: {result['model']}\")    print(f\"   Size: {result['size']}\")    print(f\"   Revised prompt: {result['revised_prompt'][:80]}...\")        # Display the image    display_generated_image(result)        # Analyze the generated image with vision model    print(f\"\\nüîç Analyzing generated image with {VISION_MODEL}...\")    print(\"-\" * 80)        analysis = analyze_image(        image_b64=result['b64_json'],        question=\"Describe this image in detail, focusing on the architecture and visual elements.\",        model=VISION_MODEL    )        print(f\"\\n[VISION ANALYSIS]\\n{analysis}\")        # Store for potential reuse    generated_image_result = result    generated_image_analysis = analysis    else:    print(f\"\\n‚ùå Image generation failed\")    if 'error' in result:        print(f\"   Error: {result['error']}\")print(\"\\n\" + \"=\" * 80)print(\"‚úÖ Lab 22 Complete!\")print(\"\\nüí° Functions available:\")print(\"   - generate_image(prompt, model, size, quality)\")print(\"   - analyze_image(image_b64, question, model)\")print(\"   - display_generated_image(result)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_118_8471f6a8",
   "metadata": {},
   "source": [
    "### Lab 01: Test - Temperature Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cell_119_100f4b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:40.927670Z",
     "iopub.status.busy": "2025-11-17T18:40:40.927495Z",
     "iopub.status.idle": "2025-11-17T18:40:40.950220Z",
     "shell.execute_reply": "2025-11-17T18:40:40.949644Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m1.5\u001b[39m, \u001b[32m2.0\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      3\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWrite a creative sentence\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      5\u001b[39m         temperature=temp,\n\u001b[32m      6\u001b[39m         max_tokens=\u001b[32m30\u001b[39m\n\u001b[32m      7\u001b[39m     )\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTemp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "for temp in [0.0, 0.5, 1.0, 1.5, 2.0]:\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'Write a creative sentence'}],\n",
    "        temperature=temp,\n",
    "        max_tokens=30\n",
    "    )\n",
    "    print(f'Temp {temp}: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_120_67bfb0f6",
   "metadata": {},
   "source": [
    "### Lab 01: Test - System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cell_121_f0a2faf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:40.952184Z",
     "iopub.status.busy": "2025-11-17T18:40:40.952005Z",
     "iopub.status.idle": "2025-11-17T18:40:40.969974Z",
     "shell.execute_reply": "2025-11-17T18:40:40.969463Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m system_prompts = [\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a sarcastic comedian.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a professional technical writer.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a poet.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m system_prompts:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     10\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m         messages=[\n\u001b[32m     12\u001b[39m             {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: prompt},\n\u001b[32m     13\u001b[39m             {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mDescribe the weather\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m     14\u001b[39m         ],\n\u001b[32m     15\u001b[39m         max_tokens=\u001b[32m50\u001b[39m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "system_prompts = [\n",
    "    'You are a helpful assistant.',\n",
    "    'You are a sarcastic comedian.',\n",
    "    'You are a professional technical writer.',\n",
    "    'You are a poet.'\n",
    "]\n",
    "\n",
    "for prompt in system_prompts:\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': prompt},\n",
    "            {'role': 'user', 'content': 'Describe the weather'}\n",
    "        ],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f'\\n{prompt}:\\n{response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_126_b9751fe0",
   "metadata": {},
   "source": [
    "### Lab 19: Test - Redis Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cell_127_07af662f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:40.972163Z",
     "iopub.status.busy": "2025-11-17T18:40:40.971992Z",
     "iopub.status.idle": "2025-11-17T18:40:40.994642Z",
     "shell.execute_reply": "2025-11-17T18:40:40.993887Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'redis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mredis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masyncio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mredis\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Resolve Redis connection settings without redefining earlier variables if already present\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Prefer existing globals, then environment (.env / master-lab.env), then step3_outputs\u001b[39;00m\n\u001b[32m      5\u001b[39m redis_host = \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mredis_host\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os.getenv(\u001b[33m'\u001b[39m\u001b[33mREDIS_HOST\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m step3_outputs.get(\u001b[33m'\u001b[39m\u001b[33mredisCacheHost\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'redis'"
     ]
    }
   ],
   "source": [
    "import redis.asyncio as redis\n",
    "\n",
    "# Resolve Redis connection settings without redefining earlier variables if already present\n",
    "# Prefer existing globals, then environment (.env / master-lab.env), then step3_outputs\n",
    "redis_host = globals().get('redis_host') or os.getenv('REDIS_HOST') or step3_outputs.get('redisCacheHost')\n",
    "redis_port_raw = globals().get('redis_port') or os.getenv('REDIS_PORT') or step3_outputs.get('redisCachePort', 6380)\n",
    "redis_key = globals().get('redis_key') or os.getenv('REDIS_KEY') or step3_outputs.get('redisCacheKey')\n",
    "\n",
    "# Normalize port\n",
    "try:\n",
    "    redis_port = int(redis_port_raw)\n",
    "except Exception:\n",
    "    redis_port = 6380  # fallback typical TLS port\n",
    "\n",
    "if not all([redis_host, redis_port, redis_key]):\n",
    "    raise ValueError('Missing Redis configuration (host/port/key). Ensure master-lab.env is generated and loaded.')\n",
    "\n",
    "async def test_redis():\n",
    "    # rediss (TLS). Decode responses for convenience.\n",
    "    url = f'rediss://:{redis_key}@{redis_host}:{redis_port}'\n",
    "    # OPTION B: Add socket_connect_timeout and socket_timeout parameters\n",
    "    r = await redis.from_url(\n",
    "        url,\n",
    "        encoding='utf-8',\n",
    "        decode_responses=True,\n",
    "        socket_connect_timeout=5,  # 5 second connection timeout\n",
    "        socket_timeout=5            # 5 second socket timeout\n",
    "    )\n",
    "    try:\n",
    "        info = await r.info()\n",
    "        print(f'[OK] Connected to Redis at {redis_host}:{redis_port}')\n",
    "        print(f'Redis Version      : {info.get(\"redis_version\")}')\n",
    "        print(f'Connected Clients  : {info.get(\"connected_clients\")}')\n",
    "        print(f'Used Memory        : {info.get(\"used_memory_human\")}')\n",
    "    finally:\n",
    "        await r.aclose()\n",
    "\n",
    "await test_redis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_144_db8e9800",
   "metadata": {},
   "source": [
    "### Lab 14: A2A Agents - Multi-Agent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cell_145_8d7506f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:40.996488Z",
     "iopub.status.busy": "2025-11-17T18:40:40.996344Z",
     "iopub.status.idle": "2025-11-17T18:40:41.006877Z",
     "shell.execute_reply": "2025-11-17T18:40:41.005513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent-to-Agent (A2A) Communication with Real AutoGen Agents\n",
      "================================================================================\n",
      "‚ùå AutoGen not installed: No module named 'autogen'\n",
      "\n",
      "To install AutoGen, run:\n",
      "   pip install pyautogen\n"
     ]
    }
   ],
   "source": [
    "# Lab 21: Agent-to-Agent (A2A) Communication with AutoGen\n",
    "print(\"ü§ñ Agent-to-Agent (A2A) Communication with Real AutoGen Agents\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    from autogen import ConversableAgent, config_list_from_json, config_list_from_dotenv\n",
    "    import os\n",
    "    \n",
    "    # Build Azure OpenAI configuration for AutoGen\n",
    "    config_list = []\n",
    "    \n",
    "    # Try to use APIM gateway configuration\n",
    "    if 'apim_gateway_url' in globals() and 'apim_api_key' in globals():\n",
    "        endpoint_base = apim_gateway_url.rstrip('/')\n",
    "        api_path = globals().get('inference_api_path', 'inference').strip('/')\n",
    "        \n",
    "        config_list.append({\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"api_type\": \"azure\",\n",
    "            \"api_key\": apim_api_key,\n",
    "            \"base_url\": f\"{endpoint_base}/{api_path}\",\n",
    "            \"api_version\": globals().get('api_version', '2024-06-01')\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Using APIM Gateway: {endpoint_base}/{api_path}\")\n",
    "    \n",
    "    # Fallback to environment variables if APIM not configured\n",
    "    if not config_list:\n",
    "        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        azure_key = os.getenv('AZURE_OPENAI_KEY')\n",
    "        \n",
    "        if azure_endpoint and azure_key:\n",
    "            config_list.append({\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"api_type\": \"azure\",\n",
    "                \"api_key\": azure_key,\n",
    "                \"base_url\": azure_endpoint,\n",
    "                \"api_version\": \"2024-06-01\"\n",
    "            })\n",
    "            print(f\"‚úÖ Using Azure OpenAI from environment\")\n",
    "    \n",
    "    if not config_list:\n",
    "        raise ValueError(\"No Azure OpenAI configuration found. Please run Cell 14 or set environment variables.\")\n",
    "    \n",
    "    # Configure LLM settings\n",
    "    llm_config = {\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.7,\n",
    "        \"timeout\": 120,\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìã Creating AutoGen Agents...\")\n",
    "    \n",
    "    # === Planner Agent ===\n",
    "    planner = ConversableAgent(\n",
    "        name=\"Planner\",\n",
    "        system_message=(\n",
    "            \"You are a strategic planner specialized in AI infrastructure deployment. \"\n",
    "            \"Your role is to create comprehensive, step-by-step plans for scaling AI Gateway systems. \"\n",
    "            \"Focus on: architecture design, resource allocation, deployment strategy, and timeline. \"\n",
    "            \"Be specific and actionable.\"\n",
    "        ),\n",
    "        llm_config=llm_config,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=1\n",
    "    )\n",
    "    \n",
    "    # === Critic Agent ===\n",
    "    critic = ConversableAgent(\n",
    "        name=\"Critic\",\n",
    "        system_message=(\n",
    "            \"You are a security and reliability expert who reviews deployment plans. \"\n",
    "            \"Your role is to identify risks, vulnerabilities, missing considerations, and potential failures. \"\n",
    "            \"Focus on: security gaps, scalability issues, cost concerns, and operational risks. \"\n",
    "            \"Provide specific, actionable feedback.\"\n",
    "        ),\n",
    "        llm_config=llm_config,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=1\n",
    "    )\n",
    "    \n",
    "    # === Summarizer Agent ===\n",
    "    summarizer = ConversableAgent(\n",
    "        name=\"Summarizer\",\n",
    "        system_message=(\n",
    "            \"You are a technical documentation expert who synthesizes information. \"\n",
    "            \"Your role is to combine the planner's strategy with the critic's feedback into a final, improved plan. \"\n",
    "            \"Create a clear, structured document with sections: Objectives, Key Steps, Risks, Mitigations, Timeline. \"\n",
    "            \"Be concise but comprehensive.\"\n",
    "        ),\n",
    "        llm_config=llm_config,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=1\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úÖ Planner agent created\")\n",
    "    print(\"   ‚úÖ Critic agent created\")\n",
    "    print(\"   ‚úÖ Summarizer agent created\")\n",
    "    \n",
    "    # === A2A Communication Flow ===\n",
    "    print(\"\\nüîÑ Starting A2A Communication Flow...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Task for the agents\n",
    "    task = (\n",
    "        \"Create a deployment plan for scaling an Azure AI Gateway that currently handles \"\n",
    "        \"1000 requests/day to handle 100,000 requests/day. The gateway uses APIM, Azure OpenAI, \"\n",
    "        \"Redis caching, and Cosmos DB. Consider multi-region deployment, load balancing, \"\n",
    "        \"security, cost optimization, and disaster recovery.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù Task: {task}\\n\")\n",
    "    \n",
    "    # Step 1: Planner creates initial plan\n",
    "    print(\"\\nüéØ Step 1: Planner creates deployment strategy...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    planner_response = planner.generate_reply(\n",
    "        messages=[{\"role\": \"user\", \"content\": task}]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[PLANNER OUTPUT]\\n{planner_response}\\n\")\n",
    "    \n",
    "    # Step 2: Critic reviews the plan\n",
    "    print(\"\\nüîç Step 2: Critic reviews plan for risks and gaps...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    critic_prompt = (\n",
    "        f\"Review this deployment plan and identify risks, security concerns, \"\n",
    "        f\"missing considerations, and potential failures:\\n\\n{planner_response}\"\n",
    "    )\n",
    "    \n",
    "    critic_response = critic.generate_reply(\n",
    "        messages=[{\"role\": \"user\", \"content\": critic_prompt}]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[CRITIC OUTPUT]\\n{critic_response}\\n\")\n",
    "    \n",
    "    # Step 3: Summarizer creates final improved plan\n",
    "    print(\"\\nüìä Step 3: Summarizer creates final improved plan...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    summarizer_prompt = (\n",
    "        f\"Combine the planner's strategy with the critic's feedback to create a final, \"\n",
    "        f\"improved deployment plan. Include sections: Objectives, Key Steps, Risks, \"\n",
    "        f\"Mitigations, and Timeline.\\n\\n\"\n",
    "        f\"PLANNER'S PROPOSAL:\\n{planner_response}\\n\\n\"\n",
    "        f\"CRITIC'S FEEDBACK:\\n{critic_response}\"\n",
    "    )\n",
    "    \n",
    "    final_plan = summarizer.generate_reply(\n",
    "        messages=[{\"role\": \"user\", \"content\": summarizer_prompt}]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[FINAL PLAN]\\n{final_plan}\\n\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n‚úÖ A2A Communication Complete!\")\n",
    "    print(\"\\nüìà Agent Interaction Summary:\")\n",
    "    print(f\"   1. Planner created strategic deployment plan ({len(planner_response)} chars)\")\n",
    "    print(f\"   2. Critic identified risks and gaps ({len(critic_response)} chars)\")\n",
    "    print(f\"   3. Summarizer produced final improved plan ({len(final_plan)} chars)\")\n",
    "    print(f\"\\nüí° This demonstrates real agent-to-agent collaboration where:\")\n",
    "    print(f\"   - Each agent has specialized expertise\")\n",
    "    print(f\"   - Agents build upon each other's outputs\")\n",
    "    print(f\"   - Final result is better than any single agent could produce\")\n",
    "    \n",
    "    # Store agents for potential reuse\n",
    "    agents = {\n",
    "        'planner': planner,\n",
    "        'critic': critic,\n",
    "        'summarizer': summarizer\n",
    "    }\n",
    "    \n",
    "    # Store final plan\n",
    "    a2a_final_plan = final_plan\n",
    "    \n",
    "    print(\"\\n‚úÖ Lab 21 Complete!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå AutoGen not installed: {e}\")\n",
    "    print(\"\\nTo install AutoGen, run:\")\n",
    "    print(\"   pip install pyautogen\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during A2A communication: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_146_75db1c21",
   "metadata": {},
   "source": [
    "### Lab 15: OpenAI Agents - Create Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cell_147_dd3f8cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:41.009777Z",
     "iopub.status.busy": "2025-11-17T18:40:41.009472Z",
     "iopub.status.idle": "2025-11-17T18:40:42.059493Z",
     "shell.execute_reply": "2025-11-17T18:40:42.058565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent: 90cbe8cf-2fa9-4103-9acb-50dfcb4aeee4\n",
      "Created thread: 22baba31-ce5b-486b-9abd-c0885c0fcaf2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m run.status \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mqueued\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33min_progress\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    136\u001b[39m     time.sleep(\u001b[32m0.5\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     run = \u001b[43magents_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Get response\u001b[39;00m\n\u001b[32m    140\u001b[39m messages = agents_client.messages.list(thread_id=thread.id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36m_AgentsClientStub.runs.get\u001b[39m\u001b[34m(thread_id, run_id)\u001b[39m\n\u001b[32m     86\u001b[39m user_msgs = [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m thread.messages \u001b[38;5;28;01mif\u001b[39;00m m.role == \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     87\u001b[39m user_content = user_msgs[-\u001b[32m1\u001b[39m].content[\u001b[32m0\u001b[39m].text.value \u001b[38;5;28;01mif\u001b[39;00m user_msgs \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m completion = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     89\u001b[39m     model=agent.model,\n\u001b[32m     90\u001b[39m     messages=[\n\u001b[32m     91\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: agent.instructions},\n\u001b[32m     92\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: user_content}\n\u001b[32m     93\u001b[39m     ],\n\u001b[32m     94\u001b[39m     max_tokens=\u001b[32m150\u001b[39m\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m assistant_text = completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     97\u001b[39m thread.messages.append(_Message(\u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m, assistant_text))\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Using Azure AI Agents (fallback stub if project_client is not defined)\n",
    "\n",
    "if 'project_client' not in globals():\n",
    "    # Minimal in-memory stub to avoid NameError and simulate Agents API behavior\n",
    "    import uuid\n",
    "\n",
    "    class _TextWrapper:\n",
    "        def __init__(self, value): self.value = value\n",
    "\n",
    "    class _ContentPart:\n",
    "        def __init__(self, value): self.text = _TextWrapper(value)\n",
    "\n",
    "    class _Message:\n",
    "        def __init__(self, role, content):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.role = role\n",
    "            # Match expected access pattern: msg.content[0].text.value\n",
    "            self.content = [_ContentPart(content)]\n",
    "\n",
    "    class _Agent:\n",
    "        def __init__(self, model, name, instructions):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.model = model\n",
    "            self.name = name\n",
    "            self.instructions = instructions\n",
    "\n",
    "    class _Thread:\n",
    "        def __init__(self):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.messages = []\n",
    "\n",
    "    class _Run:\n",
    "        def __init__(self, thread_id, agent_id):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.thread_id = thread_id\n",
    "            self.agent_id = agent_id\n",
    "            self.status = 'queued'\n",
    "\n",
    "    class _AgentsClientStub:\n",
    "        def __init__(self):\n",
    "            self._agents = {}\n",
    "            self._threads = {}\n",
    "            self._runs = {}\n",
    "\n",
    "        def create_agent(self, model, name, instructions):\n",
    "            agent = _Agent(model, name, instructions)\n",
    "            self._agents[agent.id] = agent\n",
    "            return agent\n",
    "\n",
    "        class threads:\n",
    "            @staticmethod\n",
    "            def create():\n",
    "                thread = _Thread()\n",
    "                _agents_client_stub._threads[thread.id] = thread\n",
    "                return thread\n",
    "\n",
    "        class messages:\n",
    "            @staticmethod\n",
    "            def create(thread_id, role, content):\n",
    "                thread = _agents_client_stub._threads[thread_id]\n",
    "                msg = _Message(role, content)\n",
    "                thread.messages.append(msg)\n",
    "                return msg\n",
    "\n",
    "            @staticmethod\n",
    "            def list(thread_id):\n",
    "                return _agents_client_stub._threads[thread_id].messages\n",
    "\n",
    "        class runs:\n",
    "            @staticmethod\n",
    "            def create(thread_id, agent_id):\n",
    "                run = _Run(thread_id, agent_id)\n",
    "                _agents_client_stub._runs[run.id] = run\n",
    "                return run\n",
    "\n",
    "            @staticmethod\n",
    "            def get(thread_id, run_id):\n",
    "                run = _agents_client_stub._runs[run_id]\n",
    "                if run.status == 'queued':\n",
    "                    run.status = 'in_progress'\n",
    "                elif run.status == 'in_progress':\n",
    "                    # Perform completion using existing Azure OpenAI client\n",
    "                    agent = _agents_client_stub._agents[run.agent_id]\n",
    "                    thread = _agents_client_stub._threads[run.thread_id]\n",
    "                    # Use last user message content\n",
    "                    user_msgs = [m for m in thread.messages if m.role == 'user']\n",
    "                    user_content = user_msgs[-1].content[0].text.value if user_msgs else \"Hello\"\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=agent.model,\n",
    "                        messages=[\n",
    "                            {'role': 'system', 'content': agent.instructions},\n",
    "                            {'role': 'user', 'content': user_content}\n",
    "                        ],\n",
    "                        max_tokens=150\n",
    "                    )\n",
    "                    assistant_text = completion.choices[0].message.content\n",
    "                    thread.messages.append(_Message('assistant', assistant_text))\n",
    "                    run.status = 'completed'\n",
    "                return run\n",
    "\n",
    "        def delete_agent(self, agent_id):\n",
    "            self._agents.pop(agent_id, None)\n",
    "\n",
    "    _agents_client_stub = _AgentsClientStub()\n",
    "    project_client = type('ProjectClientStub', (), {'agents': _agents_client_stub})()\n",
    "\n",
    "agents_client = project_client.agents\n",
    "\n",
    "# Create agent\n",
    "agent = agents_client.create_agent(\n",
    "    model='gpt-4o-mini',\n",
    "    name='test-assistant',\n",
    "    instructions='You are a helpful assistant.'\n",
    ")\n",
    "print(f'Created agent: {agent.id}')\n",
    "\n",
    "# Create thread\n",
    "thread = agents_client.threads.create()\n",
    "print(f'Created thread: {thread.id}')\n",
    "\n",
    "# Send message\n",
    "message = agents_client.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role='user',\n",
    "    content='What is Azure?'\n",
    ")\n",
    "\n",
    "# Run\n",
    "run = agents_client.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    agent_id=agent.id\n",
    ")\n",
    "\n",
    "# Wait for completion (stub transitions statuses internally)\n",
    "while run.status in ['queued', 'in_progress']:\n",
    "    time.sleep(0.5)\n",
    "    run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "# Get response\n",
    "messages = agents_client.messages.list(thread_id=thread.id)\n",
    "for msg in messages:\n",
    "    if msg.role == 'assistant':\n",
    "        print(f'Assistant: {msg.content[0].text.value}')\n",
    "\n",
    "# Cleanup\n",
    "agents_client.delete_agent(agent.id)\n",
    "print('[OK] Agent test complete (stubbed if no real project_client)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_148_3bece681",
   "metadata": {},
   "source": [
    "### Lab 16: AI Agent Service - Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cell_149_3f484305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.061754Z",
     "iopub.status.busy": "2025-11-17T18:40:42.061606Z",
     "iopub.status.idle": "2025-11-17T18:40:42.500995Z",
     "shell.execute_reply": "2025-11-17T18:40:42.499798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent Service: multi-agent test...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m done = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m pending:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     run_obj = \u001b[43magents_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthread_multi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_obj.status == \u001b[33m'\u001b[39m\u001b[33mcompleted\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     33\u001b[39m         done.append(name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36m_AgentsClientStub.runs.get\u001b[39m\u001b[34m(thread_id, run_id)\u001b[39m\n\u001b[32m     86\u001b[39m user_msgs = [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m thread.messages \u001b[38;5;28;01mif\u001b[39;00m m.role == \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     87\u001b[39m user_content = user_msgs[-\u001b[32m1\u001b[39m].content[\u001b[32m0\u001b[39m].text.value \u001b[38;5;28;01mif\u001b[39;00m user_msgs \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m completion = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     89\u001b[39m     model=agent.model,\n\u001b[32m     90\u001b[39m     messages=[\n\u001b[32m     91\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: agent.instructions},\n\u001b[32m     92\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: user_content}\n\u001b[32m     93\u001b[39m     ],\n\u001b[32m     94\u001b[39m     max_tokens=\u001b[32m150\u001b[39m\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m assistant_text = completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     97\u001b[39m thread.messages.append(_Message(\u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m, assistant_text))\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Multi-agent scenario (planning, critic, summarizer) using existing agents_client + client\n",
    "print('AI Agent Service: multi-agent test...')\n",
    "\n",
    "# Create agents\n",
    "agents = {\n",
    "    'planner': agents_client.create_agent(model='gpt-4o-mini', name='planner', instructions='Plan a concise Azure AI workshop agenda.'),\n",
    "    'critic': agents_client.create_agent(model='gpt-4o-mini', name='critic', instructions='Review a proposed agenda and point out gaps.'),\n",
    "    'summarizer': agents_client.create_agent(model='gpt-4o-mini', name='summarizer', instructions='Summarize multiple agenda perspectives clearly.')\n",
    "}\n",
    "\n",
    "# Shared thread\n",
    "thread_multi = agents_client.threads.create()\n",
    "\n",
    "# Initial user request\n",
    "agents_client.messages.create(\n",
    "    thread_id=thread_multi.id,\n",
    "    role='user',\n",
    "    content='Create a 2-hour Azure AI workshop focusing on deployment, security, and MCP integrations.'\n",
    ")\n",
    "\n",
    "# Run each agent\n",
    "runs = {name: agents_client.runs.create(thread_id=thread_multi.id, agent_id=agent.id) for name, agent in agents.items()}\n",
    "\n",
    "# Poll until all complete\n",
    "pending = set(runs.keys())\n",
    "while pending:\n",
    "    done = []\n",
    "    for name in pending:\n",
    "        run_obj = agents_client.runs.get(thread_id=thread_multi.id, run_id=runs[name].id)\n",
    "        if run_obj.status == 'completed':\n",
    "            done.append(name)\n",
    "    for d in done:\n",
    "        pending.remove(d)\n",
    "    if pending:\n",
    "        time.sleep(0.4)\n",
    "\n",
    "# Collect assistant messages\n",
    "msgs = agents_client.messages.list(thread_id=thread_multi.id)\n",
    "agent_outputs = []\n",
    "for m in msgs:\n",
    "    if m.role == 'assistant':\n",
    "        agent_outputs.append(m.content[0].text.value)\n",
    "\n",
    "# Combine via summarizer (final synthesis)\n",
    "summary_prompt = \"Combine these agent outputs into a single refined workshop plan:\\n\\n\" + \"\\n\\n---\\n\\n\".join(agent_outputs)\n",
    "agents_client.messages.create(thread_id=thread_multi.id, role='user', content=summary_prompt)\n",
    "final_run = agents_client.runs.create(thread_id=thread_multi.id, agent_id=agents['summarizer'].id)\n",
    "while True:\n",
    "    final_run = agents_client.runs.get(thread_id=thread_multi.id, run_id=final_run.id)\n",
    "    if final_run.status == 'completed':\n",
    "        break\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Extract final summary\n",
    "final_msgs = agents_client.messages.list(thread_id=thread_multi.id)\n",
    "final_response = [m.content[0].text.value for m in final_msgs if m.role == 'assistant'][-1]\n",
    "\n",
    "print('\\n[RESULT] Multi-agent workshop synthesis:\\n')\n",
    "print(final_response[:2000])  # truncate if very long\n",
    "\n",
    "# Cleanup\n",
    "for a in agents.values():\n",
    "    agents_client.delete_agent(a.id)\n",
    "print('\\n[OK] Multi-agent test complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_150_e2c4205d",
   "metadata": {},
   "source": [
    "### Lab 18: Function Calling - Multiple Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cell_151_52d1da7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.502941Z",
     "iopub.status.busy": "2025-11-17T18:40:42.502742Z",
     "iopub.status.idle": "2025-11-17T18:40:42.524393Z",
     "shell.execute_reply": "2025-11-17T18:40:42.523820Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m      1\u001b[39m functions = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mget_weather\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     }\n\u001b[32m     26\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     29\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     30\u001b[39m     messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWhat is 15 + 27?\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m     31\u001b[39m     functions=functions,\n\u001b[32m     32\u001b[39m     function_call=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.function_call:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFunction called: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.function_call.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "functions = [\n",
    "    {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get weather for a location',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'location': {'type': 'string', 'description': 'City name'}\n",
    "            },\n",
    "            'required': ['location']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'calculate',\n",
    "        'description': 'Perform calculation',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'operation': {'type': 'string', 'enum': ['add', 'subtract', 'multiply', 'divide']},\n",
    "                'a': {'type': 'number'},\n",
    "                'b': {'type': 'number'}\n",
    "            },\n",
    "            'required': ['operation', 'a', 'b']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[{'role': 'user', 'content': 'What is 15 + 27?'}],\n",
    "    functions=functions,\n",
    "    function_call='auto'\n",
    ")\n",
    "\n",
    "if response.choices[0].message.function_call:\n",
    "    print(f'Function called: {response.choices[0].message.function_call.name}')\n",
    "    print(f'Arguments: {response.choices[0].message.function_call.arguments}')\n",
    "else:\n",
    "    print('No function called')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_152_5c4f338d",
   "metadata": {},
   "source": [
    "### Lab 20: Message Storing - Store and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cell_153_5fc4f592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.526431Z",
     "iopub.status.busy": "2025-11-17T18:40:42.526167Z",
     "iopub.status.idle": "2025-11-17T18:40:42.564407Z",
     "shell.execute_reply": "2025-11-17T18:40:42.563821Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure.cosmos'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cosmos DB message storage with auto-firewall configuration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcosmos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CosmosClient, PartitionKey\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcosmos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CosmosHttpResponseError\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01msubprocess\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure.cosmos'"
     ]
    }
   ],
   "source": [
    "# Cosmos DB message storage with auto-firewall configuration\n",
    "\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from azure.cosmos.exceptions import CosmosHttpResponseError\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Resolve endpoint/key (prefer existing vars, then env, then deployment outputs; guard missing step3_outputs)\n",
    "_step3 = globals().get('step3_outputs', {}) or {}\n",
    "cosmos_endpoint = globals().get('cosmos_endpoint') or os.getenv('COSMOS_ENDPOINT') or _step3.get('cosmosDbEndpoint')\n",
    "cosmos_key = globals().get('cosmos_key') or os.getenv('COSMOS_KEY') or _step3.get('cosmosDbKey')\n",
    "\n",
    "if not cosmos_endpoint or not cosmos_key:\n",
    "    print('[WARN] Cosmos DB configuration missing (endpoint/key) - persistence disabled')\n",
    "    cosmos_enabled = False\n",
    "else:\n",
    "    cosmos_enabled = True\n",
    "\n",
    "# OPTION A: Auto-configure Cosmos DB firewall\n",
    "def auto_configure_cosmos_firewall():\n",
    "    \"\"\"Automatically add current IP to Cosmos DB firewall\"\"\"\n",
    "    try:\n",
    "        # Get current IP\n",
    "        print('[auto-fix] Detecting current IP address...')\n",
    "        result = subprocess.run(['curl', '-s', 'ifconfig.me'], capture_output=True, text=True, timeout=5)\n",
    "        current_ip = result.stdout.strip()\n",
    "        print(f'[auto-fix] Current IP: {current_ip}')\n",
    "\n",
    "        # Get Cosmos account name\n",
    "        cosmos_account = os.environ.get('COSMOS_ACCOUNT_NAME') or 'cosmos-pavavy6pu5hpa'\n",
    "        resource_group = os.environ.get('RESOURCE_GROUP') or 'lab-master-lab'\n",
    "\n",
    "        # Add IP to firewall\n",
    "        print(f'[auto-fix] Adding IP {current_ip} to Cosmos DB firewall...')\n",
    "        result = subprocess.run([\n",
    "            'az', 'cosmosdb', 'update',\n",
    "            '--resource-group', resource_group,\n",
    "            '--name', cosmos_account,\n",
    "            '--ip-range-filter', current_ip\n",
    "        ], capture_output=True, text=True, timeout=30)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print('[auto-fix] ‚úÖ Firewall updated successfully. Waiting 10s for propagation...')\n",
    "            time.sleep(10)\n",
    "            return True\n",
    "        else:\n",
    "            print(f'[auto-fix] ‚ùå Failed: {result.stderr[:200]}')\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'[auto-fix] ‚ùå Auto-configuration failed: {e}')\n",
    "        return False\n",
    "\n",
    "# Initialize client once (only if enabled)\n",
    "if cosmos_enabled and 'cosmos_client' not in globals():\n",
    "    try:\n",
    "        cred_obj = credential if 'credential' in globals() else cosmos_key\n",
    "        cosmos_client = CosmosClient(cosmos_endpoint, credential=cred_obj)\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] CosmosClient init failed: {e}')\n",
    "        cosmos_enabled = False\n",
    "\n",
    "db_name = 'chatStore'\n",
    "container_name = 'messages'\n",
    "container = None\n",
    "\n",
    "# Create database / container if network/firewall permits\n",
    "if cosmos_enabled:\n",
    "    try:\n",
    "        database = cosmos_client.create_database_if_not_exists(id=db_name)\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path='/threadId'),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except CosmosHttpResponseError as e:\n",
    "        if getattr(e, 'status_code', None) == 403:\n",
    "            print('[WARN] Cosmos DB access forbidden (likely firewall). Attempting auto-fix...')\n",
    "\n",
    "            # Try auto-fix\n",
    "            if auto_configure_cosmos_firewall():\n",
    "                # Retry connection\n",
    "                print('[auto-fix] Retrying Cosmos DB connection...')\n",
    "                try:\n",
    "                    database = cosmos_client.create_database_if_not_exists(id=db_name)\n",
    "                    container = database.create_container_if_not_exists(\n",
    "                        id=container_name,\n",
    "                        partition_key=PartitionKey(path='/threadId'),\n",
    "                        offer_throughput=400\n",
    "                    )\n",
    "                    print('[auto-fix] ‚úÖ Successfully connected to Cosmos DB after firewall update')\n",
    "                except Exception as retry_ex:\n",
    "                    print(f'[auto-fix] ‚ùå Connection still failed after firewall update: {retry_ex}')\n",
    "                    cosmos_enabled = False\n",
    "            else:\n",
    "                print('')\n",
    "                print('üìã MANUAL FIX REQUIRED:')\n",
    "                print('   Azure Portal ‚Üí Cosmos DB ‚Üí Networking ‚Üí Add my current IP ‚Üí Save')\n",
    "                print('')\n",
    "                cosmos_enabled = False\n",
    "        else:\n",
    "            print(f'[ERROR] Init Cosmos unexpected HTTP error: {e}')\n",
    "            cosmos_enabled = False\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Init Cosmos: {e}')\n",
    "        cosmos_enabled = False\n",
    "\n",
    "def store_chat_messages(thread_id: str, msgs: list):\n",
    "    \"\"\"\n",
    "    Persist chat messages (list of {'role','content'}) to Cosmos DB (if enabled),\n",
    "    otherwise no-op without raising errors.\n",
    "    \"\"\"\n",
    "    if not cosmos_enabled or container is None:\n",
    "        print('[INFO] Cosmos disabled; skipping message persistence')\n",
    "        return\n",
    "    stored = 0\n",
    "    for idx, m in enumerate(msgs):\n",
    "        try:\n",
    "            doc = {\n",
    "                'id': f'{thread_id}-{idx}',\n",
    "                'threadId': thread_id,\n",
    "                'index': idx,\n",
    "                'role': m.get('role'),\n",
    "                'content': m.get('content'),\n",
    "            }\n",
    "            container.upsert_item(doc)\n",
    "            stored += 1\n",
    "        except CosmosHttpResponseError as ex:\n",
    "            if getattr(ex, 'status_code', None) == 403:\n",
    "                print('[WARN] Firewall blocked mid-write; disabling persistence')\n",
    "                break\n",
    "            else:\n",
    "                print(f'[WARN] HTTP store failure {idx}: {ex}')\n",
    "        except Exception as ex:\n",
    "            print(f'[WARN] Failed to store message {idx}: {ex}')\n",
    "    print(f'[OK] Stored {stored}/{len(msgs)} messages in Cosmos DB' if cosmos_enabled else '[INFO] No messages stored')\n",
    "\n",
    "# Example: store existing conversation if available\n",
    "if 'conversation' in globals():\n",
    "    store_chat_messages('conv-001', conversation)\n",
    "else:\n",
    "    print('[INFO] No conversation variable found to persist')\n",
    "\n",
    "print(f'Cosmos DB endpoint: {cosmos_endpoint}')\n",
    "print(f'[OK] Message storage {\"enabled\" if cosmos_enabled else \"disabled\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_156_11cc16c9",
   "metadata": {},
   "source": [
    "### Lab 24: FinOps Framework - Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cell_157_e7744376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.566371Z",
     "iopub.status.busy": "2025-11-17T18:40:42.566219Z",
     "iopub.status.idle": "2025-11-17T18:40:42.585590Z",
     "shell.execute_reply": "2025-11-17T18:40:42.584975Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m costs = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      5\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      7\u001b[39m         max_tokens=\u001b[32m50\u001b[39m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Estimate cost (example rates)\u001b[39;00m\n\u001b[32m     10\u001b[39m     prompt_cost = response.usage.prompt_tokens * \u001b[32m0.00015\u001b[39m / \u001b[32m1000\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Simulate cost tracking\n",
    "costs = []\n",
    "for i in range(10):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'test'}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    # Estimate cost (example rates)\n",
    "    prompt_cost = response.usage.prompt_tokens * 0.00015 / 1000\n",
    "    completion_cost = response.usage.completion_tokens * 0.00060 / 1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    costs.append(total_cost)\n",
    "\n",
    "print(f'Total estimated cost: ${sum(costs):.6f}')\n",
    "print(f'Average per request: ${sum(costs)/len(costs):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_162_9ad6860a",
   "metadata": {},
   "source": [
    "<a id='kql'></a>\n",
    "### üîç Display LLM logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cell_163_e86d22f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.587402Z",
     "iopub.status.busy": "2025-11-17T18:40:42.587269Z",
     "iopub.status.idle": "2025-11-17T18:40:42.616333Z",
     "shell.execute_reply": "2025-11-17T18:40:42.615749Z"
    },
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mlet llmHeaderLogs = ApiManagementGatewayLlmLog \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33m| where DeploymentName != \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m; \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33mlet llmLogsWithSubscriptionId = llmHeaderLogs \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33m    SumTotalTokens      = sum(TotalTokens) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33m  by SubscriptionId, DeploymentName\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Resolve Log Analytics workspace/customer ID from existing globals or environment\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"let llmHeaderLogs = ApiManagementGatewayLlmLog \\\n",
    "| where DeploymentName != ''; \\\n",
    "let llmLogsWithSubscriptionId = llmHeaderLogs \\\n",
    "| join kind=leftouter ApiManagementGatewayLogs on CorrelationId \\\n",
    "| project \\\n",
    "    SubscriptionId = ApimSubscriptionId, DeploymentName, TotalTokens; \\\n",
    "llmLogsWithSubscriptionId \\\n",
    "| summarize \\\n",
    "    SumTotalTokens      = sum(TotalTokens) \\\n",
    "  by SubscriptionId, DeploymentName\"\n",
    "\n",
    "# Resolve Log Analytics workspace/customer ID from existing globals or environment\n",
    "if 'log_analytics_id' not in globals() or not log_analytics_id:\n",
    "    log_analytics_id = (\n",
    "        os.getenv('LOG_ANALYTICS_WORKSPACE_ID')\n",
    "        or (step1_outputs.get('logAnalyticsWorkspaceId') if 'step1_outputs' in globals() else None)\n",
    "        or (step1_outputs.get('logAnalyticsCustomerId') if 'step1_outputs' in globals() else None)\n",
    "    )\n",
    "\n",
    "if not log_analytics_id:\n",
    "    print('[WARN] Log Analytics workspace ID not configured - analytics features disabled')\n",
    "    print('       This is a monitoring feature and notebook continues without it.')\n",
    "    print('')\n",
    "    print('üìã TO ENABLE VIA CLI:')\n",
    "    print('   # List Log Analytics workspaces')\n",
    "    print('   az monitor log-analytics workspace list --resource-group lab-master-lab \\\\')\n",
    "    print('     --query \"[].{name:name, customerId:customerId, location:location}\" -o table')\n",
    "    print('')\n",
    "    print('   # Get workspace details')\n",
    "    print('   WORKSPACE_NAME=$(az monitor log-analytics workspace list --resource-group lab-master-lab \\\\')\n",
    "    print('     --query \"[0].name\" -o tsv)')\n",
    "    print('   WORKSPACE_ID=$(az monitor log-analytics workspace show --resource-group lab-master-lab \\\\')\n",
    "    print('     --workspace-name $WORKSPACE_NAME --query customerId -o tsv)')\n",
    "    print('   export LOG_ANALYTICS_WORKSPACE_ID=$WORKSPACE_ID')\n",
    "    print('')\n",
    "    print('   # Check if APIM is sending logs to workspace')\n",
    "    print('   az monitor diagnostic-settings list \\\\')\n",
    "    print('     --resource /subscriptions/$(az account show --query id -o tsv)/resourceGroups/lab-master-lab/providers/Microsoft.ApiManagement/service/apim-pavavy6pu5hpa \\\\')\n",
    "    print('     --query \"value[].{name:name, workspaceId:workspaceId}\" -o table')\n",
    "    print('')\n",
    "    print('   # If no diagnostic settings, create one:')\n",
    "    print('   APIM_ID=\"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/lab-master-lab/providers/Microsoft.ApiManagement/service/apim-pavavy6pu5hpa\"')\n",
    "    print('   WORKSPACE_RESOURCE_ID=\"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/lab-master-lab/providers/Microsoft.OperationalInsights/workspaces/$WORKSPACE_NAME\"')\n",
    "    print('   az monitor diagnostic-settings create --resource $APIM_ID --name \"apim-to-log-analytics\" \\\\')\n",
    "    print('     --workspace $WORKSPACE_RESOURCE_ID \\\\')\n",
    "    print('     --logs \\'[{\"category\": \"GatewayLogs\", \"enabled\": true}]\\' \\\\')\n",
    "    print('     --metrics \\'[{\"category\": \"AllMetrics\", \"enabled\": true}]\\'')\n",
    "    print('')\n",
    "    print('   # Note: Log Analytics has 5-10 minute ingestion delay - wait before querying')\n",
    "    print('')\n",
    "    print('üìã TO ENABLE VIA PORTAL:')\n",
    "    print('   1. Azure Portal ‚Üí Log Analytics workspaces ‚Üí Copy Workspace ID')\n",
    "    print('   2. Azure Portal ‚Üí API Management ‚Üí Diagnostic settings ‚Üí Add diagnostic setting')\n",
    "    print('   3. Select: GatewayLogs, AllMetrics ‚Üí Send to Log Analytics workspace')\n",
    "    print('   4. Save and wait 10 minutes for data ingestion')\n",
    "    print('')\n",
    "    print('[OK] Log analytics features: disabled (optional)')\n",
    "    print('')\n",
    "    analytics_enabled = False\n",
    "else:\n",
    "    # Try to run the query\n",
    "    try:\n",
    "        output = utils.run(\n",
    "            f\"az monitor log-analytics query -w {log_analytics_id} --analytics-query \\\"{query}\\\"\",\n",
    "            \"Retrieved log analytics query output\",\n",
    "            \"Failed to retrieve log analytics query output\"\n",
    "        )\n",
    "        if output.success and output.json_data:\n",
    "            table = output.json_data\n",
    "            # Normalize typical Azure Monitor response into a DataFrame if needed\n",
    "            if isinstance(table, dict) and 'tables' in table and table['tables']:\n",
    "                t = table['tables'][0]\n",
    "                cols = [c.get('name') for c in t.get('columns', [])]\n",
    "                rows = t.get('rows', [])\n",
    "                df = pd.DataFrame(rows, columns=cols or None)\n",
    "            else:\n",
    "                df = pd.DataFrame(table)\n",
    "            display(df)\n",
    "            analytics_enabled = True\n",
    "        else:\n",
    "            print(f'[WARN] Log Analytics query failed (may not have data yet)')\n",
    "            print('       Wait 10 minutes after enabling diagnostic settings for data to appear')\n",
    "            print('')\n",
    "            analytics_enabled = False\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Log Analytics query error: {e}')\n",
    "        print('       Verify workspace ID is correct and diagnostic settings are enabled')\n",
    "        print('')\n",
    "        analytics_enabled = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28819f5b",
   "metadata": {},
   "source": [
    "### Exercise 2.6: AI-Generated Sales Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_201_4805765f",
   "metadata": {},
   "source": [
    "### Agent Dependency Note for MCP Labs\n",
    "\n",
    "- Ensures `openai>=2.2,<3` to satisfy `openai-agents==0.4.1`.\n",
    "- Performs on-demand install/upgrade only if version mismatch or module missing.\n",
    "\n",
    "\n",
    "To force a clean reinstall manually:\n",
    "\n",
    "```bash\n",
    "pip uninstall -y openai openai-agents\n",
    "pip install \"openai>=2.2,<3\" openai-agents==0.4.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Phase 3: Advanced Semantic Kernel + AutoGen Features\n",
    "\n",
    "This section demonstrates advanced agentic AI patterns using:\n",
    "- **Semantic Kernel 1.x**: Plugins, function calling, streaming, agents, vector search\n",
    "- **AutoGen**: Multi-agent conversations, tool registration, orchestration\n",
    "- **Hybrid Patterns**: Combining SK and AutoGen capabilities\n",
    "\n",
    "All demonstrations route through the APIM AI Gateway configured in earlier sections.\n",
    "\n",
    "**Prerequisites**:\n",
    "- All earlier cells executed successfully\n",
    "- Variables available: `apim_gateway_url`, `subscription_key_both`, `headers_both`, `deployment_name`\n",
    "- Packages installed: `semantic-kernel>=1.0.0`, `pyautogen>=0.2.0`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 1: SK Plugin for Gateway-Routed Function Calling\n",
    "\n",
    "**Purpose**: SK Plugin for Gateway-Routed Function Calling\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.618435Z",
     "iopub.status.busy": "2025-11-17T18:40:42.618246Z",
     "iopub.status.idle": "2025-11-17T18:40:42.660546Z",
     "shell.execute_reply": "2025-11-17T18:40:42.659804Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01masyncio\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernel_function\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatCompletion\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_kernel'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Plugin with Function Calling via APIM Gateway\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK plugin creation with kernel_function decorator\n",
    "- Automatic function calling with FunctionChoiceBehavior.Auto()\n",
    "- Routing SK chat completion through APIM gateway\n",
    "- Multi-step planning with automatic function invocation\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Function Calling Plugin via APIM Gateway\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create SK Plugin with Kernel Functions\n",
    "# ============================================================================\n",
    "\n",
    "class WorkshopPlugin:\n",
    "    \"\"\"Custom plugin for AI Gateway workshop demonstrations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get the current UTC time\")\n",
    "    def get_current_time(self) -> str:\n",
    "        \"\"\"Returns current UTC time in ISO format.\"\"\"\n",
    "        return datetime.utcnow().isoformat()\n",
    "\n",
    "    @kernel_function(description=\"Get weather information for a city\")\n",
    "    def get_weather(self, city: str) -> str:\n",
    "        \"\"\"\n",
    "        Get simulated weather for a city.\n",
    "\n",
    "        Args:\n",
    "            city: Name of the city\n",
    "        \"\"\"\n",
    "        # Simulated weather data\n",
    "        weather_data = {\n",
    "            \"seattle\": \"Rainy, 55¬∞F (13¬∞C)\",\n",
    "            \"san francisco\": \"Foggy, 62¬∞F (17¬∞C)\",\n",
    "            \"boston\": \"Cloudy, 48¬∞F (9¬∞C)\",\n",
    "            \"paris\": \"Partly cloudy, 15¬∞C (59¬∞F)\",\n",
    "        }\n",
    "        city_lower = city.lower()\n",
    "        return weather_data.get(city_lower, f\"Weather data unavailable for {city}\")\n",
    "\n",
    "    @kernel_function(description=\"Calculate the square of a number\")\n",
    "    def calculate_square(self, number: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate square of a number.\n",
    "\n",
    "        Args:\n",
    "            number: Number to square\n",
    "        \"\"\"\n",
    "        return number * number\n",
    "\n",
    "print(\"\\n‚úì Workshop plugin created with 3 functions\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Configure Custom Azure OpenAI Client for APIM\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure gateway URL is available from existing notebook variables\n",
    "if 'apim_gateway_url' not in globals():\n",
    "    if 'APIM_GATEWAY_URL' in globals():\n",
    "        apim_gateway_url = APIM_GATEWAY_URL\n",
    "    elif 'step1_outputs' in globals():\n",
    "        apim_gateway_url = step1_outputs.get('apimGatewayUrl')\n",
    "    else:\n",
    "        raise RuntimeError(\"APIM gateway URL not found. Define APIM_GATEWAY_URL or step1_outputs['apimGatewayUrl'].\")\n",
    "\n",
    "# Derive subscription key if not already defined\n",
    "if 'subscription_key_both' not in globals():\n",
    "    if 'APIM_API_KEY' in globals():\n",
    "        subscription_key_both = APIM_API_KEY\n",
    "    elif 'subs' in globals() and isinstance(subs, list) and subs:\n",
    "        subscription_key_both = subs[0].get('key')\n",
    "    elif 'step1_outputs' in globals():\n",
    "        # Try to pull a key from apimSubscriptions array if present\n",
    "        subs_list = step1_outputs.get('apimSubscriptions', [])\n",
    "        subscription_key_both = next(\n",
    "            (s.get('primaryKey') or s.get('key') for s in subs_list if isinstance(s, dict)),\n",
    "            None\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"Unable to derive subscription key. Define subscription_key_both manually.\")\n",
    "    if not subscription_key_both:\n",
    "        raise RuntimeError(\"Derived subscription_key_both is empty. Provide a valid APIM subscription key.\")\n",
    "\n",
    "# Prepare headers if not already present\n",
    "if 'headers_both' not in globals():\n",
    "    headers_both = {\n",
    "        \"api-key\": subscription_key_both,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "# Create custom client pointing to APIM gateway (ensure correct inference path to avoid 404)\n",
    "# Normalize and append inference path (expected by APIM route rewrite)\n",
    "if 'inference_api_path' not in globals():\n",
    "    if 'INFERENCE_API_PATH' in globals():\n",
    "        inference_api_path = INFERENCE_API_PATH.strip('/')\n",
    "    elif 'step2_outputs' in globals():\n",
    "        inference_api_path = step2_outputs.get('inferenceAPIPath', 'inference').strip('/')\n",
    "    else:\n",
    "        inference_api_path = 'inference'\n",
    "\n",
    "# Ensure single trailing slash on base\n",
    "base_url = apim_gateway_url.rstrip('/') + '/'\n",
    "gateway_inference_endpoint = base_url + inference_api_path\n",
    "\n",
    "# Update/openai_endpoint variable (fix earlier missing slash issue)\n",
    "openai_endpoint = gateway_inference_endpoint\n",
    "\n",
    "custom_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=gateway_inference_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,  # From existing notebook variables\n",
    "    default_headers=headers_both    # From existing notebook variables\n",
    ")\n",
    "\n",
    "print(\"‚úì Custom Azure OpenAI client configured for APIM gateway\")\n",
    "print(f\"  Base Gateway URL: {apim_gateway_url}\")\n",
    "print(f\"  Inference Endpoint: {gateway_inference_endpoint}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Initialize Semantic Kernel with Plugin\n",
    "# ============================================================================\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service with custom client\n",
    "chat_service = AzureChatCompletion(\n",
    "    service_id=\"apim_chat\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=custom_client,\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Add the workshop plugin\n",
    "kernel.add_plugin(\n",
    "    WorkshopPlugin(),\n",
    "    plugin_name=\"Workshop\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Semantic Kernel initialized\")\n",
    "print(\"  Service: Azure OpenAI via APIM\")\n",
    "print(\"  Plugin: WorkshopPlugin (3 functions)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Configure Auto Function Calling\n",
    "# ============================================================================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"apim_chat\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Enable automatic function calling\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"‚úì Execution settings configured\")\n",
    "print(\"  Function calling: Automatic\")\n",
    "print(\"  Max tokens: 500\")\n",
    "print(\"  Temperature: 0.7\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Function Calling Examples\n",
    "# ============================================================================\n",
    "\n",
    "async def run_sk_function_calling():\n",
    "    \"\"\"Execute SK function calling examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Simple Function Call\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create chat history\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What time is it right now?\")\n",
    "\n",
    "    # Get response (SK will automatically call get_current_time function)\n",
    "    result = await chat_service.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What time is it right now?\")\n",
    "    print(f\"Assistant: {result}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Multi-Step Function Calling\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history2 = ChatHistory()\n",
    "    history2.add_user_message(\n",
    "        \"What's the weather in Seattle and what's the square of 12?\"\n",
    "    )\n",
    "\n",
    "    result2 = await chat_service.get_chat_message_content(\n",
    "        chat_history=history2,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What's the weather in Seattle and what's the square of 12?\")\n",
    "    print(f\"Assistant: {result2}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Complex Planning\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history3 = ChatHistory()\n",
    "    history3.add_user_message(\n",
    "        \"First tell me the current time, then check the weather in Paris, \"\n",
    "        \"and finally calculate the square of 7. Present all results.\"\n",
    "    )\n",
    "\n",
    "    result3 = await chat_service.get_chat_message_content(\n",
    "        chat_history=history3,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: First tell me the current time, then check the weather in Paris,\")\n",
    "    print(f\"      and finally calculate the square of 7. Present all results.\")\n",
    "    print(f\"Assistant: {result3}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FUNCTION CALLING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total examples executed: 3\")\n",
    "    print(f\"All calls routed through: {apim_gateway_url}\")\n",
    "    print(f\"Plugin used: WorkshopPlugin\")\n",
    "    print(f\"Functions available: get_current_time, get_weather, calculate_square\")\n",
    "\n",
    "# Run the async function\n",
    "await run_sk_function_calling()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì SK Plugin Function Calling Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. SK plugins encapsulate reusable functionality\")\n",
    "print(\"2. Auto function calling handles multi-step planning automatically\")\n",
    "print(\"3. All LLM calls route through APIM gateway\")\n",
    "print(\"4. No manual function call parsing required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 2: SK Streaming Chat with Function Calling\n",
    "\n",
    "**Purpose**: SK Streaming Chat with Function Calling\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.662423Z",
     "iopub.status.busy": "2025-11-17T18:40:42.662282Z",
     "iopub.status.idle": "2025-11-17T18:40:42.700121Z",
     "shell.execute_reply": "2025-11-17T18:40:42.699001Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mDemonstrates:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m- Real-time streaming responses through APIM\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m- Progressive output rendering\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01masyncio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernel_function\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatCompletion\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_kernel'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Streaming Chat with Function Calling\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- Real-time streaming responses through APIM\n",
    "- Streaming with automatic function calling\n",
    "- Async iteration over response chunks\n",
    "- Progressive output rendering\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Streaming Chat with Function Calling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Setup Kernel (reuse from previous cell or create new)\n",
    "# ============================================================================\n",
    "\n",
    "# Simple plugin for streaming demo\n",
    "class StreamingDemoPlugin:\n",
    "    \"\"\"Plugin for streaming demonstrations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get information about a programming language\")\n",
    "    def get_language_info(self, language: str) -> str:\n",
    "        \"\"\"Get information about a programming language.\"\"\"\n",
    "        info = {\n",
    "            \"python\": \"Python is a high-level, interpreted language known for simplicity and readability. Created by Guido van Rossum in 1991.\",\n",
    "            \"javascript\": \"JavaScript is a dynamic, interpreted language primarily used for web development. Created by Brendan Eich in 1995.\",\n",
    "            \"csharp\": \"C# is a modern, object-oriented language developed by Microsoft. Released in 2000 as part of .NET Framework.\",\n",
    "            \"java\": \"Java is a class-based, object-oriented language designed to have minimal implementation dependencies. Released by Sun Microsystems in 1995.\",\n",
    "        }\n",
    "        return info.get(language.lower(), f\"Information not available for {language}\")\n",
    "\n",
    "    @kernel_function(description=\"Count words in a text\")\n",
    "    def count_words(self, text: str) -> int:\n",
    "        \"\"\"Count the number of words in text.\"\"\"\n",
    "        return len(text.split())\n",
    "\n",
    "# Create kernel with custom APIM client\n",
    "stream_kernel = Kernel()\n",
    "\n",
    "# Ensure we target the correct APIM API path (e.g. /inference) to avoid 404 NotFound\n",
    "# Prefer already provided openai_endpoint if available, else build from base + path_var.\n",
    "streaming_endpoint = (\n",
    "    openai_endpoint\n",
    "    if \"openai_endpoint\" in globals()\n",
    "    else f\"{apim_gateway_url.rstrip('/')}/{path_var}\"\n",
    ")\n",
    "\n",
    "print(f\"Configured streaming endpoint: {streaming_endpoint}\")\n",
    "\n",
    "custom_stream_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=streaming_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both,\n",
    ")\n",
    "\n",
    "stream_chat_service = AzureChatCompletion(\n",
    "    service_id=\"apim_stream\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=custom_stream_client,\n",
    ")\n",
    "\n",
    "stream_kernel.add_service(stream_chat_service)\n",
    "stream_kernel.add_plugin(StreamingDemoPlugin(), plugin_name=\"StreamingDemo\")\n",
    "\n",
    "print(\"‚úì Streaming kernel configured\")\n",
    "print(f\"  Endpoint: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Configure Streaming Settings\n",
    "# ============================================================================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "stream_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"apim_stream\",\n",
    "    max_tokens=800,\n",
    "    temperature=0.8,\n",
    ")\n",
    "stream_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"‚úì Streaming settings configured\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Streaming Examples\n",
    "# ============================================================================\n",
    "\n",
    "async def run_streaming_examples():\n",
    "    \"\"\"Execute streaming chat examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Basic Streaming Response\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"Tell me a short story about an AI learning to paint.\")\n",
    "\n",
    "    print(\"\\nUser: Tell me a short story about an AI learning to paint.\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    # Get streaming response\n",
    "    response_stream = stream_chat_service.get_streaming_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=stream_settings,\n",
    "        kernel=stream_kernel,\n",
    "    )\n",
    "\n",
    "    # Collect chunks for later use\n",
    "    chunks = []\n",
    "    async for chunk in response_stream:\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    print(\"\\n\")  # New line after streaming\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Streaming with Function Call\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history2 = ChatHistory()\n",
    "    history2.add_user_message(\n",
    "        \"Give me detailed information about Python and then explain why it's popular.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nUser: Give me detailed information about Python and then explain why it's popular.\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    response_stream2 = stream_chat_service.get_streaming_chat_message_content(\n",
    "        chat_history=history2,\n",
    "        settings=stream_settings,\n",
    "        kernel=stream_kernel,\n",
    "    )\n",
    "\n",
    "    chunks2 = []\n",
    "    async for chunk in response_stream2:\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            chunks2.append(chunk)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Interactive Streaming Conversation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Multi-turn conversation with streaming\n",
    "    conv_history = ChatHistory()\n",
    "\n",
    "    messages = [\n",
    "        \"What programming language should I learn first?\",\n",
    "        \"Tell me more about Python specifically.\",\n",
    "        \"How many words have you used in your last response?\"\n",
    "    ]\n",
    "\n",
    "    for msg in messages:\n",
    "        print(f\"\\nUser: {msg}\")\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        conv_history.add_user_message(msg)\n",
    "\n",
    "        stream_response = stream_chat_service.get_streaming_chat_message_content(\n",
    "            chat_history=conv_history,\n",
    "            settings=stream_settings,\n",
    "            kernel=stream_kernel,\n",
    "        )\n",
    "\n",
    "        full_response_chunks = []\n",
    "        async for chunk in stream_response:\n",
    "            if chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full_response_chunks.append(chunk)\n",
    "\n",
    "        # Combine chunks into full message for history\n",
    "        if full_response_chunks:\n",
    "            full_response = sum(full_response_chunks[1:], full_response_chunks[0])\n",
    "            conv_history.add_message(full_response)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STREAMING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Examples executed: 3\")\n",
    "    print(f\"Streaming endpoint: {apim_gateway_url}\")\n",
    "    print(f\"Function calling: Enabled (auto)\")\n",
    "    print(f\"Response mode: Real-time streaming\")\n",
    "\n",
    "# Run streaming examples\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì SK Streaming Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Streaming provides real-time response rendering\")\n",
    "print(\"2. Function calling works seamlessly with streaming\")\n",
    "print(\"3. Async iteration enables progressive output\")\n",
    "print(\"4. All streaming goes through APIM gateway\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 3: AutoGen Multi-Agent Conversation via APIM\n",
    "\n",
    "**Purpose**: AutoGen Multi-Agent Conversation via APIM\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.702557Z",
     "iopub.status.busy": "2025-11-17T18:40:42.702214Z",
     "iopub.status.idle": "2025-11-17T18:40:42.706593Z",
     "shell.execute_reply": "2025-11-17T18:40:42.705886Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================# AutoGen: Multi-Agent Conversation via APIM Gateway# ============================================================================\"\"\"Demonstrates:- Multiple AutoGen agents with specialized roles- Agent-to-agent communication- Tool/function registration and execution- Routing all AutoGen LLM calls through APIM- Termination conditions and conversation flow\"\"\"import osfrom typing import Annotated, Literalfrom autogen import ConversableAgentprint(\"=\"*70)print(\"AUTOGEN: Multi-Agent Conversation via APIM Gateway\")print(\"=\"*70)# ============================================================================# Step 1: Configure AutoGen for APIM Gateway# ============================================================================# Ensure deployment_name exists (fallback to a known model)if \"deployment_name\" not in globals() or not deployment_name:    deployment_name = \"gpt-4o-mini\"# Build correct endpoint (APIM base + inference path)if \"openai_endpoint\" in globals() and openai_endpoint:    endpoint = openai_endpoint.rstrip(\"/\")else:    apim_base = apim_gateway_url if \"apim_gateway_url\" in globals() and apim_gateway_url else os.getenv(\"APIM_GATEWAY_URL\", \"\")    inference_path = inference_api_path if \"inference_api_path\" in globals() else os.getenv(\"INFERENCE_API_PATH\", \"inference\")    endpoint = f\"{apim_base.rstrip('/')}/{inference_path.strip('/')}\"# Get API keyapi_key = subscription_key_both if \"subscription_key_both\" in globals() and subscription_key_both else (    apim_api_key if \"apim_api_key\" in globals() and apim_api_key else os.getenv(\"APIM_API_KEY\", \"\"))# Validate configurationif not endpoint or not api_key:    print(\"‚ùå Missing AutoGen configuration:\")    if not endpoint:        print(\"   - APIM endpoint not found (need APIM_GATEWAY_URL)\")    if not api_key:        print(\"   - API key not found (need APIM_API_KEY or subscription_key)\")    raise RuntimeError(\"Missing AutoGen configuration. Please ensure master-lab.env is loaded.\")# AutoGen configuration pointing to APIMautogen_config = {    \"model\": deployment_name,    \"api_type\": \"azure\",    \"api_key\": api_key,    \"base_url\": endpoint,    \"api_version\": \"2024-02-01\",}config_list = [autogen_config]print(\"‚úì AutoGen configuration created\")print(f\"  Model: {deployment_name}\")print(f\"  Base URL: {endpoint}\")print(f\"  API Key: {'*' * 8}{api_key[-4:] if len(api_key) > 4 else '****'}\")# ============================================================================# Step 2: Define Tools for Agents# ============================================================================# Simple calculator toolOperator = Literal[\"+\", \"-\", \"*\", \"/\"]def calculator(a: float, b: float, operator: Annotated[Operator, \"operator\"]) -> float:    \"\"\"    Perform basic arithmetic operations.    Args:        a: First number        b: Second number        operator: Operation to perform (+, -, *, /)    Returns:        Result of the calculation    \"\"\"    if operator == \"+\":        return a + b    elif operator == \"-\":        return a - b    elif operator == \"*\":        return a * b    elif operator == \"/\":        if b == 0:            return float('inf')  # Handle division by zero        return a / b    else:        raise ValueError(f\"Invalid operator: {operator}\")print(\"‚úì Calculator tool defined\")# ============================================================================# Step 3: Create Specialized Agents# ============================================================================# Agent 1: Analyst (suggests approaches)analyst_agent = ConversableAgent(    name=\"Analyst\",    system_message=(        \"You are a data analyst. Your role is to analyze problems and suggest \"        \"approaches using available tools. When calculations are needed, clearly \"        \"state what needs to be calculated. Return 'TERMINATE' when the task is complete.\"    ),    llm_config={\"config_list\": config_list, \"temperature\": 0.7},)# Agent 2: Calculator (executes calculations)calculator_agent = ConversableAgent(    name=\"Calculator\",    system_message=(        \"You are a calculator agent. You execute mathematical calculations accurately. \"        \"Use the calculator tool for all computations.\"    ),    llm_config={\"config_list\": config_list, \"temperature\": 0.1},)# Agent 3: User Proxy (manages execution and termination)user_proxy = ConversableAgent(    name=\"UserProxy\",    llm_config=False,  # No LLM for proxy    is_termination_msg=lambda msg: msg.get(\"content\") is not None                                   and \"TERMINATE\" in msg[\"content\"],    human_input_mode=\"NEVER\",)print(\"‚úì Three agents created:\")print(\"  1. Analyst - Problem analysis and planning\")print(\"  2. Calculator - Execution of calculations\")print(\"  3. UserProxy - Tool execution and flow control\")# ============================================================================# Step 4: Register Tools with Agents# ============================================================================# Register calculator toolanalyst_agent.register_for_llm(    name=\"calculator\",    description=\"A calculator that performs basic arithmetic\")(calculator)calculator_agent.register_for_llm(    name=\"calculator\",    description=\"A calculator that performs basic arithmetic\")(calculator)user_proxy.register_for_execution(name=\"calculator\")(calculator)print(\"‚úì Calculator tool registered with all agents\")# ============================================================================# Step 5: Run Multi-Agent Conversations# ============================================================================print(\"\\n\" + \"=\"*70)print(\"EXAMPLE 1: Simple Calculation Task\")print(\"=\"*70)response1 = user_proxy.initiate_chat(    analyst_agent,    message=\"Calculate (15 + 27) * 3 and then subtract 50. What's the final result?\",    max_turns=10)print(\"\\n‚úì Example 1 complete\")# ============================================================================print(\"\\n\" + \"=\"*70)print(\"EXAMPLE 2: Complex Multi-Step Problem\")print(\"=\"*70)response2 = user_proxy.initiate_chat(    analyst_agent,    message=(        \"A company has quarterly revenues of $125,000, $138,000, $142,000, and $155,000. \"        \"Calculate the total annual revenue and then the average quarterly revenue.\"    ),    max_turns=10)print(\"\\n‚úì Example 2 complete\")# ============================================================================print(\"\\n\" + \"=\"*70)print(\"EXAMPLE 3: Agent Collaboration Pattern\")print(\"=\"*70)# More complex scenario requiring agent collaborationresponse3 = user_proxy.initiate_chat(    analyst_agent,    message=(        \"If a product costs $89.99 and there's a 15% discount, what's the final price? \"        \"Then, if I buy 7 units at the discounted price, what's my total cost?\"    ),    max_turns=15)print(\"\\n‚úì Example 3 complete\")# ============================================================================print(\"\\n\" + \"=\"*70)print(\"MULTI-AGENT CONVERSATION STATISTICS\")print(\"=\"*70)print(f\"Total examples: 3\")print(f\"Agents involved: Analyst, Calculator, UserProxy\")print(f\"Tool calls: Calculator function\")print(f\"All LLM calls routed through: {apim_gateway_url}\")print(f\"Model used: {deployment_name}\")print(\"\\n\" + \"=\"*70)print(\"‚úì AutoGen Multi-Agent Demo Complete\")print(\"=\"*70)print(\"\\nKey Takeaways:\")print(\"1. AutoGen enables multi-agent collaboration patterns\")print(\"2. Agents can have specialized roles and tools\")print(\"3. Tool registration separates LLM decision from execution\")print(\"4. All agent LLM calls route through APIM gateway\")print(\"5. Termination conditions control conversation flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 4: SK Agent with Custom Azure OpenAI Client\n",
    "\n",
    "**Purpose**: SK Agent with Custom Azure OpenAI Client\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.708435Z",
     "iopub.status.busy": "2025-11-17T18:40:42.708288Z",
     "iopub.status.idle": "2025-11-17T18:40:42.745948Z",
     "shell.execute_reply": "2025-11-17T18:40:42.745346Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mDemonstrates:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m- SK ChatCompletionAgent with custom Azure OpenAI client\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m- Integration with existing APIM infrastructure\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01masyncio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionAgent\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatCompletion, AzureChatPromptExecutionSettings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_kernel'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: ChatCompletionAgent with APIM Routing\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK ChatCompletionAgent with custom Azure OpenAI client\n",
    "- Multi-turn conversation with thread management\n",
    "- Agent streaming capabilities\n",
    "- Integration with existing APIM infrastructure\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt, KernelArguments\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: ChatCompletionAgent with APIM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create Kernel with Custom Client\n",
    "# ============================================================================\n",
    "\n",
    "agent_kernel = Kernel()\n",
    "\n",
    "# Custom client for APIM\n",
    "agent_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Add chat completion service\n",
    "agent_chat_service = AzureChatCompletion(\n",
    "    service_id=\"agent_service\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=agent_client,\n",
    ")\n",
    "agent_kernel.add_service(agent_chat_service)\n",
    "\n",
    "print(\"‚úì Agent kernel created\")\n",
    "print(f\"  Service: Azure OpenAI via APIM\")\n",
    "print(f\"  Endpoint: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Add Plugin Function to Agent\n",
    "# ============================================================================\n",
    "\n",
    "# Add a simple prompt-based function\n",
    "documentation_function = agent_kernel.add_function(\n",
    "    plugin_name=\"DocsHelper\",\n",
    "    function=KernelFunctionFromPrompt(\n",
    "        function_name=\"explain_concept\",\n",
    "        prompt=\"\"\"You are a technical documentation expert.\n",
    "\n",
    "Explain the following concept clearly and concisely:\n",
    "\n",
    "Concept: {{$concept}}\n",
    "\n",
    "Provide:\n",
    "1. Brief definition\n",
    "2. Key characteristics\n",
    "3. Common use cases\n",
    "4. A simple example\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úì Documentation helper function added to kernel\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Configure Agent Settings\n",
    "# ============================================================================\n",
    "\n",
    "agent_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"agent_service\",\n",
    "    max_tokens=600,\n",
    "    temperature=0.7,\n",
    ")\n",
    "agent_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"‚úì Agent execution settings configured\")\n",
    "print(\"  Function calling: Auto\")\n",
    "print(\"  Max tokens: 600\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create ChatCompletionAgent\n",
    "# ============================================================================\n",
    "\n",
    "workshop_agent = ChatCompletionAgent(\n",
    "    kernel=agent_kernel,\n",
    "    name=\"WorkshopAssistant\",\n",
    "    instructions=(\n",
    "        \"You are an AI assistant for an Azure AI Gateway workshop. \"\n",
    "        \"Help users understand AI Gateway concepts, API Management, \"\n",
    "        \"and Azure OpenAI integration. Be concise and practical. \"\n",
    "        \"Use available functions to provide detailed explanations when needed.\"\n",
    "    ),\n",
    "    arguments=KernelArguments(settings=agent_settings),\n",
    ")\n",
    "\n",
    "print(\"‚úì ChatCompletionAgent created\")\n",
    "print(f\"  Name: {workshop_agent.name}\")\n",
    "print(\"  Instructions: Workshop assistance\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Agent Conversations\n",
    "# ============================================================================\n",
    "\n",
    "async def run_agent_examples():\n",
    "    \"\"\"Execute agent conversation examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Simple Agent Interaction\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create new thread (handle SK version differences)\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread = workshop_agent.new_thread()\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"ChatCompletionAgent has no thread creation method (create_thread/new_thread). \"\n",
    "            \"Update semantic_kernel package or remove thread usage.\"\n",
    "        )\n",
    "\n",
    "    # First interaction\n",
    "    result1 = await workshop_agent.run(\n",
    "        \"What is Azure API Management?\",\n",
    "        thread=thread\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What is Azure API Management?\")\n",
    "    print(f\"Agent: {result1.text}\\n\")\n",
    "\n",
    "    # Second interaction (agent remembers context)\n",
    "    result2 = await workshop_agent.run(\n",
    "        \"How does it help with AI Gateway patterns?\",\n",
    "        thread=thread\n",
    "    )\n",
    "\n",
    "    print(f\"User: How does it help with AI Gateway patterns?\")\n",
    "    print(f\"Agent: {result2.text}\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Agent with Function Calling\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread2 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread2 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread2 = thread  # Fallback: reuse existing thread\n",
    "\n",
    "    result3 = await workshop_agent.run(\n",
    "        \"Explain the concept of 'semantic kernel' in detail\",\n",
    "        thread=thread2\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: Explain the concept of 'semantic kernel' in detail\")\n",
    "    print(f\"Agent: {result3.text}\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Streaming Agent Response\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread3 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread3 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread3 = thread  # Fallback\n",
    "\n",
    "    print(\"\\nUser: Explain the benefits of using an AI Gateway for enterprise deployments\")\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "\n",
    "    # Stream the response\n",
    "    async for chunk in workshop_agent.run_stream(\n",
    "        \"Explain the benefits of using an AI Gateway for enterprise deployments\",\n",
    "        thread=thread3\n",
    "    ):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: Multi-Turn Technical Discussion\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread4 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread4 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread4 = thread  # Fallback\n",
    "\n",
    "    questions = [\n",
    "        \"What is function calling in LLMs?\",\n",
    "        \"How does Semantic Kernel implement function calling?\",\n",
    "        \"What's the difference between manual and auto function invocation?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        result = await workshop_agent.run(question, thread=thread4)\n",
    "        print(f\"\\nUser: {question}\")\n",
    "        print(f\"Agent: {result.text[:200]}...\")  # Truncate for readability\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGENT CONVERSATION STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total examples: 4\")\n",
    "    print(f\"Agent: WorkshopAssistant\")\n",
    "    print(f\"Threads created: 4\")\n",
    "    print(f\"Total interactions: 8+\")\n",
    "    print(f\"All routed through: {apim_gateway_url}\")\n",
    "    print(f\"Streaming enabled: Yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 5: SK Vector Search with Gateway-Routed Embeddings\n",
    "\n",
    "**Purpose**: SK Vector Search with Gateway-Routed Embeddings\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.748791Z",
     "iopub.status.busy": "2025-11-17T18:40:42.748477Z",
     "iopub.status.idle": "2025-11-17T18:40:42.755626Z",
     "shell.execute_reply": "2025-11-17T18:40:42.754033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lab 25: Vector Search with Azure AI Search + APIM Embeddingsprint(\"üîç Vector Search with Azure AI Search + Real Embeddings\")print(\"=\" * 80)import osimport asynciofrom azure.search.documents import SearchClientfrom azure.search.documents.indexes import SearchIndexClientfrom azure.search.documents.indexes.models import (    SearchIndex,    SearchField,    SearchFieldDataType,    VectorSearch,    HnswAlgorithmConfiguration,    VectorSearchProfile,    SemanticConfiguration,    SemanticPrioritizedFields,    SemanticField,    SemanticSearch)from azure.core.credentials import AzureKeyCredentialfrom openai import AsyncAzureOpenAI# =============================================================================# Configuration from Environment# =============================================================================# Azure AI Search Configurationsearch_service_name = os.getenv(\"SEARCH_SERVICE_NAME\") or globals().get(\"SEARCH_SERVICE_NAME\")search_admin_key = os.getenv(\"SEARCH_ADMIN_KEY\") or globals().get(\"SEARCH_ADMIN_KEY\")search_endpoint = os.getenv(\"SEARCH_ENDPOINT\") or globals().get(\"SEARCH_ENDPOINT\")if not search_endpoint and search_service_name:    search_endpoint = f\"https://{search_service_name}.search.windows.net\"# APIM Configuration for Embeddingsapim_gateway_url = os.getenv(\"APIM_GATEWAY_URL\") or globals().get(\"apim_gateway_url\")apim_api_key = os.getenv(\"APIM_API_KEY\") or globals().get(\"apim_api_key\")inference_api_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")# Model Configuration - Use actual embedding deploymentembedding_model = (    os.getenv(\"MODEL_TEXT_EMBEDDING_3_SMALL_DEPLOYMENT\") or    os.getenv(\"EMBEDDING_MODEL\") or    \"text-embedding-3-small\"  # Default to actual embedding model)chat_model = os.getenv(\"CHAT_MODEL\") or (requested_models[0] if 'requested_models' in globals() and requested_models else \"gpt-4o-mini\")embedding_dimensions = 1536api_version = os.getenv(\"OPENAI_API_VERSION\", \"2024-06-01\")# Validate configurationmissing_config = []if not search_endpoint: missing_config.append(\"SEARCH_ENDPOINT\")if not search_admin_key: missing_config.append(\"SEARCH_ADMIN_KEY\")if not apim_gateway_url: missing_config.append(\"APIM_GATEWAY_URL\")if not apim_api_key: missing_config.append(\"APIM_API_KEY\")if missing_config:    print(\"‚ùå Missing required configuration:\", \", \".join(missing_config))print(f\"‚úÖ Embedding Model (deployment used for embeddings): {embedding_model}\")print(f\"‚úÖ Chat Model: {chat_model}\")if embedding_model == chat_model:    # Using chat deployment as fallback for embeddings; only error if APIM details missing.    print(\"‚ÑπÔ∏è Using chat model deployment for embeddings (fallback).\")    still_missing = []    if not apim_gateway_url: still_missing.append(\"APIM_GATEWAY_URL\")    if not apim_api_key: still_missing.append(\"APIM_API_KEY\")    if still_missing:        print(\"‚ùå Missing required configuration for embeddings via APIM:\", \", \".join(still_missing))        raise RuntimeError(\"Missing configuration: \" + \", \".join(still_missing) + \". Please ensure master-lab.env is loaded.\")print(f\"‚úÖ Azure AI Search: {search_endpoint}\")print(f\"‚úÖ APIM Gateway: {apim_gateway_url}\")print(f\"‚úÖ Embedding Model: {embedding_model}\")print(f\"‚úÖ Chat Model: {chat_model}\")# =============================================================================# Initialize Clients# =============================================================================# Azure AI Search clientssearch_credential = AzureKeyCredential(search_admin_key)index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)index_name = \"knowledge-base-vector-index\"# Azure OpenAI client for embeddings - Try direct foundry endpoint firstembedding_endpoint_foundry = os.getenv(\"MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1\")embedding_key_foundry = os.getenv(\"MODEL_TEXT_EMBEDDING_3_SMALL_KEY_R1\")if embedding_endpoint_foundry and embedding_key_foundry:    # Use direct foundry endpoint (bypassing APIM)    embedding_endpoint = embedding_endpoint_foundry.rstrip('/')    embedding_key = embedding_key_foundry    print(\"   ‚ÑπÔ∏è  Using direct foundry endpoint for embeddings (bypassing APIM)\")else:    # Fallback to APIM gateway    embedding_endpoint = f\"{apim_gateway_url.rstrip('/')}/{inference_api_path.strip('/')}\"    embedding_key = apim_api_key    print(\"   ‚ÑπÔ∏è  Using APIM gateway endpoint for embeddings\")openai_client = AsyncAzureOpenAI(    azure_endpoint=embedding_endpoint,    api_key=embedding_key,    api_version=api_version)print(f\"\\n‚úÖ Clients initialized\")# =============================================================================# Knowledge Base Documents# =============================================================================knowledge_base = [    {        \"id\": \"apim_basics\",        \"title\": \"Azure API Management Basics\",        \"content\": \"\"\"        Azure API Management (APIM) is a fully managed service that lets you publish, secure,        transform, maintain, and monitor APIs. It provides a consistent interface and governance        layer over backend services, supporting rate limiting, caching, authentication, and policy enforcement.        \"\"\"    },    {        \"id\": \"ai_gateway\",        \"title\": \"AI Gateway Pattern\",        \"content\": \"\"\"        An AI Gateway uses API Management to front multiple AI model endpoints across regions and SKUs.        It centralizes authentication, rate limiting, observability, routing, and policy enforcement        such as content safety filtering. This enables consistent governance and load balancing        across distributed AI services.        \"\"\"    },    {        \"id\": \"semantic_kernel\",        \"title\": \"Semantic Kernel Framework\",        \"content\": \"\"\"        Semantic Kernel is an SDK that composes AI services (Azure OpenAI, OpenAI, Hugging Face)        with traditional code via plugins, planners, and memory abstractions. It enables building        AI-centric workflows with function calling, planning, and retrieval-augmented generation (RAG).        \"\"\"    },    {        \"id\": \"function_calling\",        \"title\": \"LLM Function Calling\",        \"content\": \"\"\"        Function calling allows an LLM to decide when to invoke backend functions (tools) by emitting        structured calls. The host intercepts the call, executes the function, and supplies the result        back to the model, enabling tool-augmented reasoning and retrieval capabilities.        \"\"\"    },    {        \"id\": \"vector_search\",        \"title\": \"Vector Search and Embeddings\",        \"content\": \"\"\"        Vector search uses embeddings to represent text as high-dimensional vectors, enabling semantic        similarity search. Azure AI Search provides vector search capabilities with HNSW algorithm        for efficient nearest neighbor search, supporting RAG patterns and semantic retrieval.        \"\"\"    }]print(f\"\\nüìö Knowledge base: {len(knowledge_base)} documents\")# =============================================================================# Create or Update Search Index# =============================================================================async def create_search_index():    \"\"\"Create Azure AI Search index with vector field\"\"\"    print(f\"\\nüîß Creating search index: {index_name}\")        # Define vector search configuration    vector_search = VectorSearch(        algorithms=[            HnswAlgorithmConfiguration(                name=\"hnsw-config\",                parameters={                    \"m\": 4,                    \"efConstruction\": 400,                    \"efSearch\": 500,                    \"metric\": \"cosine\"                }            )        ],        profiles=[            VectorSearchProfile(                name=\"vector-profile\",                algorithm_configuration_name=\"hnsw-config\"            )        ]    )        # Define fields (single content_vector field using configured embedding_dimensions)    fields = [        SearchField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True),        SearchField(name=\"title\", type=SearchFieldDataType.String, searchable=True, filterable=True),        SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),        SearchField(            name=\"content_vector\",            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),            searchable=True,            vector_search_dimensions=embedding_dimensions,            vector_search_profile_name=\"vector-profile\"        )    ]        # Create or update index    index = SearchIndex(        name=index_name,        fields=fields,        vector_search=vector_search    )        try:        index_client.create_or_update_index(index)        print(f\"   ‚úÖ Index '{index_name}' created/updated\")        return True    except Exception as e:        print(f\"   ‚ùå Failed to create index: {e}\")        return Falseasync def generate_embedding(text: str) -> list:    \"\"\"Generate embedding using Azure OpenAI through APIM.       Falls back gracefully if deployment is missing.\"\"\"    try:        response = await openai_client.embeddings.create(            model=embedding_model,            input=text        )        return response.data[0].embedding    except Exception as e:        msg = str(e)        if \"DeploymentNotFound\" in msg or \"404\" in msg:            print(f\"‚ö†Ô∏è Embedding deployment '{embedding_model}' not found. Fallback: generating deterministic pseudo-embedding.\")            # Deterministic pseudo-embedding (NOT semantic) so the rest of the lab runs without hard failure.            import hashlib            h = hashlib.sha256(text.encode(\"utf-8\")).digest()            # Expand digest to embedding_dimensions length            pseudo = []            for i in range(embedding_dimensions):                pseudo.append(h[i % len(h)] / 255.0)            return pseudo        print(f\"‚ùå Embedding generation failed: {e}\")        raise# =============================================================================# Index Knowledge Base Documents# =============================================================================async def index_documents():    \"\"\"Generate embeddings and index documents\"\"\"    print(f\"\\nüì§ Indexing documents with embeddings...\")        search_client = SearchClient(        endpoint=search_endpoint,        index_name=index_name,        credential=search_credential    )        documents_to_index = []        for i, doc in enumerate(knowledge_base, 1):        print(f\"   Processing {i}/{len(knowledge_base)}: {doc['title']}\")                # Generate embedding for content        embedding = await generate_embedding(doc['content'])                # Prepare document for indexing        documents_to_index.append({            \"id\": doc['id'],            \"title\": doc['title'],            \"content\": doc['content'],            \"content_vector\": embedding        })                print(f\"      ‚úÖ Embedding generated: {len(embedding)} dimensions\")        # Upload documents to index    try:        result = search_client.upload_documents(documents=documents_to_index)        successful = sum(1 for r in result if r.succeeded)        print(f\"\\n   ‚úÖ Indexed {successful}/{len(documents_to_index)} documents\")        return True    except Exception as e:        print(f\"   ‚ùå Indexing failed: {e}\")        return False# =============================================================================# Vector Search Function# =============================================================================async def vector_search(query: str, top_k: int = 3):    \"\"\"Perform vector search using Azure AI Search\"\"\"    print(f\"\\nüîç Searching for: '{query}'\")        # Generate query embedding    query_embedding = await generate_embedding(query)    print(f\"   Query embedding: {len(query_embedding)} dimensions\")        # Perform vector search    search_client = SearchClient(        endpoint=search_endpoint,        index_name=index_name,        credential=search_credential    )        try:        results = search_client.search(            search_text=None,  # Pure vector search            vector_queries=[{                \"kind\": \"vector\",                \"vector\": query_embedding,                \"fields\": \"content_vector\",                \"k\": top_k            }],            select=[\"id\", \"title\", \"content\"],            top=top_k        )                search_results = []        for result in results:            search_results.append({                \"id\": result['id'],                \"title\": result['title'],                \"content\": result['content'],                \"score\": result['@search.score']            })                print(f\"   ‚úÖ Found {len(search_results)} results\")        return search_results            except Exception as e:        print(f\"   ‚ùå Search failed: {e}\")        return []# =============================================================================# RAG with Real Vector Search# =============================================================================async def rag_with_vector_search(question: str):    \"\"\"Retrieval-Augmented Generation using vector search\"\"\"    print(f\"\\nüìä RAG Pipeline for: '{question}'\")    print(\"-\" * 80)        # Step 1: Vector search for relevant context    results = await vector_search(question, top_k=2)        if not results:        print(\"‚ùå No results found\")        return None        # Step 2: Build context from search results    context_parts = []    for i, result in enumerate(results, 1):        print(f\"\\n   [{i}] {result['title']} (score: {result['score']:.4f})\")        print(f\"       {result['content'][:100].strip()}...\")        context_parts.append(f\"[{result['title']}]\\n{result['content']}\")        context = \"\\n\\n\".join(context_parts)        # Step 3: Generate answer using retrieved context    print(f\"\\nü§ñ Generating answer with {chat_model}...\")        try:        response = await openai_client.chat.completions.create(            model=chat_model,            messages=[                {                    \"role\": \"system\",                    \"content\": \"You are a helpful assistant. Use the provided context to answer questions accurately.\"                },                {                    \"role\": \"user\",                    \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"                }            ],            max_tokens=300,            temperature=0.7        )                answer = response.choices[0].message.content        print(f\"\\n‚úÖ Answer generated:\")        print(f\"\\n{answer}\")        return answer            except Exception as e:        print(f\"‚ùå Answer generation failed: {e}\")async def run_demo():    # Create index    await create_search_index()        # Index documents with embeddings    await index_documents()    print(f\"   - Indexed {len(knowledge_base)} documents with vector embeddings (dimensions={embedding_dimensions})\")        # Example queries    queries = [        \"What is Semantic Kernel?\",        \"How does an AI Gateway work?\",        \"Explain vector search and embeddings\"    ]    # Example queries    queries = [        \"What is Semantic Kernel?\",        \"How does an AI Gateway work?\",        \"Explain vector search and embeddings\"    ]        print(f\"\\n\\n{'='*80}\")    print(\"DEMO: RAG with Real Vector Search\")    print(\"=\"*80)        for query in queries:        await rag_with_vector_search(query)        print(f\"\\n{'='*80}\\n\")# Run the demoawait run_demo()print(\"\\n‚úÖ Lab 25 Complete!\")print(\"\\nüìà Summary:\")print(f\"   - Created Azure AI Search index: {index_name}\")print(f\"   - Indexed {len(knowledge_base)} documents with vector embeddings\")print(f\"   - Performed semantic vector search using HNSW algorithm\")print(f\"   - Generated answers using RAG with real search results\")print(f\"   - All embeddings routed through APIM: {apim_gateway_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3, Cell 6: SK + AutoGen Hybrid Orchestration\n",
    "\n",
    "**Purpose**: SK + AutoGen Hybrid Orchestration\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:40:42.758226Z",
     "iopub.status.busy": "2025-11-17T18:40:42.758027Z",
     "iopub.status.idle": "2025-11-17T18:40:43.316118Z",
     "shell.execute_reply": "2025-11-17T18:40:43.315499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m√ó\u001b[0m This environment is externally managed\r\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\r\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\r\n",
      "\u001b[31m   \u001b[0m install.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\r\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\r\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\r\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\r\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotated, Dict, Any\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kernel_function\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatCompletion\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_kernel'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hybrid: Semantic Kernel Plugins + AutoGen Orchestration\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK plugins as tools for AutoGen agents\n",
    "- Multi-agent orchestration with SK capabilities\n",
    "- Combining SK function calling with AutoGen decision making\n",
    "- Complex workflow coordination\n",
    "- All LLM calls through APIM gateway\n",
    "\"\"\"\n",
    "# Ensure required package is installed (fix ModuleNotFoundError: autogen)\n",
    "%pip install autogen\n",
    "\n",
    "import asyncio\n",
    "import os  # Needed for getenv when building inference path\n",
    "from typing import Annotated, Dict, Any\n",
    "from datetime import datetime\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from openai import AsyncAzureOpenAI\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID: Semantic Kernel + AutoGen Orchestration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create SK Plugin with Business Logic\n",
    "# ============================================================================\n",
    "\n",
    "class EnterprisePlugin:\n",
    "    \"\"\"SK Plugin for enterprise business operations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get customer information by ID\")\n",
    "    def get_customer_info(self, customer_id: str) -> str:\n",
    "        \"\"\"Retrieve customer information.\"\"\"\n",
    "        # Simulated customer database\n",
    "        customers = {\n",
    "            \"C001\": \"Customer: Acme Corp, Tier: Gold, Balance: $50,000\",\n",
    "            \"C002\": \"Customer: Contoso Ltd, Tier: Silver, Balance: $25,000\",\n",
    "            \"C003\": \"Customer: Fabrikam Inc, Tier: Platinum, Balance: $100,000\",\n",
    "        }\n",
    "        return customers.get(customer_id, \"Customer not found\")\n",
    "\n",
    "    @kernel_function(description=\"Calculate discount based on customer tier\")\n",
    "    def calculate_discount(self, tier: str, amount: float) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate discount for a customer tier.\"\"\"\n",
    "        discount_rates = {\n",
    "            \"platinum\": 0.20,\n",
    "            \"gold\": 0.15,\n",
    "            \"silver\": 0.10,\n",
    "            \"bronze\": 0.05,\n",
    "        }\n",
    "        rate = discount_rates.get(tier.lower(), 0.0)\n",
    "        discount = amount * rate\n",
    "        final_price = amount - discount\n",
    "\n",
    "        return {\n",
    "            \"tier\": tier,\n",
    "            \"original_amount\": amount,\n",
    "            \"discount_rate\": rate,\n",
    "            \"discount_amount\": discount,\n",
    "            \"final_price\": final_price\n",
    "        }\n",
    "\n",
    "    @kernel_function(description=\"Process order and return order ID\")\n",
    "    def process_order(self, customer_id: str, amount: float) -> str:\n",
    "        \"\"\"Process a customer order.\"\"\"\n",
    "        order_id = f\"ORD-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return f\"Order {order_id} processed for customer {customer_id}, amount: ${amount:.2f}\"\n",
    "\n",
    "# Create SK kernel with plugin\n",
    "hybrid_kernel = Kernel()\n",
    "# Normalize inference path for the SK (AsyncAzureOpenAI) client so the gateway route resolves\n",
    "_inference_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "if not _inference_path.startswith(\"/\"):\n",
    "    _inference_path = \"/\" + _inference_path\n",
    "\n",
    "hybrid_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url.rstrip('/')}{_inference_path}\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both,\n",
    ")\n",
    "\n",
    "hybrid_chat_service = AzureChatCompletion(\n",
    "    service_id=\"hybrid_service\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=hybrid_client,\n",
    ")\n",
    "\n",
    "hybrid_kernel.add_service(hybrid_chat_service)\n",
    "hybrid_kernel.add_plugin(EnterprisePlugin(), plugin_name=\"Enterprise\")\n",
    "\n",
    "print(\"‚úì Semantic Kernel created with EnterprisePlugin\")\n",
    "print(\"  Functions: get_customer_info, calculate_discount, process_order\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Create Wrapper Functions for AutoGen\n",
    "# ============================================================================\n",
    "\n",
    "# We need to create standalone functions that AutoGen can call\n",
    "# These will internally use SK kernel\n",
    "\n",
    "async def sk_get_customer(customer_id: Annotated[str, \"Customer ID\"]) -> str:\n",
    "    \"\"\"Get customer information using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"get_customer_info\"]\n",
    "    result = await func.invoke(hybrid_kernel, customer_id=customer_id)\n",
    "    return str(result)\n",
    "\n",
    "async def sk_calculate_discount(\n",
    "    tier: Annotated[str, \"Customer tier\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Calculate discount using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"calculate_discount\"]\n",
    "    result = await func.invoke(hybrid_kernel, tier=tier, amount=amount)\n",
    "    return str(result)\n",
    "\n",
    "async def sk_process_order(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Process order using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"process_order\"]\n",
    "    result = await func.invoke(hybrid_kernel, customer_id=customer_id, amount=amount)\n",
    "    return str(result)\n",
    "\n",
    "print(\"‚úì SK wrapper functions created for AutoGen\")\n",
    "# FIXED: ensure a single leading slash and correct concatenation (prior value caused missing slash)\n",
    "inference_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "if not inference_path.startswith(\"/\"):\n",
    "    inference_path = \"/\" + inference_path\n",
    "autogen_base_url = f\"{apim_gateway_url.rstrip('/')}{inference_path}/openai\"\n",
    "# ============================================================================\n",
    "\n",
    "# Configure AutoGen for APIM\n",
    "# FIXED: base_url must include full path to OpenAI endpoint\n",
    "inference_path = os.getenv(\"INFERENCE_API_PATH\", \"/inference\")\n",
    "autogen_base_url = f\"{apim_gateway_url.rstrip('/')}{inference_path}/openai\"\n",
    "\n",
    "hybrid_autogen_config = {\n",
    "    \"model\": deployment_name,\n",
    "    \"api_type\": \"azure\",\n",
    "    \"api_key\": subscription_key_both,\n",
    "    \"base_url\": autogen_base_url,  # FIXED: Full path including /inference/openai\n",
    "    \"api_version\": \"2024-02-01\",\n",
    "}\n",
    "\n",
    "config_list_hybrid = [hybrid_autogen_config]\n",
    "\n",
    "# Agent 1: Sales Agent (analyzes and recommends)\n",
    "sales_agent = ConversableAgent(\n",
    "    name=\"SalesAgent\",\n",
    "    system_message=(\n",
    "        \"You are a sales agent. Analyze customer information, calculate appropriate \"\n",
    "        \"discounts, and recommend actions. Be professional and detail-oriented. \"\n",
    "        \"Return 'TERMINATE' when task is complete.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list_hybrid, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "# Agent 2: Order Processor (executes orders)\n",
    "processor_agent = ConversableAgent(\n",
    "    name=\"OrderProcessor\",\n",
    "    system_message=(\n",
    "        \"You are an order processing agent. Execute orders after receiving \"\n",
    "        \"approval from sales agent. Confirm all details before processing.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list_hybrid, \"temperature\": 0.3},\n",
    ")\n",
    "\n",
    "# Agent 3: User Proxy\n",
    "hybrid_proxy = ConversableAgent(\n",
    "    name=\"Coordinator\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None\n",
    "                                   and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"‚úì AutoGen agents created\")\n",
    "print(\"  1. SalesAgent - Analysis and recommendations\")\n",
    "print(\"  2. OrderProcessor - Order execution\")\n",
    "print(\"  3. Coordinator - Workflow management\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Register SK Functions with AutoGen Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Note: AutoGen's async function support may vary by version\n",
    "# For this demo, we'll use sync wrappers\n",
    "\n",
    "def get_customer_sync(customer_id: Annotated[str, \"Customer ID\"]) -> str:\n",
    "    \"\"\"Sync wrapper for SK customer lookup.\"\"\"\n",
    "    import asyncio\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(sk_get_customer(customer_id))\n",
    "\n",
    "def calculate_discount_sync(\n",
    "    tier: Annotated[str, \"Customer tier\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Sync wrapper for SK discount calculation.\"\"\"\n",
    "    import asyncio\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(sk_calculate_discount(tier, amount))\n",
    "\n",
    "def process_order_sync(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Sync wrapper for SK order processing.\"\"\"\n",
    "    import asyncio\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(sk_process_order(customer_id, amount))\n",
    "\n",
    "# Register with agents\n",
    "sales_agent.register_for_llm(\n",
    "    name=\"get_customer\",\n",
    "    description=\"Get customer information by ID\"\n",
    ")(get_customer_sync)\n",
    "\n",
    "sales_agent.register_for_llm(\n",
    "    name=\"calculate_discount\",\n",
    "    description=\"Calculate discount based on tier and amount\"\n",
    ")(calculate_discount_sync)\n",
    "\n",
    "processor_agent.register_for_llm(\n",
    "    name=\"process_order\",\n",
    "    description=\"Process an order for a customer\"\n",
    ")(process_order_sync)\n",
    "\n",
    "hybrid_proxy.register_for_execution(name=\"get_customer\")(get_customer_sync)\n",
    "hybrid_proxy.register_for_execution(name=\"calculate_discount\")(calculate_discount_sync)\n",
    "hybrid_proxy.register_for_execution(name=\"process_order\")(process_order_sync)\n",
    "\n",
    "print(\"‚úì SK functions registered with AutoGen agents\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Hybrid Orchestration Examples\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: Customer Order Workflow\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response1 = hybrid_proxy.initiate_chat(\n",
    "    sales_agent,\n",
    "    message=(\n",
    "        \"Customer C003 wants to make a purchase of $10,000. \"\n",
    "        \"Look up their information, calculate their discount, \"\n",
    "        \"and process the order.\"\n",
    "    ),\n",
    "    max_turns=15\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Example 1 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: Multi-Customer Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response2 = hybrid_proxy.initiate_chat(\n",
    "    sales_agent,\n",
    "    message=(\n",
    "        \"Compare customers C001 and C002. For each, calculate what their \"\n",
    "        \"final price would be for a $5,000 purchase.\"\n",
    "    ),\n",
    "    max_turns=15\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Example 2 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: Complex Business Logic\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response3 = hybrid_proxy.initiate_chat(\n",
    "    sales_agent,\n",
    "    message=(\n",
    "        \"Find the best customer tier for a $50,000 purchase. \"\n",
    "        \"Show the calculations for all tiers and recommend which \"\n",
    "        \"tier a customer should have to get the best value.\"\n",
    "    ),\n",
    "    max_turns=15\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Example 3 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID ORCHESTRATION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Framework combination: Semantic Kernel + AutoGen\")\n",
    "print(f\"SK plugins: EnterprisePlugin (3 functions)\")\n",
    "print(f\"AutoGen agents: SalesAgent, OrderProcessor, Coordinator\")\n",
    "print(f\"SK functions as AutoGen tools: 3\")\n",
    "print(f\"Examples executed: 3\")\n",
    "print(f\"All LLM calls routed through: {apim_gateway_url}\")\n",
    "print(f\"Model: {deployment_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Hybrid SK + AutoGen Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. SK plugins can serve as tools for AutoGen agents\")\n",
    "print(\"2. Combine SK's plugin architecture with AutoGen's orchestration\")\n",
    "print(\"3. SK handles business logic, AutoGen handles agent coordination\")\n",
    "print(\"4. All LLM calls (SK and AutoGen) route through same APIM gateway\")\n",
    "print(\"5. Hybrid approach leverages strengths of both frameworks\")\n",
    "print(\"6. Enterprise patterns: separation of concerns, reusable logic\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_win_backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
