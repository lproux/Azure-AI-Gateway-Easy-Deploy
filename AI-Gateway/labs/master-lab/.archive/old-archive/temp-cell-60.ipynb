{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell_0_dcf63404",
   "metadata": {},
   "source": [
    "# Master AI Gateway Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell_1_eea3122c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:47.053479Z",
     "iopub.status.busy": "2025-11-15T16:27:47.053161Z",
     "iopub.status.idle": "2025-11-15T16:27:47.062889Z",
     "shell.execute_reply": "2025-11-15T16:27:47.061667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRun these cells (-1.x) in order before using legacy sections.\\nOrder:\\n  (-1.1) Env Loader\\n  (-1.2) Dependencies Install\\n  (-1.3) Azure CLI & Service Principal\\n  (-1.4) Endpoint Normalizer\\n  (upcoming) (-1.5) Deployment Helpers\\n  (upcoming) (-1.6) Unified Deployment Orchestrator\\n  (upcoming) (-1.7) Unified Policy Application\\n  (upcoming) (-1.8) Unified MCP Initialization\\nLegacy cells retained below for reference.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (-1.0) Section -1: Consolidated Provisioning & Initialization\n",
    "\"\"\"\n",
    "Run these cells (-1.x) in order before using legacy sections.\n",
    "Order:\n",
    "  (-1.1) Env Loader\n",
    "  (-1.2) Dependencies Install\n",
    "  (-1.3) Azure CLI & Service Principal\n",
    "  (-1.4) Endpoint Normalizer\n",
    "  (upcoming) (-1.5) Deployment Helpers\n",
    "  (upcoming) (-1.6) Unified Deployment Orchestrator\n",
    "  (upcoming) (-1.7) Unified Policy Application\n",
    "  (upcoming) (-1.8) Unified MCP Initialization\n",
    "Legacy cells retained below for reference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell_2_d100dc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:47.093393Z",
     "iopub.status.busy": "2025-11-15T16:27:47.093125Z",
     "iopub.status.idle": "2025-11-15T16:27:47.122460Z",
     "shell.execute_reply": "2025-11-15T16:27:47.121892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] ‚úÖ Derived APIM_SERVICE = apim-pavavy6pu5hpa\n",
      "[env] ‚úÖ Using default API_ID = inference-api\n",
      "[env] ‚úÖ BICEP_DIR = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "[env] ‚úÖ Loaded 32 environment variables\n",
      "[env] ‚úÖ Configuration: lab-master-lab @ uksouth\n",
      "[env] ‚úÖ APIM Gateway: https://apim-pavavy6pu5hpa.azure-api.net...\n"
     ]
    }
   ],
   "source": [
    "# (-1.1) Consolidated Environment Loader (Enhanced)\n",
    "\"\"\"\n",
    "Single source of truth for environment configuration.\n",
    "Enhancements:\n",
    "- Auto-creates master-lab.env if missing\n",
    "- Loads and validates environment variables\n",
    "- Derives APIM_SERVICE from APIM_GATEWAY_URL if missing\n",
    "- Sets BICEP_DIR for deployment files\n",
    "- Provides NotebookConfig dataclass for structured access\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import re, os\n",
    "\n",
    "ENV_FILE = Path('master-lab.env')\n",
    "TEMPLATE = \"\"\"# master-lab.env (auto-generated template)\n",
    "SUBSCRIPTION_ID=\n",
    "RESOURCE_GROUP=\n",
    "LOCATION=uksouth\n",
    "APIM_GATEWAY_URL=\n",
    "APIM_SERVICE=\n",
    "API_ID=inference-api\n",
    "INFERENCE_API_PATH=/inference\n",
    "OPENAI_ENDPOINT=\n",
    "MODEL_SKU=gpt-4o-mini\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class NotebookConfig:\n",
    "    \"\"\"Structured configuration object\"\"\"\n",
    "    subscription_id: str = \"\"\n",
    "    resource_group: str = \"\"\n",
    "    location: str = \"uksouth\"\n",
    "    apim_gateway_url: str = \"\"\n",
    "    apim_service: str = \"\"\n",
    "    api_id: str = \"inference-api\"\n",
    "    inference_api_path: str = \"/inference\"\n",
    "    openai_endpoint: Optional[str] = None\n",
    "    model_sku: str = \"gpt-4o-mini\"\n",
    "\n",
    "def ensure_env():\n",
    "    \"\"\"Load environment file, create if missing\"\"\"\n",
    "    if not ENV_FILE.exists():\n",
    "        ENV_FILE.write_text(TEMPLATE, encoding='utf-8')\n",
    "        print(f\"[env] Created {ENV_FILE} - PLEASE FILL IN VALUES\")\n",
    "        return {}\n",
    "\n",
    "    env = {}\n",
    "    for line in ENV_FILE.read_text(encoding='utf-8').splitlines():\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            key, value = key.strip(), value.strip()\n",
    "            if value:  # Only set non-empty values\n",
    "                env[key] = value\n",
    "                os.environ[key] = value\n",
    "\n",
    "    # Auto-derive APIM_SERVICE if missing\n",
    "    if 'APIM_SERVICE' not in env and 'APIM_GATEWAY_URL' in env:\n",
    "        match = re.search(r'//([^.]+)', env['APIM_GATEWAY_URL'])\n",
    "        if match:\n",
    "            env['APIM_SERVICE'] = match.group(1)\n",
    "            os.environ['APIM_SERVICE'] = env['APIM_SERVICE']\n",
    "            print(f\"[env] ‚úÖ Derived APIM_SERVICE = {env['APIM_SERVICE']}\")\n",
    "\n",
    "    # Set default API_ID if missing\n",
    "    if 'API_ID' not in env:\n",
    "        env['API_ID'] = 'inference-api'\n",
    "        os.environ['API_ID'] = env['API_ID']\n",
    "        print(f\"[env] ‚úÖ Using default API_ID = {env['API_ID']}\")\n",
    "\n",
    "    return env\n",
    "\n",
    "# Load environment\n",
    "ENV = ensure_env()\n",
    "\n",
    "# Create config object for structured access\n",
    "config = NotebookConfig(\n",
    "    subscription_id=ENV.get('SUBSCRIPTION_ID', ''),\n",
    "    resource_group=ENV.get('RESOURCE_GROUP', ''),\n",
    "    location=ENV.get('LOCATION', 'uksouth'),\n",
    "    apim_gateway_url=ENV.get('APIM_GATEWAY_URL', ''),\n",
    "    apim_service=ENV.get('APIM_SERVICE', ''),\n",
    "    api_id=ENV.get('API_ID', 'inference-api'),\n",
    "    inference_api_path=ENV.get('INFERENCE_API_PATH', '/inference'),\n",
    "    openai_endpoint=ENV.get('OPENAI_ENDPOINT'),\n",
    "    model_sku=ENV.get('MODEL_SKU', 'gpt-4o-mini')\n",
    ")\n",
    "\n",
    "# Set BICEP_DIR for deployment files\n",
    "# HAVE TO CAHNGE IN FINAL COMMIT\n",
    "BICEP_DIR = Path(\"archive/scripts\")\n",
    "if not BICEP_DIR.exists():\n",
    "    print(f\"[env] ‚ö†Ô∏è  BICEP_DIR not found: {BICEP_DIR.resolve()}\")\n",
    "    BICEP_DIR = Path(\".\")  # Fallback\n",
    "else:\n",
    "    print(f\"[env] ‚úÖ BICEP_DIR = {BICEP_DIR.resolve()}\")\n",
    "\n",
    "os.environ['BICEP_DIR'] = str(BICEP_DIR.resolve())\n",
    "\n",
    "# Summary\n",
    "print(f\"[env] ‚úÖ Loaded {len(ENV)} environment variables\")\n",
    "print(f\"[env] ‚úÖ Configuration: {config.resource_group} @ {config.location}\")\n",
    "if config.apim_gateway_url:\n",
    "    print(f\"[env] ‚úÖ APIM Gateway: {config.apim_gateway_url[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell_3_41f69468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:47.124983Z",
     "iopub.status.busy": "2025-11-15T16:27:47.124750Z",
     "iopub.status.idle": "2025-11-15T16:27:47.729939Z",
     "shell.execute_reply": "2025-11-15T16:27:47.729172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deps] /usr/bin/python3 -m pip install -r requirements.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[deps] ‚ö†Ô∏è pip exit 1 stderr: \u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m√ó\u001b[0m This environment is externally managed\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xy\n"
     ]
    }
   ],
   "source": [
    "# (-1.2) Dependencies Install (Consolidated)\n",
    "import sys, subprocess, pathlib, shlex\n",
    "REQ_FILE = pathlib.Path('requirements.txt')\n",
    "if REQ_FILE.exists():\n",
    "    cmd=[sys.executable,'-m','pip','install','-r',str(REQ_FILE)]\n",
    "    print('[deps]',' '.join(shlex.quote(c) for c in cmd))\n",
    "    r=subprocess.run(cmd,capture_output=True,text=True)\n",
    "    print(r.stdout[:800])\n",
    "    if r.returncode==0: print('[deps] ‚úÖ complete')\n",
    "    else: print('[deps] ‚ö†Ô∏è pip exit',r.returncode,'stderr:',r.stderr[:200])\n",
    "else:\n",
    "    print('[deps] requirements.txt missing; skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell_4_820d7759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:47.731744Z",
     "iopub.status.busy": "2025-11-15T16:27:47.731600Z",
     "iopub.status.idle": "2025-11-15T16:27:49.842430Z",
     "shell.execute_reply": "2025-11-15T16:27:49.841556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az resolved: /usr/bin/az (reason=ranked selection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az version: azure-cli                         2.78.0 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] Active subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[azure] Loading existing credentials file\n",
      "  SUBSCRIPTION_ID=d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  AZURE_CLIENT_ID=4a5d0f1a-578e-479a-8ba9-05770ae9ce6b\n",
      "  AZURE_TENANT_ID=2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "  AZURE_CLIENT_SECRET=***\n"
     ]
    }
   ],
   "source": [
    "# (-1.3) Azure CLI & Service Principal Setup (Consolidated v2)\n",
    "import json, os, shutil, subprocess, sys, time\n",
    "from pathlib import Path\n",
    "AZ_CREDS_FILE=Path('.azure-credentials.env')\n",
    "\n",
    "OS_RELEASE = {}\n",
    "try:\n",
    "    if Path('/etc/os-release').exists():\n",
    "        for line in Path('/etc/os-release').read_text().splitlines():\n",
    "            if '=' in line:\n",
    "                k,v=line.split('=',1)\n",
    "                OS_RELEASE[k]=v.strip().strip('\"')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ARCH_LINUX = OS_RELEASE.get('ID') == 'arch'\n",
    "CODESPACES = bool(os.environ.get('CODESPACES')) or bool(os.environ.get('CODESPACE_NAME'))\n",
    "\n",
    "def resolve_az_cli():\n",
    "    # 1. Explicit override\n",
    "    override=os.environ.get('AZURE_CLI_PATH')\n",
    "    if override and Path(override).exists():\n",
    "        return override, 'env AZURE_CLI_PATH'\n",
    "    candidates = []\n",
    "    # which-based\n",
    "    for name in ['az','az.cmd','az.exe']:\n",
    "        p=shutil.which(name)\n",
    "        if p: candidates.append(p)\n",
    "    # Common Linux / macOS locations\n",
    "    candidates += [\n",
    "        '/usr/bin/az', '/usr/local/bin/az', '/snap/bin/az', '/opt/homebrew/bin/az'\n",
    "    ]\n",
    "    # Codespaces typical path (if pip user install)\n",
    "    if CODESPACES:\n",
    "        candidates.append(str(Path.home()/'.local/bin/az'))\n",
    "    # Windows typical install locations\n",
    "    candidates += [\n",
    "        'C:/Program Files (x86)/Microsoft SDKs/Azure/CLI2/wbin/az.cmd',\n",
    "        'C:/Program Files/Microsoft SDKs/Azure/CLI2/wbin/az.cmd'\n",
    "    ]\n",
    "    # Home azure-cli shim\n",
    "    home_cli = Path.home()/'.azure-cli/az'\n",
    "    candidates.append(str(home_cli))\n",
    "    # Remove non-existing\n",
    "    existing=[c for c in candidates if c and Path(c).exists()]\n",
    "    if not existing:\n",
    "        # Last-resort: if a pip install put az inside .venv Scripts\n",
    "        venv_az = Path(sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "        if venv_az.exists():\n",
    "            return str(venv_az), 'venv fallback'\n",
    "        return None, 'not found'\n",
    "    # Rank: prefer system-level (exclude .venv & Scripts) then shortest path\n",
    "    def rank(p):\n",
    "        p_low=p.lower()\n",
    "        penalty = 1000 if ('.venv' in p_low or 'scripts' in p_low) else 0\n",
    "        return penalty, len(p)\n",
    "    existing.sort(key=rank)\n",
    "    chosen=existing[0]\n",
    "    return chosen, 'ranked selection'\n",
    "\n",
    "az_cli, reason = resolve_az_cli()\n",
    "print(f'[azure] az resolved: {az_cli or \"NOT FOUND\"} (reason={reason})')\n",
    "if not az_cli:\n",
    "    if ARCH_LINUX:\n",
    "        print('[azure] Arch Linux detected. Install Azure CLI: sudo pacman -S azure-cli')\n",
    "    else:\n",
    "        print('[azure] Install Azure CLI: https://learn.microsoft.com/cli/azure/install-azure-cli')\n",
    "    raise SystemExit('Azure CLI not found.')\n",
    "\n",
    "os.environ['AZ_CLI']=az_cli\n",
    "# Quick version check with short timeout\n",
    "try:\n",
    "    ver=subprocess.run([az_cli,'--version'],capture_output=True,text=True,timeout=4)\n",
    "    if ver.returncode==0:\n",
    "        first_line=ver.stdout.splitlines()[0] if ver.stdout else ''\n",
    "        print('[azure] az version:', first_line)\n",
    "    else:\n",
    "        print('[azure] az --version exit', ver.returncode)\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('[azure] WARN: az version check timed out (continuing)')\n",
    "except Exception as e:\n",
    "    print('[azure] WARN: az version check error:', e)\n",
    "\n",
    "# Subscription discovery (robust with timeout retries)\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')  # existing env takes precedence\n",
    "sub_proc = None\n",
    "if not subscription_id:\n",
    "    attempts = 2\n",
    "    for attempt in range(1, attempts + 1):\n",
    "        try:\n",
    "            timeout_sec = 8 if attempt == 1 else 20  # longer second attempt\n",
    "            sub_proc = subprocess.run(\n",
    "                [az_cli, 'account', 'show', '--output', 'json'],\n",
    "                capture_output=True, text=True, timeout=timeout_sec\n",
    "            )\n",
    "            if sub_proc.returncode == 0:\n",
    "                try:\n",
    "                    sub = json.loads(sub_proc.stdout)\n",
    "                    subscription_id = sub.get('id')\n",
    "                    print('[azure] Active subscription:', subscription_id)\n",
    "                    if subscription_id:\n",
    "                        os.environ.setdefault('SUBSCRIPTION_ID', subscription_id)\n",
    "                except Exception as e:\n",
    "                    print('[azure] Parse error account show:', e)\n",
    "                break\n",
    "            else:\n",
    "                print(f'[azure] account show failed (rc={sub_proc.returncode}): {sub_proc.stderr[:200]}')\n",
    "                break  # non-timeout failure; do not retry\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f'[azure] account show timed out (attempt {attempt}/{attempts}, timeout={timeout_sec}s)')\n",
    "            if attempt < attempts:\n",
    "                time.sleep(retry_delay_sec)  # use existing retry delay variable\n",
    "            else:\n",
    "                print('[azure] ERROR: account show timed out; skipping subscription discovery')\n",
    "else:\n",
    "    print('[azure] Using existing SUBSCRIPTION_ID from environment:', subscription_id)\n",
    "\n",
    "# Ensure Service Principal\n",
    "sp_env_keys=['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID']\n",
    "creds_present=all(os.environ.get(k) for k in sp_env_keys)\n",
    "if creds_present:\n",
    "    print('[azure] SP credentials already present; skipping creation')\n",
    "elif AZ_CREDS_FILE.exists():\n",
    "    print('[azure] Loading existing credentials file')\n",
    "    for line in AZ_CREDS_FILE.read_text().splitlines():\n",
    "        if line.strip() and '=' in line:\n",
    "            k,v=line.split('=',1); os.environ.setdefault(k.strip(),v.strip())\n",
    "else:\n",
    "    if not os.environ.get('SUBSCRIPTION_ID'):\n",
    "        print('[azure] Cannot create SP: missing SUBSCRIPTION_ID')\n",
    "    else:\n",
    "        print('[azure] Creating new service principal (Contributor)')\n",
    "        sp_cmd=[az_cli,'ad','sp','create-for-rbac','--name','ai-gateway-sp','--role','Contributor','--scopes',f\"/subscriptions/{os.environ.get('SUBSCRIPTION_ID','')}\",\"--sdk-auth\"]\n",
    "        r=subprocess.run(sp_cmd,capture_output=True,text=True,timeout=40)\n",
    "        if r.returncode!=0:\n",
    "            print('[azure] SP creation failed:', r.stderr[:300])\n",
    "        else:\n",
    "            data=json.loads(r.stdout)\n",
    "            mapping={'clientId':'AZURE_CLIENT_ID','clientSecret':'AZURE_CLIENT_SECRET','tenantId':'AZURE_TENANT_ID','subscriptionId':'SUBSCRIPTION_ID'}\n",
    "            for src,dst in mapping.items():\n",
    "                if src in data:\n",
    "                    os.environ[dst]=data[src]\n",
    "            lines=[f'{k}={os.environ[k]}' for k in mapping.values() if k in os.environ]\n",
    "            AZ_CREDS_FILE.write_text('\\n'.join(lines))\n",
    "            print('[azure] SP created & credentials saved (.azure-credentials.env)')\n",
    "\n",
    "# Masked summary\n",
    "for k in ['SUBSCRIPTION_ID','AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET']:\n",
    "    v=os.environ.get(k)\n",
    "    if not v: continue\n",
    "    masked='***' if 'SECRET' in k else v\n",
    "    print(f'  {k}={masked}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell_4b_msal_helper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:49.844570Z",
     "iopub.status.busy": "2025-11-15T16:27:49.844368Z",
     "iopub.status.idle": "2025-11-15T16:27:49.850356Z",
     "shell.execute_reply": "2025-11-15T16:27:49.849345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[msal] MSAL cache flush helpers loaded\n",
      "[msal] Available functions: flush_msal_cache(), az_with_msal_retry()\n"
     ]
    }
   ],
   "source": [
    "# (-1.3b) MSAL Cache Flush Helper\n",
    "\"\"\"Helper function to flush MSAL cache when Azure CLI encounters MSAL corruption.\n",
    "\n",
    "The MSAL error 'Can't get attribute NormalizedResponse' indicates cache corruption.\n",
    "This helper safely clears the MSAL cache and retries Azure CLI operations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def flush_msal_cache():\n",
    "    \"\"\"Flush MSAL cache directories to resolve cache corruption.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if cache was flushed successfully\n",
    "    \"\"\"\n",
    "    msal_cache_dirs = [\n",
    "        Path.home() / '.azure' / 'msal_token_cache.bin',\n",
    "        Path.home() / '.azure' / 'msal_token_cache.json',\n",
    "        Path.home() / '.azure' / 'msal_http_cache',\n",
    "        Path.home() / '.azure' / 'service_principal_entries.bin',\n",
    "    ]\n",
    "    \n",
    "    flushed = []\n",
    "    for cache_path in msal_cache_dirs:\n",
    "        try:\n",
    "            if cache_path.exists():\n",
    "                if cache_path.is_file():\n",
    "                    cache_path.unlink()\n",
    "                    flushed.append(str(cache_path))\n",
    "                elif cache_path.is_dir():\n",
    "                    shutil.rmtree(cache_path)\n",
    "                    flushed.append(str(cache_path))\n",
    "        except Exception as e:\n",
    "            print(f'[msal] Warning: Could not remove {cache_path}: {e}')\n",
    "    \n",
    "    if flushed:\n",
    "        print(f'[msal] Flushed {len(flushed)} cache entries')\n",
    "        return True\n",
    "    else:\n",
    "        print('[msal] No cache entries found to flush')\n",
    "        return False\n",
    "\n",
    "def az_with_msal_retry(az_cli, command_args, **kwargs):\n",
    "    \"\"\"Execute Azure CLI command with automatic MSAL cache flush on error.\n",
    "    \n",
    "    Args:\n",
    "        az_cli: Path to az CLI executable\n",
    "        command_args: List of command arguments (e.g., ['account', 'show'])\n",
    "        **kwargs: Additional arguments for subprocess.run()\n",
    "    \n",
    "    Returns:\n",
    "        subprocess.CompletedProcess: Result of the command\n",
    "    \"\"\"\n",
    "    # Ensure capture_output and text are set\n",
    "    kwargs.setdefault('capture_output', True)\n",
    "    kwargs.setdefault('text', True)\n",
    "    kwargs.setdefault('timeout', 30)\n",
    "    \n",
    "    # First attempt\n",
    "    result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "    \n",
    "    # Check for MSAL error\n",
    "    if result.returncode != 0 and 'NormalizedResponse' in result.stderr:\n",
    "        print('[msal] MSAL cache corruption detected, flushing cache...')\n",
    "        flush_msal_cache()\n",
    "        \n",
    "        # Re-login if needed\n",
    "        print('[msal] Re-authenticating...')\n",
    "        login_result = subprocess.run(\n",
    "            [az_cli, 'login'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if login_result.returncode == 0:\n",
    "            print('[msal] Re-authentication successful, retrying command...')\n",
    "            # Retry the original command\n",
    "            result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "        else:\n",
    "            print(f'[msal] Re-authentication failed: {login_result.stderr[:200]}')\n",
    "    \n",
    "    return result\n",
    "\n",
    "print('[msal] MSAL cache flush helpers loaded')\n",
    "print('[msal] Available functions: flush_msal_cache(), az_with_msal_retry()')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell_5_c9ea7412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:49.853012Z",
     "iopub.status.busy": "2025-11-15T16:27:49.852753Z",
     "iopub.status.idle": "2025-11-15T16:27:49.872259Z",
     "shell.execute_reply": "2025-11-15T16:27:49.871345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[endpoint] Derived from APIM_GATEWAY_URL + INFERENCE_API_PATH\n",
      "[endpoint] OPENAI_ENDPOINT = https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "[endpoint] Persisted derived endpoint to master-lab.env\n",
      "[endpoint] Derived convenience vars: OPENAI_API_BASE, OPENAI_MODELS_URL\n"
     ]
    }
   ],
   "source": [
    "# (-1.4) Endpoint Normalizer & Derived Variables\n",
    "\"\"\"\n",
    "Derives OPENAI_ENDPOINT and related derived variables if missing.\n",
    "Logic priority:\n",
    "1. Use explicit OPENAI_ENDPOINT if set (leave unchanged).\n",
    "2. Else if APIM_GATEWAY_URL + INFERENCE_API_PATH present -> compose.\n",
    "3. Else attempt Foundry style endpoints (AZURE_OPENAI_ENDPOINT, AI_FOUNDRY_ENDPOINT).\n",
    "Persist back to master-lab.env if value was newly derived.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os, re\n",
    "env_path=Path('master-lab.env')\n",
    "text=env_path.read_text() if env_path.exists() else ''\n",
    "get=lambda k: os.environ.get(k) or re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE).group(1).strip() if re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE) else ''\n",
    "openai_endpoint=get('OPENAI_ENDPOINT')\n",
    "modified=False\n",
    "if openai_endpoint:\n",
    "    print('[endpoint] Existing OPENAI_ENDPOINT found; using as-is')\n",
    "else:\n",
    "    apim=get('APIM_GATEWAY_URL')\n",
    "    path_var=get('INFERENCE_API_PATH') or '/inference'\n",
    "    if apim:\n",
    "        openai_endpoint=apim.rstrip('/')+path_var\n",
    "        print('[endpoint] Derived from APIM_GATEWAY_URL + INFERENCE_API_PATH')\n",
    "        modified=True\n",
    "    else:\n",
    "        fallback=get('AZURE_OPENAI_ENDPOINT') or get('AI_FOUNDRY_ENDPOINT')\n",
    "        if fallback:\n",
    "            openai_endpoint=fallback.rstrip('/')\n",
    "            print('[endpoint] Derived from Foundry/Azure fallback endpoint')\n",
    "            modified=True\n",
    "        else:\n",
    "            print('[endpoint] Unable to derive endpoint; please set OPENAI_ENDPOINT manually in master-lab.env')\n",
    "if openai_endpoint:\n",
    "    os.environ['OPENAI_ENDPOINT']=openai_endpoint\n",
    "    print('[endpoint] OPENAI_ENDPOINT =', openai_endpoint)\n",
    "    if modified and env_path.exists():\n",
    "        # update file\n",
    "        lines=[]\n",
    "        found=False\n",
    "        for line in text.splitlines():\n",
    "            if line.startswith('OPENAI_ENDPOINT='):\n",
    "                lines.append(f'OPENAI_ENDPOINT={openai_endpoint}')\n",
    "                found=True\n",
    "            else:\n",
    "                lines.append(line)\n",
    "        if not found:\n",
    "            lines.append(f'OPENAI_ENDPOINT={openai_endpoint}')\n",
    "        env_path.write_text('\\n'.join(lines))\n",
    "        print('[endpoint] Persisted derived endpoint to master-lab.env')\n",
    "# Convenience derived variables (could be referenced later)\n",
    "os.environ.setdefault('OPENAI_API_BASE', openai_endpoint)\n",
    "os.environ.setdefault('OPENAI_MODELS_URL', openai_endpoint.rstrip('/') + '/models')\n",
    "print('[endpoint] Derived convenience vars: OPENAI_API_BASE, OPENAI_MODELS_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell_6_6bbec029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:49.875325Z",
     "iopub.status.busy": "2025-11-15T16:27:49.875107Z",
     "iopub.status.idle": "2025-11-15T16:27:51.397864Z",
     "shell.execute_reply": "2025-11-15T16:27:51.397240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[az] version: azure-cli                         2.78.0 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[az] account: ME-MngEnvMCAP592090-lproux-1 d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n"
     ]
    }
   ],
   "source": [
    "# (-1.5) Unified az() Helper & Login Check\n",
    "\"\"\"Provides a cached az CLI executor with:\n",
    "- Path reuse via AZ_CLI env (expects (-1.3) run first)\n",
    "- Automatic login prompt if account show fails and no service principal creds\n",
    "- Timeout controls & JSON parsing convenience\n",
    "Usage:\n",
    "    ok, data = az('account show', json_out=True)\n",
    "    ok, text = az('apim list --resource-group X')\n",
    "\"\"\"\n",
    "import os, subprocess, json, shlex\n",
    "from pathlib import Path\n",
    "AZ_CLI = os.environ.get('AZ_CLI') or os.environ.get('AZURE_CLI_PATH')\n",
    "_cached_version=None\n",
    "\n",
    "def az(cmd:str, json_out:bool=False, timeout:int=25, login_if_needed:bool=True):\n",
    "    global _cached_version\n",
    "    if not AZ_CLI:\n",
    "        return False, 'AZ_CLI not set; run (-1.3) first.'\n",
    "    parts=[AZ_CLI]+shlex.split(cmd)\n",
    "    try:\n",
    "        proc=subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, f'timeout after {timeout}s: {cmd}'\n",
    "    if proc.returncode!=0:\n",
    "        stderr=proc.stderr.strip()\n",
    "        if login_if_needed and 'az login' in stderr.lower():\n",
    "            # If SP creds exist, attempt non-interactive login; else instruct.\n",
    "            sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET'])\n",
    "            if sp_ok:\n",
    "                sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} \"\n",
    "                        f\"-p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "                print('[az] Attempting SP login ...')\n",
    "                lp=subprocess.run([AZ_CLI]+shlex.split(sp_cmd),capture_output=True,text=True,timeout=40)\n",
    "                if lp.returncode==0:\n",
    "                    print('[az] SP login successful; retrying command')\n",
    "                    return az(cmd,json_out=json_out,timeout=timeout,login_if_needed=False)\n",
    "                else:\n",
    "                    print('[az] SP login failed:', lp.stderr[:180])\n",
    "            else:\n",
    "                print('[az] Interactive login required: run \"az login\" in a terminal.')\n",
    "        return False, stderr or proc.stdout\n",
    "    out=proc.stdout\n",
    "    if json_out:\n",
    "        try:\n",
    "            return True, json.loads(out or '{}')\n",
    "        except Exception as e:\n",
    "            return False, f'json parse error: {e}\\nRaw: {out[:200]}'\n",
    "    return True, out\n",
    "\n",
    "# Cache version lazily\n",
    "if not _cached_version:\n",
    "    ok, ver = az('--version', json_out=False, timeout=5, login_if_needed=False)\n",
    "    if ok:\n",
    "        _cached_version=ver.splitlines()[0] if ver else ''\n",
    "        print('[az] version:', _cached_version)\n",
    "    else:\n",
    "        print('[az] version check skipped:', ver[:120])\n",
    "\n",
    "# Quick account context (suppresses login if SP already authenticated)\n",
    "ok, acct = az('account show', json_out=True, timeout=10)\n",
    "if ok:\n",
    "    print('[az] account:', acct.get('name'), acct.get('id'))\n",
    "else:\n",
    "    print('[az] account show issue:', acct[:160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell_7_778421b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:51.400683Z",
     "iopub.status.busy": "2025-11-15T16:27:51.400290Z",
     "iopub.status.idle": "2025-11-15T16:27:51.409787Z",
     "shell.execute_reply": "2025-11-15T16:27:51.408809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shim] AzureOpenAI shim ready.\n",
      "[deploy] helpers ready\n"
     ]
    }
   ],
   "source": [
    "# (-1.6) Deployment Helpers (Consolidated)\n",
    "\"\"\"Utilities for ARM/Bicep deployments via az CLI.\n",
    "Depends on az() from (-1.5).\n",
    "Functions:\n",
    "  compile_bicep(bicep_path) -> str json_template_path\n",
    "  deploy_template(rg, name, template_file, params: dict) -> (ok, result_json)\n",
    "  get_deployment_outputs(rg, name) -> dict outputs or {}\n",
    "  ensure_deployment(rg, name, template, params, skip_if_exists=True)\n",
    "\"\"\"\n",
    "import os, json, tempfile, pathlib, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "def compile_bicep(bicep_path:str):\n",
    "    b=Path(bicep_path)\n",
    "    if not b.exists():\n",
    "        raise FileNotFoundError(f'Bicep file not found: {bicep_path}')\n",
    "    out_json = b.with_suffix('.json')\n",
    "    ok, res = az(f'bicep build --file {shlex.quote(str(b))}')\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Failed bicep build: {res}')\n",
    "    if not out_json.exists():\n",
    "        raise RuntimeError(f'Expected compiled template missing: {out_json}')\n",
    "    print('[deploy] compiled', bicep_path, '->', out_json)\n",
    "    return str(out_json)\n",
    "\n",
    "def deploy_template(rg:str, name:str, template_file:str, params:dict):\n",
    "    param_args=[]\n",
    "    for k,v in params.items():\n",
    "        if isinstance(v, (dict,list)):\n",
    "            # Write complex params to temp file\n",
    "            tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "            tmp.write_text(json.dumps({\"value\": v}, indent=2))\n",
    "            param_args.append(f'{k}=@{tmp}')\n",
    "        else:\n",
    "            param_args.append(f'{k}={json.dumps(v)}')\n",
    "    params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "    cmd=f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}'\n",
    "    print('[deploy] running:', cmd)\n",
    "    ok, res = az(cmd, json_out=True, timeout=600)\n",
    "    return ok, res\n",
    "\n",
    "def get_deployment_outputs(rg:str, name:str):\n",
    "    ok,res = az(f'deployment group show --resource-group {rg} --name {name}', json_out=True)\n",
    "    if not ok:\n",
    "        print('[deploy] show failed:', res[:140])\n",
    "        return {}\n",
    "    outputs = res.get('properties',{}).get('outputs',{})\n",
    "    simplified={k: v.get('value') for k,v in outputs.items()} if isinstance(outputs, dict) else {}\n",
    "    print('[deploy] outputs keys:', ', '.join(simplified.keys()))\n",
    "    return simplified\n",
    "\n",
    "def check_deployment_exists(rg:str, name:str):\n",
    "    ok,res=az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=15)\n",
    "    return ok and res.get('name')==name\n",
    "\n",
    "def ensure_deployment(rg:str, name:str, bicep_file:str, params:dict, skip_if_exists:bool=True):\n",
    "    if skip_if_exists and check_deployment_exists(rg,name):\n",
    "        print('[deploy] existing deployment found:', name)\n",
    "        return get_deployment_outputs(rg,name)\n",
    "    template=compile_bicep(bicep_file) if bicep_file.endswith('.bicep') else bicep_file\n",
    "    ok,res=deploy_template(rg,name,template,params)\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Deployment {name} failed: {res}')\n",
    "    return get_deployment_outputs(rg,name)\n",
    "\n",
    "# AzureOpenAI Compatibility Import Shim\n",
    "# Some cells use: from openai import AzureOpenAI\n",
    "# Provide a unified accessor that can adapt if future SDK reorganizes paths.\n",
    "\n",
    "def get_azure_openai_client(**kwargs):\n",
    "    try:\n",
    "        from openai import AzureOpenAI  # standard location\n",
    "        return AzureOpenAI(**kwargs)\n",
    "    except ImportError as ex:\n",
    "        raise ImportError(\"AzureOpenAI class not found; ensure openai>=2.2,<3 installed.\") from ex\n",
    "\n",
    "print('[shim] AzureOpenAI shim ready.')\n",
    "\n",
    "print('[deploy] helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell_8_a9abfe41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:51.412052Z",
     "iopub.status.busy": "2025-11-15T16:27:51.411803Z",
     "iopub.status.idle": "2025-11-15T16:27:51.421734Z",
     "shell.execute_reply": "2025-11-15T16:27:51.420981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] apply_policies(policies) ready with auto-discovery (using Azure REST API)\n"
     ]
    }
   ],
   "source": [
    "# (-1.7) Unified Policy Application with Auto-Discovery\n",
    "\n",
    "\"\"\"Applies one or more API Management policies to the target API using Azure REST API.\n",
    "\n",
    "Provide policies as a list of (policy_name, policy_xml_string).\n",
    "\n",
    "Automatically discovers the API ID if not set in environment.\n",
    "Creates policy payloads and invokes az rest to apply them.\n",
    "\n",
    "Requires ENV values: RESOURCE_GROUP, APIM_SERVICE (service name)\n",
    "Optional: API_ID (will be auto-discovered if not provided)\n",
    "\n",
    "Note: Uses Azure REST API because 'az apim api policy' command is not available in all CLI versions.\n",
    "\"\"\"\n",
    "\n",
    "import os, json as json_module, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_POLICY_ENV=['RESOURCE_GROUP','APIM_SERVICE']\n",
    "\n",
    "missing=[k for k in REQUIRED_POLICY_ENV if not os.environ.get(k)]\n",
    "\n",
    "if missing:\n",
    "    print('[policy] Missing env vars; set:', ', '.join(missing))\n",
    "else:\n",
    "    def discover_api_id():\n",
    "        \"\"\"Discover the API ID from APIM instance.\"\"\"\n",
    "        service = os.environ['APIM_SERVICE']\n",
    "        rg = os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get subscription ID\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print('[policy] Failed to get subscription ID')\n",
    "            return None\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "\n",
    "        # List APIs using REST API\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{service}/apis?api-version=2022-08-01')\n",
    "\n",
    "        print('[policy] Discovering APIs in APIM instance...')\n",
    "        ok, result = az(f'rest --method get --url \"{url}\"', json_out=True, timeout=60)\n",
    "\n",
    "        if not ok or not result:\n",
    "            print('[policy] Failed to list APIs')\n",
    "            return None\n",
    "\n",
    "        apis = result.get('value', [])\n",
    "\n",
    "        if not apis:\n",
    "            print('[policy] ERROR: No APIs found in APIM instance')\n",
    "            print('[policy] HINT: You may need to deploy the infrastructure first')\n",
    "            return None\n",
    "\n",
    "        # Prefer APIs with 'openai' in the name\n",
    "        openai_apis = [api for api in apis if 'openai' in api.get('name', '').lower()]\n",
    "\n",
    "        if openai_apis:\n",
    "            api_id = openai_apis[0]['name']\n",
    "            print(f'[policy] Found OpenAI API: {api_id}')\n",
    "        else:\n",
    "            api_id = apis[0]['name']\n",
    "            print(f'[policy] Using first available API: {api_id}')\n",
    "\n",
    "        return api_id\n",
    "\n",
    "    def apply_policies(policies):\n",
    "        service=os.environ['APIM_SERVICE']\n",
    "        rg=os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get or discover API_ID\n",
    "        api_id = os.environ.get('API_ID')\n",
    "\n",
    "        if not api_id:\n",
    "            print('[policy] API_ID not set in environment, discovering...')\n",
    "            api_id = discover_api_id()\n",
    "\n",
    "            if not api_id:\n",
    "                print('[policy] ERROR: Could not discover API ID')\n",
    "                print('[policy] HINT: Set API_ID environment variable or deploy infrastructure')\n",
    "                return\n",
    "\n",
    "            # Save for future use\n",
    "            os.environ['API_ID'] = api_id\n",
    "            print(f'[policy] Saved API_ID to environment: {api_id}')\n",
    "\n",
    "        # Get subscription ID\n",
    "        print('[policy] Getting subscription ID...')\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print(f'[policy] Failed to get subscription ID: {sub_result}')\n",
    "            return\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "        print(f'[policy] Subscription ID: {subscription_id}')\n",
    "        print(f'[policy] Using API ID: {api_id}')\n",
    "\n",
    "        for name, xml in policies:\n",
    "            xml = xml.strip()\n",
    "\n",
    "            # Azure REST API endpoint for APIM policy\n",
    "            url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "                   f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "                   f'/service/{service}/apis/{api_id}/policies/policy?api-version=2022-08-01')\n",
    "\n",
    "            # Policy payload in Azure format\n",
    "            policy_payload = {\n",
    "                \"properties\": {\n",
    "                    \"value\": xml,\n",
    "                    \"format\": \"xml\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write JSON payload to temp file (Windows-friendly)\n",
    "            payload_file = Path(tempfile.gettempdir()) / f'apim-{name}-payload.json'\n",
    "            with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "                json_module.dump(policy_payload, f, indent=2)\n",
    "\n",
    "            print(f'[policy] Applying {name} via REST API...')\n",
    "\n",
    "            # Use az rest command with @file syntax for body\n",
    "            cmd = f'rest --method put --url \"{url}\" --body @\"{payload_file}\" --headers \"Content-Type=application/json\"'\n",
    "\n",
    "            ok, res = az(cmd, json_out=False, timeout=120)\n",
    "\n",
    "            # Clean up temp file\n",
    "            try:\n",
    "                payload_file.unlink()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if ok:\n",
    "                print(f'[policy] {name} applied successfully')\n",
    "            else:\n",
    "                error_msg = str(res)[:400] if res else 'Unknown error'\n",
    "                print(f'[policy] {name} failed: {error_msg}')\n",
    "\n",
    "    print('[policy] apply_policies(policies) ready with auto-discovery (using Azure REST API)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell_9_9200941f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:51.423434Z",
     "iopub.status.busy": "2025-11-15T16:27:51.423270Z",
     "iopub.status.idle": "2025-11-15T16:27:51.618961Z",
     "shell.execute_reply": "2025-11-15T16:27:51.618217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing MCP Client...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MCP Client initialized successfully!\n",
      "\n",
      "üì° Deployed MCP Servers:\n",
      "   1. Excel Analytics: http://excel-mcp-72998.eastus.azurecontainer.io:8000\n",
      "   2. Research Documents: http://docs-mcp-72998.eastus.azurecontainer.io:8000\n",
      "   3. Weather: http://weather-mcp-72998.eastus.azurecontainer.io:8080\n",
      "\n",
      "   5. GitHub: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n",
      "   6. Spotify: http://spotify-mcp-72998.uksouth.azurecontainer.io:8080\n",
      "   7. Product Catalog: http://mcp-product-catalog-72998.uksouth.azurecontainer.io:8080\n",
      "   8. Place Order: http://mcp-place-order-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "üí° All servers initialized from .mcp-servers-config\n",
      "   Access via: mcp.github, mcp.weather, etc.\n"
     ]
    }
   ],
   "source": [
    "# (-1.8) Unified MCP Initialization (Updated to use MCPClient)\n",
    "\"\"\"Initializes all MCP servers using MCPClient from notebook_mcp_helpers.\n",
    "Reads configuration from .mcp-servers-config file.\n",
    "Creates a global 'mcp' object with attributes for each server:\n",
    "  mcp.excel, mcp.docs, mcp.weather, mcp.github,\n",
    "  mcp.spotify, mcp.product_catalog, mcp.place_order\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "# Check if already initialized\n",
    "if 'mcp' in globals() and hasattr(mcp, 'github'):\n",
    "    print(\"‚ö†Ô∏è  MCP Client already initialized. Skipping re-initialization.\")\n",
    "    print(f\"   Excel: {mcp.excel.server_url}\")\n",
    "    print(f\"   Docs: {mcp.docs.server_url}\")\n",
    "    print(f\"   Weather: {mcp.weather.server_url}\")\n",
    "    print(f\"\")\n",
    "    print(f\"   GitHub: {mcp.github.server_url}\")\n",
    "    print(f\"   Spotify: {mcp.spotify.server_url}\")\n",
    "    print(f\"   Product Catalog: {mcp.product_catalog.server_url}\")\n",
    "    print(f\"   Place Order: {mcp.place_order.server_url}\")\n",
    "else:\n",
    "    print(\"üîÑ Initializing MCP Client...\")\n",
    "    try:\n",
    "        mcp = MCPClient()\n",
    "        print(\"‚úÖ MCP Client initialized successfully!\")\n",
    "        print()\n",
    "        print(f\"üì° Deployed MCP Servers:\")\n",
    "        print(f\"   1. Excel Analytics: {mcp.excel.server_url}\")\n",
    "        print(f\"   2. Research Documents: {mcp.docs.server_url}\")\n",
    "        print(f\"   3. Weather: {mcp.weather.server_url}\")\n",
    "        print(f\"\")\n",
    "        print(f\"   5. GitHub: {mcp.github.server_url}\")\n",
    "        print(f\"   6. Spotify: {mcp.spotify.server_url}\")\n",
    "        print(f\"   7. Product Catalog: {mcp.product_catalog.server_url}\")\n",
    "        print(f\"   8. Place Order: {mcp.place_order.server_url}\")\n",
    "        print()\n",
    "        print(f\"üí° All servers initialized from .mcp-servers-config\")\n",
    "        print(f\"   Access via: mcp.github, mcp.weather, etc.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize MCP Client: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# For backward compatibility: create MCP_SERVERS dict\n",
    "MCP_SERVERS = {\n",
    "    'excel': mcp.excel,\n",
    "    'docs': mcp.docs,\n",
    "    'weather': mcp.weather,\n",
    "    'github': mcp.github,\n",
    "    'spotify': mcp.spotify,\n",
    "    'product_catalog': mcp.product_catalog,\n",
    "    'place_order': mcp.place_order,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell_10_78abcae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:51.621089Z",
     "iopub.status.busy": "2025-11-15T16:27:51.620924Z",
     "iopub.status.idle": "2025-11-15T16:27:53.197783Z",
     "shell.execute_reply": "2025-11-15T16:27:53.196662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureOps] SP credential init failed: name 'ClientSecretCredential' is not defined\n",
      "[AzureOps] AzureCliCredential failed (defer login): name 'AzureCliCredential' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureOps] CLI: /usr/bin/az\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AzureOps] login status: OK\n",
      "[AzureOps] version: azure-cli                         2.78.0 *\n",
      "[AzureOps] strategy: sdk\n"
     ]
    }
   ],
   "source": [
    "# (-1.9) Unified AzureOps Wrapper (Enhanced SDK Strategy)\n",
    "\"\"\"High-level Azure operations wrapper consolidating:\n",
    "- CLI resolution & version\n",
    "- Service principal / interactive login fallback\n",
    "- Generic az() invocation (JSON/text)\n",
    "- Resource group ensure (CLI or SDK)\n",
    "- Bicep compile (CLI) + group deployment (CLI or SDK)\n",
    "- AI Foundry model deployments (SDK)\n",
    "- APIM policy fragments + API policy apply (with rollback)\n",
    "- Deployment outputs retrieval & simplification\n",
    "- MCP server health probing\n",
    "\n",
    "Strategy:\n",
    "    AzureOps(strategy='sdk' | 'cli')  # default 'sdk' to favor richer status & long-running handling.\n",
    "\n",
    "Example:\n",
    "    AZ_OPS = AzureOps(strategy='sdk')\n",
    "    AZ_OPS.ensure_login()\n",
    "    AZ_OPS.ensure_resource_group(rg, location)\n",
    "    tpl = AZ_OPS.compile_bicep('deploy-01-core.bicep')\n",
    "    ok, res = AZ_OPS.deploy_group(rg,'core',tpl, params={})\n",
    "    outputs = AZ_OPS.get_deployment_outputs(rg,'core')\n",
    "    AZ_OPS.ensure_policy_fragment(rg, service, 'semanticCacheFragment', xml)\n",
    "    AZ_OPS.apply_api_policy_with_fragments(rg, service, api_id, ['semanticCacheFragment','contentSafetyFragment'])\n",
    "\n",
    "NOTE: Legacy helper cells remain for reference; prefer AzureOps going forward.\n",
    "\"\"\"\n",
    "import os, shutil, subprocess, json, time, shlex, tempfile, sys, socket\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Optional Azure SDK imports (defer errors until used)\n",
    "try:\n",
    "    from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "    from azure.mgmt.resource import ResourceManagementClient\n",
    "    from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "    from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "except Exception as _sdk_err:\n",
    "    _AZURE_SDK_IMPORT_ERROR = _sdk_err\n",
    "else:\n",
    "    _AZURE_SDK_IMPORT_ERROR = None\n",
    "\n",
    "class DeploymentError(Exception):\n",
    "    pass\n",
    "class PolicyError(Exception):\n",
    "    pass\n",
    "class ModelDeploymentError(Exception):\n",
    "    pass\n",
    "\n",
    "class AzureOps:\n",
    "    def __init__(self, strategy: str = 'sdk'):\n",
    "        self.strategy = strategy.lower()\n",
    "        if self.strategy not in {'sdk','cli'}:\n",
    "            self.strategy = 'sdk'\n",
    "        self.az_cli = None\n",
    "        self.version = None\n",
    "        self.subscription_id = os.environ.get('SUBSCRIPTION_ID') or os.environ.get('AZURE_SUBSCRIPTION_ID') or ''\n",
    "        self.credential = None\n",
    "        self.resource_client: Optional[ResourceManagementClient] = None\n",
    "        self.cog_client: Optional[CognitiveServicesManagementClient] = None\n",
    "        self._resolve_cli()\n",
    "        self._init_credentials_if_possible()\n",
    "        self._cache_version()\n",
    "\n",
    "    # ---------- CLI RESOLUTION ----------\n",
    "    def _resolve_cli(self):\n",
    "        override = os.environ.get('AZURE_CLI_PATH')\n",
    "        if override and Path(override).exists():\n",
    "            self.az_cli = override\n",
    "        else:\n",
    "            candidates = []\n",
    "            for name in ['az','az.cmd','az.exe']:\n",
    "                p = shutil.which(name)\n",
    "                if p: candidates.append(p)\n",
    "            candidates += [ '/usr/bin/az','/usr/local/bin/az', str(Path.home()/'.local/bin/az'), str(Path.home()/'.azure-cli/az') ]\n",
    "            existing = [c for c in candidates if c and Path(c).exists()]\n",
    "            if not existing:\n",
    "                venv = Path(os.environ.get('VIRTUAL_ENV','') or sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "                if venv.exists(): existing=[str(venv)]\n",
    "            if existing:\n",
    "                def rank(p):\n",
    "                    pl=p.lower(); penalty=1000 if '.venv' in pl or 'scripts' in pl else 0\n",
    "                    return penalty, len(p)\n",
    "                existing.sort(key=rank)\n",
    "                self.az_cli = existing[0]\n",
    "            else:\n",
    "                self.az_cli = 'az'\n",
    "        os.environ['AZ_CLI'] = self.az_cli\n",
    "\n",
    "    # ---------- GENERIC az() INVOCATION ----------\n",
    "    def _run(self, parts, timeout=30):\n",
    "        try:\n",
    "            return subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            class Dummy: returncode=1; stdout=''; stderr=f'timeout>{timeout}s'\n",
    "            return Dummy()\n",
    "\n",
    "    def az(self, cmd: str, json_out=False, timeout=30, login_retry=True) -> Tuple[bool, str | Dict]:\n",
    "        parts=[self.az_cli]+shlex.split(cmd)\n",
    "        proc=self._run(parts,timeout)\n",
    "        if proc.returncode!=0:\n",
    "            stderr=proc.stderr.strip()\n",
    "            if login_retry and 'az login' in stderr.lower():\n",
    "                if self.ensure_login(silent=True):\n",
    "                    return self.az(cmd,json_out=json_out,timeout=timeout,login_retry=False)\n",
    "            return False, stderr or proc.stdout\n",
    "        out=proc.stdout\n",
    "        if json_out:\n",
    "            try:\n",
    "                return True, json.loads(out or '{}')\n",
    "            except Exception as e:\n",
    "                return False, f'json parse error: {e}\\n{out[:200]}'\n",
    "        return True, out\n",
    "\n",
    "    def _cache_version(self):\n",
    "        ok, ver = self.az('--version', json_out=False, timeout=6, login_retry=False)\n",
    "        if ok:\n",
    "            self.version = ver.splitlines()[0] if ver else ''\n",
    "\n",
    "    # ---------- AUTHENTICATION ----------\n",
    "    def _init_credentials_if_possible(self):\n",
    "        # Service Principal first\n",
    "        sp_keys = ['AZURE_TENANT_ID','AZURE_CLIENT_ID','AZURE_CLIENT_SECRET']\n",
    "        if all(os.environ.get(k) for k in sp_keys):\n",
    "            try:\n",
    "                self.credential = ClientSecretCredential(\n",
    "                    tenant_id=os.environ['AZURE_TENANT_ID'],\n",
    "                    client_id=os.environ['AZURE_CLIENT_ID'],\n",
    "                    client_secret=os.environ['AZURE_CLIENT_SECRET']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] SP credential init failed:', e)\n",
    "                self.credential=None\n",
    "        if self.credential is None:\n",
    "            try:\n",
    "                self.credential = AzureCliCredential()\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] AzureCliCredential failed (defer login):', e)\n",
    "                self.credential=None\n",
    "        # Resource client if SDK chosen\n",
    "        if self.strategy=='sdk' and self.credential and self.subscription_id:\n",
    "            if _AZURE_SDK_IMPORT_ERROR:\n",
    "                print('[AzureOps] SDK import error; fallback to CLI deployments:', _AZURE_SDK_IMPORT_ERROR)\n",
    "                self.strategy='cli'\n",
    "                return\n",
    "            try:\n",
    "                self.resource_client = ResourceManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] ResourceManagementClient init failed:', e)\n",
    "                self.resource_client=None\n",
    "            try:\n",
    "                self.cog_client = CognitiveServicesManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] CognitiveServicesManagementClient init failed:', e)\n",
    "                self.cog_client=None\n",
    "\n",
    "    def ensure_login(self, silent=False):\n",
    "        ok,_ = self.az('account show', json_out=True, login_retry=False, timeout=8)\n",
    "        if ok:\n",
    "            acct_id = _.get('id') if isinstance(_,dict) else None\n",
    "            if acct_id and not self.subscription_id:\n",
    "                self.subscription_id = acct_id\n",
    "            return True\n",
    "        # Attempt SP non-interactive if creds exist\n",
    "        sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID'])\n",
    "        if sp_ok:\n",
    "            sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} -p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "            proc=self._run([self.az_cli]+shlex.split(sp_cmd),timeout=40)\n",
    "            if proc.returncode==0:\n",
    "                if not silent: print('[AzureOps] SP login successful')\n",
    "                return True\n",
    "            else:\n",
    "                if not silent: print('[AzureOps] SP login failed:', proc.stderr[:160])\n",
    "        if not silent:\n",
    "            print('[AzureOps] Interactive login required: run \"az login\" in terminal')\n",
    "        return False\n",
    "\n",
    "    # ---------- RESOURCE GROUP ----------\n",
    "    def ensure_resource_group(self, rg: str, location: str) -> bool:\n",
    "        if self.strategy=='sdk' and self.resource_client:\n",
    "            try:\n",
    "                self.resource_client.resource_groups.create_or_update(rg, {'location': location})\n",
    "                print('[AzureOps] RG ensured (sdk):', rg)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] RG ensure failed (sdk):', e)\n",
    "        # CLI fallback\n",
    "        ok,res=self.az(f'group exists --name {rg}', json_out=False)\n",
    "        exists = ok and res.strip()=='true'\n",
    "        if exists:\n",
    "            print('[AzureOps] RG exists:', rg); return True\n",
    "        ok,_=self.az(f'group create --name {rg} --location {location}', json_out=True, timeout=120)\n",
    "        print('[AzureOps] RG created' if ok else '[AzureOps] RG create failed')\n",
    "        return ok\n",
    "\n",
    "    # ---------- BICEP COMPILE ----------\n",
    "    def compile_bicep(self, path: str) -> str:\n",
    "        b=Path(path); out=b.with_suffix('.json')\n",
    "        ok,res=self.az(f'bicep build --file {shlex.quote(str(b))}', json_out=False)\n",
    "        if not ok or not out.exists():\n",
    "            raise DeploymentError(f'Bicep compile failed: {res}')\n",
    "        print('[AzureOps] compiled', path, '->', out)\n",
    "        return str(out)\n",
    "\n",
    "    # ---------- DEPLOYMENT (CLI OR SDK) ----------\n",
    "    def _deploy_group_cli(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        param_args=[]\n",
    "        for k,v in params.items():\n",
    "            if isinstance(v,(dict,list)):\n",
    "                tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "                tmp.write_text(json.dumps({\"value\":v}))\n",
    "                param_args.append(f'{k}=@{tmp}')\n",
    "            else:\n",
    "                param_args.append(f'{k}={json.dumps(v)}')\n",
    "        params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "        cmd=(f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}')\n",
    "        print('[AzureOps] deploy(cli):', cmd)\n",
    "        ok,res=self.az(cmd,json_out=True,timeout=timeout)\n",
    "        return ok,res\n",
    "\n",
    "    def _deploy_group_sdk(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if not self.resource_client:\n",
    "            print('[AzureOps] SDK resource_client missing; fallback to CLI')\n",
    "            return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "        template = json.loads(Path(template_file).read_text(encoding='utf-8'))\n",
    "        # Convert params to ARM expected {k:{\"value\":v}}\n",
    "        arm_params={k:{'value':v} for k,v in params.items()}\n",
    "        properties={'mode':'Incremental','template':template,'parameters':arm_params}\n",
    "        print('[AzureOps] deploy(sdk):', name)\n",
    "        poller = self.resource_client.deployments.begin_create_or_update(rg,name,{'properties':properties})\n",
    "        start=time.time();\n",
    "        while not poller.done():\n",
    "            time.sleep(30)\n",
    "            elapsed=int(time.time()-start)\n",
    "            if elapsed%120<30:  # periodic status\n",
    "                print(f'  [AzureOps] deploying... {elapsed}s')\n",
    "        result=poller.result()\n",
    "        state=getattr(result.properties,'provisioning_state',None)\n",
    "        ok = state=='Succeeded'\n",
    "        if ok:\n",
    "            print('[AzureOps] deployment succeeded:', name)\n",
    "        else:\n",
    "            print('[AzureOps] deployment state:', state)\n",
    "        return ok, {'properties':{'outputs': getattr(result.properties,'outputs',{})}}\n",
    "\n",
    "    def deploy_group(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if self.strategy=='sdk':\n",
    "            return self._deploy_group_sdk(rg,name,template_file,params,timeout)\n",
    "        return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "\n",
    "    def get_deployment_outputs(self, rg: str, name: str) -> Dict[str,str]:\n",
    "        # Attempt CLI first for uniformity\n",
    "        ok,res=self.az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=60)\n",
    "        if ok and isinstance(res,dict):\n",
    "            outputs=res.get('properties',{}).get('outputs',{})\n",
    "            return {k:v.get('value') for k,v in outputs.items()} if isinstance(outputs,dict) else {}\n",
    "        # SDK fallback if available\n",
    "        if self.resource_client:\n",
    "            try:\n",
    "                dep=self.resource_client.deployments.get(rg,name)\n",
    "                outs=getattr(dep.properties,'outputs',{})\n",
    "                return {k:v.get('value') for k,v in outs.items()} if isinstance(outs,dict) else {}\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] outputs retrieval failed (sdk):', e)\n",
    "        return {}\n",
    "\n",
    "    # ---------- MODEL DEPLOYMENTS (AI Foundry) ----------\n",
    "    def deploy_models_via_sdk(self, rg: str, foundries: List[dict], models_config: Dict[str,List[dict]]):\n",
    "        if not self.cog_client:\n",
    "            print('[AzureOps] Cognitive Services client not initialized; skipping model deployments')\n",
    "            return {'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        existing_accounts={acc.name:acc for acc in self.cog_client.accounts.list_by_resource_group(rg)}\n",
    "        results={'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        # Ensure accounts\n",
    "        for f in foundries:\n",
    "            name=f['name']; location=f['location']\n",
    "            if name in existing_accounts:\n",
    "                print(f'  [AzureOps] foundry exists: {name}')\n",
    "            else:\n",
    "                print(f'  [AzureOps] creating foundry: {name}')\n",
    "                try:\n",
    "                    account_params=Account(location=location, sku=CogSku(name='S0'), kind='AIServices', properties={'customSubDomainName':name.lower(),'publicNetworkAccess':'Enabled','allowProjectManagement':True}, identity={'type':'SystemAssigned'})\n",
    "                    poll=self.cog_client.accounts.begin_create(rg,name,account_params)\n",
    "                    poll.result(timeout=600)\n",
    "                    print(f'    [AzureOps] created {name}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [AzureOps] create failed {name}: {e}'); continue\n",
    "        # Deploy models\n",
    "        for f in foundries:\n",
    "            name=f['name']; short=name.split('-')[0]\n",
    "            models=models_config.get(short,[])\n",
    "            print(f'  [AzureOps] models for {name}: {len(models)}')\n",
    "            for m in models:\n",
    "                mname=m['name']\n",
    "                try:\n",
    "                    # Exists check\n",
    "                    try:\n",
    "                        existing=self.cog_client.deployments.get(rg,name,mname)\n",
    "                        if existing.properties.provisioning_state=='Succeeded':\n",
    "                            print(f'    [skip] {mname} already')\n",
    "                            results['skipped'].append(f'{short}/{mname}')\n",
    "                            continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    dep_params=Deployment(sku=CogSku(name=m['sku'],capacity=m['capacity']), properties=DeploymentProperties(model=DeploymentModel(format=m['format'],name=m['name'],version=m['version'])))\n",
    "                    poll=self.cog_client.deployments.begin_create_or_update(rg,name,mname,dep_params)\n",
    "                    poll.result(timeout=900)\n",
    "                    print(f'    [ok] {mname}')\n",
    "                    results['succeeded'].append(f'{short}/{mname}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [fail] {mname}: {e}')\n",
    "                    results['failed'].append({'model':f'{short}/{mname}','error':str(e)})\n",
    "        return results\n",
    "\n",
    "    # ---------- POLICY FRAGMENTS & API POLICY ----------\n",
    "    def ensure_policy_fragment(self, rg: str, service: str, fragment_name: str, xml_policy: str):\n",
    "        body={\"properties\":{\"format\":\"xml\",\"value\":xml_policy.strip()}}\n",
    "        url=(f'https://management.azure.com/subscriptions/{self.subscription_id}/resourceGroups/{rg}/providers/Microsoft.ApiManagement/service/{service}/policyFragments/{fragment_name}?api-version=2023-03-01-preview')\n",
    "        body_json=json.dumps(body)\n",
    "        ok,res=self.az(f\"rest --method put --url {shlex.quote(url)} --body {shlex.quote(body_json)} --headers Content-Type=application/json\", json_out=True, timeout=120)\n",
    "        if ok:\n",
    "            print(f'[AzureOps] fragment ensured: {fragment_name}')\n",
    "        else:\n",
    "            print(f'[AzureOps] fragment failed {fragment_name}: {str(res)[:160]}')\n",
    "        return ok\n",
    "\n",
    "    def backup_api_policy(self, rg: str, service: str, api_id: str):\n",
    "        ok,res=self.az(f'apim api policy show --resource-group {rg} --service-name {service} --api-id {api_id}', json_out=False, timeout=60)\n",
    "        if not ok:\n",
    "            print('[AzureOps] no existing policy (show failed)'); return None\n",
    "        backup_dir=Path('.apim-policy-backups'); backup_dir.mkdir(exist_ok=True)\n",
    "        ts=time.strftime('%Y%m%d-%H%M%S')\n",
    "        file=backup_dir/f'{api_id}-{ts}.xml'\n",
    "        file.write_text(res)\n",
    "        print('[AzureOps] policy backed up:', file)\n",
    "        return str(file)\n",
    "\n",
    "    def apply_api_policy_with_fragments(self, rg: str, service: str, api_id: str, fragments: List[str], extra_inbound: str=''):\n",
    "        self.backup_api_policy(rg,service,api_id)\n",
    "        fragment_tags='\\n'.join(f'        <fragment ref=\"{f}\" />' for f in fragments)\n",
    "        inbound = f\"<inbound>\\n        <base />\\n{fragment_tags}\\n{extra_inbound}\\n    </inbound>\".rstrip()\n",
    "        policy_xml=f\"<policies>\\n{inbound}\\n    <backend><base /></backend>\\n    <outbound><base /></outbound>\\n    <on-error><base /></on-error>\\n</policies>\"\n",
    "        tmp=Path(tempfile.gettempdir())/f'apim-{api_id}-policy.xml'\n",
    "        tmp.write_text(policy_xml)\n",
    "        ok,res=self.az(f'apim api policy create --resource-group {rg} --service-name {service} --api-id {api_id} --xml-path {tmp}', json_out=False, timeout=180)\n",
    "        if not ok:\n",
    "            raise PolicyError(f'Policy apply failed: {res}')\n",
    "        print('[AzureOps] API policy applied with fragments:', fragments)\n",
    "        return True\n",
    "\n",
    "    # ---------- MCP HEALTH ----------\n",
    "    def mcp_health(self, servers: Dict[str,object]) -> Dict[str,Dict[str,str]]:\n",
    "        summary={}\n",
    "        for name,client in servers.items():\n",
    "            url=getattr(client,'server_url',None) or getattr(client,'url',None) or ''\n",
    "            status='unknown'; latency_ms='-'\n",
    "            if url.startswith('http'):  # basic TCP connect\n",
    "                try:\n",
    "                    host=url.split('//',1)[1].split('/',1)[0].split(':')[0]\n",
    "                    port=443 if url.startswith('https') else (int(url.split(':')[2].split('/')[0]) if ':' in url[8:] else 80)\n",
    "                    s=socket.socket(); s.settimeout(3); start=time.time(); s.connect((host,port)); s.close(); latency_ms=int((time.time()-start)*1000); status='ok'\n",
    "                except Exception:\n",
    "                    status='unreachable'\n",
    "            summary[name]={'url':url,'status':status,'latency_ms':latency_ms}\n",
    "        return summary\n",
    "\n",
    "# Instantiate global wrapper (prefer sdk)\n",
    "AZ_OPS = AzureOps(strategy=os.environ.get('AZ_OPS_STRATEGY','sdk'))\n",
    "print('[AzureOps] CLI:', AZ_OPS.az_cli)\n",
    "az_ok = AZ_OPS.ensure_login(silent=True)\n",
    "print('[AzureOps] login status:', 'OK' if az_ok else 'AUTH REQUIRED')\n",
    "if AZ_OPS.version: print('[AzureOps] version:', AZ_OPS.version)\n",
    "print('[AzureOps] strategy:', AZ_OPS.strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_11_c260b025",
   "metadata": {},
   "source": [
    "# Section -1: Consolidated Provisioning & Initialization\n",
    "\n",
    "This section provides an optimized, minimal set of cells to run the entire lab setup end-to-end.\n",
    "Run these in order, then skip legacy duplicates below. Original cells are retained for reference.\n",
    "\n",
    "Order:\n",
    "1. Env Loader & Masked Summary\n",
    "2. Dependency Installation\n",
    "3. Azure Auth + CLI + Service Principal\n",
    "4. Deployment Helpers (compile, deploy, utilities)\n",
    "5. Main 4-Step Deployment\n",
    "6. Generate master-lab.env\n",
    "7. Endpoint Normalizer (OPENAI + Inference)\n",
    "8. Unified Policy Application (Semantic Cache + Content Safety + others)\n",
    "9. Unified MCP Initialization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell_12_086d2df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.201030Z",
     "iopub.status.busy": "2025-11-15T16:27:53.200671Z",
     "iopub.status.idle": "2025-11-15T16:27:53.308839Z",
     "shell.execute_reply": "2025-11-15T16:27:53.308022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] Summary (masked=True)\n",
      "\n",
      "[apim]\n",
      "  APIM_API_ID = inference-api\n",
      "  APIM_API_KEY = b64e*************************cb0\n",
      "  APIM_GATEWAY_URL = https://apim-pavavy6pu5hpa.azure-api.net\n",
      "  APIM_NAME = apimmcpwksp321028\n",
      "  APIM_SERVICE = apim-pavavy6pu5hpa\n",
      "  APIM_SERVICE_ID = /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resourceGroups/lab-master-lab/providers/Microsoft.ApiManagement/service/apim-pavavy6pu5hpa\n",
      "  APIM_SERVICE_NAME = apim-pavavy6pu5hpa\n",
      "  INFERENCE_API_PATH = inference\n",
      "  LOCATION = uksouth\n",
      "  RESOURCE_GROUP = lab-master-lab\n",
      "\n",
      "[redis]\n",
      "  REDIS_HOST = redis-pavavy6pu5hpa.uksouth.redis.azure.net\n",
      "  REDIS_KEY = MOEW*************************************J0=\n",
      "  REDIS_PORT = 10000\n",
      "\n",
      "[search]\n",
      "  SEARCH_ADMIN_KEY = B5dq*********************************************SgB\n",
      "  SEARCH_ENDPOINT = https://search-pavavy6pu5hpa.search.windows.net\n",
      "  SEARCH_SERVICE_NAME = search-pavavy6pu5hpa\n",
      "\n",
      "[cosmos]\n",
      "  COSMOS_ACCOUNT_NAME = cosmos-pavavy6pu5hpa\n",
      "  COSMOS_ENDPOINT = https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "  COSMOS_KEY = KL11*********************************************************************************w==\n",
      "\n",
      "[content_safety]\n",
      "  CONTENT_SAFETY_ENDPOINT = https://contentsafety-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  CONTENT_SAFETY_KEY = 5ZvG*****************************************************************************6p1\n",
      "\n",
      "[models]\n",
      "  DEPLOYMENT_PREFIX = master-lab\n",
      "  OPENAI_MODELS_URL = https://apim-pavavy6pu5hpa.azure-api.netinference/models\n",
      "\n",
      "[other]\n",
      "  ACR_LOGIN_SERVER = acrmcpwksp321028.azurecr.io\n",
      "  ACR_NAME = acrmcpwksp321028\n",
      "  API_ID = inference-api\n",
      "  AZURE_CLIENT_ID = 4a5d0f1a-578e-479a-8ba9-05770ae9ce6b\n",
      "  AZURE_CLIENT_SECRET = lXV8*********************************aIr\n",
      "  AZURE_SUBSCRIPTION_ID = d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  AZURE_TENANT_ID = 2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "  AZ_CLI = /usr/bin/az\n",
      "  BICEP_DIR = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "  CLAUDECODE = 1\n",
      "  CLAUDE_CODE_ENTRYPOINT = cli\n",
      "  CLICOLOR = 1\n",
      "  CLICOLOR_FORCE = 1\n",
      "  CONTAINER_APP_ENV_ID = /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resourceGroups/lab-master-lab/providers/Microsoft.App/managedEnvironments/cae-pavavy6pu5hpa\n",
      "  CONTAINER_REGISTRY = acrpavavy6pu5hpa.azurecr.io\n",
      "  COREPACK_ENABLE_AUTO_PIN = 0\n",
      "  DBUS_SESSION_BUS_ADDRESS = unix:path=/run/user/1000/bus\n",
      "  DISPLAY = :0\n",
      "  FORCE_COLOR = 1\n",
      "  FOUNDRY_PROJECT_ENDPOINT = <empty>\n",
      "  GIT_EDITOR = true\n",
      "  GIT_PAGER = cat\n",
      "  HOME = /home/lproux\n",
      "  HOMEBREW_CELLAR = /home/linuxbrew/.linuxbrew/Cellar\n",
      "  HOMEBREW_PREFIX = /home/linuxbrew/.linuxbrew\n",
      "  HOMEBREW_REPOSITORY = /home/linuxbrew/.linuxbrew/Homebrew\n",
      "  HOSTTYPE = x86_64\n",
      "  INFOPATH = /home/linuxbrew/.linuxbrew/share/info:\n",
      "  JPY_PARENT_PID = 1562\n",
      "  LANG = C.UTF-8\n",
      "  LB_ENABLED = false\n",
      "  LB_GPT4O_MINI_ENDPOINTS = <empty>\n",
      "  LB_REGIONS = <empty>\n",
      "  LESSCLOSE = /usr/bin/lesspipe %s %s\n",
      "  LESSOPEN = | /usr/bin/lesspipe %s\n",
      "  LOGNAME = lproux\n",
      "  LS_COLORS = rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=00:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.avif=01;35:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:*~=00;90:*#=00;90:*.bak=00;90:*.crdownload=00;90:*.dpkg-dist=00;90:*.dpkg-new=00;90:*.dpkg-old=00;90:*.dpkg-tmp=00;90:*.old=00;90:*.orig=00;90:*.part=00;90:*.rej=00;90:*.rpmnew=00;90:*.rpmorig=00;90:*.rpmsave=00;90:*.swp=00;90:*.tmp=00;90:*.ucf-dist=00;90:*.ucf-new=00;90:*.ucf-old=00;90:\n",
      "  MCP_SERVER_GITHUB_URL = https://mcp-github-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_MS_LEARN_URL = https://mcp-ms-learn-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_ONCALL_URL = https://mcp-oncall-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_PLACE_ORDER_URL = https://mcp-place-order-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_PRODUCT_CATALOG_URL = https://mcp-product-catalog-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_SPOTIFY_URL = https://mcp-spotify-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MCP_SERVER_WEATHER_URL = https://mcp-weather-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "  MPLBACKEND = module://matplotlib_inline.backend_inline\n",
      "  NAME = SurfacIT\n",
      "  NoDefaultCurrentDirectoryInExePath = 1\n",
      "  OLDPWD = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub\n",
      "  OPENAI_API_BASE = https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "  OPENAI_ENDPOINT = https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "  OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE = delta\n",
      "  PAGER = cat\n",
      "  PATH = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/.venv/bin:/home/lproux/.local/bin:/home/lproux/bin:/home/lproux/.local/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Users/lproux/AppData/Roaming/Code/User/globalStorage/github.copilot-chat/debugCommand:/mnt/c/Users/lproux/AppData/Roaming/Code/User/globalStorage/github.copilot-chat/copilotCli:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files/nodejs/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/GitHub CLI/:/mnt/c/Users/lproux/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code Insiders/bin:/mnt/c/Users/lproux/AppData/Roaming/npm:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/lproux/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/lproux/.local/bin:/mnt/c/Users/lproux/.local/bin:/mnt/c/Users/lproux/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code Insiders/bin:/mnt/c/Users/lproux/AppData/Roaming/npm:/mnt/c/Users/lproux/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/lproux/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/lproux/.vscode/extensions/ms-python.debugpy-2025.16.0-win32-x64/bundled/scripts/noConfigScripts:/snap/bin\n",
      "  PULSE_SERVER = unix:/mnt/wslg/PulseServer\n",
      "  PWD = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "  PYDEVD_USE_FRAME_EVAL = NO\n",
      "  RANDOM_SUFFIX = 24774\n",
      "  SHELL = /bin/bash\n",
      "  SHLVL = 2\n",
      "  STORAGE_ACCOUNT = stmcpwksp321028\n",
      "  SUBSCRIPTION_ID = d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  TERM = xterm-color\n",
      "  USER = lproux\n",
      "  VIRTUAL_ENV = /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/.venv\n",
      "  VIRTUAL_ENV_PROMPT = (.venv) \n",
      "  WAYLAND_DISPLAY = wayland-0\n",
      "  WSL2_GUI_APPS_ENABLED = 1\n",
      "  WSLENV = <empty>\n",
      "  WSL_DISTRO_NAME = Ubuntu\n",
      "  WSL_INTEROP = /run/WSL/295_interop\n",
      "  XDG_DATA_DIRS = /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "  XDG_RUNTIME_DIR = /run/user/1000/\n",
      "  _ = /home/lproux/.local/bin/jupyter\n",
      "\n",
      "[load-balancing] Model Pools & Region Mapping\n",
      "  (no pools or region map defined)\n",
      "\n",
      "[env] Loader ready. Use ENV.get('KEY') in subsequent cells.\n"
     ]
    }
   ],
   "source": [
    "# === Unified Environment Loader & Load Balancing Overview ===\n",
    "\"\"\"\n",
    "This cell provides a single source of truth for configuration:\n",
    "- Auto-creates `master-lab.env` if missing (non-secret template placeholders).\n",
    "- Loads key=value pairs (duplicates allowed) and merges with current process env.\n",
    "- Masks sensitive values when displaying (KEY, SECRET, TOKEN, PASSWORD, API_KEY substrings).\n",
    "- Ensures `.gitignore` patterns include env files (both global and lab-specific).\n",
    "- Displays load balancing pools and region mapping across models.\n",
    "\n",
    "Duplication Policy: Allowed. Later cells still using os.getenv will continue working;\n",
    "new code should prefer ENV.get(\"NAME\").\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "ENV_FILE = Path(\"master-lab.env\")\n",
    "\n",
    "# Template includes both model names and deployment names (user may fill in later)\n",
    "_DEFAULT_ENV_TEMPLATE = \"\"\"# master-lab.env - autogenerated template (fill real credentials)\n",
    "# Lines beginning with # are comments. Duplicates permitted; last occurrence wins.\n",
    "# Core Azure API Management / Resource settings\n",
    "APIM_GATEWAY_URL=\n",
    "APIM_API_KEY=\n",
    "RESOURCE_GROUP=\n",
    "LOCATION=\n",
    "INFERENCE_API_PATH=inference\n",
    "\n",
    "# Model identifiers (by model and by deployment)\n",
    "DALL_E3_MODEL=dall-e-3\n",
    "DALL_E_DEPLOYMENT=\n",
    "FLUX_MODEL=FLUX.1-Kontext-pro\n",
    "FLUX_DEPLOYMENT=\n",
    "VISION_MODEL=gpt-4o-mini\n",
    "VISION_DEPLOYMENT=\n",
    "\n",
    "# Optional regional mapping (comma-separated model:region pairs)\n",
    "MODEL_REGION_MAP=dall-e-3:westus3,FLUX.1-Kontext-pro:eastus,gpt-4o-mini:swedencentral\n",
    "\n",
    "# Pools for probabilistic or round-robin balancing (models separated by '|')\n",
    "IMAGE_MODEL_POOL=dall-e-3|FLUX.1-Kontext-pro\n",
    "VISION_MODEL_POOL=gpt-4o-mini\n",
    "\n",
    "# Other service keys (fill as needed)\n",
    "REDIS_HOST=\n",
    "REDIS_PORT=\n",
    "REDIS_KEY=\n",
    "SEARCH_SERVICE_NAME=\n",
    "SEARCH_ENDPOINT=\n",
    "SEARCH_ADMIN_KEY=\n",
    "COSMOS_ACCOUNT_NAME=\n",
    "COSMOS_ENDPOINT=\n",
    "COSMOS_KEY=\n",
    "CONTENT_SAFETY_ENDPOINT=\n",
    "CONTENT_SAFETY_KEY=\n",
    "\"\"\".strip() + \"\\n\"\n",
    "\n",
    "_SENSITIVE_SUBSTRINGS = [\"KEY\", \"SECRET\", \"TOKEN\", \"PASSWORD\", \"API_KEY\"]\n",
    "\n",
    "def create_env_file_if_missing(path: Path = ENV_FILE):\n",
    "    if not path.exists():\n",
    "        path.write_text(_DEFAULT_ENV_TEMPLATE, encoding=\"utf-8\")\n",
    "        print(f\"[env] Created missing env file: {path}\")\n",
    "\n",
    "def parse_env_lines(lines: Iterable[str]) -> Dict[str, str]:\n",
    "    data: Dict[str, str] = {}\n",
    "    for raw in lines:\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"): # comment / empty\n",
    "            continue\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\"=\", 1)\n",
    "        data[k.strip()] = v.strip()\n",
    "    return data\n",
    "\n",
    "def load_env(path: Path = ENV_FILE) -> Dict[str, str]:\n",
    "    create_env_file_if_missing(path)\n",
    "    file_text = path.read_text(encoding=\"utf-8\")\n",
    "    data = parse_env_lines(file_text.splitlines())\n",
    "    # Merge in process env (does not overwrite file values)\n",
    "    for k, v in os.environ.items():\n",
    "        if k not in data:\n",
    "            data[k] = v\n",
    "    return data\n",
    "\n",
    "def mask_value(v: str, keep_start: int = 4, keep_end: int = 3) -> str:\n",
    "    if v is None or v == \"\":\n",
    "        return \"<empty>\"\n",
    "    if len(v) <= keep_start + keep_end + 2:\n",
    "        return v  # too short to mask meaningfully\n",
    "    return v[:keep_start] + \"*\" * (len(v) - keep_start - keep_end) + v[-keep_end:]\n",
    "\n",
    "def is_sensitive(key: str) -> bool:\n",
    "    u = key.upper()\n",
    "    return any(sub in u for sub in _SENSITIVE_SUBSTRINGS)\n",
    "\n",
    "def ensure_gitignore_patterns():\n",
    "    patterns_to_add = [\"*.env\", \"master-lab.env\"]\n",
    "    # Walk a few ancestor levels to update existing .gitignore files\n",
    "    checked = []\n",
    "    for base in [Path(\".\"), Path(\"..\"), Path(\"../..\"), Path(\"../../..\")]:\n",
    "        gi = base / \".gitignore\"\n",
    "        if gi.exists():\n",
    "            try:\n",
    "                lines = gi.read_text(encoding=\"utf-8\").splitlines()\n",
    "            except Exception:\n",
    "                continue\n",
    "            changed = False\n",
    "            for p in patterns_to_add:\n",
    "                if not any(line.strip() == p for line in lines):\n",
    "                    lines.append(p)\n",
    "                    changed = True\n",
    "            if changed:\n",
    "                gi.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "                print(f\"[env] Updated gitignore: {gi} (added env patterns)\")\n",
    "            checked.append(str(gi))\n",
    "    if not checked:\n",
    "        print(\"[env] No .gitignore files found in scanned paths (env patterns not globally verified).\")\n",
    "\n",
    "def categorize_keys(env: Dict[str, str]) -> Dict[str, Dict[str, str]]:\n",
    "    categories = {\n",
    "        \"apim\": {},\n",
    "        \"redis\": {},\n",
    "        \"search\": {},\n",
    "        \"cosmos\": {},\n",
    "        \"content_safety\": {},\n",
    "        \"models\": {},\n",
    "        \"other\": {},\n",
    "    }\n",
    "    for k, v in env.items():\n",
    "        ku = k.upper()\n",
    "        if ku.startswith(\"APIM\") or ku in {\"RESOURCE_GROUP\", \"LOCATION\", \"INFERENCE_API_PATH\"}:\n",
    "            categories[\"apim\"][k] = v\n",
    "        elif ku.startswith(\"REDIS\"):\n",
    "            categories[\"redis\"][k] = v\n",
    "        elif ku.startswith(\"SEARCH\"):\n",
    "            categories[\"search\"][k] = v\n",
    "        elif ku.startswith(\"COSMOS\"):\n",
    "            categories[\"cosmos\"][k] = v\n",
    "        elif ku.startswith(\"CONTENT_SAFETY\"):\n",
    "            categories[\"content_safety\"][k] = v\n",
    "        elif \"MODEL\" in ku or \"DEPLOYMENT\" in ku or ku.endswith(\"_POOL\"):\n",
    "            categories[\"models\"][k] = v\n",
    "        else:\n",
    "            categories[\"other\"][k] = v\n",
    "    return categories\n",
    "\n",
    "def parse_region_map(env: Dict[str, str]) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    raw = env.get(\"MODEL_REGION_MAP\", \"\")\n",
    "    for part in raw.split(','):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if ':' in part:\n",
    "            model, region = part.split(':', 1)\n",
    "            mapping[model.strip()] = region.strip()\n",
    "    return mapping\n",
    "\n",
    "def show_load_balancing(env: Dict[str, str]):\n",
    "    print(\"\\n[load-balancing] Model Pools & Region Mapping\")\n",
    "    region_map = parse_region_map(env)\n",
    "    pools = [k for k in env.keys() if k.endswith(\"_POOL\")]\n",
    "    if not pools and not region_map:\n",
    "        print(\"  (no pools or region map defined)\")\n",
    "        return\n",
    "    for pk in pools:\n",
    "        models = [m.strip() for m in env.get(pk, \"\").split('|') if m.strip()]\n",
    "        print(f\"  Pool {pk}: {len(models)} model(s)\")\n",
    "        for m in models:\n",
    "            reg = region_map.get(m, \"<no-region>\")\n",
    "            print(f\"    - {m} @ {reg}\")\n",
    "    if region_map:\n",
    "        print(\"\\n  Region Map (all models):\")\n",
    "        for m, r in region_map.items():\n",
    "            print(f\"    {m}: {r}\")\n",
    "\n",
    "def list_env(env: Dict[str, str], mask: bool = True):\n",
    "    cats = categorize_keys(env)\n",
    "    print(\"[env] Summary (masked=\" + str(mask) + \")\")\n",
    "    for cname, items in cats.items():\n",
    "        if not items:\n",
    "            continue\n",
    "        print(f\"\\n[{cname}]\")\n",
    "        for k, v in sorted(items.items()):\n",
    "            display_v = mask_value(v) if (mask and is_sensitive(k)) else (v if v else \"<empty>\")\n",
    "            print(f\"  {k} = {display_v}\")\n",
    "\n",
    "# Execute setup\n",
    "ENV = load_env()\n",
    "ensure_gitignore_patterns()\n",
    "list_env(ENV, mask=True)\n",
    "show_load_balancing(ENV)\n",
    "print(\"\\n[env] Loader ready. Use ENV.get('KEY') in subsequent cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell_13_fad9bf81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.310858Z",
     "iopub.status.busy": "2025-11-15T16:27:53.310674Z",
     "iopub.status.idle": "2025-11-15T16:27:53.315452Z",
     "shell.execute_reply": "2025-11-15T16:27:53.314931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deps] requirements.txt missing at c:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab/requirements.txt; skip.\n"
     ]
    }
   ],
   "source": [
    "# Unified Dependencies Install (replaces older dependency cell)\n",
    "import os, sys, subprocess, pathlib, shlex\n",
    "LAB_ROOT = pathlib.Path(r\"c:\\Users\\lproux\\OneDrive - Microsoft\\bkp\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab\")\n",
    "REQ_FILE = LAB_ROOT / \"requirements.txt\"\n",
    "if REQ_FILE.exists():\n",
    "    print(f\"[deps] Installing from {REQ_FILE} (idempotent)\")\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(REQ_FILE)]\n",
    "    print(\"[deps] Command:\", \" \".join(shlex.quote(c) for c in cmd))\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"[deps][stderr]\", result.stderr[:400])\n",
    "        if result.returncode == 0:\n",
    "            print(\"[deps] ‚úÖ Requirements installed / already satisfied.\")\n",
    "        else:\n",
    "            print(f\"[deps] ‚ö†Ô∏è pip exited with code {result.returncode}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[deps] ‚ùå Installation error: {e}\")\n",
    "else:\n",
    "    print(f\"[deps] requirements.txt missing at {REQ_FILE}; skip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_14_da5ac2e2",
   "metadata": {},
   "source": [
    "## ‚úÖ Optimized Execution Order (Cells 1‚Äì25 Refactor)\n",
    "\n",
    "Recommended run sequence for clean provisioning & testing:\n",
    "1. Environment Loader (already executed) ‚Äì establishes `ENV` and masking.\n",
    "2. Dependencies Install (new unified cell) ‚Äì ensures Python packages present.\n",
    "3. Azure Auth & CLI Setup ‚Äì resolves `az`, creates Service Principal if missing, sets subscription/rg/location.\n",
    "4. Deployment Helper Functions ‚Äì (original helper cell kept) defines utility functions.\n",
    "5. Main Deployment (4 steps) ‚Äì provisions core, AI Foundry, supporting services, MCP servers.\n",
    "6. Generate `master-lab.env` ‚Äì writes consolidated outputs.\n",
    "7. OPENAI Endpoint/Inference Path Normalizer ‚Äì derives `OPENAI_ENDPOINT` if missing.\n",
    "8. Unified APIM Policy Application ‚Äì applies content-safety + semantic caching policies post-deployment.\n",
    "9. Unified MCP Initialization ‚Äì initializes all deployed MCP servers once.\n",
    "10. Import Libraries ‚Äì (original imports cell) after environment & deployment.\n",
    "\n",
    "Deprecated cells replaced by stubs:\n",
    "- Old semantic caching policy cell\n",
    "- Redundant Azure CLI resolution cells\n",
    "- Duplicate MCP initialization cells (2 vs 5 servers)\n",
    "- Legacy `load_dotenv` environment loader\n",
    "- Separate Service Principal creation & config cells\n",
    "\n",
    "Rationale:\n",
    "- Prevent policy application before backend/API exist.\n",
    "- Single Azure CLI resolution reduces timeouts & path drift.\n",
    "- One MCP client avoids partial initialization confusion.\n",
    "- Centralized environment variable evolution (adds derived `OPENAI_ENDPOINT`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell_15_85f5150d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.317817Z",
     "iopub.status.busy": "2025-11-15T16:27:53.317474Z",
     "iopub.status.idle": "2025-11-15T16:27:53.546978Z",
     "shell.execute_reply": "2025-11-15T16:27:53.546345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAB 01: Semantic Caching Configuration\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[32m     13\u001b[39m backend_id = \u001b[33m\"\u001b[39m\u001b[33minference-backend-pool\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LAB 01: Semantic Caching with Azure Redis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 01: Semantic Caching Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "embeddings_backend_id = \"foundry1\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend ID: {backend_id}\")\n",
    "print(f\"[policy] Embeddings Backend ID: {embeddings_backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Semantic caching policy with API-KEY authentication\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <azure-openai-semantic-cache-lookup\n",
    "            score-threshold=\"0.8\"\n",
    "            embeddings-backend-id=\"{embeddings_backend_id}\"\n",
    "            embeddings-backend-auth=\"system-assigned\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <azure-openai-semantic-cache-store duration=\"120\" />\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying semantic-cache via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Policy will take ~30-60 seconds to propagate\")\n",
    "print(\"[NEXT] Run the cells below to test semantic caching behavior\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_16_59b5974e",
   "metadata": {},
   "source": [
    "# SECTION 0 - DEPLOY AND CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_17_382108c7",
   "metadata": {},
   "source": [
    "### Dependency Alignment Notes\n",
    "\n",
    "This notebook now performs multi-strategy installation for `openai` + `openai-agents`:\n",
    "\n",
    "Installation Order:\n",
    "1. Preferred spec (env `OPENAI_PREFERRED_SPEC`, default `openai>=2.2,<3`)\n",
    "2. Fallback specs list (env `OPENAI_FALLBACK_SPECS`)\n",
    "3. Agent fallbacks (env `OPENAI_AGENTS_FALLBACK_VERSIONS`) combined with all openai specs.\n",
    "\n",
    "Why previous attempts failed:\n",
    "- The target spec may not exist (mirror lag / version not published).\n",
    "- `openai-agents==0.4.1` might require an earlier major of `openai`.\n",
    "- Network or index restrictions prevented download.\n",
    "\n",
    "Override Examples:\n",
    "\n",
    "```bash\n",
    "export OPENAI_PREFERRED_SPEC=\"openai==1.60.1\"\n",
    "export OPENAI_FALLBACK_SPECS=\"openai==1.54.0,openai==1.40.0\"\n",
    "export OPENAI_AGENTS_PREFERRED_VERSION=\"0.3.0\"\n",
    "export OPENAI_AGENTS_FALLBACK_VERSIONS=\"0.2.0\"\n",
    "```\n",
    "\n",
    "Dry Run (no installs):\n",
    "\n",
    "```bash\n",
    "export DRY_RUN=1\n",
    "```\n",
    "\n",
    "Then rerun the first dependency cell.\n",
    "\n",
    "If ALL attempts fail:\n",
    "- Check connectivity: `pip index versions openai`\n",
    "- Try manual: `python -m pip install openai==1.60.1 openai-agents==0.3.0`\n",
    "- Consider updating notebook logic if `openai-agents` is deprecated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_18_9925d274",
   "metadata": {},
   "source": [
    "### Environment Standardization\n",
    "\n",
    "The notebook now **always loads** `master-lab.env` (and intentionally ignores a legacy `.env` if present). This ensures consistency across all mid-range cells (50‚Äì90) and later diagnostics.\n",
    "\n",
    "Key points:\n",
    "- Precedence: `master-lab.env` > previously loaded `.env`.\n",
    "- If `python-dotenv` isn't installed, a manual parser is used.\n",
    "- A legacy `.env` file is detected but not sourced (informational notice only).\n",
    "- Downstream MCP initialization and Azure deployment cells rely on values sourced here‚Äîre-run this cell first after any env changes.\n",
    "\n",
    "If servers still show unreachable statuses:\n",
    "1. Confirm URL entries in `master-lab.env` match those in `.mcp-servers-config` (config overrides env inside the improved MCP cell).\n",
    "2. Check for network/firewall restrictions (timeouts vs connection errors distinguished in diagnostics).\n",
    "3. For non-HTTP package/stdio servers, ensure local installation or runtime adapter before expecting probe success.\n",
    "\n",
    "Proceed to run the improved MCP diagnostics cell at the bottom, then re-run cells 50‚Äì90."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_19_7f8d96e1",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Run this first to install all dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_20_360bb0f9",
   "metadata": {},
   "source": [
    "<a id='init'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell_21_579ba0a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.548819Z",
     "iopub.status.busy": "2025-11-15T16:27:53.548652Z",
     "iopub.status.idle": "2025-11-15T16:27:53.561352Z",
     "shell.execute_reply": "2025-11-15T16:27:53.560635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Azure CLI resolved: /usr/bin/az\n",
      "[OK] API_ID from environment: inference-api\n"
     ]
    }
   ],
   "source": [
    "# APIM policy apply helper (patched Azure CLI resolution with autodiscovery)\n",
    "import shutil, subprocess, os, sys, textwrap, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "AZ_CANDIDATES = [\n",
    "    shutil.which(\"az\"),\n",
    "    str(Path(sys.prefix) / \"bin\" / \"az\"),\n",
    "]\n",
    "\n",
    "AZ_CANDIDATES += [c for c in [os.getenv(\"AZURE_CLI_PATH\"), os.getenv(\"AZ_PATH\")] if c]\n",
    "az_cli = next((c for c in AZ_CANDIDATES if c and Path(c).exists()), None)\n",
    "\n",
    "if not az_cli:\n",
    "    raise SystemExit(\"[FATAL] Azure CLI 'az' not found. Install it before continuing.\")\n",
    "\n",
    "print(f\"[INFO] Azure CLI resolved: {az_cli}\")\n",
    "\n",
    "# Ensure ENV is defined\n",
    "ENV = os.environ\n",
    "RESOURCE_GROUP = ENV.get(\"RESOURCE_GROUP\") or os.getenv(\"RESOURCE_GROUP\") or \"lab-master-lab\"\n",
    "APIM_SERVICE = ENV.get(\"APIM_SERVICE_NAME\") or os.getenv(\"APIM_SERVICE_NAME\") or \"apim-pavavy6pu5hpa\"\n",
    "\n",
    "# Autodiscover API_ID from APIM service\n",
    "def autodiscover_api_id():\n",
    "    \"\"\"Auto-discover the inference API ID from APIM service.\"\"\"\n",
    "    try:\n",
    "        # Get subscription ID\n",
    "        subscription_id_local = globals().get(\"subscription_id\")\n",
    "        if not subscription_id_local:\n",
    "            result_sub = subprocess.run([az_cli, \"account\", \"show\"], capture_output=True, text=True, timeout=30)\n",
    "            if result_sub.returncode != 0:\n",
    "                return None\n",
    "            import json as json_module\n",
    "            sub_info = json_module.loads(result_sub.stdout)\n",
    "            subscription_id_local = sub_info.get(\"id\")\n",
    "        \n",
    "        if not subscription_id_local:\n",
    "            return None\n",
    "        \n",
    "        # Query APIM for APIs\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id_local}'\n",
    "               f'/resourceGroups/{RESOURCE_GROUP}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{APIM_SERVICE}/apis?api-version=2022-08-01')\n",
    "        \n",
    "        result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                               capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            return None\n",
    "        \n",
    "        import json as json_module\n",
    "        apis_data = json_module.loads(result.stdout)\n",
    "        apis = apis_data.get('value', [])\n",
    "        \n",
    "        # Find inference API\n",
    "        for api in apis:\n",
    "            api_id = api.get('name', '')\n",
    "            api_props = api.get('properties', {})\n",
    "            api_name = api_props.get('displayName', '').lower()\n",
    "            api_path = api_props.get('path', '').lower()\n",
    "            \n",
    "            if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                return api_id\n",
    "        \n",
    "        # Fallback to inference-api if exists\n",
    "        for api in apis:\n",
    "            if api.get('name') == 'inference-api':\n",
    "                return 'inference-api'\n",
    "        \n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Try to autodiscover, fallback to env or default\n",
    "API_ID = ENV.get(\"APIM_API_ID\") or os.getenv(\"APIM_API_ID\")\n",
    "\n",
    "if not API_ID:\n",
    "    print(\"[*] Auto-discovering API_ID from APIM service...\")\n",
    "    discovered_api_id = autodiscover_api_id()\n",
    "    if discovered_api_id:\n",
    "        API_ID = discovered_api_id\n",
    "        os.environ['APIM_API_ID'] = API_ID\n",
    "        print(f\"[OK] API_ID auto-discovered: {API_ID}\")\n",
    "    else:\n",
    "        # Fallback to default\n",
    "        API_ID = \"inference-api\"\n",
    "        os.environ['APIM_API_ID'] = API_ID\n",
    "        print(f\"[!] Could not auto-discover API_ID, using default: {API_ID}\")\n",
    "else:\n",
    "    print(f\"[OK] API_ID from environment: {API_ID}\")\n",
    "\n",
    "policy_xml = \"\"\"<policies>\n",
    "  <inbound>\n",
    "    <base />\n",
    "    <set-header name=\"X-Policy-Applied\" exists-action=\"override\">\n",
    "      <value>content-safety</value>\n",
    "    </set-header>\n",
    "  </inbound>\n",
    "  <backend>\n",
    "    <base />\n",
    "  </backend>\n",
    "  <outbound>\n",
    "    <base />\n",
    "  </outbound>\n",
    "  <on-error>\n",
    "    <base />\n",
    "  </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "def apply_policy(xml_str: str, label: str):\n",
    "    \"\"\"Apply APIM policy using Azure REST API.\"\"\"\n",
    "    import json as json_module\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Prefer existing subscription_id if already defined in notebook\n",
    "    subscription_id_local = globals().get(\"subscription_id\")\n",
    "    if not subscription_id_local:\n",
    "        result_sub = subprocess.run([az_cli, \"account\", \"show\"], capture_output=True, text=True, timeout=30)\n",
    "        if result_sub.returncode != 0:\n",
    "            print(f\"[ERROR] Failed to get subscription ID\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            sub_info = json_module.loads(result_sub.stdout)\n",
    "            subscription_id_local = sub_info[\"id\"]\n",
    "        except Exception:\n",
    "            print(f\"[ERROR] Could not parse subscription info\")\n",
    "            return\n",
    "\n",
    "    # Azure REST API endpoint for APIM policy\n",
    "    url = (f'https://management.azure.com/subscriptions/{subscription_id_local}'\n",
    "           f'/resourceGroups/{RESOURCE_GROUP}/providers/Microsoft.ApiManagement'\n",
    "           f'/service/{APIM_SERVICE}/apis/{API_ID}/policies/policy?api-version=2022-08-01')\n",
    "    # Policy payload in Azure format\n",
    "    policy_payload = {\n",
    "        \"properties\": {\n",
    "            \"value\": xml_str.strip(),\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write JSON payload to temp file\n",
    "    payload_file = Path(tempfile.gettempdir()) / f'apim-{label}-payload.json'\n",
    "    with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "        json_module.dump(policy_payload, f, indent=2)\n",
    "\n",
    "    # Build az rest command\n",
    "    cmd = [az_cli, \"rest\", \"--method\", \"put\", \"--url\", url, \"--body\", f\"@{payload_file}\", \"--headers\", \"Content-Type=application/json\"]\n",
    "    print(f\"[*] Applying {label} policy via REST API\")\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"[ERROR] {label} policy timed out after 120 seconds\")\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {label} policy failed: {e}\")\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return\n",
    "    else:\n",
    "        # Clean up temp file\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if result.returncode == 0:\n",
    "            print(f\"[SUCCESS] {label} policy applied\")\n",
    "        else:\n",
    "            print(f\"[ERROR] {label} policy failed rc={result.returncode}\\nSTDERR: {result.stderr[:400]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell_22_d87c1715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.563516Z",
     "iopub.status.busy": "2025-11-15T16:27:53.563312Z",
     "iopub.status.idle": "2025-11-15T16:27:53.594528Z",
     "shell.execute_reply": "2025-11-15T16:27:53.593843Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01masyncio\u001b[39;00m, \u001b[34;01mrandom\u001b[39;00m, \u001b[34;01mbase64\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mmpl\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Consolidated imports (fixed syntax & separated lines)\n",
    "import asyncio, random, base64\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display\n",
    "\n",
    "# Azure SDK / OpenAI imports (first time in notebook above this cell)\n",
    "from openai import AzureOpenAI, AsyncAzureOpenAI\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# MCP imports\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "# Local utilities\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add shared path for utils (already used elsewhere)\n",
    "import sys\n",
    "sys.path.insert(1, '../../shared')\n",
    "import utils\n",
    "\n",
    "# Matplotlib defaults\n",
    "mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "print('[OK] All libraries imported')\n",
    "\n",
    "# Configure APIM API_ID for policy applications with autodiscovery\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check if API_ID already set from previous cell\n",
    "API_ID = os.getenv('APIM_API_ID')\n",
    "\n",
    "if not API_ID:\n",
    "    print('[*] Auto-discovering APIM_API_ID...')\n",
    "    \n",
    "    # Get APIM configuration\n",
    "    RESOURCE_GROUP = os.getenv('RESOURCE_GROUP', 'lab-master-lab')\n",
    "    APIM_SERVICE = os.getenv('APIM_SERVICE_NAME', 'apim-pavavy6pu5hpa')\n",
    "    \n",
    "    # Find Azure CLI\n",
    "    az_cli = shutil.which(\"az\")\n",
    "    if az_cli:\n",
    "        try:\n",
    "            # Get subscription ID\n",
    "            subscription_id_local = globals().get(\"subscription_id\")\n",
    "            if not subscription_id_local:\n",
    "                result_sub = subprocess.run([az_cli, \"account\", \"show\"], capture_output=True, text=True, timeout=30)\n",
    "                if result_sub.returncode == 0:\n",
    "                    import json as json_module\n",
    "                    sub_info = json_module.loads(result_sub.stdout)\n",
    "                    subscription_id_local = sub_info.get(\"id\")\n",
    "            \n",
    "            if subscription_id_local:\n",
    "                # Query APIM for APIs\n",
    "                url = (f'https://management.azure.com/subscriptions/{subscription_id_local}'\n",
    "                       f'/resourceGroups/{RESOURCE_GROUP}/providers/Microsoft.ApiManagement'\n",
    "                       f'/service/{APIM_SERVICE}/apis?api-version=2022-08-01')\n",
    "                \n",
    "                result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                                       capture_output=True, text=True, timeout=60)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    import json as json_module\n",
    "                    apis_data = json_module.loads(result.stdout)\n",
    "                    apis = apis_data.get('value', [])\n",
    "                    \n",
    "                    # Find inference API\n",
    "                    for api in apis:\n",
    "                        api_id = api.get('name', '')\n",
    "                        api_props = api.get('properties', {})\n",
    "                        api_name = api_props.get('displayName', '').lower()\n",
    "                        api_path = api_props.get('path', '').lower()\n",
    "                        \n",
    "                        if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                            API_ID = api_id\n",
    "                            break\n",
    "                    \n",
    "                    if not API_ID:\n",
    "                        # Fallback to inference-api if exists\n",
    "                        for api in apis:\n",
    "                            if api.get('name') == 'inference-api':\n",
    "                                API_ID = 'inference-api'\n",
    "                                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    if not API_ID:\n",
    "        # Final fallback\n",
    "        API_ID = 'inference-api'\n",
    "        print(f'[!] Could not auto-discover, using default: {API_ID}')\n",
    "    else:\n",
    "        print(f'[OK] Auto-discovered APIM_API_ID: {API_ID}')\n",
    "    \n",
    "    os.environ['APIM_API_ID'] = API_ID\n",
    "else:\n",
    "    print(f'[OK] APIM_API_ID from environment: {API_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_23_343c991d",
   "metadata": {},
   "source": [
    "### Load Environment Variables from Deployment Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell_24_637b493c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.596960Z",
     "iopub.status.busy": "2025-11-15T16:27:53.596687Z",
     "iopub.status.idle": "2025-11-15T16:27:53.621179Z",
     "shell.execute_reply": "2025-11-15T16:27:53.620021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded environment from master-lab.env\n",
      "[OK] APIM Gateway URL: https://apim-pavavy6pu5hpa.azure-api.net\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from deployment\n",
    "env_file = 'master-lab.env'\n",
    "if os.path.exists(env_file):\n",
    "    load_dotenv(env_file)\n",
    "    print(f'[OK] Loaded environment from {env_file}')\n",
    "\n",
    "    # Verify key variables are loaded\n",
    "    apim_url = os.getenv('APIM_GATEWAY_URL')\n",
    "    if apim_url:\n",
    "        print(f'[OK] APIM Gateway URL: {apim_url}')\n",
    "    else:\n",
    "        print('[!] Warning: APIM_GATEWAY_URL not found in .env')\n",
    "else:\n",
    "    print(f'[!] {env_file} not found. Run deployment cells first.')\n",
    "    print('[!] Cells 10-17 will deploy infrastructure and create the .env file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_25_8b3b6457",
   "metadata": {},
   "source": [
    "### Master Lab Configuration\n",
    "\n",
    "Set deployment configuration for all 4 deployment steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell_26_13f05f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.624100Z",
     "iopub.status.busy": "2025-11-15T16:27:53.623878Z",
     "iopub.status.idle": "2025-11-15T16:27:53.628447Z",
     "shell.execute_reply": "2025-11-15T16:27:53.627614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Configuration set\n",
      "  Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  Resource Group: lab-master-lab\n",
      "  Location: uksouth\n",
      "  Deployment Prefix: master-lab\n"
     ]
    }
   ],
   "source": [
    "# Master Lab Configuration\n",
    "\n",
    "# IMPORTANT: Set your Azure subscription ID\n",
    "# Get this from: Azure Portal > Subscriptions > Copy Subscription ID\n",
    "subscription_id = 'd334f2cd-3efd-494e-9fd3-2470b1a13e4c'  # Replace with your subscription ID\n",
    "\n",
    "deployment_name_prefix = 'master-lab'\n",
    "resource_group_name = 'lab-master-lab'\n",
    "location = 'uksouth'\n",
    "\n",
    "# Deployment names for each step\n",
    "deployment_step1 = f'{deployment_name_prefix}-01-core'\n",
    "deployment_step2 = f'{deployment_name_prefix}-02-ai-foundry'\n",
    "deployment_step3 = f'{deployment_name_prefix}-03-supporting'\n",
    "deployment_step4 = f'{deployment_name_prefix}-04-mcp'\n",
    "\n",
    "print('[OK] Configuration set')\n",
    "print(f'  Subscription ID: {subscription_id}')\n",
    "print(f'  Resource Group: {resource_group_name}')\n",
    "print(f'  Location: {location}')\n",
    "print(f'  Deployment Prefix: {deployment_name_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_27_9aa17b08",
   "metadata": {},
   "source": [
    "### Deployment Helper Functions\n",
    "\n",
    "Azure SDK functions for deployment management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell_28_1ecfb480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.630989Z",
     "iopub.status.busy": "2025-11-15T16:27:53.630707Z",
     "iopub.status.idle": "2025-11-15T16:27:53.674607Z",
     "shell.execute_reply": "2025-11-15T16:27:53.673857Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmgmt\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResourceManagementClient\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientSecretCredential, AzureCliCredential\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[*] Initializing Azure authentication...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "\n",
    "print('[*] Initializing Azure authentication...')\n",
    "print()\n",
    "\n",
    "# Try to load Service Principal credentials from .azure-credentials.env\n",
    "credentials_file = '.azure-credentials.env'\n",
    "credential = None\n",
    "\n",
    "if os.path.exists(credentials_file):\n",
    "    print(f'[*] Found {credentials_file}, using Service Principal authentication')\n",
    "    load_dotenv(credentials_file)\n",
    "\n",
    "    tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "    if tenant_id and client_id and client_secret:\n",
    "        try:\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id,\n",
    "                client_id=client_id,\n",
    "                client_secret=client_secret\n",
    "            )\n",
    "            print('[OK] Service Principal credentials loaded')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to create Service Principal credential: {e}')\n",
    "            credential = None\n",
    "    else:\n",
    "        print('[ERROR] Missing credentials in .azure-credentials.env')\n",
    "        print('[INFO] Required: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET')\n",
    "else:\n",
    "    print(f'[*] {credentials_file} not found')\n",
    "    print('[INFO] Run: create_service_principal.ps1 to create Service Principal')\n",
    "\n",
    "# Fallback to Azure CLI credential if Service Principal not available\n",
    "if credential is None:\n",
    "    print('[*] Falling back to Azure CLI authentication...')\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        print('[OK] Using Azure CLI credentials')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Azure CLI authentication failed: {e}')\n",
    "        print()\n",
    "        print('[ERROR] Authentication failed. Options:')\n",
    "        print('  1. Create Service Principal: run create_service_principal.ps1')\n",
    "        print('  2. Clear Azure CLI cache and re-login:')\n",
    "        print('     - Delete: %USERPROFILE%\\\\.azure')\n",
    "        print('     - Run: az login')\n",
    "        raise Exception('Authentication failed')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify subscription ID from config\n",
    "if not subscription_id or len(subscription_id) < 10:\n",
    "    raise Exception('Please set your subscription_id in Cell 11')\n",
    "\n",
    "print(f'[OK] Using Subscription ID: {subscription_id}')\n",
    "\n",
    "# Create Resource Management Client\n",
    "print('[*] Creating Azure Resource Management client...')\n",
    "try:\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "    # Test connection by listing resource groups\n",
    "    list(resource_client.resource_groups.list())\n",
    "    print('[OK] Azure SDK initialized and connection verified')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to initialize Resource Management client: {e}')\n",
    "    print()\n",
    "    print('[INFO] If you see MSAL or cache errors, try clearing Azure CLI cache:')\n",
    "    print('       rd /s /q \"%USERPROFILE%\\\\.azure\"')\n",
    "    print('       az login')\n",
    "    raise e\n",
    "\n",
    "print()\n",
    "\n",
    "def compile_bicep(bicep_file):\n",
    "    \"\"\"Compile Bicep to JSON - SIMPLIFIED: Just use existing JSON files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path if string\n",
    "    bicep_path = Path(bicep_file)\n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    print(f'[*] Looking for template: {json_path.name}...')\n",
    "    \n",
    "    # Check if JSON exists\n",
    "    if json_path.exists():\n",
    "        print(f'[OK] Found existing template: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    \n",
    "    # JSON doesn't exist - this is an error since we don't want to compile\n",
    "    print(f'[ERROR] Template not found: {json_path}')\n",
    "    print(f'[INFO] Expected location: {json_path.absolute()}')\n",
    "    print(f'[INFO] Please ensure Bicep templates are pre-compiled')\n",
    "    print(f'[INFO] Or set BICEP_DIR to the correct scripts directory')\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_resource_group_exists(rg_name):\n",
    "    \"\"\"Check if resource group exists\"\"\"\n",
    "    try:\n",
    "        resource_client.resource_groups.get(rg_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_deployment_exists(rg_name, deployment_name):\n",
    "    \"\"\"Check if deployment exists and succeeded - Fixed to return boolean only\"\"\"\n",
    "    try:\n",
    "        deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "        return deployment.properties.provisioning_state == 'Succeeded'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_template(rg_name, deployment_name, template_file, parameters_dict):\n",
    "    \"\"\"Deploy ARM template using Azure SDK\"\"\"\n",
    "    print(f'[*] Deploying {deployment_name}...')\n",
    "\n",
    "    # Read template\n",
    "    with open(template_file, 'r', encoding='utf-8') as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # Prepare deployment properties\n",
    "    deployment_properties = {\n",
    "        'mode': 'Incremental',\n",
    "        'template': template,\n",
    "        'parameters': parameters_dict\n",
    "    }\n",
    "\n",
    "    # Start deployment\n",
    "    print('[*] Starting deployment...')\n",
    "    deployment_async = resource_client.deployments.begin_create_or_update(\n",
    "        rg_name,\n",
    "        deployment_name,\n",
    "        {'properties': deployment_properties}\n",
    "    )\n",
    "\n",
    "    # Poll deployment status\n",
    "    print('[*] Deployment in progress. Polling status...')\n",
    "    start_time = time.time()\n",
    "    last_update = start_time\n",
    "\n",
    "    while not deployment_async.done():\n",
    "        time.sleep(30)\n",
    "        elapsed = time.time() - start_time\n",
    "        if time.time() - last_update >= 60:\n",
    "            mins = int(elapsed / 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f'[*] Still deploying... {mins}m {secs}s elapsed')\n",
    "            last_update = time.time()\n",
    "\n",
    "    # Get result\n",
    "    deployment_result = deployment_async.result()\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    if deployment_result.properties.provisioning_state == 'Succeeded':\n",
    "        print(f'[OK] Deployment succeeded in {mins}m {secs}s')\n",
    "        return True, deployment_result\n",
    "    else:\n",
    "        print(f'[ERROR] Deployment failed: {deployment_result.properties.provisioning_state}')\n",
    "        if deployment_result.properties.error:\n",
    "            print(f'[ERROR] Error: {deployment_result.properties.error.message}')\n",
    "        return False, deployment_result\n",
    "\n",
    "def get_deployment_outputs(rg_name, deployment_name):\n",
    "    \"\"\"Get deployment outputs\"\"\"\n",
    "    deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "    if deployment.properties.outputs:\n",
    "        return {k: v['value'] for k, v in deployment.properties.outputs.items()}\n",
    "    return {}\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_29_a7330fb3",
   "metadata": {},
   "source": [
    "### Main Deployment - All 4 Steps\n",
    "\n",
    "Deploys all infrastructure in sequence:\n",
    "1. Core (APIM, Log Analytics, App Insights) - ~10 min\n",
    "2. AI Foundry (3 hubs + 14 models) - ~15 min\n",
    "3. Supporting Services (Redis, Search, Cosmos, Content Safety) - ~10 min\n",
    "4. MCP Servers (Container Apps + 7 servers) - ~5 min\n",
    "\n",
    "**Total time: ~40 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.677161Z",
     "iopub.status.busy": "2025-11-15T16:27:53.676848Z",
     "iopub.status.idle": "2025-11-15T16:27:53.706258Z",
     "shell.execute_reply": "2025-11-15T16:27:53.705689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FILE LOCATION DIAGNOSTIC\n",
      "======================================================================\n",
      "\n",
      "Current Working Directory: /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab\n",
      "\n",
      "BICEP_DIR from env: /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "\n",
      "BICEP_DIR variable: archive/scripts\n",
      "BICEP_DIR exists: True\n",
      "BICEP_DIR absolute: /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "\n",
      "Checking potential template locations:\n",
      "‚úì /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "  Found 5 JSON templates:\n",
      "    - deploy-01-core.json\n",
      "    - deploy-02-ai-foundry.json\n",
      "    - deploy-02c-apim-api.json\n",
      "    - deploy-03-supporting.json\n",
      "    - deploy-04-mcp.json\n",
      "‚úó /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/scripts\n",
      "‚úó /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/../archive/scripts\n",
      "‚úó /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/C:/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "\n",
      "RECOMMENDED FIX:\n",
      "Set BICEP_DIR to: /mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\n",
      "\n",
      "Run this before Cell 32:\n",
      "BICEP_DIR = Path(r\"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts\")\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: Check file locations\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print('='*70)\n",
    "print('FILE LOCATION DIAGNOSTIC')\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "# Check current working directory\n",
    "cwd = Path.cwd()\n",
    "print(f'Current Working Directory: {cwd}')\n",
    "print()\n",
    "\n",
    "# Check BICEP_DIR environment variable\n",
    "bicep_dir_env = os.getenv('BICEP_DIR', 'archive/scripts')\n",
    "print(f'BICEP_DIR from env: {bicep_dir_env}')\n",
    "print()\n",
    "\n",
    "# Check if BICEP_DIR variable exists in globals\n",
    "if 'BICEP_DIR' in globals():\n",
    "    print(f'BICEP_DIR variable: {BICEP_DIR}')\n",
    "    print(f'BICEP_DIR exists: {BICEP_DIR.exists()}')\n",
    "    print(f'BICEP_DIR absolute: {BICEP_DIR.absolute()}')\n",
    "else:\n",
    "    print('BICEP_DIR variable not defined yet')\n",
    "print()\n",
    "\n",
    "# Try different potential locations\n",
    "potential_dirs = [\n",
    "    Path('archive/scripts'),\n",
    "    Path('scripts'),\n",
    "    Path('../archive/scripts'),\n",
    "    Path(r'C:/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/archive/scripts'),\n",
    "]\n",
    "\n",
    "print('Checking potential template locations:')\n",
    "for test_dir in potential_dirs:\n",
    "    exists = test_dir.exists()\n",
    "    marker = '‚úì' if exists else '‚úó'\n",
    "    print(f'{marker} {test_dir.absolute()}')\n",
    "    \n",
    "    if exists:\n",
    "        # Check for JSON files\n",
    "        json_files = list(test_dir.glob('deploy-*.json'))\n",
    "        if json_files:\n",
    "            print(f'  Found {len(json_files)} JSON templates:')\n",
    "            for jf in json_files:\n",
    "                print(f'    - {jf.name}')\n",
    "print()\n",
    "\n",
    "# Recommended fix\n",
    "print('RECOMMENDED FIX:')\n",
    "for test_dir in potential_dirs:\n",
    "    if test_dir.exists() and list(test_dir.glob('deploy-*.json')):\n",
    "        print(f'Set BICEP_DIR to: {test_dir.absolute()}')\n",
    "        print(f'\\nRun this before Cell 32:')\n",
    "        print(f'BICEP_DIR = Path(r\"{test_dir.absolute()}\")')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell_30_1a0a2a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.708303Z",
     "iopub.status.busy": "2025-11-15T16:27:53.708128Z",
     "iopub.status.idle": "2025-11-15T16:27:53.784138Z",
     "shell.execute_reply": "2025-11-15T16:27:53.783530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)\n",
      "======================================================================\n",
      "\n",
      "[*] Step 0: Ensuring resource group exists...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_resource_group_exists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Ensure resource group exists\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[*] Step 0: Ensuring resource group exists...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_resource_group_exists\u001b[49m(resource_group_name):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[*] Creating resource group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_group_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m     resource_client.resource_groups.create_or_update(\n\u001b[32m     20\u001b[39m         resource_group_name,\n\u001b[32m     21\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m: location}\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'check_resource_group_exists' is not defined"
     ]
    }
   ],
   "source": [
    "print('=' * 70)\n",
    "# Load BICEP_DIR (set by Cell 3)\n",
    "BICEP_DIR = Path(os.getenv('BICEP_DIR', 'archive/scripts'))\n",
    "if not BICEP_DIR.exists():\n",
    "    print(f\"[deploy] ‚ö†Ô∏è  BICEP_DIR not found: {BICEP_DIR}\")\n",
    "    print(f\"[deploy] Looking in current directory instead\")\n",
    "    BICEP_DIR = Path(\".\")\n",
    "\n",
    "print('MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Ensure resource group exists\n",
    "print('[*] Step 0: Ensuring resource group exists...')\n",
    "if not check_resource_group_exists(resource_group_name):\n",
    "    print(f'[*] Creating resource group: {resource_group_name}')\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {'location': location}\n",
    "    )\n",
    "    print('[OK] Resource group created')\n",
    "else:\n",
    "    print('[OK] Resource group already exists')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: CORE INFRASTRUCTURE (Bicep - as before)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 1: CORE INFRASTRUCTURE')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Log Analytics, App Insights, API Management')\n",
    "print('[*] Estimated time: ~10 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step1 = 'master-lab-01-core'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step1):\n",
    "    print('[OK] Step 1 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 1 not found. Deploying...')\n",
    "\n",
    "    # Compile and deploy\n",
    "    # Fix: original compile_bicep used Path.replace(old, new) causing TypeError.\n",
    "    # Provide safe wrapper using Path.with_suffix('.json').\n",
    "    # Added resilient az CLI discovery & FileNotFoundError handling.\n",
    "    # Enhanced: auto-install bicep if missing; richer diagnostics; fallback to direct bicep use if JSON not produced.\n",
    "    def compile_bicep_safe(bicep_path: Path):\n",
    "        \"\"\"SIMPLIFIED: Just use existing JSON files - no compilation\"\"\"\n",
    "        if not bicep_path.exists():\n",
    "            print(f'[ERROR] Bicep file not found: {bicep_path}')\n",
    "            return None\n",
    "        \n",
    "        json_path = bicep_path.with_suffix('.json')\n",
    "        \n",
    "        if json_path.exists():\n",
    "            print(f'[OK] Using existing template: {json_path.name}')\n",
    "            return str(json_path)\n",
    "        \n",
    "        print(f'[ERROR] JSON template not found: {json_path}')\n",
    "        print(f'[INFO] Expected at: {json_path.absolute()}')\n",
    "        return None\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-01-core.bicep')\n",
    "\n",
    "    # Load parameters\n",
    "    with open(BICEP_DIR / 'params-01-core.json') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # Extract only the 'parameters' section from ARM parameter file\n",
    "    params_dict = params.get('parameters', {})\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step1, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 1 deployment failed')\n",
    "\n",
    "    print('[OK] Step 1 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# Get Step 1 outputs (with fallback to saved file)\n",
    "step1_outputs = None\n",
    "try:\n",
    "    step1_outputs = get_deployment_outputs(resource_group_name, deployment_step1)\n",
    "    print('[OK] Step 1 outputs retrieved from deployment')\n",
    "except Exception as e:\n",
    "    print(f'[WARN] Failed to retrieve Step 1 outputs from deployment: {str(e)}')\n",
    "    # Try loading from saved file\n",
    "    step1_output_file = BICEP_DIR / 'step1-outputs.json'\n",
    "    if step1_output_file.exists():\n",
    "        try:\n",
    "            with open(step1_output_file) as f:\n",
    "                step1_outputs = json.load(f)\n",
    "            print(f'[OK] Step 1 outputs loaded from {step1_output_file.name}')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Failed to load from file: {str(e2)}')\n",
    "    \n",
    "if not step1_outputs:\n",
    "    print('[ERROR] Cannot proceed without Step 1 outputs')\n",
    "    print('[INFO] Please ensure Step 1 deployment completed or step1-outputs.json exists')\n",
    "    raise Exception('Cannot proceed without Step 1 outputs')\n",
    "\n",
    "print(f\"  - APIM Gateway: {step1_outputs.get('apimGatewayUrl', 'N/A')}\")\n",
    "print(f\"  - Log Analytics: {step1_outputs.get('logAnalyticsWorkspaceId', 'N/A')[:60]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: AI FOUNDRY (RESILIENT PYTHON APPROACH)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 2: AI FOUNDRY (RESILIENT DEPLOYMENT)')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: 3 Foundry hubs, 3 projects, AI models')\n",
    "print('[*] Estimated time: ~15 minutes')\n",
    "print()\n",
    "\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "\n",
    "cog_client = CognitiveServicesManagementClient(credential, subscription_id)\n",
    "\n",
    "# Configuration\n",
    "resource_suffix = 'pavavy6pu5hpa'  # Consistent suffix\n",
    "foundries = [\n",
    "    {'name': f'foundry1-{resource_suffix}', 'location': 'uksouth', 'project': 'master-lab-foundry1'},\n",
    "    {'name': f'foundry2-{resource_suffix}', 'location': 'eastus', 'project': 'master-lab-foundry2'},\n",
    "    {'name': f'foundry3-{resource_suffix}', 'location': 'norwayeast', 'project': 'master-lab-foundry3'}\n",
    "]\n",
    "\n",
    "models_config = {\n",
    "    'foundry1': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4o', 'format': 'OpenAI', 'version': '2024-08-06', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'text-embedding-3-small', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "        {'name': 'text-embedding-3-large', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "        {'name': 'dall-e-3', 'format': 'OpenAI', 'version': '3.0', 'sku': 'Standard', 'capacity': 1},\n",
    "        {'name': 'gpt-4o-realtime-preview', 'format': 'OpenAI', 'version': '2024-10-01', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry2': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry3': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Phase 2a: Check/Create Foundry Hubs\n",
    "print('[*] Phase 2a: AI Foundry Hubs')\n",
    "existing_accounts = {acc.name: acc for acc in cog_client.accounts.list_by_resource_group(resource_group_name)}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    if foundry_name in existing_accounts:\n",
    "        print(f'  [OK] {foundry_name} already exists')\n",
    "    else:\n",
    "        print(f'  [*] Creating {foundry_name}...')\n",
    "        try:\n",
    "            account_params = Account(\n",
    "                location=foundry['location'],\n",
    "                sku=CogSku(name='S0'),\n",
    "                kind='AIServices',\n",
    "                properties={\n",
    "                    'customSubDomainName': foundry_name.lower(),\n",
    "                    'publicNetworkAccess': 'Enabled',\n",
    "                    'allowProjectManagement': True\n",
    "                },\n",
    "                identity={'type': 'SystemAssigned'}\n",
    "            )\n",
    "            poller = cog_client.accounts.begin_create(resource_group_name, foundry_name, account_params)\n",
    "            poller.result(timeout=300)\n",
    "            print(f'  [OK] {foundry_name} created')\n",
    "        except Exception as e:\n",
    "            print(f'  [ERROR] Failed: {str(e)[:100]}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Phase 2b: Deploy Models (Resilient)\n",
    "print('[*] Phase 2b: AI Models (Resilient)')\n",
    "deployment_results = {'succeeded': [], 'failed': [], 'skipped': []}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    short_name = foundry_name.split('-')[0]\n",
    "    models = models_config.get(short_name, [])\n",
    "\n",
    "    print(f'  [*] {foundry_name}: {len(models)} models')\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model['name']\n",
    "        try:\n",
    "            # Check if exists\n",
    "            existing = cog_client.deployments.get(resource_group_name, foundry_name, model_name)\n",
    "            if existing.properties.provisioning_state == 'Succeeded':\n",
    "                deployment_results['skipped'].append(f'{short_name}/{model_name}')\n",
    "                print(f'    [OK] {model_name} already deployed')\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            print(f'    [*] Deploying {model_name}...')\n",
    "            deployment_params = Deployment(\n",
    "                sku=CogSku(name=model['sku'], capacity=model['capacity']),\n",
    "                properties=DeploymentProperties(\n",
    "                    model=DeploymentModel(\n",
    "                        format=model['format'],\n",
    "                        name=model['name'],\n",
    "                        version=model['version']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            poller = cog_client.deployments.begin_create_or_update(\n",
    "                resource_group_name, foundry_name, model_name, deployment_params\n",
    "            )\n",
    "            poller.result(timeout=600)\n",
    "            deployment_results['succeeded'].append(f'{short_name}/{model_name}')\n",
    "            print(f'    [OK] {model_name} deployed')\n",
    "        except Exception as e:\n",
    "            deployment_results['failed'].append({'model': f'{short_name}/{model_name}', 'error': str(e)})\n",
    "            print(f'    [SKIP] {model_name} failed: {str(e)[:80]}')\n",
    "\n",
    "print()\n",
    "print(f'[OK] Models: {len(deployment_results[\"succeeded\"])} deployed, {len(deployment_results[\"skipped\"])} skipped, {len(deployment_results[\"failed\"])} failed')\n",
    "print('[*] Phase 2c: APIM Inference API')\n",
    "\n",
    "deployment_step2c = 'master-lab-02c-apim-api'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step2c):\n",
    "    print('[OK] APIM API already configured. Skipping...')\n",
    "else:\n",
    "    print('[*] Configuring APIM Inference API...')\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-02c-apim-api.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 2c')\n",
    "\n",
    "    params_dict = {\n",
    "        'apimLoggerId': {'value': step1_outputs['apimLoggerId']},\n",
    "        'appInsightsId': {'value': step1_outputs['appInsightsId']},\n",
    "        'appInsightsInstrumentationKey': {'value': step1_outputs['appInsightsInstrumentationKey']},\n",
    "        'inferenceAPIPath': {'value': 'inference'},\n",
    "        'inferenceAPIType': {'value': 'AzureOpenAI'}\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step2c, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 2c deployment failed')\n",
    "\n",
    "    print('[OK] APIM API configured')\n",
    "\n",
    "print('[OK] Step 2 complete')\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SUPPORTING SERVICES (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print()\n",
    "\n",
    "deployment_step3 = 'master-lab-03-supporting'\n",
    "if check_deployment_exists(resource_group_name, deployment_step3):\n",
    "    print('[OK] Step 3 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 3 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-03-supporting.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 3')\n",
    "\n",
    "    params_dict = {}\n",
    "    if os.path.exists(BICEP_DIR / 'params-03-supporting.json'):\n",
    "        with open(BICEP_DIR / 'params-03-supporting.json') as f:\n",
    "            params = json.load(f)\n",
    "        # Extract only the 'parameters' section from ARM parameter file\n",
    "        params_dict = params.get('parameters', {})\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step3, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 3 deployment failed')\n",
    "    print('[OK] Step 3 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    print('[OK] Step 3 outputs retrieved')\n",
    "except Exception:\n",
    "    step3_outputs = {}\n",
    "    print('[*] No Step 3 outputs available')\n",
    "# =============================================================================\n",
    "# STEP 4: MCP SERVERS (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 4: MCP SERVERS')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Container Apps + 7 MCP servers')\n",
    "print('[*] Estimated time: ~5 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step4 = 'master-lab-04-mcp'\n",
    "if check_deployment_exists(resource_group_name, deployment_step4):\n",
    "    print('[OK] Step 4 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 4 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-04-mcp.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 4')\n",
    "\n",
    "    params_dict = {\n",
    "        'logAnalyticsCustomerId': {'value': step1_outputs.get('logAnalyticsCustomerId', '')},\n",
    "        'logAnalyticsPrimarySharedKey': {'value': step1_outputs.get('logAnalyticsPrimarySharedKey', '')},\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step4, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 4 deployment failed')\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    print('[OK] Step 4 outputs retrieved')\n",
    "except Exception:\n",
    "    step4_outputs = {}\n",
    "    print('[*] No Step 4 outputs available')\n",
    "\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT COMPLETE\n",
    "# =============================================================================\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "total_mins = int(total_elapsed / 60)\n",
    "total_secs = int(total_elapsed % 60)\n",
    "\n",
    "print('=' * 70)\n",
    "print('DEPLOYMENT COMPLETE')\n",
    "print('=' * 70)\n",
    "print(f'[OK] Total time: {total_mins}m {total_secs}s')\n",
    "print()\n",
    "print('[OK] All 4 steps deployed successfully!')\n",
    "print('[OK] Next: Run Cell 18-19 to generate master-lab.env')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_31_c1f724ac",
   "metadata": {},
   "source": [
    "### Generate .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell_32_9a09bb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:53.786965Z",
     "iopub.status.busy": "2025-11-15T16:27:53.786642Z",
     "iopub.status.idle": "2025-11-15T16:27:56.723428Z",
     "shell.execute_reply": "2025-11-15T16:27:56.722401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating master-lab.env...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deploy] outputs keys: contentSafetyEndpoint, contentSafetyKey, cosmosDbAccountName, cosmosDbEndpoint, cosmosDbKey, redisCacheHost, redisCacheKey, redisCachePort, searchServiceAdminKey, searchServiceEndpoint, searchServiceName\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deploy] outputs keys: containerAppEnvId, containerRegistryLoginServer, containerRegistryName, mcpServerUrls\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'step1_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m         step4_outputs = {}\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Get API key from APIM subscriptions (prefer step1 outputs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m apim_subscriptions = step1_outputs.get(\u001b[33m'\u001b[39m\u001b[33mapimSubscriptions\u001b[39m\u001b[33m'\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mstep1_outputs\u001b[49m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m     33\u001b[39m api_key = apim_subscriptions[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m apim_subscriptions \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Auto-discover APIM API_ID from deployed APIM service\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'step1_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('[*] Generating master-lab.env...')\n",
    "\n",
    "# Ensure step2_outputs, step3_outputs and step4_outputs exist (safe fallback to empty dicts)\n",
    "try:\n",
    "    step2_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step2_outputs = get_deployment_outputs(resource_group_name, deployment_step2c)\n",
    "    except Exception:\n",
    "        step2_outputs = {}\n",
    "\n",
    "try:\n",
    "    step3_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    except Exception:\n",
    "        step3_outputs = {}\n",
    "\n",
    "try:\n",
    "    step4_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    except Exception:\n",
    "        step4_outputs = {}\n",
    "\n",
    "# Get API key from APIM subscriptions (prefer step1 outputs)\n",
    "apim_subscriptions = step1_outputs.get('apimSubscriptions', []) if isinstance(step1_outputs, dict) else []\n",
    "api_key = apim_subscriptions[0]['key'] if apim_subscriptions else 'N/A'\n",
    "\n",
    "# Auto-discover APIM API_ID from deployed APIM service\n",
    "print('[*] Auto-discovering APIM_API_ID...')\n",
    "discovered_api_id = None\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    import json as json_module\n",
    "    import shutil\n",
    "    \n",
    "    # Get APIM service name from step1 outputs\n",
    "    apim_service_name = step1_outputs.get('apimServiceName', 'apim-pavavy6pu5hpa')\n",
    "    \n",
    "    # Find Azure CLI\n",
    "    az_cli = shutil.which(\"az\")\n",
    "    if az_cli and subscription_id:\n",
    "        # Query APIM for APIs\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{resource_group_name}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{apim_service_name}/apis?api-version=2022-08-01')\n",
    "        \n",
    "        result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                               capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            apis_data = json_module.loads(result.stdout)\n",
    "            apis = apis_data.get('value', [])\n",
    "            \n",
    "            # Find inference API\n",
    "            for api in apis:\n",
    "                api_id = api.get('name', '')\n",
    "                api_props = api.get('properties', {})\n",
    "                api_name = api_props.get('displayName', '').lower()\n",
    "                api_path = api_props.get('path', '').lower()\n",
    "                \n",
    "                if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                    discovered_api_id = api_id\n",
    "                    print(f'[OK] Auto-discovered APIM_API_ID: {discovered_api_id}')\n",
    "                    break\n",
    "            \n",
    "            if not discovered_api_id:\n",
    "                # Fallback to inference-api if exists\n",
    "                for api in apis:\n",
    "                    if api.get('name') == 'inference-api':\n",
    "                        discovered_api_id = 'inference-api'\n",
    "                        print(f'[OK] Found APIM_API_ID: {discovered_api_id}')\n",
    "                        break\n",
    "except Exception as e:\n",
    "    print(f'[!] Could not auto-discover APIM_API_ID: {e}')\n",
    "\n",
    "# Use discovered ID or fallback to default\n",
    "apim_api_id = discovered_api_id if discovered_api_id else 'inference-api'\n",
    "if not discovered_api_id:\n",
    "    print(f'[!] Using default APIM_API_ID: {apim_api_id}')\n",
    "\n",
    "# Set in environment for downstream use\n",
    "os.environ['APIM_API_ID'] = apim_api_id\n",
    "\n",
    "# Build .env content with grouped structure\n",
    "env_content = f\"\"\"# Master AI Gateway Lab - Deployment Outputs\n",
    "# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "# Resource Group: {resource_group_name}\n",
    "\n",
    "# ===========================================\n",
    "# APIM (API Management)\n",
    "# ===========================================\n",
    "APIM_GATEWAY_URL={step1_outputs.get('apimGatewayUrl', '')}\n",
    "APIM_SERVICE_ID={step1_outputs.get('apimServiceId', '')}\n",
    "APIM_SERVICE_NAME={step1_outputs.get('apimServiceName', '')}\n",
    "APIM_API_KEY={api_key}\n",
    "APIM_API_ID={apim_api_id}\n",
    "\n",
    "# ===========================================\n",
    "# AI Foundry\n",
    "# ===========================================\n",
    "FOUNDRY_PROJECT_ENDPOINT={step2_outputs.get('foundryProjectEndpoint', '')}\n",
    "INFERENCE_API_PATH={step2_outputs.get('inferenceAPIPath', 'inference')}\n",
    "\"\"\"\n",
    "\n",
    "# ===========================================\n",
    "# AI Models (Multi-Region Load Balancing)\n",
    "# ===========================================\n",
    "# Extract foundry deployment information from step2_outputs\n",
    "foundries_data = step2_outputs.get('foundries', []) if isinstance(step2_outputs, dict) else []\n",
    "\n",
    "# Region mapping for display\n",
    "region_names = {\n",
    "    'uksouth': 'UK South',\n",
    "    'eastus': 'East US',\n",
    "    'norwayeast': 'Norway East'\n",
    "}\n",
    "\n",
    "# Track endpoints for load balancing\n",
    "lb_endpoints = []\n",
    "lb_regions = []\n",
    "\n",
    "env_content += \"\\n# ===========================================\\n\"\n",
    "env_content += \"# AI Models (Multi-Region Load Balancing)\\n\"\n",
    "env_content += \"# ===========================================\\n\\n\"\n",
    "\n",
    "# Process each foundry (region)\n",
    "for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "    if not isinstance(foundry_info, dict):\n",
    "        continue\n",
    "\n",
    "    foundry_name = foundry_info.get('name', '')\n",
    "    location = foundry_info.get('location', '')\n",
    "    endpoint = foundry_info.get('endpoint', '')\n",
    "    key = foundry_info.get('key', '')\n",
    "    models = foundry_info.get('models', [])\n",
    "\n",
    "    # Add region to load balancing config\n",
    "    if location:\n",
    "        lb_regions.append(location)\n",
    "\n",
    "    # Add comment for region\n",
    "    region_display = region_names.get(location, location)\n",
    "    env_content += f\"# Region {idx} ({region_display}) - {foundry_name}\\n\"\n",
    "\n",
    "    # Process each model in this foundry\n",
    "    for model_name in models:\n",
    "        # Normalize model name for env var (replace hyphens with underscores, uppercase)\n",
    "        model_var = model_name.upper().replace('-', '_').replace('.', '_')\n",
    "\n",
    "        # Add endpoint and key for this model in this region\n",
    "        env_content += f\"MODEL_{model_var}_ENDPOINT_R{idx}={endpoint}\\n\"\n",
    "        env_content += f\"MODEL_{model_var}_KEY_R{idx}={key}\\n\"\n",
    "\n",
    "        # Track gpt-4o-mini endpoints for load balancing\n",
    "        if model_name == 'gpt-4o-mini' and endpoint:\n",
    "            lb_endpoints.append(endpoint)\n",
    "\n",
    "    env_content += \"\\n\"\n",
    "\n",
    "# Add load balancing configuration\n",
    "env_content += \"# Load Balancing Configuration\\n\"\n",
    "env_content += f\"LB_REGIONS={','.join(lb_regions)}\\n\"\n",
    "env_content += f\"LB_GPT4O_MINI_ENDPOINTS={','.join(lb_endpoints)}\\n\"\n",
    "env_content += f\"LB_ENABLED={'true' if len(lb_endpoints) > 1 else 'false'}\\n\"\n",
    "env_content += \"\\n\"\n",
    "\n",
    "# Continue with supporting services\n",
    "env_content += f\"\"\"# ===========================================\n",
    "# Supporting Services\n",
    "# ===========================================\n",
    "\n",
    "# Redis (Semantic Caching)\n",
    "REDIS_HOST={step3_outputs.get('redisCacheHost', '')}\n",
    "REDIS_PORT={step3_outputs.get('redisCachePort', 10000)}\n",
    "REDIS_KEY={step3_outputs.get('redisCacheKey', '')}\n",
    "\n",
    "# Azure Cognitive Search\n",
    "SEARCH_SERVICE_NAME={step3_outputs.get('searchServiceName', '')}\n",
    "SEARCH_ENDPOINT={step3_outputs.get('searchServiceEndpoint', '')}\n",
    "SEARCH_ADMIN_KEY={step3_outputs.get('searchServiceAdminKey', '')}\n",
    "\n",
    "# Cosmos DB\n",
    "COSMOS_ACCOUNT_NAME={step3_outputs.get('cosmosDbAccountName', '')}\n",
    "COSMOS_ENDPOINT={step3_outputs.get('cosmosDbEndpoint', '')}\n",
    "COSMOS_KEY={step3_outputs.get('cosmosDbKey', '')}\n",
    "\n",
    "# Content Safety\n",
    "CONTENT_SAFETY_ENDPOINT={step3_outputs.get('contentSafetyEndpoint', '')}\n",
    "CONTENT_SAFETY_KEY={step3_outputs.get('contentSafetyKey', '')}\n",
    "\n",
    "# ===========================================\n",
    "# MCP Servers\n",
    "# ===========================================\n",
    "CONTAINER_REGISTRY={step4_outputs.get('containerRegistryLoginServer', '')}\n",
    "CONTAINER_APP_ENV_ID={step4_outputs.get('containerAppEnvId', '')}\n",
    "\"\"\"\n",
    "\n",
    "# Add MCP server URLs (safe handling if not present)\n",
    "mcp_urls = step4_outputs.get('mcpServerUrls', []) if isinstance(step4_outputs, dict) else []\n",
    "for mcp in mcp_urls:\n",
    "    # Guard against missing fields\n",
    "    name = mcp.get('name') if isinstance(mcp, dict) else None\n",
    "    url = mcp.get('url') if isinstance(mcp, dict) else None\n",
    "    if name and url:\n",
    "        var_name = f\"MCP_SERVER_{name.upper().replace('-', '_')}_URL\"\n",
    "        env_content += f\"{var_name}={url}\\n\"\n",
    "\n",
    "env_content += f\"\"\"\n",
    "# ===========================================\n",
    "# Deployment Info\n",
    "# ===========================================\n",
    "RESOURCE_GROUP={resource_group_name}\n",
    "LOCATION={location}\n",
    "DEPLOYMENT_PREFIX={deployment_name_prefix}\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "env_file = 'master-lab.env'\n",
    "with open(env_file, 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(f'[OK] Created {env_file}')\n",
    "print(f'[OK] File location: {os.path.abspath(env_file)}')\n",
    "\n",
    "# Display summary of model deployments\n",
    "if foundries_data:\n",
    "    print()\n",
    "    print('[*] Model Deployment Summary:')\n",
    "    for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "        if isinstance(foundry_info, dict):\n",
    "            location = foundry_info.get('location', 'unknown')\n",
    "            models = foundry_info.get('models', [])\n",
    "            region_display = region_names.get(location, location)\n",
    "            print(f'  Region {idx} ({region_display}): {len(models)} models')\n",
    "            for model in models:\n",
    "                print(f'    - {model}')\n",
    "\n",
    "# Display load balancing info\n",
    "if len(lb_endpoints) > 1:\n",
    "    print()\n",
    "    print(f'[OK] Load Balancing: ENABLED ({len(lb_endpoints)} regions)')\n",
    "    print(f'[OK] LB Regions: {\", \".join(lb_regions)}')\n",
    "else:\n",
    "    print()\n",
    "    print('[!] Load Balancing: Disabled (requires 2+ regions with gpt-4o-mini)')\n",
    "\n",
    "print()\n",
    "print('[OK] You can now load this in all lab tests:')\n",
    "print('  from dotenv import load_dotenv')\n",
    "print('  load_dotenv(\"master-lab.env\")')\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('SETUP COMPLETE - ALL LABS READY')\n",
    "print('=' * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_33_bbe53d04",
   "metadata": {},
   "source": [
    "# Master AI Gateway Lab - 25 Labs Consolidated\n",
    "\n",
    "**One deployment. All features. Fully expanded tests.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Initialization](#init)\n",
    "- [Workshop Routes](#routes)\n",
    "- [Lab 01: Zero to Production](#lab01)\n",
    "- [Lab 02: Backend Pool Load Balancing](#lab02)\n",
    "- [Lab 03: Built-in Logging](#lab03)\n",
    "- [Lab 04: Token Metrics Emitting](#lab04)\n",
    "- [Lab 05: Token Rate Limiting](#lab05)\n",
    "- [Lab 06: Access Controlling](#lab06)\n",
    "- [Lab 07: Content Safety](#lab07)\n",
    "- [Lab 08: Model Routing](#lab08)\n",
    "- [Lab 09: AI Foundry SDK](#lab09)\n",
    "- [Lab 10: AI Foundry DeepSeek](#lab10)\n",
    "- [Lab 11: Model Context Protocol](#lab11)\n",
    "- [Lab 12: MCP from API](#lab12)\n",
    "- [Lab 13: MCP Client Authorization](#lab13)\n",
    "- [Lab 14: MCP A2A Agents](#lab14)\n",
    "- [Lab 15: OpenAI Agents](#lab15)\n",
    "- [Lab 16: AI Agent Service](#lab16)\n",
    "- [Lab 17: Realtime MCP Agents](#lab17)\n",
    "- [Lab 18: Function Calling](#lab18)\n",
    "- [Lab 19: Semantic Caching](#lab19)\n",
    "- [Lab 20: Message Storing](#lab20)\n",
    "- [Lab 21: Vector Searching](#lab21)\n",
    "- [Lab 22: Image Generation](#lab22)\n",
    "- [Lab 23: Multi-Server Orchestration](#lab23)\n",
    "- [Lab 24: FinOps Framework](#lab24)\n",
    "- [Lab 25: Secure Responses API](#lab25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_34_8b15779b",
   "metadata": {},
   "source": [
    "The following labs (01-10) cover essential Azure API Management features for AI workloads:\n",
    "\n",
    "- **Lab 01:** Zero to Production - Foundation setup and basic chat completion\n",
    "- **Lab 02:** Backend Pool Load Balancing - Multi-region routing and failover\n",
    "- **Lab 03:** Built-in Logging - Observability with Log Analytics and App Insights\n",
    "- **Lab 04:** Token Metrics Emitting - Cost monitoring and capacity planning\n",
    "- **Lab 05:** Token Rate Limiting - Quota management and abuse prevention\n",
    "- **Lab 06:** Access Controlling - OAuth 2.0 and Entra ID authentication\n",
    "- **Lab 07:** Content Safety - Harmful content detection and filtering\n",
    "- **Lab 08:** Model Routing - Intelligent model selection by criteria\n",
    "- **Lab 09:** AI Foundry SDK - Advanced AI capabilities and model catalog\n",
    "- **Lab 10:** AI Foundry DeepSeek - Open-source reasoning model integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_35_0a0d7ce7",
   "metadata": {},
   "source": [
    "<a id='lab01'></a>\n",
    "\n",
    "## Lab 01: Zero to Production\n",
    "\n",
    "![flow](../../images/GPT-4o-inferencing.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Learn the fundamentals of deploying and testing Azure OpenAI through API Management, establishing the foundation for all advanced labs.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Basic Chat Completion:** Send prompts to GPT-4o-mini and receive AI-generated responses\n",
    "- **Streaming Responses:** Handle real-time streaming output for better user experience\n",
    "- **Request Patterns:** Understand the HTTP request/response cycle through APIM gateway\n",
    "- **API Key Management:** Secure API access using APIM subscription keys\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](../../zero-to-production/result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Basic chat completion returns valid responses\n",
    "- Streaming works correctly with incremental tokens\n",
    "- Multiple requests complete successfully\n",
    "- Response times are < 2 seconds for simple prompts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_36_1f195b0a",
   "metadata": {},
   "source": [
    "### Test 1: Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell_37_7bb1f71e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:56.726481Z",
     "iopub.status.busy": "2025-11-15T16:27:56.726272Z",
     "iopub.status.idle": "2025-11-15T16:27:56.753743Z",
     "shell.execute_reply": "2025-11-15T16:27:56.752986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAB 02: Token Metrics Configuration\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[32m     13\u001b[39m backend_id = \u001b[33m\"\u001b[39m\u001b[33minference-backend-pool\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LAB 02: Token Metrics (OpenAI API Monitoring)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 02: Token Metrics Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend ID: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Token metrics policy with API-KEY authentication\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "        <azure-openai-emit-token-metric namespace=\"openai\">\n",
    "            <dimension name=\"Subscription ID\" value=\"@(context.Subscription.Id)\" />\n",
    "            <dimension name=\"Client IP\" value=\"@(context.Request.IpAddress)\" />\n",
    "            <dimension name=\"API ID\" value=\"@(context.Api.Id)\" />\n",
    "            <dimension name=\"User ID\" value=\"@(context.Request.Headers.GetValueOrDefault(&quot;x-user-id&quot;, &quot;N/A&quot;))\" />\n",
    "        </azure-openai-emit-token-metric>\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying token-metrics via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Metrics will be available in Azure Monitor\")\n",
    "print(\"[NEXT] Run the cells below to test token metrics\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell_38_5f96f861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:56.755875Z",
     "iopub.status.busy": "2025-11-15T16:27:56.755699Z",
     "iopub.status.idle": "2025-11-15T16:27:56.768377Z",
     "shell.execute_reply": "2025-11-15T16:27:56.767569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] APIM Gateway URL: https://apim-pavavy6pu5hpa.azure-api.net\n",
      "[OK] Inference API Path: inference\n",
      "[INFO] Candidate API versions: ['2024-08-01-preview', '2024-07-18', '2024-06-01-preview']\n",
      "[OK] Resolved Azure Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "[*] Attempting chat with api-version=2024-08-01-preview ...\n",
      "[WARN] Failed with 2024-08-01-preview: No module named 'openai'\n",
      "[*] Attempting chat with api-version=2024-07-18 ...\n",
      "[WARN] Failed with 2024-07-18: No module named 'openai'\n",
      "[*] Attempting chat with api-version=2024-06-01-preview ...\n",
      "[WARN] Failed with 2024-06-01-preview: No module named 'openai'\n",
      "[ERROR] All candidate API versions failed.\n",
      "  Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "  Model: gpt-4o-mini\n",
      "  Last error: No module named 'openai'\n",
      "\n",
      "Troubleshooting:\n",
      "  1. Confirm APIM policy rewrites to the backend OpenAI path correctly.\n",
      "  2. Verify the model deployment exists and is named exactly gpt-4o-mini.\n",
      "  3. Check Log Analytics / APIM trace for internal exception details.\n",
      "  4. If 500 persists, test direct backend (bypass APIM) to isolate gateway policy issues.\n",
      "\n",
      "[diag] Expected request pattern:\n",
      "  https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=<selected>\n",
      "[diag] If APIM operation base path != /inference adjust INFERENCE_API_PATH accordingly.\n"
     ]
    }
   ],
   "source": [
    "# Lab 01: Test 1 - Basic Chat Completion\n",
    "# This cell initializes the AzureOpenAI client and tests basic chat completion\n",
    "\n",
    "# NOTE: Core imports (os, AzureOpenAI, dotenv) already done in earlier cells (avoid re-import)\n",
    "\n",
    "# Pull required config from existing environment (already loaded via earlier env loader)\n",
    "apim_gateway_url = os.getenv('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.getenv('APIM_API_KEY')\n",
    "inference_api_path = os.getenv('INFERENCE_API_PATH', 'inference').strip('/')\n",
    "\n",
    "missing = [k for k, v in [('APIM_GATEWAY_URL', apim_gateway_url), ('APIM_API_KEY', apim_api_key)] if not v]\n",
    "if missing:\n",
    "    print(f'[ERROR] Missing required env vars: {\", \".join(missing)}')\n",
    "    print('Run deployment + env generation cells first.')\n",
    "else:\n",
    "    print(f'[OK] APIM Gateway URL: {apim_gateway_url}')\n",
    "    print(f'[OK] Inference API Path: {inference_api_path}')\n",
    "\n",
    "# Determine correct azure_endpoint:\n",
    "# Expect external path to be: /{inference_api_path}/openai/deployments/...\n",
    "# Only append inference path if not already present at end.\n",
    "if apim_gateway_url:\n",
    "    base = apim_gateway_url.rstrip('/')\n",
    "    if base.lower().endswith(f'/{inference_api_path.lower()}'):\n",
    "        azure_endpoint = base\n",
    "    else:\n",
    "        azure_endpoint = f'{base}/{inference_api_path}'\n",
    "else:\n",
    "    azure_endpoint = ''\n",
    "\n",
    "# Ordered list of API versions to try (first success wins)\n",
    "preferred = os.getenv('OPENAI_API_VERSION')\n",
    "api_versions = [v for v in [\n",
    "    preferred,\n",
    "    '2024-08-01-preview',\n",
    "    '2024-07-18',\n",
    "    '2024-06-01-preview'\n",
    "] if v]\n",
    "\n",
    "print(f'[INFO] Candidate API versions: {api_versions}')\n",
    "print(f'[OK] Resolved Azure Endpoint: {azure_endpoint}')\n",
    "\n",
    "model_name = 'gpt-4o-mini'\n",
    "prompt_user = 'Explain Azure API Management in one concise sentence.'\n",
    "\n",
    "def try_chat(api_version: str):\n",
    "    from openai import AzureOpenAI  # available already; safe local use without re-importing globally\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_key=apim_api_key,\n",
    "        api_version=api_version,\n",
    "        timeout=30\n",
    "    )\n",
    "    return client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a concise helpful assistant.'},\n",
    "            {'role': 'user', 'content': prompt_user}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if not apim_gateway_url or not apim_api_key:\n",
    "    # Abort early ‚Äì environment not ready\n",
    "    pass\n",
    "else:\n",
    "    last_error = None\n",
    "    successful_version = None\n",
    "    response = None\n",
    "\n",
    "    for ver in api_versions:\n",
    "        print(f'[*] Attempting chat with api-version={ver} ...')\n",
    "        try:\n",
    "            response = try_chat(ver)\n",
    "            successful_version = ver\n",
    "            print(f'[OK] Chat success with version {ver}')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            # Inspect for transient 500 to decide if retry is meaningful\n",
    "            msg = str(e)\n",
    "            if '429' in msg:\n",
    "                print(f'[WARN] Throttled on {ver}; retrying next version/backoff.')\n",
    "            elif '500' in msg:\n",
    "                print(f'[WARN] Backend 500 with {ver}; will try fallback.')\n",
    "            elif '404' in msg or 'Not Found' in msg:\n",
    "                print(f'[WARN] Path/version mismatch for {ver}; trying next.')\n",
    "            else:\n",
    "                print(f'[WARN] Failed with {ver}: {msg[:140]}')\n",
    "        # continue loop\n",
    "\n",
    "    if not successful_version:\n",
    "        print('[ERROR] All candidate API versions failed.')\n",
    "        print(f'  Endpoint: {azure_endpoint}')\n",
    "        print(f'  Model: {model_name}')\n",
    "        if last_error:\n",
    "            print(f'  Last error: {last_error}')\n",
    "        print('\\nTroubleshooting:')\n",
    "        print('  1. Confirm APIM policy rewrites to the backend OpenAI path correctly.')\n",
    "        print('  2. Verify the model deployment exists and is named exactly gpt-4o-mini.')\n",
    "        print('  3. Check Log Analytics / APIM trace for internal exception details.')\n",
    "        print('  4. If 500 persists, test direct backend (bypass APIM) to isolate gateway policy issues.')\n",
    "    else:\n",
    "        # Extract content safely (SDK variants may differ)\n",
    "        try:\n",
    "            content = response.choices[0].message.content\n",
    "        except AttributeError:\n",
    "            # Fallback for older shapes\n",
    "            content = response.choices[0].message.get('content', '')\n",
    "        print(f'[SUCCESS] Version {successful_version} response:\\n{content}')\n",
    "        print('[OK] Lab 01 Test 1: Basic chat completion succeeded.')\n",
    "\n",
    "# Optional diagnostic: show fully qualified request pattern for verification\n",
    "print('\\n[diag] Expected request pattern:')\n",
    "print(f'  {azure_endpoint}/openai/deployments/{model_name}/chat/completions?api-version=<selected>')\n",
    "print('[diag] If APIM operation base path != /inference adjust INFERENCE_API_PATH accordingly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_39_0478a7f3",
   "metadata": {},
   "source": [
    "### Test 2: Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell_40_4d2ace70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:56.771125Z",
     "iopub.status.busy": "2025-11-15T16:27:56.770924Z",
     "iopub.status.idle": "2025-11-15T16:27:56.776697Z",
     "shell.execute_reply": "2025-11-15T16:27:56.775896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Testing streaming...\n",
      "[ERROR] Streaming exception: name 'client' is not defined\n",
      "[HINT] If this persists, verify the APIM operation allows streaming and the backend model supports it.\n"
     ]
    }
   ],
   "source": [
    "# Lab 01: Test 2 - Streaming Response (robust with fallback)\n",
    "\n",
    "print('[*] Testing streaming...')\n",
    "\n",
    "prompt_messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant. Stream numbers.'},\n",
    "    {'role': 'user', 'content': 'Count from 1 to 5'}\n",
    "]\n",
    "\n",
    "def stream_completion():\n",
    "    return client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompt_messages,\n",
    "        max_tokens=32,\n",
    "        temperature=0.2,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "def non_stream_completion():\n",
    "    return client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompt_messages,\n",
    "        max_tokens=32,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "try:\n",
    "    stream = stream_completion()\n",
    "    had_output = False\n",
    "    for chunk in stream:\n",
    "        try:\n",
    "            # Support both delta.content and delta with list of content parts\n",
    "            if chunk.choices:\n",
    "                delta = getattr(chunk.choices[0], 'delta', None)\n",
    "                if delta:\n",
    "                    piece = getattr(delta, 'content', None)\n",
    "                    if piece:\n",
    "                        print(piece, end='', flush=True)\n",
    "                        had_output = True\n",
    "        except Exception:\n",
    "            # Ignore malformed chunk pieces\n",
    "            pass\n",
    "    if not had_output:\n",
    "        print('[WARN] Stream yielded no incremental content; backend may not support streaming through APIM.')\n",
    "        raise RuntimeError('Empty stream')\n",
    "    print()  # newline after stream\n",
    "    print('[OK] Streaming works!')\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if '500' in msg or 'Internal server error' in msg:\n",
    "        print(f'[WARN] Streaming failed with backend 500: {msg[:140]}')\n",
    "        print('[INFO] Falling back to non-streaming completion (APIM policy or backend may not support streaming).')\n",
    "        try:\n",
    "            resp = non_stream_completion()\n",
    "            try:\n",
    "                full = resp.choices[0].message.content\n",
    "            except AttributeError:\n",
    "                full = resp.choices[0].message.get('content', '')\n",
    "            print(full)\n",
    "            print('[OK] Fallback non-streaming completion succeeded.')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Fallback non-streaming also failed: {e2}')\n",
    "            print('[HINT] Check APIM policies (buffering, rewrite), model deployment name, and gateway trace.')\n",
    "    else:\n",
    "        print(f'[ERROR] Streaming exception: {msg}')\n",
    "        print('[HINT] If this persists, verify the APIM operation allows streaming and the backend model supports it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_41_d7ea554a",
   "metadata": {},
   "source": [
    "### Test 3: Multiple Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell_42_ccbab37e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:56.778933Z",
     "iopub.status.busy": "2025-11-15T16:27:56.778745Z",
     "iopub.status.idle": "2025-11-15T16:27:56.806314Z",
     "shell.execute_reply": "2025-11-15T16:27:56.805300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AzureOpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m     azure_endpoint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mazure_endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_api_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m api_version = os.getenv(\u001b[33m'\u001b[39m\u001b[33mOPENAI_API_VERSION\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m2024-08-01-preview\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m client = \u001b[43mAzureOpenAI\u001b[49m(\n\u001b[32m     13\u001b[39m     azure_endpoint=azure_endpoint,\n\u001b[32m     14\u001b[39m     api_key=apim_api_key,  \u001b[38;5;66;03m# still supply subscription key (API key) in case policy allows it\u001b[39;00m\n\u001b[32m     15\u001b[39m     api_version=api_version,\n\u001b[32m     16\u001b[39m     timeout=\u001b[32m30\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Acquire JWT if current APIM policy requires it (JWT-only or dual auth)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# DefaultAzureCredential already imported earlier (cell 22); reuse without re-import\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'AzureOpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure AzureOpenAI client exists (reuse existing imports / env)\n",
    "if 'client' not in globals():\n",
    "    apim_gateway_url = os.getenv('APIM_GATEWAY_URL')\n",
    "    apim_api_key = os.getenv('APIM_API_KEY')\n",
    "    inference_api_path = os.getenv('INFERENCE_API_PATH', 'inference').strip('/')\n",
    "    if not apim_gateway_url or not apim_api_key:\n",
    "        raise RuntimeError('Missing APIM_GATEWAY_URL or APIM_API_KEY in environment. Run earlier env/deployment cells.')\n",
    "    azure_endpoint = apim_gateway_url.rstrip('/')\n",
    "    if not azure_endpoint.lower().endswith(f'/{inference_api_path.lower()}'):\n",
    "        azure_endpoint = f\"{azure_endpoint}/{inference_api_path}\"\n",
    "    api_version = os.getenv('OPENAI_API_VERSION') or '2024-08-01-preview'\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_key=apim_api_key,  # still supply subscription key (API key) in case policy allows it\n",
    "        api_version=api_version,\n",
    "        timeout=30\n",
    "    )\n",
    "    # Acquire JWT if current APIM policy requires it (JWT-only or dual auth)\n",
    "    # DefaultAzureCredential already imported earlier (cell 22); reuse without re-import\n",
    "    try:\n",
    "        credential = DefaultAzureCredential()\n",
    "        # Audience accepted by the APIM JWT-only policy (see policy cell 59)\n",
    "        jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    except Exception as _jwt_err:\n",
    "        jwt_token = None\n",
    "        print(f\"[auth] WARN: Unable to acquire JWT token: {_jwt_err}\")\n",
    "\n",
    "# Use JWT header if available (APIM will ignore if policy does not require it)\n",
    "extra_headers = {}\n",
    "if 'jwt_token' in globals() and jwt_token:\n",
    "    extra_headers['Authorization'] = f'Bearer {jwt_token}'\n",
    "\n",
    "for i in range(5):\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': f'Request {i+1}'}],\n",
    "        max_tokens=10,\n",
    "        extra_headers=extra_headers  # inject Authorization when present\n",
    "    )\n",
    "    try:\n",
    "        content = resp.choices[0].message.content\n",
    "    except AttributeError:\n",
    "        content = resp.choices[0].message.get('content', '')\n",
    "    print(f'Request {i+1}: {content}')\n",
    "\n",
    "utils.print_ok('Lab 01 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_43_67b478de",
   "metadata": {},
   "source": [
    "<a id='lab02'></a>\n",
    "\n",
    "## Lab 02: Backend Pool Load Balancing\n",
    "\n",
    "![Backend Pool Load Balancing](../../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "üìñ [Workshop Guide](https://azure-samples.github.io/AI-Gateway/docs/azure-openai/dynamic-failover)\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "Master multi-region load balancing with priority-based routing and automatic failover across Azure regions.\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Priority Routing:** Configure priority 1 (UK South) with fallback to priority 2 regions\n",
    "- **Round-Robin Distribution:** Balance traffic across Sweden Central and West Europe (50/50 weight)\n",
    "- **Automatic Retry:** APIM retries on HTTP 429 (rate limit) transparently\n",
    "- **Regional Headers:** Track which region served each request via `x-ms-region` header\n",
    "- **Performance Analysis:** Visualize response times and regional distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Backend Pool Configuration\n",
    "\n",
    "Azure API Management supports three load balancing strategies:\n",
    "\n",
    "<details>\n",
    "<summary><b>1. Round-Robin Distribution</b></summary>\n",
    "\n",
    "Distributes requests evenly across all backends with equal weight.\n",
    "\n",
    "**Configuration:**\n",
    "- All backends have the same priority level\n",
    "- Equal weight distribution (or default weights)\n",
    "- Requests rotate sequentially through backends\n",
    "\n",
    "**Use Case:** When all regions have equal capacity and you want even distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>2. Priority-Based Routing</b></summary>\n",
    "\n",
    "Lower priority values receive traffic first, with automatic failover to higher priority backends.\n",
    "\n",
    "**Example Configuration:**\n",
    "- **East US:** Priority 1 (primary)\n",
    "- **West US:** Priority 2 (fallback)\n",
    "- **Sweden Central:** Priority 3 (fallback)\n",
    "\n",
    "**Use Case:** When you have a preferred region for latency or cost reasons.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><b>3. Weighted Load Balancing</b></summary>\n",
    "\n",
    "Assigns different traffic proportions within the same priority level.\n",
    "\n",
    "**Example Configuration:**\n",
    "- **East US:** Priority 1, Weight 100\n",
    "- **West US:** Priority 2, Weight 50\n",
    "- **Sweden Central:** Priority 2, Weight 50\n",
    "\n",
    "When Priority 1 is unavailable, traffic splits 50/50 between Priority 2 backends.\n",
    "\n",
    "**Use Case:** When backends have different capacities or you want controlled traffic distribution.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Circuit Breaker Configuration\n",
    "\n",
    "> **üí° Tip:** Each backend should have a circuit breaker rule to handle failures gracefully.\n",
    "\n",
    "**Recommended Settings:**\n",
    "- **Failure Count:** 1 (trip after single failure)\n",
    "- **Failure Interval:** 5 minutes\n",
    "- **Custom Range:** HTTP 429 (rate limit)\n",
    "- **Trip Duration:** 1 minute\n",
    "- **Retry-After Header:** Enabled\n",
    "\n",
    "This configuration ensures that when a backend hits its rate limit (HTTP 429), APIM automatically routes traffic to other backends for 1 minute.\n",
    "\n",
    "---\n",
    "\n",
    "### Monitoring Regional Distribution\n",
    "\n",
    "> **‚ö†Ô∏è Note:** The `x-ms-region` header in responses indicates which backend processed the request.\n",
    "\n",
    "This header allows you to:\n",
    "- Verify load distribution patterns\n",
    "- Monitor failover behavior\n",
    "- Analyze regional performance\n",
    "- Debug routing issues\n",
    "\n",
    "**Example Response Headers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell_44_f7e0fe5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:56.809192Z",
     "iopub.status.busy": "2025-11-15T16:27:56.808878Z",
     "iopub.status.idle": "2025-11-15T16:27:56.836850Z",
     "shell.execute_reply": "2025-11-15T16:27:56.835598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAB 03: Load Balancing Configuration\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[32m     13\u001b[39m backend_id = \u001b[33m\"\u001b[39m\u001b[33minference-backend-pool\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LAB 03: Load Balancing with Retry Logic\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 03: Load Balancing Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend Pool: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Load balancing policy with API-KEY authentication and retry logic\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\"\n",
    "               condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n",
    "            <forward-request buffer-request-body=\"true\" />\n",
    "        </retry>\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "        <choose>\n",
    "            <when condition=\"@(context.Response.StatusCode == 503)\">\n",
    "                <return-response>\n",
    "                    <set-status code=\"503\" reason=\"Service Unavailable\" />\n",
    "                    <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "                        <value>application/json</value>\n",
    "                    </set-header>\n",
    "                    <set-body>{{\"error\": {{\"code\": \"ServiceUnavailable\", \"message\": \"Service temporarily unavailable\"}}}}</set-body>\n",
    "                </return-response>\n",
    "            </when>\n",
    "        </choose>\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying load-balancing via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Load balancing will distribute requests across backend pool\")\n",
    "print(\"[INFO] Retry logic will handle 429 (rate limit) and 503 (unavailable) errors\")\n",
    "print(\"[NEXT] Run load balancing tests in cells below\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_45_7d2cb75c",
   "metadata": {},
   "source": [
    "### Test 1: Load Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell_46_c665adef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:56.839984Z",
     "iopub.status.busy": "2025-11-15T16:27:56.839681Z",
     "iopub.status.idle": "2025-11-15T16:27:59.433990Z",
     "shell.execute_reply": "2025-11-15T16:27:59.433141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing load balancing across 3 regions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1: 0.52s - Region: Unknown - Backend: Unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 2: 0.27s - Region: Unknown - Backend: Unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 3: 0.27s - Region: Unknown - Backend: Unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 4: 0.26s - Region: Unknown - Backend: Unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 5: 0.24s - Region: Unknown - Backend: Unknown\n",
      "\n",
      "Average response time: 0.31s\n",
      "\n",
      "Region Distribution:\n",
      "  Unknown: 5 requests (100.0%)\n",
      "\n",
      "[INFO] All regions showing as \"Unknown\" - region headers may not be configured in APIM\n",
      "\n",
      "üìã TO ADD REGION HEADERS VIA APIM POLICY:\n",
      "   1. Azure Portal ‚Üí API Management ‚Üí APIs ‚Üí inference-api\n",
      "   2. Click \"All operations\" ‚Üí Outbound processing ‚Üí Add policy\n",
      "   3. Add this XML to <outbound> section:\n",
      "\n",
      "   <set-header name=\"x-ms-region\" exists-action=\"override\">\n",
      "       <value>@(context.Deployment.Region)</value>\n",
      "   </set-header>\n",
      "   <set-header name=\"x-ms-backend-id\" exists-action=\"override\">\n",
      "       <value>@(context.Request.MatchedParameters.GetValueOrDefault(\"backend-id\", \"unknown\"))</value>\n",
      "   </set-header>\n",
      "\n",
      "   4. Save the policy\n",
      "\n",
      "‚ÑπÔ∏è  Region detection is informational only - load balancing still works\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m‚ÑπÔ∏è  Region detection is informational only - load balancing still works\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mutils\u001b[49m.print_ok(\u001b[33m'\u001b[39m\u001b[33mLoad balancing test complete!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "print('Testing load balancing across 3 regions...')\n",
    "responses = []\n",
    "regions = []  # Track which region processed each request\n",
    "backend_ids = []  # Track which backend served each request\n",
    "\n",
    "# Use requests library to access HTTP headers (OpenAI SDK doesn't expose them)\n",
    "import requests\n",
    "\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "\n",
    "    # Make direct HTTP request to capture headers\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=f\"{apim_gateway_url}/{inference_api_path}/openai/deployments/gpt-4o-mini/chat/completions?api-version={api_version}\",\n",
    "            headers={\n",
    "                \"api-key\": apim_api_key,\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": f\"Test {i+1}\"}],\n",
    "                \"max_tokens\": 5\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        responses.append(elapsed)\n",
    "        \n",
    "        # Extract region and backend info from response headers\n",
    "        region = response.headers.get('x-ms-region', 'Unknown')\n",
    "        backend_id = response.headers.get('x-ms-backend-id', 'Unknown')\n",
    "        \n",
    "        regions.append(region)\n",
    "        backend_ids.append(backend_id)\n",
    "        \n",
    "        print(f'Request {i+1}: {elapsed:.2f}s - Region: {region} - Backend: {backend_id}')\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'[WARN] Request {i+1} failed: {e}')\n",
    "        responses.append(0)\n",
    "        regions.append('Error')\n",
    "        backend_ids.append('Error')\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "\n",
    "avg_time = sum(responses) / len(responses) if responses else 0\n",
    "print(f'\\nAverage response time: {avg_time:.2f}s')\n",
    "\n",
    "# Display region distribution\n",
    "from collections import Counter\n",
    "region_counts = Counter(regions)\n",
    "print(f'\\nRegion Distribution:')\n",
    "for region, count in region_counts.items():\n",
    "    print(f'  {region}: {count} requests ({count/len(regions)*100:.1f}%)')\n",
    "\n",
    "# Check if region detection is working\n",
    "unknown_count = region_counts.get('Unknown', 0)\n",
    "if unknown_count == len(regions):\n",
    "    print('')\n",
    "    print('[INFO] All regions showing as \"Unknown\" - region headers may not be configured in APIM')\n",
    "    print('')\n",
    "    print('üìã TO ADD REGION HEADERS VIA APIM POLICY:')\n",
    "    print('   1. Azure Portal ‚Üí API Management ‚Üí APIs ‚Üí inference-api')\n",
    "    print('   2. Click \"All operations\" ‚Üí Outbound processing ‚Üí Add policy')\n",
    "    print('   3. Add this XML to <outbound> section:')\n",
    "    print('')\n",
    "    print('   <set-header name=\"x-ms-region\" exists-action=\"override\">')\n",
    "    print('       <value>@(context.Deployment.Region)</value>')\n",
    "    print('   </set-header>')\n",
    "    print('   <set-header name=\"x-ms-backend-id\" exists-action=\"override\">')\n",
    "    print('       <value>@(context.Request.MatchedParameters.GetValueOrDefault(\"backend-id\", \"unknown\"))</value>')\n",
    "    print('   </set-header>')\n",
    "    print('')\n",
    "    print('   4. Save the policy')\n",
    "    print('')\n",
    "    print('‚ÑπÔ∏è  Region detection is informational only - load balancing still works')\n",
    "    print('')\n",
    "\n",
    "utils.print_ok('Load balancing test complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_47_c20a7ffc",
   "metadata": {},
   "source": [
    "### Test 2: Visualize Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell_48_b37f6034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.437303Z",
     "iopub.status.busy": "2025-11-15T16:27:59.437102Z",
     "iopub.status.idle": "2025-11-15T16:27:59.466216Z",
     "shell.execute_reply": "2025-11-15T16:27:59.465348Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Create DataFrame with response times and regions\n",
    "df = pd.DataFrame({\n",
    "    'Request': range(1, len(responses)+1),\n",
    "    'Time (s)': responses,\n",
    "    'Region': regions\n",
    "})\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Response times with region colors\n",
    "region_colors = {'Unknown': 'gray'}\n",
    "unique_regions = [r for r in set(regions) if r != 'Unknown']\n",
    "color_palette = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for idx, region in enumerate(unique_regions):\n",
    "    region_colors[region] = color_palette[idx % len(color_palette)]\n",
    "\n",
    "colors = [region_colors.get(r, 'gray') for r in regions]\n",
    "\n",
    "ax1.scatter(df['Request'], df['Time (s)'], c=colors, alpha=0.6, s=100)\n",
    "ax1.axhline(y=avg_time, color='r', linestyle='--', label=f'Average: {avg_time:.2f}s')\n",
    "ax1.set_xlabel('Request Number')\n",
    "ax1.set_ylabel('Response Time (s)')\n",
    "ax1.set_title('Load Balancing Response Times by Region')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Create custom legend for regions\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=region_colors[r], label=r) for r in set(regions)]\n",
    "ax1.legend(handles=legend_elements + [plt.Line2D([0], [0], color='r', linestyle='--', label=f'Avg: {avg_time:.2f}s')],\n",
    "          loc='upper right')\n",
    "\n",
    "# Plot 2: Region distribution bar chart\n",
    "region_counts = Counter(regions)\n",
    "regions_list = list(region_counts.keys())\n",
    "counts_list = list(region_counts.values())\n",
    "\n",
    "bars = ax2.bar(regions_list, counts_list, color=[region_colors.get(r, 'gray') for r in regions_list])\n",
    "ax2.set_xlabel('Region')\n",
    "ax2.set_ylabel('Number of Requests')\n",
    "ax2.set_title('Request Distribution Across Regions')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "utils.print_ok('Lab 02 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_49_35b632c0",
   "metadata": {},
   "source": [
    "Implement comprehensive observability using Azure Log Analytics and Application Insights for AI gateway monitoring.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Log Analytics Integration:** Automatic logging of all APIM requests and responses\n",
    "- **Application Insights:** Track performance metrics, failures, and dependencies\n",
    "- **Diagnostic Settings:** Configure what data to log and where to send it\n",
    "- **Query Language (KQL):** Write queries to analyze request patterns\n",
    "- **Dashboard Creation:** Build monitoring dashboards for AI gateway operations\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- All API requests logged to Log Analytics workspace\n",
    "- Application Insights captures latency metrics\n",
    "- KQL queries return request data successfully\n",
    "- Can trace individual requests end-to-end\n",
    "- Dashboards show real-time gateway health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell_50_233ef817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.469895Z",
     "iopub.status.busy": "2025-11-15T16:27:59.469410Z",
     "iopub.status.idle": "2025-11-15T16:27:59.498942Z",
     "shell.execute_reply": "2025-11-15T16:27:59.497748Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AuthenticationError\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Ensure client exists (reuse previously initialized one; avoid duplicate imports)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "from openai import AuthenticationError\n",
    "\n",
    "# Ensure client exists (reuse previously initialized one; avoid duplicate imports)\n",
    "if 'client' not in globals():\n",
    "    import os\n",
    "    apim_gateway_url = os.getenv('APIM_GATEWAY_URL')\n",
    "    apim_api_key = os.getenv('APIM_API_KEY')\n",
    "    inference_api_path = os.getenv('INFERENCE_API_PATH', 'inference').strip('/')\n",
    "    if apim_gateway_url and apim_api_key:\n",
    "        from openai import AzureOpenAI\n",
    "        endpoint = apim_gateway_url.rstrip('/')\n",
    "        if not endpoint.lower().endswith(f'/{inference_api_path.lower()}'):\n",
    "            endpoint = f\"{endpoint}/{inference_api_path}\"\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=apim_api_key,\n",
    "            api_version=os.getenv('OPENAI_API_VERSION') or '2024-08-01-preview',\n",
    "            timeout=30\n",
    "        )\n",
    "    else:\n",
    "        print('[logs] Missing APIM_GATEWAY_URL/APIM_API_KEY; abort log generation.')\n",
    "        utils.print_ok('Lab 03 skipped (env missing)')\n",
    "        raise SystemExit\n",
    "\n",
    "success_count = 0\n",
    "for i in range(10):\n",
    "    try:\n",
    "        client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{'role': 'user', 'content': f'Log test {i}'}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        success_count += 1\n",
    "    except AuthenticationError as e:\n",
    "        # Graceful handling of missing data action (RBAC) on APIM managed identity\n",
    "        principal_id = step1_outputs.get('apimPrincipalId')\n",
    "        print('[logs] AuthenticationError encountered; stopping further requests.')\n",
    "        print(f'[logs] Principal lacking OpenAI data action: {principal_id}')\n",
    "        print('[fix] Assign role (one-time) to allow logging:')\n",
    "        print('       az role assignment create \\\\')\n",
    "        print(f'         --assignee {principal_id} \\\\')\n",
    "        print('         --role \"Cognitive Services OpenAI User\" \\\\')\n",
    "        print(f'         --scope /subscriptions/{subscription_id}')\n",
    "        print('[alt] Or switch APIM backend auth to key-based if MI not needed.')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f'[logs] Non-auth error on iteration {i}: {e}')\n",
    "        break\n",
    "\n",
    "print(f'[logs] Chat completion attempts succeeded: {success_count}/10')\n",
    "utils.print_ok('Lab 03: Logs generated (check Log Analytics for successful calls)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_51_7bbce6e3",
   "metadata": {},
   "source": [
    "<a id='lab04'></a>\n",
    "\n",
    "## Lab 04: Token Metrics Emitting\n",
    "\n",
    "![flow](../../images/ai-gateway.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Track and emit token usage metrics for cost monitoring and capacity planning across all AI requests.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Token Counting:** Capture prompt tokens, completion tokens, and total tokens\n",
    "- **Custom Metrics:** Emit token metrics to Application Insights\n",
    "- **Cost Calculation:** Understand token-based pricing and cost attribution\n",
    "- **Usage Patterns:** Analyze token consumption trends over time\n",
    "- **Quota Management:** Track usage against allocated quotas\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](../../token-metrics-emitting/result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Token metrics logged for every request\n",
    "- Custom Application Insights metrics show token usage\n",
    "- Can query total tokens consumed per time period\n",
    "- Cost estimates available based on token pricing\n",
    "- Alerts configured for unusual token consumption\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell_52_d2e70a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.501728Z",
     "iopub.status.busy": "2025-11-15T16:27:59.501471Z",
     "iopub.status.idle": "2025-11-15T16:27:59.520414Z",
     "shell.execute_reply": "2025-11-15T16:27:59.519349Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m total_tokens = \u001b[32m0\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      4\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mTell me about AI\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      6\u001b[39m         max_tokens=\u001b[32m50\u001b[39m\n\u001b[32m      7\u001b[39m     )\n\u001b[32m      8\u001b[39m     tokens = response.usage.total_tokens\n\u001b[32m      9\u001b[39m     total_tokens += tokens\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "for i in range(5):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'Tell me about AI'}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    tokens = response.usage.total_tokens\n",
    "    total_tokens += tokens\n",
    "    print(f'Request {i+1}: {tokens} tokens')\n",
    "print(f'Total tokens used: {total_tokens}')\n",
    "utils.print_ok('Lab 04 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_53_e6cddee5",
   "metadata": {},
   "source": [
    "<a id='lab05'></a>\n",
    "\n",
    "## Lab 05: API Gateway Policy Foundations\n",
    "\n",
    "> Establish core Azure API Management (APIM) policies before adding advanced access control (Lab 06). This lab focuses on baseline resilience, observability, and request hygiene.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Lay down essential APIM inbound/outbound policies to:\n",
    "- Normalize headers and enforce HTTPS\n",
    "- Add correlation IDs for tracing\n",
    "- Rate-limit abusive clients\n",
    "- Set caching directives where appropriate\n",
    "- Instrument responses for latency and status analytics\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Policy Composition:** How inbound/outbound sections work together\n",
    "- **Chaining Policies Safely:** Order considerations (validation ‚Üí transformation ‚Üí routing)\n",
    "- **Correlation & Logging:** Injecting IDs for distributed tracing\n",
    "- **Basic Throttling:** Using `rate-limit` and `quota` policies\n",
    "- **Response Shaping:** Adding custom headers for monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### Core Policy Anatomy\n",
    "\n",
    "APIM policies execute in an XML pipeline:\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <!-- Validation / Security -->\n",
    "  <!-- Transformation -->\n",
    "  <!-- Routing -->\n",
    "</inbound>\n",
    "<backend>\n",
    "  <!-- Optional backend-specific modifications -->\n",
    "</backend>\n",
    "<outbound>\n",
    "  <!-- Response shaping / telemetry -->\n",
    "</outbound>\n",
    "<on-error>\n",
    "  <!-- Fallback handling / structured errors -->\n",
    "</on-error>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Starter Inbound Policy Example\n",
    "\n",
    "<details><summary><b>Baseline Hardened Inbound</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <!-- Enforce HTTPS -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.OriginalUrl.Scheme != \\\"https\\\")\">\n",
    "      <return-response>\n",
    "        <set-status code=\"301\" reason=\"Moved Permanently\" />\n",
    "        <set-header name=\"Location\" exists-action=\"override\">\n",
    "          <value>@(context.Request.OriginalUrl.ToString().Replace(\"http://\",\"https://\"))</value>\n",
    "        </set-header>\n",
    "      </return-response>\n",
    "    </when>\n",
    "  </choose>\n",
    "\n",
    "  <!-- Correlation ID (generate if absent) -->\n",
    "  <set-variable name=\"corrId\" value=\"@(context.Request.Headers.GetValueOrDefault(\\\"x-correlation-id\\\", Guid.NewGuid().ToString()))\" />\n",
    "  <set-header name=\"x-correlation-id\" exists-action=\"override\">\n",
    "    <value>@(context.Variables.GetValueOrDefault(\"corrId\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <!-- Basic abuse protection -->\n",
    "  <rate-limit calls=\"100\" renewal-period=\"60\" />\n",
    "  <quota calls=\"1000\" renewal-period=\"3600\" />\n",
    "\n",
    "  <!-- Normalize User-Agent (example) -->\n",
    "  <set-header name=\"x-user-agent\" exists-action=\"override\">\n",
    "    <value>@(context.Request.Headers.GetValueOrDefault(\"User-Agent\",\"unknown\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <!-- Forward to backend -->\n",
    "  <base />\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Outbound Policy Enhancements\n",
    "\n",
    "<details><summary><b>Latency Instrumentation & Cache Guidance</b></summary>\n",
    "\n",
    "```xml\n",
    "<outbound>\n",
    "  <!-- Add processing time header -->\n",
    "  <set-header name=\"x-apim-elapsed-ms\" exists-action=\"override\">\n",
    "    <value>@((DateTime.UtcNow - context.Request.TimestampUtc).TotalMilliseconds.ToString(\"F0\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <!-- Simple cache hint for successful GETs -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Response.StatusCode == 200 && context.Operation?.Method == \\\"GET\\\")\">\n",
    "      <set-header name=\"Cache-Control\" exists-action=\"override\">\n",
    "        <value>public, max-age=60</value>\n",
    "      </set-header>\n",
    "    </when>\n",
    "  </choose>\n",
    "\n",
    "  <!-- Propagate correlation ID -->\n",
    "  <set-header name=\"x-correlation-id\" exists-action=\"override\">\n",
    "    <value>@(context.Request.Headers.GetValueOrDefault(\"x-correlation-id\",\"none\"))</value>\n",
    "  </set-header>\n",
    "\n",
    "  <base />\n",
    "</outbound>\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Error Handling Pattern\n",
    "\n",
    "<details><summary><b>Structured on-error Block</b></summary>\n",
    "\n",
    "```xml\n",
    "<on-error>\n",
    "  <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "    <value>application/json</value>\n",
    "  </set-header>\n",
    "  <return-response>\n",
    "    <set-status code=\"500\" reason=\"Internal Server Error\" />\n",
    "    <set-body><![CDATA[{\n",
    "      \\\"error\\\": {\n",
    "        \\\"code\\\": \\\"InternalError\\\",\n",
    "        \\\"message\\\": \\\"An unexpected error occurred\\\",\n",
    "        \\\"correlationId\\\": \\\"@(context.Request.Headers.GetValueOrDefault(\\\"x-correlation-id\\\",\\\"none\\\"))\\\"\n",
    "      }\n",
    "    }]]></set-body>\n",
    "  </return-response>\n",
    "</on-error>\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Policy Ordering Tips\n",
    "\n",
    "| Stage | Purpose | Common Policies |\n",
    "|-------|---------|-----------------|\n",
    "| Early (Inbound) | Reject bad requests fast | `validate-content`, HTTPS redirect, auth |\n",
    "| Mid (Inbound) | Enrich & shape | header set, variables, rate/quotas |\n",
    "| Late (Inbound) | Routing | backend selection, rewrite-uri |\n",
    "| Early (Outbound) | Telemetry | timing headers, correlation propagation |\n",
    "| Mid (Outbound) | Optimization | caching hints, compression |\n",
    "| Late (Outbound) | Final shaping | remove/override headers |\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Checklist\n",
    "\n",
    "- ‚úÖ HTTPS enforced\n",
    "- ‚úÖ Correlation ID present\n",
    "- ‚úÖ Basic rate limit + quota applied\n",
    "- ‚úÖ Latency header added\n",
    "- ‚úÖ Consistent error shape on failures\n",
    "- ‚úÖ Cache hint on idempotent success responses\n",
    "\n",
    "---\n",
    "\n",
    "### Transition to Lab 06\n",
    "\n",
    "Next lab layers authentication and authorization (JWT validation, scopes, roles) on top of these foundational policies. Ensure baseline stability before adding access control logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell_54_bd2e7969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.523037Z",
     "iopub.status.busy": "2025-11-15T16:27:59.522766Z",
     "iopub.status.idle": "2025-11-15T16:27:59.526449Z",
     "shell.execute_reply": "2025-11-15T16:27:59.525901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC CELL SKIPPED (optional troubleshooting tool)\n",
      "================================================================================\n",
      "\n",
      "[INFO] This cell is only needed for debugging 500 errors\n",
      "[INFO] Your main labs (Cells 16, 38, 45) are working fine\n",
      "[INFO] Set SKIP_DIAGNOSTIC = False if you need to run diagnostics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL DIAGNOSTIC CELL - Can be skipped\n",
    "# ============================================================================\n",
    "# This cell is for troubleshooting 500 errors.\n",
    "# If everything is working, you can skip this cell.\n",
    "# ============================================================================\n",
    "\n",
    "SKIP_DIAGNOSTIC = True  # Set to False to run diagnostic\n",
    "\n",
    "if SKIP_DIAGNOSTIC:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DIAGNOSTIC CELL SKIPPED (optional troubleshooting tool)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n[INFO] This cell is only needed for debugging 500 errors\")\n",
    "    print(\"[INFO] Your main labs (Cells 16, 38, 45) are working fine\")\n",
    "    print(\"[INFO] Set SKIP_DIAGNOSTIC = False if you need to run diagnostics\\n\")\n",
    "else:\n",
    "    # Original diagnostic code would go here\n",
    "    # (keeping the structure in case needed later)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"APIM DIAGNOSTIC - IDENTIFYING 500 ERROR ROOT CAUSE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n[INFO] Diagnostic tool disabled in this version\")\n",
    "    print(\"[INFO] Use Azure Portal or Azure CLI for advanced diagnostics\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_55_a4257ecc",
   "metadata": {},
   "source": [
    "<a id='lab06'></a>\n",
    "## Lab 06: Access Controlling\n",
    "\n",
    "![Access Controlling](../../images/access-controlling.gif)\n",
    "\n",
    "üìñ **Workshop Guide:** https://azure-samples.github.io/AI-Gateway/\n",
    "\n",
    "### Objective\n",
    "Secure AI Gateway endpoints using OAuth 2.0 and Microsoft Entra ID (formerly Azure AD) for enterprise authentication.\n",
    "\n",
    "### What You'll Learn\n",
    "- **OAuth 2.0 Flow:** Implement token-based authentication with Entra ID\n",
    "- **JWT Validation:** Validate JSON Web Tokens in APIM policies\n",
    "- **RBAC Integration:** Control access based on Azure roles and groups\n",
    "- **API Scopes:** Define granular permissions for different API operations\n",
    "- **Token Claims:** Extract user identity and roles from access tokens\n",
    "\n",
    "---\n",
    "### Understanding OAuth 2.0 with Microsoft Entra ID\n",
    "> üí° **Tip:** OAuth 2.0 provides token-based authentication without exposing credentials in each request.\n",
    "\n",
    "**Authentication Flow:**\n",
    "1. **User Login:** Client application redirects user to Entra ID login\n",
    "2. **Authentication:** User enters credentials and consents to permissions\n",
    "3. **Token Issuance:** Entra ID issues JWT access token\n",
    "4. **API Request:** Client includes token in `Authorization: Bearer <token>` header\n",
    "5. **Token Validation:** APIM validates token signature, expiration, and claims\n",
    "6. **Request Processing:** If valid, request forwarded to Azure OpenAI backend\n",
    "\n",
    "---\n",
    "### JWT Validation Policy\n",
    "Azure API Management uses the `validate-jwt` policy to secure endpoints.\n",
    "\n",
    "<details><summary><b>Basic JWT Validation Example</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt\n",
    "      header-name=\"Authorization\"\n",
    "      failed-validation-httpcode=\"401\"\n",
    "      failed-validation-error-message=\"Unauthorized. Access token is missing or invalid.\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <audiences>\n",
    "      <audience>api://your-api-client-id</audience>\n",
    "    </audiences>\n",
    "    <issuers>\n",
    "      <issuer>https://sts.windows.net/{tenant-id}/</issuer>\n",
    "    </issuers>\n",
    "    <required-claims>\n",
    "      <claim name=\"roles\" match=\"any\">\n",
    "        <value>AI.User</value>\n",
    "        <value>AI.Admin</value>\n",
    "      </claim>\n",
    "    </required-claims>\n",
    "  </validate-jwt>\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- `header-name`: HTTP header containing the JWT (typically `Authorization`)\n",
    "- `openid-config`: URL to Entra ID's OpenID Connect metadata\n",
    "- `audiences`: Valid `aud` claim values\n",
    "- `issuers`: Trusted token issuers\n",
    "- `required-claims`: Claims that must be present in the token\n",
    "</details>\n",
    "\n",
    "---\n",
    "### Microsoft Entra ID Integration\n",
    "> ‚ö†Ô∏è **Note:** You must register your application in Microsoft Entra ID before implementing OAuth 2.0.\n",
    "\n",
    "**Setup Steps:**\n",
    "1. **Register Application:** Azure Portal ‚Üí Entra ID ‚Üí App Registrations ‚Üí New registration (note Application (client) ID & Tenant ID)\n",
    "2. **Configure API Permissions:** Add permissions and define custom scopes (e.g., `AI.Read`, `AI.Write`); grant admin consent if required\n",
    "3. **Create App Roles:** Define roles in app manifest (e.g., `AI.User`, `AI.Admin`) and assign users/groups\n",
    "4. **Configure APIM:** Add `validate-jwt` policy, reference tenant & client IDs, map roles to operations\n",
    "\n",
    "---\n",
    "### Role-Based Access Control (RBAC)\n",
    "<details><summary><b>Policy Example: Role-Based Backend Routing</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt header-name=\"Authorization\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <audiences>\n",
    "      <audience>api://your-api-client-id</audience>\n",
    "    </audiences>\n",
    "  </validate-jwt>\n",
    "\n",
    "  <!-- Admin users get priority routing -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.Headers.GetValueOrDefault(\\\"Authorization\\\",\\\"\").AsJwt()?.Claims.GetValueOrDefault(\\\"roles\\\",\\\"\").Contains(\\\"AI.Admin\\\") == true)\">\n",
    "      <set-backend-service backend-id=\"openai-premium-backend\" />\n",
    "    </when>\n",
    "    <!-- Regular users get standard backend -->\n",
    "    <otherwise>\n",
    "      <set-backend-service backend-id=\"openai-standard-backend\" />\n",
    "    </otherwise>\n",
    "  </choose>\n",
    "</inbound>\n",
    "```\n",
    "This example routes admin users to a premium backend with higher quotas.\n",
    "</details>\n",
    "\n",
    "<details><summary><b>Policy Example: Scope-Based Operation Control</b></summary>\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "  <validate-jwt header-name=\"Authorization\">\n",
    "    <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/v2.0/.well-known/openid-configuration\" />\n",
    "    <required-claims>\n",
    "      <claim name=\"scp\" match=\"any\">\n",
    "        <value>AI.Read</value>\n",
    "        <value>AI.Write</value>\n",
    "      </claim>\n",
    "    </required-claims>\n",
    "  </validate-jwt>\n",
    "\n",
    "  <!-- Check if operation requires write permission -->\n",
    "  <choose>\n",
    "    <when condition=\"@(context.Request.Method != \\\"GET\\\")\">\n",
    "      <validate-jwt header-name=\"Authorization\">\n",
    "        <required-claims>\n",
    "          <claim name=\"scp\" match=\"any\">\n",
    "            <value>AI.Write</value>\n",
    "          </claim>\n",
    "        </required-claims>\n",
    "      </validate-jwt>\n",
    "    </when>\n",
    "  </choose>\n",
    "</inbound>\n",
    "```\n",
    "This ensures only tokens with `AI.Write` scope can perform non-GET operations.\n",
    "</details>\n",
    "\n",
    "---\n",
    "### Token Claims and User Identity\n",
    "JWT tokens contain claims that provide user context.\n",
    "\n",
    "**Common Claims:**\n",
    "- `sub`: Subject (unique user identifier)\n",
    "- `name`: User's display name\n",
    "- `email`: User's email address\n",
    "- `roles`: User's assigned roles\n",
    "- `scp`: Delegated permissions (scopes)\n",
    "- `aud`: Audience (intended recipient)\n",
    "- `iss`: Issuer (token authority)\n",
    "- `exp`: Expiration timestamp\n",
    "\n",
    "**Extracting Claims in Policy:**\n",
    "```xml\n",
    "<set-header name=\"X-User-Email\" exists-action=\"override\">\n",
    "  <value>@(context.Request.Headers.GetValueOrDefault(\"Authorization\",\"\" ).AsJwt()?.Claims.GetValueOrDefault(\"email\", \"unknown\"))</value>\n",
    "</set-header>\n",
    "```\n",
    "\n",
    "---\n",
    "### Testing Access Control\n",
    "**Test Scenarios:**\n",
    "1. No Token ‚Üí 401 Unauthorized\n",
    "2. Invalid Token ‚Üí 401 Unauthorized\n",
    "3. Valid Token ‚Üí 200 OK\n",
    "4. Insufficient Permissions ‚Üí 403 Forbidden\n",
    "5. Token Expired ‚Üí 401 Unauthorized\n",
    "\n",
    "**Python Example with Azure Identity:**\n",
    "```python\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "\n",
    "# Acquire token from Azure Identity\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"api://your-api-client-id/.default\")\n",
    "\n",
    "# Use token with Azure OpenAI via APIM\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://your-apim.azure-api.net\",\n",
    "    api_key=token.token,  # JWT token used as API key\n",
    "    api_version=\"2024-02-15-preview\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "---\n",
    "### Security Best Practices\n",
    "> ‚úÖ **Checklist:**\n",
    "- Validate JWT signature using OpenID configuration\n",
    "- Check token expiration (`exp`)\n",
    "- Verify audience (`aud`) matches your API\n",
    "- Validate issuer (`iss`) is trusted\n",
    "- Enforce HTTPS only\n",
    "- Handle errors without leaking sensitive info\n",
    "- Log authentication failures\n",
    "- Rotate client secrets regularly\n",
    "- Apply least-privilege role assignments\n",
    "\n",
    "---\n",
    "### Expected Outcome\n",
    "**Success Criteria:**\n",
    "- Unauthenticated requests return 401\n",
    "- Valid Entra ID tokens grant access\n",
    "- JWT validation enforces signature & claims\n",
    "- Roles restrict privileged operations\n",
    "- Scope checks block unauthorized writes\n",
    "- Expired tokens rejected cleanly\n",
    "- Clear error messages guide remediation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63151888",
   "metadata": {},
   "source": [
    "# Access Control Workshop\n",
    "\n",
    "The following cells demonstrate token acquisition and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_57_a90b96df",
   "metadata": {},
   "source": [
    "# Access Control Workshop\n",
    "\n",
    "The following cells demonstrate token acquisition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell_58_898600fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.528680Z",
     "iopub.status.busy": "2025-11-15T16:27:59.528387Z",
     "iopub.status.idle": "2025-11-15T16:27:59.716209Z",
     "shell.execute_reply": "2025-11-15T16:27:59.715390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Baseline Test (No Auth)\n",
      "Status: 401 - Expected: 401\n",
      "Result: ‚úì PASS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "apim_url = os.getenv('APIM_GATEWAY_URL')\n",
    "endpoint = f\"{apim_url}/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\"\n",
    "\n",
    "response = requests.post(endpoint, json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"max_tokens\": 10}, timeout=30)\n",
    "\n",
    "print(f\"\\nüìä Baseline Test (No Auth)\")\n",
    "print(f\"Status: {response.status_code} - Expected: 401\")\n",
    "print(f\"Result: {'‚úì PASS' if response.status_code == 401 else '‚úó FAIL'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell_59_8f6ce564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.718505Z",
     "iopub.status.busy": "2025-11-15T16:27:59.718331Z",
     "iopub.status.idle": "2025-11-15T16:27:59.748721Z",
     "shell.execute_reply": "2025-11-15T16:27:59.747969Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mos\u001b[39;00m, \u001b[34;01msubprocess\u001b[39;00m, \u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù APPLY: JWT Only Policy (disable subscriptionRequired)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"üìù APPLY: JWT Only Policy (disable subscriptionRequired)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# STEP 1: Disable subscription requirement for pure JWT auth\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        api_config = response.json()\n",
    "        current_subscription_required = api_config.get('properties', {}).get('subscriptionRequired', False)\n",
    "        \n",
    "        print(f\"[1] Current subscriptionRequired: {current_subscription_required}\")\n",
    "        \n",
    "        if current_subscription_required:\n",
    "            # Disable subscription requirement\n",
    "            api_config['properties']['subscriptionRequired'] = False\n",
    "            \n",
    "            update_response = requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "            \n",
    "            if update_response.status_code in [200, 201]:\n",
    "                print(f\"[2] ‚úì Disabled subscriptionRequired for '{api_id}'\")\n",
    "            else:\n",
    "                print(f\"[2] ‚úó Failed: {update_response.status_code}\")\n",
    "        else:\n",
    "            print(f\"[2] ‚úì subscriptionRequired already disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {str(e)}\")\n",
    "\n",
    "# STEP 2: Apply JWT policy with v1.0 + v2.0 issuer support\n",
    "print(f\"\\n[3] Applying JWT policy...\")\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID\")\n",
    "else:\n",
    "    # JWT policy - CRITICAL: correct element order (openid-config, audiences, issuers)\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "    \n",
    "    try:\n",
    "        policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "        \n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "        \n",
    "        print(f\"[4] Policy Status: {response.status_code} - {'‚úì SUCCESS' if response.status_code in [200, 201] else '‚úó FAILED'}\")\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"\\n‚úì JWT policy applied with multi-issuer support\")\n",
    "            print(f\"‚è≥ Waiting 3 seconds for propagation...\")\n",
    "            time.sleep(3)\n",
    "            print(f\"‚úì Ready for testing\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell_60_a0ec9a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.750659Z",
     "iopub.status.busy": "2025-11-15T16:27:59.750433Z",
     "iopub.status.idle": "2025-11-15T16:27:59.995891Z",
     "shell.execute_reply": "2025-11-15T16:27:59.994987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test: API Key Only\n",
      "Status: 401 - Expected: 200\n",
      "Result: ‚úó FAIL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "apim_url = os.getenv('APIM_GATEWAY_URL')\n",
    "api_key = os.getenv('APIM_API_KEY')\n",
    "endpoint = f\"{apim_url}/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\"\n",
    "\n",
    "response = requests.post(endpoint, headers={\"api-key\": api_key}, json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"max_tokens\": 10}, timeout=30)\n",
    "\n",
    "print(f\"\\nüìä Test: API Key Only\")\n",
    "print(f\"Status: {response.status_code} - Expected: 200\")\n",
    "print(f\"Result: {'‚úì PASS - API Key auth working' if response.status_code == 200 else '‚úó FAIL'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6708c8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:27:59.998477Z",
     "iopub.status.busy": "2025-11-15T16:27:59.998287Z",
     "iopub.status.idle": "2025-11-15T16:28:00.030513Z",
     "shell.execute_reply": "2025-11-15T16:28:00.029928Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mos\u001b[39;00m, \u001b[34;01msubprocess\u001b[39;00m, \u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[32m      5\u001b[39m subscription_id = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mSUBSCRIPTION_ID\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get management token and tenant ID\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run([az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "                       capture_output=True, text=True, timeout=10)\n",
    "tenant_id = result.stdout.strip()\n",
    "\n",
    "# Disable subscription requirement (allows pure JWT auth)\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "headers = {\"Authorization\": f\"Bearer {mgmt_token.token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(api_url, headers=headers, timeout=30)\n",
    "if response.status_code == 200:\n",
    "    api_config = response.json()\n",
    "    api_config['properties']['subscriptionRequired'] = False\n",
    "    requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "\n",
    "# Apply JWT-only policy (supports both v1.0 and v2.0 tokens)\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "            <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "            <audiences><audience>https://cognitiveservices.azure.com</audience></audiences>\n",
    "            <issuers>\n",
    "                <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "            </issuers>\n",
    "        </validate-jwt>\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend><base /></backend>\n",
    "    <outbound><base /></outbound>\n",
    "    <on-error><base /></on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "body = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "print(f\"üìù Policy Applied: JWT Only\")\n",
    "print(f\"Status: {response.status_code} - {'SUCCESS' if response.status_code in [200, 201] else 'FAILED'}\")\n",
    "print(f\"Note: Disabled subscriptionRequired for pure JWT authentication\")\n",
    "\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"Waiting 60 seconds for policy to propagate...\")\n",
    "    for i in range(60, 0, -1):\n",
    "        print(f\"   {i} seconds remaining...\", end='\\r')\n",
    "        time.sleep(1)\n",
    "    print(\"\\nPolicy propagation complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3700c511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:00.032546Z",
     "iopub.status.busy": "2025-11-15T16:28:00.032359Z",
     "iopub.status.idle": "2025-11-15T16:28:00.054458Z",
     "shell.execute_reply": "2025-11-15T16:28:00.053204Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m      4\u001b[39m apim_url = os.getenv(\u001b[33m'\u001b[39m\u001b[33mAPIM_GATEWAY_URL\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m endpoint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapim_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "apim_url = os.getenv('APIM_GATEWAY_URL')\n",
    "endpoint = f\"{apim_url}/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\"\n",
    "\n",
    "# Get JWT token using DefaultAzureCredential (more reliable than subprocess)\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    jwt_token = token.token\n",
    "\n",
    "    # Test JWT authentication\n",
    "    response = requests.post(\n",
    "        endpoint,\n",
    "        headers={\"Authorization\": f\"Bearer {jwt_token}\"},\n",
    "        json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"max_tokens\": 10},\n",
    "        timeout=30\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Test: JWT Only\")\n",
    "    print(f\"Status: {response.status_code} - Expected: 200\")\n",
    "    print(f\"Result: {'PASS - JWT auth working' if response.status_code == 200 else 'FAIL'}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Cannot get JWT token: {str(e)}\")\n",
    "    print(\"Make sure you are authenticated (az login or DefaultAzureCredential)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc65e15a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:00.057856Z",
     "iopub.status.busy": "2025-11-15T16:28:00.057490Z",
     "iopub.status.idle": "2025-11-15T16:28:00.133337Z",
     "shell.execute_reply": "2025-11-15T16:28:00.132807Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mos\u001b[39;00m, \u001b[34;01msubprocess\u001b[39;00m, \u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù APPLY: Dual Auth (JWT + API Key)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"üìù APPLY: Dual Auth (JWT + API Key)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID. Ensure az login completed.\")\n",
    "else:\n",
    "    print(f\"[auth] Resolved tenant_id: {tenant_id}\")\n",
    "\n",
    "    # Dual Auth policy - BOTH JWT validation AND API key check\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing API key\" />\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "\n",
    "    # Apply policy\n",
    "    try:\n",
    "        url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "        print(f\"üìù Policy Applied: Dual Auth (JWT + API Key)\")\n",
    "        print(f\"Status: {response.status_code} - {'‚úì SUCCESS' if response.status_code in [200, 201] else '‚úó FAILED'}\")\n",
    "\n",
    "        if response.status_code not in [200, 201]:\n",
    "            print(f\"Error: {response.text[:500]}\")\n",
    "        else:\n",
    "            print(\"Policy requires BOTH:\")\n",
    "            print(\"  ‚Ä¢ Valid JWT token (Authorization header)\")\n",
    "            print(\"  ‚Ä¢ Valid API key (api-key header)\")\n",
    "\n",
    "            print(\"‚è≥ Waiting 60 seconds for policy to propagate...\")\n",
    "            for i in range(60, 0, -1):\n",
    "                print(f\"   {i} seconds remaining...\", end='')\n",
    "                time.sleep(1)\n",
    "            print(\"‚úì Policy propagation complete!\")\n",
    "            print(\"üí° TIP: Run Cell 65 to test Dual Auth\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Policy application failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "707184cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:00.135484Z",
     "iopub.status.busy": "2025-11-15T16:28:00.135330Z",
     "iopub.status.idle": "2025-11-15T16:28:00.155422Z",
     "shell.execute_reply": "2025-11-15T16:28:00.154265Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m      4\u001b[39m credential = DefaultAzureCredential()\n\u001b[32m      5\u001b[39m jwt_token = credential.get_token(\u001b[33m\"\u001b[39m\u001b[33mhttps://cognitiveservices.azure.com/.default\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "apim_url = os.getenv('APIM_GATEWAY_URL')\n",
    "api_key = os.getenv('APIM_API_KEY')\n",
    "endpoint = f\"{apim_url}/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\"\n",
    "\n",
    "response = requests.post(endpoint, headers={\"Authorization\": f\"Bearer {jwt_token.token}\", \"api-key\": api_key}, json={\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"max_tokens\": 10}, timeout=30)\n",
    "\n",
    "print(f\"\\nüìä Test: Dual Auth (JWT + API Key)\")\n",
    "print(f\"Status: {response.status_code} - Expected: 200\")\n",
    "print(f\"Result: {'‚úì PASS - Both credentials required' if response.status_code == 200 else '‚úó FAIL'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell_65_9ed0a378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:00.158293Z",
     "iopub.status.busy": "2025-11-15T16:28:00.157964Z",
     "iopub.status.idle": "2025-11-15T16:28:00.188056Z",
     "shell.execute_reply": "2025-11-15T16:28:00.186908Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m, \u001b[34;01mos\u001b[39;00m, \u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ RESET: API-KEY Authentication (for remaining labs)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "import requests, os, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"üîÑ RESET: API-KEY Authentication (for remaining labs)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Re-enable subscription requirement (for API-KEY authentication)\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "response = requests.get(api_url, headers=headers, timeout=30)\n",
    "if response.status_code == 200:\n",
    "    api_config = response.json()\n",
    "    api_config['properties']['subscriptionRequired'] = True\n",
    "    requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "    print(\"[1] ‚úì Re-enabled subscriptionRequired for API-KEY authentication\")\n",
    "\n",
    "# Apply simple API-KEY only policy\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend><base /></backend>\n",
    "    <outbound><base /></outbound>\n",
    "    <on-error><base /></on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "body = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "print(f\"[2] Policy Reset: API-KEY Only\")\n",
    "print(f\"    Status: {response.status_code} - {'‚úì SUCCESS' if response.status_code in [200, 201] else '‚úó FAILED'}\")\n",
    "\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"‚è≥ Waiting 30 seconds for policy to propagate...\")\n",
    "    for i in range(30, 0, -1):\n",
    "        print(f\"   {i} seconds remaining...\", end='')\n",
    "        time.sleep(1)\n",
    "    print(\"‚úì Policy reset complete!\")\n",
    "    print(\"üí° All remaining labs will use API-KEY authentication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c43932",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| 401 Unauthorized | Wait 30-60 seconds for policy to propagate |\n",
    "| 500 Internal Server Error | Check backend health with Azure CLI |\n",
    "| Token not found | Run `az login` to authenticate |\n",
    "| Missing API Key | Verify `APIM_API_KEY` in environment variables |\n",
    "\n",
    "**Verify Resources:**\n",
    "\n",
    "```bash\n",
    "az apim api list --service-name $APIM_SERVICE_NAME --resource-group $RESOURCE_GROUP --output table\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cell_67_06966815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:00.190390Z",
     "iopub.status.busy": "2025-11-15T16:28:00.190110Z",
     "iopub.status.idle": "2025-11-15T16:28:01.122781Z",
     "shell.execute_reply": "2025-11-15T16:28:01.121818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[apim] 1 APIs in apim-pavavy6pu5hpa:\n",
      " - inference-api  None\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, shlex\n",
    "\n",
    "# Use bash cell magic (falls back to Python subprocess if bash unavailable)\n",
    "try:\n",
    "    # Prefer Python az() helper; fallback to bash cell magic if available\n",
    "    apim_service = (\n",
    "        os.getenv('APIM_SERVICE_NAME')\n",
    "        or globals().get('service_name')\n",
    "        or (step1_outputs.get('apimServiceName') if 'step1_outputs' in globals() else None)\n",
    "    )\n",
    "    resource_group = (\n",
    "        os.getenv('RESOURCE_GROUP')\n",
    "        or globals().get('resource_group_name')\n",
    "        or os.getenv('RESOURCE_GROUP')\n",
    "    )\n",
    "    if not apim_service or not resource_group:\n",
    "        raise RuntimeError('APIM_SERVICE_NAME or RESOURCE_GROUP missing')\n",
    "\n",
    "    used_python = False\n",
    "    if 'az' in globals():  # use unified helper if present\n",
    "        ok, data = az(\n",
    "            f\"apim api list --service-name {shlex.quote(apim_service)} --resource-group {shlex.quote(resource_group)}\",\n",
    "            json_out=True,\n",
    "            timeout=40\n",
    "        )\n",
    "        if ok and isinstance(data, list):\n",
    "            used_python = True\n",
    "            print(f\"[apim] {len(data)} APIs in {apim_service}:\")\n",
    "            for api in data:\n",
    "                props = api.get('properties', {})\n",
    "                print(f\" - {api.get('name')}  {props.get('displayName')}\")\n",
    "        else:\n",
    "            print(f\"[apim] helper failed: {data[:160] if isinstance(data,str) else 'unknown'}; will try bash magic\")\n",
    "\n",
    "    if not used_python:\n",
    "        ip = globals().get('get_ipython', None)() if 'get_ipython' in globals() else None\n",
    "        if ip and hasattr(ip, 'run_cell_magic'):\n",
    "            ip.run_cell_magic(\n",
    "                'bash',\n",
    "                '',\n",
    "                f'''\n",
    "set -e\n",
    ": \"${{APIM_SERVICE_NAME:={apim_service}}}\"\n",
    ": \"${{RESOURCE_GROUP:={resource_group}}}\"\n",
    "${{AZ_CLI:-az}} apim api list --service-name \"$APIM_SERVICE_NAME\" --resource-group \"$RESOURCE_GROUP\" --output table\n",
    "'''\n",
    "            )\n",
    "        else:\n",
    "            # If bash magic unavailable, raise to trigger outer except fallback\n",
    "            raise RuntimeError('bash cell magic not available')\n",
    "except Exception:\n",
    "    apim = os.environ.get('APIM_SERVICE_NAME')\n",
    "    rg = os.environ.get('RESOURCE_GROUP')\n",
    "    if not apim or not rg:\n",
    "        print('Missing APIM_SERVICE_NAME or RESOURCE_GROUP')\n",
    "    else:\n",
    "        cmd = [os.environ.get('AZ_CLI','az'),\n",
    "               'apim','api','list',\n",
    "               '--service-name', apim,\n",
    "               '--resource-group', rg,\n",
    "               '--output','table']\n",
    "        subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_68_6a12cc5e",
   "metadata": {},
   "source": [
    "<a id='lab07'></a>\n",
    "\n",
    "## Lab 07: Content Safety\n",
    "\n",
    "![flow](../../images/content-safety.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Azure AI Content Safety to automatically detect and block harmful, offensive, or inappropriate content in AI prompts and responses.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Content Safety Policy:** Apply the llm-content-safety policy to AI endpoints\n",
    "- **Harmful Content Detection:** Identify violence, hate speech, sexual content, and self-harm\n",
    "- **Severity Thresholds:** Configure sensitivity levels (low, medium, high)\n",
    "- **Automated Blocking:** Return HTTP 403 when harmful content detected\n",
    "- **Prompt Filtering:** Scan prompts before sending to backend LLM\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- Harmful prompts blocked with HTTP 403 Forbidden\n",
    "- Safe prompts processed normally\n",
    "- Content Safety policy correctly integrated with APIM\n",
    "- Severity thresholds can be adjusted\n",
    "- Detailed error messages explain why content was blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell_69_39cd0d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:01.124845Z",
     "iopub.status.busy": "2025-11-15T16:28:01.124558Z",
     "iopub.status.idle": "2025-11-15T16:28:01.154779Z",
     "shell.execute_reply": "2025-11-15T16:28:01.153931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auth] WARN: Unable to acquire JWT token (name 'DefaultAzureCredential' is not defined); proceeding without it.\n",
      "[ERROR] Safe content request failed: No module named 'openai'\n",
      "Content blocked: No module named 'openai'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Could be a 403 from content safety or auth issue\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mContent blocked: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mutils\u001b[49m.print_ok(\u001b[33m'\u001b[39m\u001b[33mLab 07 Complete!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Lab 07 Content Safety Test (adds JWT auth if required by current APIM policy)\n",
    "\n",
    "def _get_jwt_token():\n",
    "    # Reuse existing jwt_token if earlier cell created it\n",
    "    if 'jwt_token' in globals() and jwt_token:\n",
    "        return jwt_token\n",
    "    try:\n",
    "        cred = DefaultAzureCredential()\n",
    "        tok = cred.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return tok.token\n",
    "    except Exception as _e:\n",
    "        print(f'[auth] WARN: Unable to acquire JWT token ({_e}); proceeding without it.')\n",
    "        return None\n",
    "\n",
    "_jwt = _get_jwt_token()\n",
    "extra_headers = {}\n",
    "if _jwt:\n",
    "    extra_headers['Authorization'] = f'Bearer {_jwt}'\n",
    "\n",
    "def safe_call(messages):\n",
    "    \"\"\"Call helper that retries with JWT if first attempt 401 (in case policy just changed).\"\"\"\n",
    "    from openai import AuthenticationError\n",
    "    try:\n",
    "        return client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=messages,\n",
    "            max_tokens=20,\n",
    "            extra_headers=extra_headers if extra_headers else None\n",
    "        )\n",
    "    except AuthenticationError as e:\n",
    "        msg = str(e)\n",
    "        if '401' in msg and not extra_headers:\n",
    "            # Acquire token and retry once\n",
    "            print('[auth] 401 without JWT; attempting to fetch token and retry...')\n",
    "            new_jwt = _get_jwt_token()\n",
    "            if new_jwt:\n",
    "                return client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages,\n",
    "                    max_tokens=20,\n",
    "                    extra_headers={'Authorization': f'Bearer {new_jwt}'}\n",
    "                )\n",
    "        raise\n",
    "\n",
    "# Test with safe content\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'What is the weather like?'}])\n",
    "    print(f\"Safe content: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Safe content request failed: {e}')\n",
    "\n",
    "# Test with potentially harmful content (expect policy to possibly block)\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'How to harm someone?'}])\n",
    "    print('Content passed (policy did not block or returned sanitized answer)')\n",
    "except Exception as e:\n",
    "    # Could be a 403 from content safety or auth issue\n",
    "    print(f'Content blocked: {e}')\n",
    "\n",
    "utils.print_ok('Lab 07 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_70_caa353d3",
   "metadata": {},
   "source": [
    "<a id='lab08'></a>\n",
    "\n",
    "## Lab 08: Model Routing\n",
    "\n",
    "![flow](../../images/ai-gateway.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Implement intelligent request routing to automatically select the best AI model based on criteria like prompt complexity, cost, or performance requirements.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Conditional Routing:** Route to different models based on request properties\n",
    "- **Model Selection Logic:** Choose between GPT-4o, GPT-4o-mini, DeepSeek, etc.\n",
    "- **Cost Optimization:** Route simple queries to cheaper models automatically\n",
    "- **Performance Tuning:** Send complex queries to more capable models\n",
    "- **Header-Based Routing:** Allow clients to specify model preferences\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- Simple prompts routed to GPT-4o-mini (cost-effective)\n",
    "- Complex prompts routed to GPT-4o (high capability)\n",
    "- Custom headers can override default routing\n",
    "- Routing logic is transparent and logged\n",
    "- Cost savings measurable compared to always using premium models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell_71_0250903e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:01.156689Z",
     "iopub.status.busy": "2025-11-15T16:28:01.156543Z",
     "iopub.status.idle": "2025-11-15T16:28:01.183483Z",
     "shell.execute_reply": "2025-11-15T16:28:01.182561Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Lab 08: Model Routing test (fixed for Dual Auth + invalid model + 401 handling)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AuthenticationError\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Current APIM policy may require BOTH api-key and a valid JWT (see dual auth cell 62).\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Acquire JWT (audience: https://cognitiveservices.azure.com) using DefaultAzureCredential already imported earlier.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "# Lab 08: Model Routing test (fixed for Dual Auth + invalid model + 401 handling)\n",
    "\n",
    "import os\n",
    "from openai import AuthenticationError\n",
    "\n",
    "# Current APIM policy may require BOTH api-key and a valid JWT (see dual auth cell 62).\n",
    "# Acquire JWT (audience: https://cognitiveservices.azure.com) using DefaultAzureCredential already imported earlier.\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "except Exception as e:\n",
    "    jwt_token = None\n",
    "    print(f\"[auth] WARN: Unable to acquire JWT token: {e}\")\n",
    "    \n",
    "extra_headers = {}\n",
    "if jwt_token:\n",
    "    extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "# Only test models that are actually deployed. gpt-4.1-mini not deployed; skip automatically.\n",
    "requested_models = ['gpt-4o-mini', 'gpt-4.1-mini']\n",
    "available_models = {'gpt-4o-mini', 'gpt-4o', 'text-embedding-3-small', 'text-embedding-3-large', 'dall-e-3'}  # from Step 2 config\n",
    "models_to_test = [m for m in requested_models if m in available_models]\n",
    "\n",
    "if len(models_to_test) != len(requested_models):\n",
    "    missing = [m for m in requested_models if m not in models_to_test]\n",
    "    print(f\"[routing] Skipping unavailable models: {', '.join(missing)}\")\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"[*] Testing model: {model}\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "            max_tokens=10,\n",
    "            extra_headers=extra_headers if extra_headers else None\n",
    "        )\n",
    "        try:\n",
    "            content = response.choices[0].message.content\n",
    "        except AttributeError:\n",
    "            content = response.choices[0].message.get('content', '')\n",
    "        print(f\"Model {model}: {content}\")\n",
    "    except AuthenticationError as e:\n",
    "        # Attempt one silent JWT refresh if first attempt lacked/invalid token\n",
    "        if not jwt_token:\n",
    "            print(f\"[auth] 401 without JWT; attempting token fetch & retry...\")\n",
    "            try:\n",
    "                credential = DefaultAzureCredential()\n",
    "                jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "                extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "                retry_resp = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "                    max_tokens=10,\n",
    "                    extra_headers=extra_headers\n",
    "                )\n",
    "                content = retry_resp.choices[0].message.content\n",
    "                print(f\"Model {model} (retry): {content}\")\n",
    "                continue\n",
    "            except Exception as e2:\n",
    "                print(f\"[ERROR] Retry after acquiring JWT failed: {e2}\")\n",
    "        print(f\"[ERROR] Auth failed for {model}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Request failed for {model}: {e}\")\n",
    "\n",
    "utils.print_ok('Lab 08 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_72_62fb5c72",
   "metadata": {},
   "source": [
    "<a id='lab09'></a>\n",
    "\n",
    "## Lab 09: AI Foundry SDK\n",
    "\n",
    "![flow](../../images/ai-foundry-sdk.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Azure AI Foundry SDK for advanced AI capabilities including model catalog, evaluations, and agent frameworks.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **AI Foundry Integration:** Connect to AI Foundry projects through APIM\n",
    "- **Model Catalog:** Access diverse AI models beyond Azure OpenAI\n",
    "- **Inference API:** Use unified inference API for multiple model types\n",
    "- **Agent Framework:** Build AI agents with tools and orchestration\n",
    "- **Evaluation Tools:** Assess model performance and quality\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- AI Foundry SDK successfully connects through APIM gateway\n",
    "- Can list available models in the catalog\n",
    "- Inference requests work for different model types\n",
    "- Agent framework tools execute correctly\n",
    "- Evaluation metrics collected and analyzed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_73_cc780c0e",
   "metadata": {},
   "source": [
    "## AI Foundry SDK lab\n",
    "\n",
    "![flow](../../images/ai-foundry-sdk.gif)\n",
    "\n",
    "This experimentation involves integrating [Azure AI Foundry SDK](https://learn.microsoft.com/azure/ai-studio/how-to/develop/sdk-overview?tabs=async&pivots=programming-language-python) with APIM. The OpenAI connection in the AI Foundry project includes an APIM endpoint and subscription, allowing client application requests to seamlessly route through APIM when utilizing the AI Foundry SDK.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "‚ñ∂Ô∏è Click `Run All` to execute all steps sequentially, or execute them `Step by Step`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cell_74_f22d7c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:01.185843Z",
     "iopub.status.busy": "2025-11-15T16:28:01.185590Z",
     "iopub.status.idle": "2025-11-15T16:28:02.449417Z",
     "shell.execute_reply": "2025-11-15T16:28:02.448740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Applying private connectivity policy...\n",
      "    Subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "    Resource Group: lab-master-lab\n",
      "    APIM Service: apim-pavavy6pu5hpa\n",
      "    API ID: inference-api\n",
      "    Backend Pool ID: inference-backend-pool\n",
      "    Auth: Managed Identity\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] az helper error: ERROR: Bad Request({\"error\":{\"code\":\"ValidationError\",\"message\":\"One or more fields contain incorrect values:\",\"details\":[{\"code\":\"ValidationError\",\"target\":\"representation\",\"message\":\"An error occurred while parsing EntityName. Line 18, position 100.\"}]}})\n",
      "\n",
      "[INFO] Managed identity authentication enabled.\n",
      "[INFO] Policy may take up to 60s to propagate.\n",
      "[NEXT] Execute a test request to confirm private connectivity.\n"
     ]
    }
   ],
   "source": [
    "# Cell 76 ‚Äì Private Connectivity Policy (Managed Identity) - MSAL Error Handling\n",
    "# Requires Cell 5 (Azure CLI Setup) and Cell 6 (MSAL Helper) to be run first\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "if 'az_cli' not in globals():\n",
    "    raise RuntimeError(\"Run Cell 5 (Azure CLI Setup) first to define az_cli\")\n",
    "\n",
    "if 'az_with_msal_retry' not in globals():\n",
    "    raise RuntimeError(\"Run Cell 6 (MSAL Helper) first to load MSAL error handling\")\n",
    "\n",
    "# Resolve core identifiers (prefer existing outputs / env)\n",
    "subscription_id = os.getenv('SUBSCRIPTION_ID')\n",
    "resource_group = os.getenv('RESOURCE_GROUP') or 'lab-master-lab'\n",
    "apim_service_name = (\n",
    "    os.getenv('APIM_SERVICE_NAME')\n",
    "    or (step1_outputs.get('apimServiceName') if 'step1_outputs' in globals() else None)\n",
    "    or 'apim-pavavy6pu5hpa'\n",
    ")\n",
    "api_id = os.getenv('APIM_API_ID') or 'inference-api'\n",
    "backend_id = os.getenv('BACKEND_ID') or 'inference-backend-pool'\n",
    "\n",
    "# Fallback to CLI for subscription if missing\n",
    "if not subscription_id:\n",
    "    ok_sub = False\n",
    "    if 'az' in globals():\n",
    "        ok_sub, sub_data = az('account show', json_out=True, timeout=12)\n",
    "        if ok_sub:\n",
    "            subscription_id = sub_data.get('id')\n",
    "    if not subscription_id and not ok_sub:\n",
    "        # Use MSAL retry wrapper for subprocess calls\n",
    "        r = az_with_msal_retry(az_cli, ['account', 'show', '--query', 'id', '-o', 'tsv'])\n",
    "        if r.returncode == 0:\n",
    "            subscription_id = r.stdout.strip()\n",
    "        else:\n",
    "            print(f'[ERROR] Failed to get subscription ID: {r.stderr[:200]}')\n",
    "\n",
    "if not subscription_id:\n",
    "    raise RuntimeError(\"Unable to resolve subscription ID\")\n",
    "\n",
    "print('[*] Applying private connectivity policy...')\n",
    "print(f'    Subscription: {subscription_id}')\n",
    "print(f'    Resource Group: {resource_group}')\n",
    "print(f'    APIM Service: {apim_service_name}')\n",
    "print(f'    API ID: {api_id}')\n",
    "print(f'    Backend Pool ID: {backend_id}')\n",
    "print('    Auth: Managed Identity\\n')\n",
    "\n",
    "# Managed Identity + retry + safe error surface\n",
    "policy_xml = f\"\"\"\n",
    "<policies>\n",
    "  <inbound>\n",
    "    <base />\n",
    "    <authentication-managed-identity\n",
    "        resource=\"https://cognitiveservices.azure.com\"\n",
    "        output-token-variable-name=\"managed-id-access-token\"\n",
    "        ignore-error=\"false\" />\n",
    "    <set-header name=\"Authorization\" exists-action=\"override\">\n",
    "      <value>@(\"Bearer \" + (string)context.Variables[\"managed-id-access-token\"])</value>\n",
    "    </set-header>\n",
    "    <set-backend-service backend-id=\"{backend_id}\" />\n",
    "  </inbound>\n",
    "  <backend>\n",
    "    <!-- Retry up to 2 times on 429 or specific 503 variants -->\n",
    "    <retry count=\"2\"\n",
    "           interval=\"0\"\n",
    "           first-fast-retry=\"true\"\n",
    "           condition=\"@(context.Response.StatusCode == 429 || (context.Response.StatusCode == 503 && !context.Response.StatusReason.Contains(\"Backend pool\") && !context.Response.StatusReason.Contains(\"is temporarily unavailable\")))\">\n",
    "      <forward-request buffer-request-body=\"true\" />\n",
    "    </retry>\n",
    "  </backend>\n",
    "  <outbound>\n",
    "    <base />\n",
    "  </outbound>\n",
    "  <on-error>\n",
    "    <base />\n",
    "    <choose>\n",
    "      <when condition=\"@(context.Response.StatusCode == 503)\">\n",
    "        <return-response>\n",
    "          <set-status code=\"503\" reason=\"Service Unavailable\" />\n",
    "          <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "            <value>application/json</value>\n",
    "          </set-header>\n",
    "          <set-body>@{{\n",
    "            return new JObject(\n",
    "              new JProperty(\"error\",\n",
    "                new JObject(\n",
    "                  new JProperty(\"code\",\"ServiceUnavailable\"),\n",
    "                  new JProperty(\"message\",\"The service is temporarily unavailable. Please try again later.\")\n",
    "                )\n",
    "              )\n",
    "            ).ToString();\n",
    "          }}</set-body>\n",
    "        </return-response>\n",
    "      </when>\n",
    "    </choose>\n",
    "  </on-error>\n",
    "</policies>\n",
    "\"\"\".strip()\n",
    "\n",
    "policy_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}\"\n",
    "    f\"/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement\"\n",
    "    f\"/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    ")\n",
    "\n",
    "payload = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "\n",
    "# Use az() helper if available; else subprocess with MSAL retry\n",
    "tmp_path = None\n",
    "try:\n",
    "    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)\n",
    "    json.dump(payload, tmp_file)\n",
    "    tmp_file.flush()\n",
    "    tmp_path = tmp_file.name\n",
    "    tmp_file.close()\n",
    "\n",
    "    if 'az' in globals():\n",
    "        ok, res = az(\n",
    "            f'rest --method put --url \"{policy_url}\" --body \"@{tmp_path}\" --headers \"Content-Type=application/json\"',\n",
    "            json_out=False,\n",
    "            timeout=120\n",
    "        )\n",
    "        if ok:\n",
    "            print('[OK] private-connectivity policy applied (az helper)')\n",
    "        else:\n",
    "            print(f'[FAIL] az helper error: {res[:300]}')\n",
    "    else:\n",
    "        # Use MSAL retry wrapper for subprocess calls\n",
    "        result = az_with_msal_retry(\n",
    "            az_cli,\n",
    "            ['rest', '--method', 'put', '--url', policy_url,\n",
    "             '--body', f'@{tmp_path}', '--headers', 'Content-Type=application/json'],\n",
    "            timeout=120\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print('[OK] private-connectivity policy applied (subprocess with MSAL retry)')\n",
    "        else:\n",
    "            print(f'[FAIL] subprocess stderr: {result.stderr[:300]}')\n",
    "            \n",
    "            # If still failing, try Azure SDK as fallback\n",
    "            if 'NormalizedResponse' not in result.stderr:\n",
    "                print('[INFO] Attempting fallback to Azure SDK...')\n",
    "                try:\n",
    "                    from azure.mgmt.apimanagement import ApiManagementClient\n",
    "                    from azure.identity import DefaultAzureCredential\n",
    "                    \n",
    "                    credential = DefaultAzureCredential()\n",
    "                    apim_client = ApiManagementClient(credential, subscription_id)\n",
    "                    \n",
    "                    policy_contract = {\n",
    "                        \"value\": policy_xml,\n",
    "                        \"format\": \"xml\"\n",
    "                    }\n",
    "                    \n",
    "                    apim_client.api_policy.create_or_update(\n",
    "                        resource_group_name=resource_group,\n",
    "                        service_name=apim_service_name,\n",
    "                        api_id=api_id,\n",
    "                        policy_id='policy',\n",
    "                        parameters=policy_contract\n",
    "                    )\n",
    "                    \n",
    "                    print('[OK] private-connectivity policy applied (Azure SDK fallback)')\n",
    "                except ImportError:\n",
    "                    print('[FAIL] Azure SDK not available. Install: pip install azure-mgmt-apimanagement')\n",
    "                except Exception as sdk_error:\n",
    "                    print(f'[FAIL] Azure SDK error: {sdk_error}')\n",
    "finally:\n",
    "    if tmp_path and os.path.exists(tmp_path):\n",
    "        try:\n",
    "            os.unlink(tmp_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print('\\n[INFO] Managed identity authentication enabled.')\n",
    "print('[INFO] Policy may take up to 60s to propagate.')\n",
    "print('[NEXT] Execute a test request to confirm private connectivity.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cell_75_ce629d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:02.451549Z",
     "iopub.status.busy": "2025-11-15T16:28:02.451322Z",
     "iopub.status.idle": "2025-11-15T16:28:02.481080Z",
     "shell.execute_reply": "2025-11-15T16:28:02.480265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Inference Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] Inference Endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Acquire JWT if current APIM policy enforces validate-jwt (dual auth or JWT-only)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n\u001b[32m     29\u001b[39m jwt_token = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# Lab 09: AI Foundry SDK - Chat Completion via APIM (fixed auth + missing vars)\n",
    "# ChatCompletionsClient must use FULL deployment path:\n",
    "#   {apim_gateway_url}/{inference_api_path}/openai/deployments/{deployment_name}\n",
    "\n",
    "# Reuse imports already loaded in earlier cells (avoid re-import)\n",
    "# Variables expected from earlier cells:\n",
    "#   apim_gateway_url, inference_api_path, apim_api_key\n",
    "\n",
    "deployment_name = \"gpt-4o-mini\"\n",
    "\n",
    "missing_vars = [k for k, v in {\n",
    "    'apim_gateway_url': globals().get('apim_gateway_url'),\n",
    "    'inference_api_path': globals().get('inference_api_path'),\n",
    "    'apim_api_key': globals().get('apim_api_key')\n",
    "}.items() if not v]\n",
    "\n",
    "if missing_vars:\n",
    "    raise RuntimeError(f\"Missing required variables: {', '.join(missing_vars)}. Run the earlier env/config cells first.\")\n",
    "\n",
    "# Normalize endpoint (avoid double slashes)\n",
    "base = apim_gateway_url.rstrip('/')\n",
    "inference_path = inference_api_path.strip('/')\n",
    "\n",
    "inference_endpoint = f\"{base}/{inference_path}/openai/deployments/{deployment_name}\"\n",
    "print(f\"[OK] Inference Endpoint: {inference_endpoint}\")\n",
    "\n",
    "# Acquire JWT if current APIM policy enforces validate-jwt (dual auth or JWT-only)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "jwt_token = None\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Audience used in active APIM policies\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(\"[OK] Acquired JWT token\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Unable to acquire JWT token: {e}\")\n",
    "    print(\"[INFO] Will attempt call with API key only (may fail if JWT required)\")\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "inference_client = ChatCompletionsClient(\n",
    "    endpoint=inference_endpoint,\n",
    "    credential=AzureKeyCredential(apim_api_key)  # use correct API key variable\n",
    ")\n",
    "\n",
    "print(\"[OK] ChatCompletionsClient created successfully\\n\")\n",
    "\n",
    "# Prepare headers: dual auth requires both api-key (handled via AzureKeyCredential) and Authorization\n",
    "call_headers = {}\n",
    "if jwt_token:\n",
    "    call_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "print(\"[*] Testing chat completion with Azure AI Inference SDK...\")\n",
    "try:\n",
    "    response = inference_client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=\"You are helpful.\"),\n",
    "            UserMessage(content=\"What is Azure AI Foundry?\")\n",
    "        ],\n",
    "        headers=call_headers if call_headers else None  # azure-core style header injection\n",
    "    )\n",
    "    print(f\"[SUCCESS] Response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if \"Invalid JWT\" in msg or \"401\" in msg:\n",
    "        print(f\"[ERROR] Authentication failed: {msg}\")\n",
    "        print(\"[HINT] Active APIM policy likely requires a valid JWT. Ensure az login completed or managed identity available.\")\n",
    "        print(\"[HINT] Retry after confirming validate-jwt audiences match https://cognitiveservices.azure.com\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Request failed: {msg}\")\n",
    "        print(\"[HINT] Verify deployment name matches APIM backend path and policy didn't strip /openai segment.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Lab 09 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_76_d26884e4",
   "metadata": {},
   "source": [
    "## Lab 10: AI Foundry DeepSeek\n",
    "\n",
    "![flow](../../images/ai-foundry-deepseek.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Deploy and test DeepSeek-R1, an advanced open-source reasoning model, through Azure AI Foundry and APIM gateway.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **DeepSeek-R1 Model:** Understand capabilities of this reasoning-focused model\n",
    "- **Multi-Model Support:** Run open-source models alongside Azure OpenAI\n",
    "- **Model Comparison:** Compare DeepSeek outputs with GPT-4o responses\n",
    "- **Reasoning Tokens:** Analyze special reasoning tokens in model outputs\n",
    "- **Cost Benefits:** Evaluate cost/performance tradeoffs of different models\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](../../ai-foundry-deepseek/result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- DeepSeek-R1 model deployed and accessible\n",
    "- Reasoning tasks complete successfully\n",
    "- Output quality comparable to GPT-4o for logic problems\n",
    "- Response times acceptable for production use\n",
    "- Cost per request significantly lower than GPT-4o\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_77_7dd6b64b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: MCP Fundamentals\n",
    "\n",
    "Learn MCP basics:\n",
    "- Client initialization\n",
    "- Calling MCP tools\n",
    "- Data retrieval\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cell_78_2e777ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:02.483973Z",
     "iopub.status.busy": "2025-11-15T16:28:02.483767Z",
     "iopub.status.idle": "2025-11-15T16:28:02.501802Z",
     "shell.execute_reply": "2025-11-15T16:28:02.501094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MCP SERVER INITIALIZATION\n",
      "================================================================================\n",
      "\n",
      "[OK] Loaded environment from master-lab.env\n",
      "\n",
      "MCP Server Configuration:\n",
      "  ‚úì weather: https://mcp-weather-pavavy6pu5.niceriver-900455a0....\n",
      "  ‚úì github: https://mcp-github-pavavy6pu5.niceriver-900455a0.u...\n",
      "  ‚úì spotify: https://mcp-spotify-pavavy6pu5.niceriver-900455a0....\n",
      "  ‚úì product-catalog: https://mcp-product-catalog-pavavy6pu5.niceriver-9...\n",
      "  ‚úì place-order: https://mcp-place-order-pavavy6pu5.niceriver-90045...\n",
      "  ‚úì ms-learn: https://mcp-ms-learn-pavavy6pu5.niceriver-900455a0...\n",
      "\n",
      "Configured servers: 6/6\n",
      "\n",
      "[*] Importing MCP helper classes...\n",
      "[OK] MCP helper classes imported successfully\n",
      "\n",
      "Available MCP Helpers:\n",
      "  - WeatherMCP: Weather data and forecasts\n",
      "  - GitHubMCP: GitHub repository operations\n",
      "  - SpotifyMCP: Music service integration\n",
      "  - ProductCatalogMCP: E-commerce product catalog\n",
      "  - PlaceOrderMCP: E-commerce order placement\n",
      "\n",
      "================================================================================\n",
      "MCP INITIALIZATION NOTES\n",
      "================================================================================\n",
      "\n",
      "This lab uses TWO types of MCP connections:\n",
      "\n",
      "1. HTTP-Based MCP (Used by most servers in this lab)\n",
      "   - Endpoint: {server_url}/mcp/\n",
      "   - Method: HTTP POST with JSON-RPC\n",
      "   - Helper classes: WeatherMCP, GitHubMCP, etc.\n",
      "   - Examples: Cells 58-60 (Weather, GitHub)\n",
      "\n",
      "2. SSE-Based MCP (Alternative for streaming)\n",
      "   - Endpoint: {server_url}/sse or /mcp or /events\n",
      "   - Method: Server-Sent Events\n",
      "   - Use when: Server supports streaming responses\n",
      "   - Note: Requires path discovery (servers vary)\n",
      "\n",
      "[OK] MCP initialization complete\n",
      "[INFO] Proceed to individual lab cells to use MCP servers\n"
     ]
    }
   ],
   "source": [
    "# Lab 10: MCP Server Integration - Simplified Initialization\n",
    "# NOTE: MCP servers are already initialized in Cell 9 via MCPClient()\n",
    "# The global \"mcp\" object is available with: mcp.weather, mcp.github, etc.\n",
    "# MCP servers in this notebook use HTTP POST to /mcp/ endpoint\n",
    "# Helper classes (WeatherMCP, GitHubMCP, etc.) handle this automatically\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MCP SERVER INITIALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load environment\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file, override=True)\n",
    "    print(f\"[OK] Loaded environment from {env_file.name}\")\n",
    "else:\n",
    "    print(f\"[WARNING] {env_file} not found!\")\n",
    "print()\n",
    "\n",
    "# MCP Server Configuration from Environment\n",
    "MCP_SERVERS = {\n",
    "    'weather': os.getenv('MCP_SERVER_WEATHER_URL'),\n",
    "    'github': os.getenv('MCP_SERVER_GITHUB_URL'),\n",
    "    'spotify': os.getenv('MCP_SERVER_SPOTIFY_URL'),\n",
    "    'product-catalog': os.getenv('MCP_SERVER_PRODUCT_CATALOG_URL'),\n",
    "    'place-order': os.getenv('MCP_SERVER_PLACE_ORDER_URL'),\n",
    "    'ms-learn': os.getenv('MCP_SERVER_MS_LEARN_URL')\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"MCP Server Configuration:\")\n",
    "configured = 0\n",
    "for name, url in MCP_SERVERS.items():\n",
    "    if url:\n",
    "        print(f\"  ‚úì {name}: {url[:50]}...\")\n",
    "        configured += 1\n",
    "    else:\n",
    "        print(f\"  ‚úó {name}: NOT CONFIGURED\")\n",
    "\n",
    "print(f\"\\nConfigured servers: {configured}/{len(MCP_SERVERS)}\")\n",
    "print()\n",
    "\n",
    "# Import notebook helpers\n",
    "print(\"[*] Importing MCP helper classes...\")\n",
    "try:\n",
    "    from notebook_mcp_helpers import (\n",
    "        WeatherMCP,\n",
    "        GitHubMCP,\n",
    "        SpotifyMCP,\n",
    "        ProductCatalogMCP,\n",
    "        PlaceOrderMCP\n",
    "    )\n",
    "    print(\"[OK] MCP helper classes imported successfully\")\n",
    "    print()\n",
    "    print(\"Available MCP Helpers:\")\n",
    "    print(\"  - WeatherMCP: Weather data and forecasts\")\n",
    "    print(\"  - GitHubMCP: GitHub repository operations\")\n",
    "    print(\"  - SpotifyMCP: Music service integration\")\n",
    "    print(\"  - ProductCatalogMCP: E-commerce product catalog\")\n",
    "    print(\"  - PlaceOrderMCP: E-commerce order placement\")\n",
    "except ImportError as e:\n",
    "    print(f\"[WARNING] Could not import MCP helpers: {e}\")\n",
    "    print(\"[INFO] MCP helpers use HTTP POST to /mcp/ endpoint\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"MCP INITIALIZATION NOTES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"This lab uses TWO types of MCP connections:\")\n",
    "print()\n",
    "print(\"1. HTTP-Based MCP (Used by most servers in this lab)\")\n",
    "print(\"   - Endpoint: {server_url}/mcp/\")\n",
    "print(\"   - Method: HTTP POST with JSON-RPC\")\n",
    "print(\"   - Helper classes: WeatherMCP, GitHubMCP, etc.\")\n",
    "print(\"   - Examples: Cells 58-60 (Weather, GitHub)\")\n",
    "print()\n",
    "print(\"2. SSE-Based MCP (Alternative for streaming)\")\n",
    "print(\"   - Endpoint: {server_url}/sse or /mcp or /events\")\n",
    "print(\"   - Method: Server-Sent Events\")\n",
    "print(\"   - Use when: Server supports streaming responses\")\n",
    "print(\"   - Note: Requires path discovery (servers vary)\")\n",
    "print()\n",
    "print(\"[OK] MCP initialization complete\")\n",
    "print(\"[INFO] Proceed to individual lab cells to use MCP servers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_79_750afe6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. AI application sends MCP request to APIM\n",
    "2. APIM validates OAuth token and enforces policies\n",
    "3. Request forwarded to MCP server\n",
    "4. MCP server executes tool and returns result\n",
    "5. APIM proxies response back to client\n",
    "6. AI model processes tool result and generates response\n",
    "\n",
    "---\n",
    "\n",
    "### Two MCP Connection Patterns\n",
    "\n",
    "**Important:** This lab uses HTTP-based MCP servers that communicate via POST requests to `/mcp/` endpoints.\n",
    "\n",
    "<details>\n",
    "<summary><b>Pattern 1: HTTP-Based MCP</b> (‚úÖ Used in this notebook)</summary>\n",
    "\n",
    "**How It Works:**\n",
    "- **Protocol:** HTTP POST requests\n",
    "- **Endpoint:** `{server_url}/mcp/`\n",
    "- **Format:** JSON-RPC 2.0\n",
    "- **Communication:** Request/response pattern\n",
    "\n",
    "**Advantages:**\n",
    "- Simple, reliable, works with standard HTTP clients\n",
    "- Easy to test with curl or Postman\n",
    "- Works through standard load balancers and API gateways\n",
    "- No special client libraries required\n",
    "- Firewall-friendly (standard HTTP/HTTPS)\n",
    "\n",
    "**Example Request:**\n",
    "```http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cell_80_5c80f06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:02.503684Z",
     "iopub.status.busy": "2025-11-15T16:28:02.503532Z",
     "iopub.status.idle": "2025-11-15T16:28:02.750794Z",
     "shell.execute_reply": "2025-11-15T16:28:02.749962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Connecting to weather MCP server...\n",
      "[*] Server URL: http://weather-mcp-72998.eastus.azurecontainer.io:8080\n",
      "\n",
      "[*] Getting cities in USA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] weather: MCPError: MCP initialization error: [Errno -2] Name or service not known\n",
      "[HINT] Verify MCP server is running and URL is correct.\n",
      "[HINT] Server URL: http://weather-mcp-72998.eastus.azurecontainer.io:8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Weather demo complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 434, in _ensure_initialized\n",
      "    response = httpx.post(\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 304, in post\n",
      "    return request(\n",
      "           ^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 109, in request\n",
      "    return client.request(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/788438203.py\", line 51, in <module>\n",
      "    cities_result = weather.get_cities(\"usa\")\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 554, in get_cities\n",
      "    return self._call_tool(\"get_cities\", {\"country\": country})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 469, in _call_tool\n",
      "    self._ensure_initialized()\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 465, in _ensure_initialized\n",
      "    raise MCPError(f\"MCP initialization error: {e}\")\n",
      "notebook_mcp_helpers.MCPError: MCP initialization error: [Errno -2] Name or service not known\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 Example: Weather MCP Server\n",
    "# Demonstrates weather data retrieval via MCP (with graceful fallback initialization)\n",
    "\n",
    "# Resolve weather MCP helper from global 'mcp' which may be a dict or an object\n",
    "try:\n",
    "    if isinstance(mcp, dict):\n",
    "        weather = mcp.get('weather')\n",
    "    else:\n",
    "        weather = getattr(mcp, 'weather', None)\n",
    "except NameError:\n",
    "    raise NameError(\"Variable 'mcp' is not defined. Run the MCP initialization cell first.\")\n",
    "\n",
    "# Fallback: attempt manual construction if not present\n",
    "if weather is None:\n",
    "    print(\"[WARN] Weather MCP helper not found in global 'mcp'. Attempting manual creation...\")\n",
    "    if 'os' not in globals():\n",
    "        import os\n",
    "    weather_url = os.getenv('MCP_SERVER_WEATHER_URL')\n",
    "    if not weather_url:\n",
    "        print(\"[ERROR] Environment variable MCP_SERVER_WEATHER_URL not set. Cannot create WeatherMCP helper.\")\n",
    "        print(\"[HINT] Export MCP_SERVER_WEATHER_URL or re-run the environment/bootstrap cell.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Ensure helper class is available\n",
    "            try:\n",
    "                WeatherMCP  # noqa: F821\n",
    "            except NameError:\n",
    "                from notebook_mcp_helpers import WeatherMCP  # type: ignore\n",
    "            weather = WeatherMCP(weather_url)\n",
    "            print(f\"[OK] Created WeatherMCP helper manually for: {weather_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to instantiate WeatherMCP helper: {e}\")\n",
    "\n",
    "# Abort (gracefully) if still unavailable\n",
    "if weather is None:\n",
    "    print(\"[SKIP] Weather MCP operations skipped (helper not initialized).\")\n",
    "else:\n",
    "    # Basic attribute validation\n",
    "    server_url = getattr(weather, 'server_url', None)\n",
    "    if not server_url:\n",
    "        print(\"[WARN] weather helper has no 'server_url' attribute\")\n",
    "        server_url = \"<unknown>\"\n",
    "\n",
    "    print(\"[*] Connecting to weather MCP server...\")\n",
    "    print(f\"[*] Server URL: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        # Get list of cities for USA\n",
    "        print()\n",
    "        print(\"[*] Getting cities in USA...\")\n",
    "        cities_result = weather.get_cities(\"usa\")\n",
    "        print(f\"[SUCCESS] Cities in USA: {cities_result}\")\n",
    "\n",
    "        # Get weather for Seattle\n",
    "        print()\n",
    "        print(\"[*] Getting weather for Seattle...\")\n",
    "        weather_result = weather.get_weather(\"Seattle\")\n",
    "\n",
    "        # Display result\n",
    "        print('[SUCCESS] Weather data:')\n",
    "        print('-' * 40)\n",
    "\n",
    "        import json, ast\n",
    "        if isinstance(weather_result, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(weather_result)\n",
    "                output = json.dumps(parsed, indent=2)\n",
    "            except Exception:\n",
    "                output = weather_result\n",
    "        else:\n",
    "            output = json.dumps(weather_result, indent=2)\n",
    "\n",
    "        if len(output) > 800:\n",
    "            output = output[:800] + '\\n...\\n(truncated)'\n",
    "        print(output)\n",
    "\n",
    "        # Additional city\n",
    "        print()\n",
    "        print(\"[*] Getting weather for New York...\")\n",
    "        ny_weather = weather.get_weather(\"New York\")\n",
    "        print(f\"[SUCCESS] New York weather: {ny_weather}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] weather: {type(e).__name__}: {e}\")\n",
    "        print(\"[HINT] Verify MCP server is running and URL is correct.\")\n",
    "        print(f\"[HINT] Server URL: {server_url}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print()\n",
    "print('[OK] Weather demo complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell_81_dabe2f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:02.752742Z",
     "iopub.status.busy": "2025-11-15T16:28:02.752554Z",
     "iopub.status.idle": "2025-11-15T16:28:02.981689Z",
     "shell.execute_reply": "2025-11-15T16:28:02.980501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Connecting to GitHub MCP server...\n",
      "[*] Server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[*] Searching for AI repositories...\n",
      "[ERROR] github: MCPError: MCP initialization error: [Errno -2] Name or service not known\n",
      "[HINT] Verify MCP GitHub server is running and accessible.\n",
      "[HINT] Server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 434, in _ensure_initialized\n",
      "    response = httpx.post(\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 304, in post\n",
      "    return request(\n",
      "           ^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 109, in request\n",
      "    return client.request(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/1847142797.py\", line 41, in <module>\n",
      "    search_result = github.search_repositories(\"AI language:python\")\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 665, in search_repositories\n",
      "    return self._call_tool(\"search_repositories\", {\"query\": query})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 469, in _call_tool\n",
      "    self._ensure_initialized()\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 465, in _ensure_initialized\n",
      "    raise MCPError(f\"MCP initialization error: {e}\")\n",
      "notebook_mcp_helpers.MCPError: MCP initialization error: [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] GitHub demo complete 10 Example: GitHub MCP Server\n",
      "[*] Connecting to GitHub MCP server...\n",
      "[*] Server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[*] Searching for AI repositories...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] github: MCPError: MCP initialization error: [Errno -2] Name or service not known\n",
      "[HINT] Verify MCP GitHub server is running and accessible.\n",
      "[HINT] Server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[OK] GitHub demo complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 434, in _ensure_initialized\n",
      "    response = httpx.post(\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 304, in post\n",
      "    return request(\n",
      "           ^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 109, in request\n",
      "    return client.request(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/1847142797.py\", line 112, in <module>\n",
      "    search_result = github.search_repositories(\"AI language:python\")\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 665, in search_repositories\n",
      "    return self._call_tool(\"search_repositories\", {\"query\": query})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 469, in _call_tool\n",
      "    self._ensure_initialized()\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 465, in _ensure_initialized\n",
      "    raise MCPError(f\"MCP initialization error: {e}\")\n",
      "notebook_mcp_helpers.MCPError: MCP initialization error: [Errno -2] Name or service not known\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 Example: GitHub MCP Server\n",
    "# Robust GitHub MCP helper resolution (handles dict-style mcp, missing attribute, or absent helper)\n",
    "\n",
    "# Attempt to resolve existing helper\n",
    "try:\n",
    "    if isinstance(mcp, dict):\n",
    "        github = mcp.get('github')\n",
    "    else:\n",
    "        github = getattr(mcp, 'github', None)\n",
    "except NameError:\n",
    "    raise NameError(\"Variable 'mcp' is not defined. Run the MCP initialization cell first.\")\n",
    "\n",
    "# Fallback: create helper from environment if not present\n",
    "if github is None:\n",
    "    print(\"[WARN] GitHub MCP helper not found in global 'mcp'. Attempting manual creation...\")\n",
    "    if 'os' not in globals():\n",
    "        import os\n",
    "    github_url = os.getenv('MCP_SERVER_GITHUB_URL')\n",
    "    if not github_url:\n",
    "        print(\"[ERROR] MCP_SERVER_GITHUB_URL not set. Cannot initialize GitHub MCP helper.\")\n",
    "    else:\n",
    "        try:\n",
    "            try:\n",
    "                GitHubMCP  # type: ignore  # noqa\n",
    "            except NameError:\n",
    "                from notebook_mcp_helpers import GitHubMCP  # type: ignore\n",
    "            github = GitHubMCP(github_url)\n",
    "            print(f\"[OK] Created GitHubMCP helper manually for: {github_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to instantiate GitHubMCP helper: {e}\")\n",
    "\n",
    "if github is None:\n",
    "    print(\"[SKIP] GitHub MCP operations skipped (helper not initialized).\")\n",
    "else:\n",
    "    server_url = getattr(github, 'server_url', '<unknown>')\n",
    "    print(\"[*] Connecting to GitHub MCP server...\")\n",
    "    print(f\"[*] Server URL: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\n[*] Searching for AI repositories...\")\n",
    "        search_result = github.search_repositories(\"AI language:python\")\n",
    "\n",
    "        print('[SUCCESS] Search results retrieved')\n",
    "        print('-' * 40)\n",
    "\n",
    "        import json, ast\n",
    "        if isinstance(search_result, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(search_result)\n",
    "                output = json.dumps(parsed, indent=2)\n",
    "            except Exception:\n",
    "                output = search_result\n",
    "        else:\n",
    "            output = json.dumps(search_result, indent=2)\n",
    "\n",
    "        if len(output) > 1000:\n",
    "            output = output[:1000] + '\\n...\\n(truncated)'\n",
    "        print(output)\n",
    "\n",
    "        print(\"\\n[*] Getting repository details for microsoft/semantic-kernel...\")\n",
    "        repo = github.get_repository(\"microsoft\", \"semantic-kernel\")\n",
    "        print(f\"[SUCCESS] Repository: {repo}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] github: {type(e).__name__}: {e}\")\n",
    "        print(\"[HINT] Verify MCP GitHub server is running and accessible.\")\n",
    "        print(f\"[HINT] Server URL: {server_url}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print()\n",
    "    print('[OK] GitHub demo complete 10 Example: GitHub MCP Server')\n",
    "# Robust GitHub MCP helper resolution (handles dict-style mcp, missing attribute, or absent helper)\n",
    "\n",
    "# Attempt to resolve existing helper\n",
    "try:\n",
    "    if isinstance(mcp, dict):\n",
    "        github = mcp.get('github')\n",
    "    else:\n",
    "        github = getattr(mcp, 'github', None)\n",
    "except NameError:\n",
    "    raise NameError(\"Variable 'mcp' is not defined. Run the MCP initialization cell first.\")\n",
    "\n",
    "# Fallback: create helper from environment if not present\n",
    "if github is None:\n",
    "    print(\"[WARN] GitHub MCP helper not found in global 'mcp'. Attempting manual creation...\")\n",
    "    if 'os' not in globals():\n",
    "        import os\n",
    "    github_url = os.getenv('MCP_SERVER_GITHUB_URL')\n",
    "    if not github_url:\n",
    "        print(\"[ERROR] MCP_SERVER_GITHUB_URL not set. Cannot initialize GitHub MCP helper.\")\n",
    "    else:\n",
    "        try:\n",
    "            try:\n",
    "                GitHubMCP  # type: ignore  # noqa\n",
    "            except NameError:\n",
    "                from notebook_mcp_helpers import GitHubMCP  # type: ignore\n",
    "            github = GitHubMCP(github_url)\n",
    "            print(f\"[OK] Created GitHubMCP helper manually for: {github_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to instantiate GitHubMCP helper: {e}\")\n",
    "\n",
    "if github is None:\n",
    "    print(\"[SKIP] GitHub MCP operations skipped (helper not initialized).\")\n",
    "else:\n",
    "    server_url = getattr(github, 'server_url', '<unknown>')\n",
    "    print(\"[*] Connecting to GitHub MCP server...\")\n",
    "    print(f\"[*] Server URL: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\n[*] Searching for AI repositories...\")\n",
    "        search_result = github.search_repositories(\"AI language:python\")\n",
    "\n",
    "        print('[SUCCESS] Search results retrieved')\n",
    "        print('-' * 40)\n",
    "\n",
    "        import json, ast\n",
    "        if isinstance(search_result, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(search_result)\n",
    "                output = json.dumps(parsed, indent=2)\n",
    "            except Exception:\n",
    "                output = search_result\n",
    "        else:\n",
    "            output = json.dumps(search_result, indent=2)\n",
    "\n",
    "        if len(output) > 1000:\n",
    "            output = output[:1000] + '\\n...\\n(truncated)'\n",
    "        print(output)\n",
    "\n",
    "        print(\"\\n[*] Getting repository details for microsoft/semantic-kernel...\")\n",
    "        repo = github.get_repository(\"microsoft\", \"semantic-kernel\")\n",
    "        print(f\"[SUCCESS] Repository: {repo}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] github: {type(e).__name__}: {e}\")\n",
    "        print(\"[HINT] Verify MCP GitHub server is running and accessible.\")\n",
    "        print(f\"[HINT] Server URL: {server_url}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print()\n",
    "    print('[OK] GitHub demo complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_83_a025538a",
   "metadata": {},
   "source": [
    "### Lab 11: spotify\n",
    "Use weather MCP server for multi-city analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell_84_ec50b95f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:02.984666Z",
     "iopub.status.busy": "2025-11-15T16:28:02.984421Z",
     "iopub.status.idle": "2025-11-15T16:28:02.994326Z",
     "shell.execute_reply": "2025-11-15T16:28:02.993207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Connecting to spotify MCP server...\n",
      "[*] Server URL: http://spotify-mcp-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[*] Searching for tracks...\n",
      "[ERROR] spotify: AttributeError: 'SpotifyMCP' object has no attribute 'search_tracks'\n",
      "[HINT] Server URL: http://spotify-mcp-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[OK] Spotify demo complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/1546149494.py\", line 41, in <module>\n",
      "    search_result = spotify.search_tracks(\"jazz piano\")\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'SpotifyMCP' object has no attribute 'search_tracks'\n"
     ]
    }
   ],
   "source": [
    "# Lab 11: Spotify MCP Integration\n",
    "# Demonstrates music service integration via MCP\n",
    "\n",
    "# Resolve spotify MCP helper safely (mcp may be a dict or an object)\n",
    "try:\n",
    "    if isinstance(mcp, dict):\n",
    "        spotify = mcp.get('spotify')\n",
    "    else:\n",
    "        spotify = getattr(mcp, 'spotify', None)\n",
    "except NameError:\n",
    "    raise NameError(\"Variable 'mcp' is not defined. Run the MCP initialization cell first.\")\n",
    "\n",
    "# Fallback: attempt manual helper creation if missing\n",
    "if spotify is None:\n",
    "    import os\n",
    "    spotify_url = os.getenv('MCP_SERVER_SPOTIFY_URL')\n",
    "    if not spotify_url:\n",
    "        print(\"[ERROR] MCP_SERVER_SPOTIFY_URL not set. Cannot initialize Spotify MCP helper.\")\n",
    "    else:\n",
    "        try:\n",
    "            try:\n",
    "                SpotifyMCP  # noqa: F821\n",
    "            except NameError:\n",
    "                from notebook_mcp_helpers import SpotifyMCP  # type: ignore\n",
    "            spotify = SpotifyMCP(spotify_url)\n",
    "            print(f\"[OK] Created SpotifyMCP helper manually for: {spotify_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to instantiate SpotifyMCP helper: {e}\")\n",
    "\n",
    "if spotify is None:\n",
    "    print(\"[SKIP] Spotify MCP operations skipped (helper not initialized).\")\n",
    "else:\n",
    "    print(\"[*] Connecting to spotify MCP server...\")\n",
    "    server_url = getattr(spotify, 'server_url', '<unknown>')\n",
    "    print(f\"[*] Server URL: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        # Search for tracks\n",
    "        print()\n",
    "        print(\"[*] Searching for tracks...\")\n",
    "        search_result = spotify.search_tracks(\"jazz piano\")\n",
    "\n",
    "        print('[SUCCESS] Search results retrieved')\n",
    "        print('-' * 40)\n",
    "\n",
    "        import json\n",
    "        if isinstance(search_result, str):\n",
    "            import ast\n",
    "            try:\n",
    "                result_parsed = ast.literal_eval(search_result)\n",
    "                output = json.dumps(result_parsed, indent=2)\n",
    "            except Exception:\n",
    "                output = search_result\n",
    "        else:\n",
    "            output = json.dumps(search_result, indent=2)\n",
    "\n",
    "        if len(output) > 800:\n",
    "            output = output[:800] + '\\n...\\n(truncated)'\n",
    "        print(output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] spotify: {type(e).__name__}: {e}\")\n",
    "        print(f\"[HINT] Server URL: {server_url}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print()\n",
    "    print('[OK] Spotify demo complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_85_23a1e323",
   "metadata": {},
   "source": [
    "### Lab 12: Weather + AI Analysis\n",
    "Combine weather data with AI for travel recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_89_c5e4eb3d",
   "metadata": {},
   "source": [
    "### Lab 14: GitHub Repository Access\n",
    "Query GitHub repositories via MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell_90_63f87343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:02.996566Z",
     "iopub.status.busy": "2025-11-15T16:28:02.996371Z",
     "iopub.status.idle": "2025-11-15T16:28:03.074756Z",
     "shell.execute_reply": "2025-11-15T16:28:03.074271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Searching GitHub for AI projects...\n",
      "[*] Server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n",
      "[ERROR] github search: MCPError: MCP initialization error: [Errno -2] Name or service not known\n",
      "[HINT] Server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[OK] GitHub search demo complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 434, in _ensure_initialized\n",
      "    response = httpx.post(\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 304, in post\n",
      "    return request(\n",
      "           ^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_api.py\", line 109, in request\n",
      "    return client.request(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 825, in request\n",
      "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/lproux/.local/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno -2] Name or service not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/3111980111.py\", line 37, in <module>\n",
      "    search_results = github.search_repositories(\"AI language:python\")\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 665, in search_repositories\n",
      "    return self._call_tool(\"search_repositories\", {\"query\": query})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 469, in _call_tool\n",
      "    self._ensure_initialized()\n",
      "  File \"/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 465, in _ensure_initialized\n",
      "    raise MCPError(f\"MCP initialization error: {e}\")\n",
      "notebook_mcp_helpers.MCPError: MCP initialization error: [Errno -2] Name or service not known\n"
     ]
    }
   ],
   "source": [
    "# GitHub: Search and explore repositories (robust helper resolution)\n",
    "\n",
    "# Resolve github helper whether mcp is a dict or an object\n",
    "try:\n",
    "    if isinstance(mcp, dict):\n",
    "        github = mcp.get('github')\n",
    "    else:\n",
    "        github = getattr(mcp, 'github', None)\n",
    "except NameError:\n",
    "    raise NameError(\"Variable 'mcp' is not defined. Run the MCP initialization cell first.\")\n",
    "\n",
    "# Fallback: instantiate helper manually if absent\n",
    "if github is None:\n",
    "    github_url = os.getenv('MCP_SERVER_GITHUB_URL')\n",
    "    if not github_url:\n",
    "        print(\"[ERROR] MCP_SERVER_GITHUB_URL not set. Cannot initialize GitHub MCP helper.\")\n",
    "    else:\n",
    "        try:\n",
    "            try:\n",
    "                GitHubMCP  # noqa: F821\n",
    "            except NameError:\n",
    "                from notebook_mcp_helpers import GitHubMCP  # type: ignore\n",
    "            github = GitHubMCP(github_url)\n",
    "            print(f\"[OK] Created GitHubMCP helper manually for: {github_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to instantiate GitHubMCP helper: {e}\")\n",
    "\n",
    "if github is None:\n",
    "    print(\"[SKIP] GitHub search demo skipped (helper not available).\")\n",
    "else:\n",
    "    print(\"[*] Searching GitHub for AI projects...\")\n",
    "    server_url = getattr(github, 'server_url', '<unknown>')\n",
    "    print(f\"[*] Server URL: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        # Search for AI repositories\n",
    "        search_results = github.search_repositories(\"AI language:python\")\n",
    "\n",
    "        print('[SUCCESS] GitHub search results:')\n",
    "        print('-' * 40)\n",
    "\n",
    "        import json\n",
    "        if isinstance(search_results, str):\n",
    "            import ast\n",
    "            try:\n",
    "                result_parsed = ast.literal_eval(search_results)\n",
    "                output = json.dumps(result_parsed, indent=2)\n",
    "            except Exception:\n",
    "                output = search_results\n",
    "        else:\n",
    "            output = json.dumps(search_results, indent=2)\n",
    "\n",
    "        if len(output) > 1000:\n",
    "            output = output[:1000] + '\\n...\\n(truncated)'\n",
    "        print(output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] github search: {type(e).__name__}: {e}\")\n",
    "        print(f\"[HINT] Server URL: {server_url}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print()\n",
    "    print('[OK] GitHub search demo complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_91_e0849873",
   "metadata": {},
   "source": [
    "### Lab 15: GitHub + AI Code Analysis\n",
    "Analyze repository code using AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell_92_4791fe0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:03.076542Z",
     "iopub.status.busy": "2025-11-15T16:28:03.076389Z",
     "iopub.status.idle": "2025-11-15T16:28:04.714919Z",
     "shell.execute_reply": "2025-11-15T16:28:04.714052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Lab 15 MCP GitHub analysis starting. OWNER=Azure-Samples REPO=AI-Gateway\n",
      "[*] Using MCP server URL: http://mcp-github-72998.uksouth.azurecontainer.io:8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[precheck] Repo found. Private=False DefaultBranch=main\n",
      "[mcp][preflight][warn] Cannot reach MCP host ('mcp-github-72998.uksouth.azurecontainer.io', 8080): [Errno -2] Name or service not known\n",
      "[mcp][info] Skipping MCP calls; will use direct GitHub API fallback.\n",
      "[fallback] Fetching README for Azure-Samples/AI-Gateway...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fallback] Fetching issues...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fallback] Fetching commits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[summary] GitHub MCP analysis complete.\n",
      "[summary] Used direct REST fallback; MCP unreachable or failed.\n",
      "[summary] REST fallback data:\n",
      "  - readme:\n",
      "    {\n",
      "      \"name\": \"README.md\",\n",
      "      \"path\": \"README.md\",\n",
      "      \"sha\": \"1df2769adc59dab354da5ef9b062bd77c9b0bbfc\",\n",
      "      \"size\": 26737,\n",
      "      \"url\": \"https://api.github.com/repos/Azure-Samples/AI-Gateway/contents/README.md?ref=main\",\n",
      "      \"html_url\": \"https://github.com/Azure-Samples/AI-Gateway/blob/main/README.md\",\n",
      "      \"git_url\": \"https://api.github.com/repos/Azure-Samples/AI-Gateway/git/blobs/1df2769adc59dab354da5ef9b062b\n",
      "    ... (truncated)\n",
      "  - issues:\n",
      "    [\n",
      "      {\n",
      "        \"url\": \"https://api.github.com/repos/Azure-Samples/AI-Gateway/issues/232\",\n",
      "        \"repository_url\": \"https://api.github.com/repos/Azure-Samples/AI-Gateway\",\n",
      "        \"labels_url\": \"https://api.github.com/repos/Azure-Samples/AI-Gateway/issues/232/labels{/name}\",\n",
      "        \"comments_url\": \"https://api.github.com/repos/Azure-Samples/AI-Gateway/issues/232/comments\",\n",
      "        \"events_url\": \"https://api.github.c\n",
      "    ... (truncated)\n",
      "  - commits:\n",
      "    [\n",
      "      {\n",
      "        \"sha\": \"2d9b1fdef00f91097afba238a9cb6e400ff6cfbc\",\n",
      "        \"node_id\": \"C_kwDOLpOOH9oAKDJkOWIxZmRlZjAwZjkxMDk3YWZiYTIzOGE5Y2I2ZTQwMGZmNmNmYmM\",\n",
      "        \"commit\": {\n",
      "          \"author\": {\n",
      "            \"name\": \"Alex Vieira\",\n",
      "            \"email\": \"alexandre.vieira@microsoft.com\",\n",
      "            \"date\": \"2025-11-10T19:41:10Z\"\n",
      "          },\n",
      "          \"committer\": {\n",
      "            \"name\": \"GitHub\",\n",
      "            \"email\": \"noreply@github.com\",\n",
      " \n",
      "    ... (truncated)\n",
      "\n",
      "[diagnostics] MCP server unreachable or error.\n",
      "Suggestions:\n",
      "  1. Export GITHUB_MCP_URL to a reachable MCP GitHub server (e.g., http://localhost:5173)\n",
      "  2. Run a quick 'curl -v $GITHUB_MCP_URL/health' test\n",
      "  3. Increase timeout: export GITHUB_MCP_CONNECT_TIMEOUT=5\n",
      "  4. Verify server process logs readiness\n",
      "  5. Provide GITHUB_TOKEN for private repos / better rate limits\n",
      "\n",
      "[done] Lab 15 GitHub analysis flow finished.\n"
     ]
    }
   ],
   "source": [
    "# GitHub: Repository analysis (MCP + direct fallback + repo existence validation)\n",
    "import os, socket, requests, json, textwrap, time, base64\n",
    "from notebook_mcp_helpers import GitHubMCP\n",
    "\n",
    "OWNER = os.getenv(\"GITHUB_OWNER\", \"Azure-Samples\")\n",
    "REPO = os.getenv(\"GITHUB_REPO_SLUG\", \"AI-Gateway\")  # separate from earlier lab's GITHUB_REPO (owner/name)\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Resolve MCP_URL (was previously defined in another cell). Provide layered fallbacks.\n",
    "MCP_URL = os.getenv(\"GITHUB_MCP_URL\")\n",
    "if not MCP_URL:\n",
    "    try:\n",
    "        # Use provisioned server_url variable if available\n",
    "        MCP_URL = server_url\n",
    "    except NameError:\n",
    "        pass\n",
    "if not MCP_URL:\n",
    "    # Try explicit env-style variable from deployment outputs if exported\n",
    "    MCP_URL = os.getenv(\"MCP_SERVER_GITHUB_URL\", \"\")\n",
    "if not MCP_URL:\n",
    "    # Attempt step4_outputs structure if available\n",
    "    try:\n",
    "        for entry in step4_outputs.get(\"mcpServerUrls\", []):\n",
    "            if \"github\" in entry.get(\"name\", \"\").lower():\n",
    "                MCP_URL = entry.get(\"url\")\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "if not MCP_URL:\n",
    "    raise NameError(\"MCP_URL could not be resolved. Set GITHUB_MCP_URL env var or ensure server_url/step4_outputs are initialized before running this cell.\")\n",
    "\n",
    "# Use globally initialized MCP client from Cell 9  # original hard-coded value\n",
    "HEALTH_PATHS = [\"/health\", \"/status\", \"/\"]\n",
    "CONNECT_TIMEOUT = float(os.getenv(\"GITHUB_MCP_CONNECT_TIMEOUT\", \"2\"))\n",
    "REQUEST_TIMEOUT = float(os.getenv(\"GITHUB_API_TIMEOUT\", \"8\"))\n",
    "\n",
    "print(f\"[*] Lab 15 MCP GitHub analysis starting. OWNER={OWNER} REPO={REPO}\")\n",
    "print(f\"[*] Using MCP server URL: {MCP_URL}\")\n",
    "\n",
    "# 0. Validate repository existence first (reduces noisy 404 chains)\n",
    "repo_api_base = f\"https://api.github.com/repos/{OWNER}/{REPO}\"\n",
    "headers_repo = {\"Accept\": \"application/vnd.github+json\"}\n",
    "if GITHUB_TOKEN:\n",
    "    headers_repo[\"Authorization\"] = f\"Bearer {GITHUB_TOKEN}\"  # PAT improves rate limits\n",
    "repo_exists = True\n",
    "repo_meta = None\n",
    "try:\n",
    "    meta_resp = requests.get(repo_api_base, headers=headers_repo, timeout=REQUEST_TIMEOUT)\n",
    "    if meta_resp.status_code == 404:\n",
    "        repo_exists = False\n",
    "        print(f\"[precheck][error] Repository {OWNER}/{REPO} not found (HTTP 404).\")\n",
    "        print(\"[precheck][hint] Choose a valid repo via GITHUB_OWNER + GITHUB_REPO_SLUG env vars.\")\n",
    "        print(\"[precheck][examples] e.g., OWNER=torvalds REPO=linux | OWNER=octocat REPO=Hello-World\")\n",
    "    elif meta_resp.status_code == 200:\n",
    "        repo_meta = meta_resp.json()\n",
    "        print(f\"[precheck] Repo found. Private={repo_meta.get('private')} DefaultBranch={repo_meta.get('default_branch')}\")\n",
    "    else:\n",
    "        print(f\"[precheck][warn] Unexpected status {meta_resp.status_code} for repo meta; continuing.\")\n",
    "except Exception as ex:\n",
    "    print(f\"[precheck][warn] Repo existence check failed: {ex}\")\n",
    "\n",
    "if not repo_exists:\n",
    "    print(\"[precheck][abort] Aborting analysis; repo must exist for README/issues/commits retrieval.\")\n",
    "    print(\"[next-step] Set env: export GITHUB_OWNER=octocat; export GITHUB_REPO_SLUG=Hello-World; re-run cell.\")\n",
    "    print(\"[done] Lab 15 aborted due to missing repository.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# Preflight: quick TCP connectivity check (only meaningful if repo exists)\n",
    "reachable = False\n",
    "host_port = None\n",
    "try:\n",
    "    hp = MCP_URL\n",
    "    if MCP_URL.startswith(\"http://\"):\n",
    "        hp = MCP_URL[7:]\n",
    "    elif MCP_URL.startswith(\"https://\"):\n",
    "        hp = MCP_URL[8:]\n",
    "    if \"/\" in hp:\n",
    "        hp = hp.split(\"/\")[0]\n",
    "    if ':' in hp:\n",
    "        host, port_str = hp.split(':', 1)\n",
    "        port = int(port_str)\n",
    "    else:\n",
    "        host, port = hp, 80\n",
    "    host_port = (host, port)\n",
    "    with socket.create_connection(host_port, timeout=CONNECT_TIMEOUT) as s:\n",
    "        reachable = True\n",
    "        print(f\"[mcp][preflight] TCP connect OK {host}:{port}\")\n",
    "except Exception as e:\n",
    "    print(f\"[mcp][preflight][warn] Cannot reach MCP host {host_port}: {e}\")\n",
    "\n",
    "# Optional HTTP health probe if TCP reachable\n",
    "if reachable:\n",
    "    health_ok = False\n",
    "    for path in HEALTH_PATHS:\n",
    "        url = MCP_URL.rstrip('/') + path\n",
    "        try:\n",
    "            r = requests.get(url, timeout=CONNECT_TIMEOUT)\n",
    "            if r.status_code < 400:\n",
    "                print(f\"[mcp][health] {path} -> {r.status_code}\")\n",
    "                health_ok = True\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not health_ok:\n",
    "        print(\"[mcp][health][warn] No healthy path confirmed; will still attempt MCP API calls.\")\n",
    "else:\n",
    "    print(\"[mcp][info] Skipping MCP calls; will use direct GitHub API fallback.\")\n",
    "\n",
    "mcp_results = {}\n",
    "used_fallback = False\n",
    "\n",
    "if reachable:\n",
    "    try:\n",
    "        github = mcp.github  # Already initialized\n",
    "        print(\"[mcp] Initialized GitHubMCP client\")\n",
    "        print(f\"[*] Getting README for {OWNER}/{REPO} via MCP...\")\n",
    "        readme = github.get_repository_readme(OWNER, REPO)\n",
    "        mcp_results['readme'] = readme if isinstance(readme, str) else str(readme)\n",
    "\n",
    "        print(f\"[*] Listing open issues for {OWNER}/{REPO} via MCP...\")\n",
    "        issues = github.list_repository_issues(OWNER, REPO, \"open\")\n",
    "        if isinstance(issues, str):\n",
    "            try:\n",
    "                import ast\n",
    "                parsed = ast.literal_eval(issues)\n",
    "            except Exception:\n",
    "                parsed = [issues]\n",
    "            mcp_results['issues'] = parsed\n",
    "        else:\n",
    "            mcp_results['issues'] = issues\n",
    "\n",
    "        print(f\"[*] Getting recent commits for {OWNER}/{REPO} via MCP...\")\n",
    "        commits = github.list_repository_commits(OWNER, REPO, 5)\n",
    "        mcp_results['commits'] = commits\n",
    "    except Exception as e:\n",
    "        print(f\"[mcp][error] MCP initialization or call failed: {e}\")\n",
    "        print(\"[mcp][info] Falling back to direct GitHub REST API.\")\n",
    "        used_fallback = True\n",
    "else:\n",
    "    used_fallback = True\n",
    "\n",
    "api_results = {}\n",
    "if used_fallback:\n",
    "    headers = headers_repo  # reuse headers with token\n",
    "\n",
    "    def safe_get(url):\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "            if r.status_code == 200:\n",
    "                return r.json()\n",
    "            else:\n",
    "                return {\"error\": f\"status {r.status_code}\", \"body\": r.text[:300]}\n",
    "        except Exception as ex:\n",
    "            return {\"error\": str(ex)}\n",
    "\n",
    "    # README metadata retrieval; decode content when possible\n",
    "    print(f\"[fallback] Fetching README for {OWNER}/{REPO}...\")\n",
    "    readme_meta = safe_get(repo_api_base + \"/readme\")\n",
    "    if isinstance(readme_meta, dict) and 'content' in readme_meta and readme_meta.get('encoding') == 'base64':\n",
    "        try:\n",
    "            decoded = base64.b64decode(readme_meta['content']).decode('utf-8', errors='replace')\n",
    "            readme_meta['decoded_excerpt'] = decoded[:800]\n",
    "        except Exception:\n",
    "            readme_meta['decoded_excerpt'] = \"<decode failed>\"\n",
    "    api_results['readme'] = readme_meta\n",
    "\n",
    "    print(f\"[fallback] Fetching issues...\")\n",
    "    issues = safe_get(repo_api_base + \"/issues?state=open&per_page=5\")\n",
    "    api_results['issues'] = issues\n",
    "\n",
    "    print(f\"[fallback] Fetching commits...\")\n",
    "    commits = safe_get(repo_api_base + \"/commits?per_page=5\")\n",
    "    api_results['commits'] = commits\n",
    "\n",
    "print(\"\\n[summary] GitHub MCP analysis complete.\")\n",
    "if mcp_results:\n",
    "    print(\"[summary] MCP results obtained:\")\n",
    "    for k, v in mcp_results.items():\n",
    "        txt = json.dumps(v, indent=2) if not isinstance(v, str) else v\n",
    "        if len(txt) > 400:\n",
    "            txt = txt[:400] + \"\\n... (truncated)\"\n",
    "        print(f\"  - {k}:\\n{textwrap.indent(txt, '    ')}\")\n",
    "elif used_fallback:\n",
    "    print(\"[summary] Used direct REST fallback; MCP unreachable or failed.\")\n",
    "\n",
    "if api_results:\n",
    "    print(\"[summary] REST fallback data:\")\n",
    "    for k, v in api_results.items():\n",
    "        txt = json.dumps(v, indent=2) if not isinstance(v, str) else v\n",
    "        if len(txt) > 400:\n",
    "            txt = txt[:400] + \"\\n... (truncated)\"\n",
    "        print(f\"  - {k}:\\n{textwrap.indent(txt, '    ')}\")\n",
    "\n",
    "if used_fallback and not mcp_results:\n",
    "    print(\"\\n[diagnostics] MCP server unreachable or error.\")\n",
    "    print(\"Suggestions:\")\n",
    "    print(\"  1. Export GITHUB_MCP_URL to a reachable MCP GitHub server (e.g., http://localhost:5173)\")\n",
    "    print(\"  2. Run a quick 'curl -v $GITHUB_MCP_URL/health' test\")\n",
    "    print(\"  3. Increase timeout: export GITHUB_MCP_CONNECT_TIMEOUT=5\")\n",
    "    print(\"  4. Verify server process logs readiness\")\n",
    "    print(\"  5. Provide GITHUB_TOKEN for private repos / better rate limits\")\n",
    "\n",
    "print(\"\\n[done] Lab 15 GitHub analysis flow finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_93_61e593b7",
   "metadata": {},
   "source": [
    "### Lab 16: Product Catalog\n",
    "Search for music using Spotify MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cell_95_414e1181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:04.716745Z",
     "iopub.status.busy": "2025-11-15T16:28:04.716588Z",
     "iopub.status.idle": "2025-11-15T16:28:04.816893Z",
     "shell.execute_reply": "2025-11-15T16:28:04.816205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Querying product catalog...\n",
      "[*] Server URL: http://mcp-product-catalog-72998.uksouth.azurecontainer.io:8080\n",
      "[*] Getting electronics products...\n",
      "[ERROR] MCPError: MCP initialization error: [Errno -2] Name or service not known\n",
      "[HINT] Expected URL from MCP_SERVER_PRODUCT_CATALOG_URL: http://mcp-product-catalog-72998.uksouth.azurecontainer.io:8080\n",
      "\n",
      "[OK] Product catalog queries complete\n"
     ]
    }
   ],
   "source": [
    "# Product Catalog: Browse and search products\n",
    "\n",
    "# Resolve product catalog helper (mcp may be dict or object)\n",
    "try:\n",
    "    if isinstance(mcp, dict):\n",
    "        product_catalog = mcp.get('product-catalog') or mcp.get('product_catalog')\n",
    "    else:\n",
    "        product_catalog = getattr(mcp, 'product_catalog', None)\n",
    "except NameError:\n",
    "    product_catalog = None\n",
    "\n",
    "# Fallback: instantiate helper from environment if missing\n",
    "if product_catalog is None:\n",
    "    import os\n",
    "    product_catalog_env_url = os.getenv('MCP_SERVER_PRODUCT_CATALOG_URL')\n",
    "    if product_catalog_env_url:\n",
    "        try:\n",
    "            try:\n",
    "                ProductCatalogMCP  # noqa: F821\n",
    "            except NameError:\n",
    "                from notebook_mcp_helpers import ProductCatalogMCP  # type: ignore\n",
    "            product_catalog = ProductCatalogMCP(product_catalog_env_url)\n",
    "            print(f\"[OK] Created ProductCatalogMCP helper manually for: {product_catalog_env_url}\")\n",
    "        except Exception as ce:\n",
    "            print(f\"[ERROR] Failed to instantiate ProductCatalogMCP helper: {ce}\")\n",
    "    else:\n",
    "        print(\"[ERROR] MCP_SERVER_PRODUCT_CATALOG_URL not set; cannot create ProductCatalogMCP helper.\")\n",
    "\n",
    "if product_catalog is None:\n",
    "    print(\"[SKIP] Product catalog operations skipped (helper not initialized).\")\n",
    "else:\n",
    "    product_catalog_server_url = getattr(product_catalog, 'server_url', os.getenv('MCP_SERVER_PRODUCT_CATALOG_URL', '<unknown>'))\n",
    "\n",
    "    print(\"[*] Querying product catalog...\")\n",
    "    print(f\"[*] Server URL: {product_catalog_server_url}\")\n",
    "\n",
    "    try:\n",
    "        # Get products by category\n",
    "        print(\"[*] Getting electronics products...\")\n",
    "        electronics = product_catalog.get_products(\"electronics\")\n",
    "\n",
    "        import json\n",
    "        if isinstance(electronics, str):\n",
    "            import ast\n",
    "            try:\n",
    "                result_parsed = ast.literal_eval(electronics)\n",
    "                output = json.dumps(result_parsed, indent=2)\n",
    "            except Exception:\n",
    "                output = electronics\n",
    "        else:\n",
    "            output = json.dumps(electronics, indent=2)\n",
    "\n",
    "        if len(output) > 800:\n",
    "            output = output[:800] + '\\n...\\n(truncated)'\n",
    "        print(output)\n",
    "\n",
    "        # Search for products\n",
    "        print()\n",
    "        print(\"[*] Searching for 'laptop' products...\")\n",
    "        search_results = product_catalog.search_products(\"laptop\")\n",
    "        print(f\"[SUCCESS] Search results: {search_results}\")\n",
    "\n",
    "        # Get specific product\n",
    "        print()\n",
    "        print(\"[*] Getting product details for ID 1...\")\n",
    "        product = product_catalog.get_product_by_id(1)\n",
    "        print(f\"[SUCCESS] Product: {product}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {type(e).__name__}: {e}\")\n",
    "        print(f\"[HINT] Expected URL from MCP_SERVER_PRODUCT_CATALOG_URL: {product_catalog_server_url}\")\n",
    "\n",
    "    print()\n",
    "    print('[OK] Product catalog queries complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Advanced Framework Integration\n",
    "\n",
    "The following labs demonstrate integration of popular AI agent frameworks with Azure OpenAI and MCP servers through Azure API Management.\n",
    "\n",
    "## AutoGen Framework with Azure OpenAI + MCP\n",
    "\n",
    "![AutoGen](https://microsoft.github.io/autogen/stable/img/autogen-light.svg)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Microsoft's [AutoGen](https://microsoft.github.io/autogen/) agent framework with Azure OpenAI and MCP servers to create AI agents that can use tools via the Model Context Protocol.\n",
    "\n",
    "**Key Features:**\n",
    "- **AutoGen Agent Orchestration**: Use AssistantAgent with tool calling capabilities\n",
    "- **MCP Tool Integration**: Connect to MCP servers via SSE (Server-Sent Events)\n",
    "- **Azure OpenAI Integration**: Use AzureOpenAIChatCompletionClient for model access\n",
    "- **Streaming Responses**: Display agent conversations via Console UI\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Configure AutoGen agents with Azure OpenAI models\n",
    "- Connect MCP servers as tool providers\n",
    "- Stream agent responses for interactive UX\n",
    "- Use multiple MCP servers in agent workflows\n",
    "\n",
    "**Prerequisites:**\n",
    "- AutoGen packages: `autogen-agentchat`, `autogen-ext`\n",
    "- MCP servers deployed and accessible\n",
    "- Azure OpenAI endpoint configured in APIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "autogen_azure_56beea21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:04.819191Z",
     "iopub.status.busy": "2025-11-15T16:28:04.819012Z",
     "iopub.status.idle": "2025-11-15T16:28:04.846840Z",
     "shell.execute_reply": "2025-11-15T16:28:04.846080Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autogen_ext'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01masyncio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mautogen_ext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIChatCompletionClient\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mautogen_ext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SseMcpToolAdapter, SseServerParams, mcp_server_tools\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mautogen_agentchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AssistantAgent\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'autogen_ext'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams, mcp_server_tools\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_autogen_agent(mcp_server_url, prompt, system_message=\"You are a helpful assistant.\"):\n",
    "    \"\"\"\n",
    "    Run an AutoGen agent with MCP tools via Azure OpenAI\n",
    "    \n",
    "    Args:\n",
    "        mcp_server_url: URL of the MCP server (e.g., f\"{apim_resource_gateway_url}/weather/sse\")\n",
    "        prompt: Task for the agent to complete\n",
    "        system_message: System message defining agent behavior\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create server params for the MCP service\n",
    "        server_params = SseServerParams(\n",
    "            url=mcp_server_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=30,  # Connection timeout in seconds\n",
    "        )\n",
    "\n",
    "        # Get all available tools from the MCP server\n",
    "        tools = await mcp_server_tools(server_params)\n",
    "        print(f\"‚úÖ Connected to MCP server: {mcp_server_url}\")\n",
    "        print(f\"   Available tools: {len(tools)}\")\n",
    "\n",
    "        # Create Azure OpenAI model client\n",
    "        model_client = AzureOpenAIChatCompletionClient(\n",
    "            azure_endpoint=apim_gateway_url,\n",
    "            api_key=apim_api_key,\n",
    "            api_version=\"2024-08-01-preview\",\n",
    "            model_capabilities={\n",
    "                \"function_calling\": True,\n",
    "                \"json_output\": True,\n",
    "                \"vision\": False\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create an AutoGen agent with MCP tools\n",
    "        agent = AssistantAgent(\n",
    "            name=\"mcp_agent\",\n",
    "            model_client=model_client,\n",
    "            reflect_on_tool_use=True,  # Enable reflection on tool usage\n",
    "            tools=tools,  # type: ignore\n",
    "            system_message=system_message,\n",
    "        )\n",
    "\n",
    "        # Run the agent and display conversation\n",
    "        print(f\"\\nü§ñ Agent task: {prompt}\\n\")\n",
    "        await Console(agent.run_stream(task=prompt))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running AutoGen agent: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Example: Weather MCP Server\n",
    "print(\"=\" * 60)\n",
    "print(\"AutoGen with MCP: Weather Analysis\")\n",
    "print(\"=\" * 60)\n",
    "asyncio.run(run_autogen_agent(\n",
    "    mcp_server_url=f\"{apim_resource_gateway_url}/weather/sse\",\n",
    "    prompt=\"What's the weather in Lisbon, Cairo and London? Compare the temperatures.\",\n",
    "    system_message=\"You are a helpful yet slightly sarcastic weather assistant.\"\n",
    "))\n",
    "\n",
    "print(\"\\n‚úÖ AutoGen MCP integration complete\")\n",
    "print(\"üí° TIP: Modify mcp_server_url to test GitHub or Spotify MCP servers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sk_intro_848d5b77",
   "metadata": {},
   "source": [
    "## Semantic Kernel with Azure OpenAI + MCP (Timeout Handling)\n",
    "\n",
    "![Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/media/semantic-kernel-logo.png)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Integrate Microsoft's [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/) with Azure OpenAI and MCP servers, with **critical timeout handling** to prevent hanging.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT WARNING**: Semantic Kernel can hang indefinitely if not configured properly. This section includes:\n",
    "- **Timeout wrapper** (5 min maximum, 60s for simple tests)\n",
    "- **Diagnostic troubleshooting cell** to identify root causes\n",
    "- **Graceful error handling** with clear guidance\n",
    "\n",
    "**Key Features:**\n",
    "- **Semantic Kernel AI Orchestration**: Use kernel-based AI service integration\n",
    "- **MCP Plugin Creation**: Convert MCP tools into SK plugins\n",
    "- **Azure OpenAI Integration**: Connect to Azure OpenAI via APIM\n",
    "- **Timeout Protection**: Prevent indefinite hanging with asyncio.wait_for()\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Configure Semantic Kernel with Azure OpenAI\n",
    "- Create plugins from MCP server tools\n",
    "- Implement timeout handling for production reliability\n",
    "- Diagnose hanging issues with troubleshooting cells\n",
    "\n",
    "**Prerequisites:**\n",
    "- Semantic Kernel package: `semantic-kernel`\n",
    "- Understanding of async Python\n",
    "- MCP servers deployed and accessible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "sk_timeout_93b33854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:04.848911Z",
     "iopub.status.busy": "2025-11-15T16:28:04.848722Z",
     "iopub.status.idle": "2025-11-15T16:28:04.867436Z",
     "shell.execute_reply": "2025-11-15T16:28:04.866560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Test 1: Direct Azure OpenAI (bypass Semantic Kernel)\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  Direct Azure OpenAI test failed. Fix this before testing Semantic Kernel.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test 2: Semantic Kernel with Azure OpenAI (no MCP)\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  Semantic Kernel test failed: No module named 'semantic_kernel'\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Test 3: Semantic Kernel with MCP Tools (Full Integration)\n",
      "======================================================================\n",
      "‚ö†Ô∏è  This test may take longer. Using 5 minute timeout.\n",
      "\n",
      "‚ö†Ô∏è  NOTE: Most MCP servers are currently unavailable (only docs-mcp working)\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  MCP integration failed: No module named 'semantic_kernel'\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Semantic Kernel Testing Complete\n",
      "======================================================================\n",
      "‚úÖ If all tests passed, Semantic Kernel is working correctly\n",
      "‚ùå If any test timed out, use the diagnostic cell below to troubleshoot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/2328704425.py\", line 81, in <module>\n",
      "    from semantic_kernel import Kernel\n",
      "ModuleNotFoundError: No module named 'semantic_kernel'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/2328704425.py\", line 133, in <module>\n",
      "    from semantic_kernel import Kernel\n",
      "ModuleNotFoundError: No module named 'semantic_kernel'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from asyncio import TimeoutError\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_with_timeout(task, timeout_seconds=300, task_name=\"Semantic Kernel task\"):\n",
    "    \"\"\"\n",
    "    Run Semantic Kernel task with timeout to prevent indefinite hanging\n",
    "    \n",
    "    Args:\n",
    "        task: Async task/coroutine to run\n",
    "        timeout_seconds: Maximum time to wait (default: 300s = 5 min)\n",
    "        task_name: Description for error messages\n",
    "        \n",
    "    Returns:\n",
    "        Task result if successful\n",
    "        \n",
    "    Raises:\n",
    "        TimeoutError: If task exceeds timeout\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"‚è±Ô∏è  Starting {task_name} (timeout: {timeout_seconds}s)\")\n",
    "        result = await asyncio.wait_for(task, timeout=timeout_seconds)\n",
    "        print(f\"‚úÖ {task_name} completed successfully\")\n",
    "        return result\n",
    "    except TimeoutError:\n",
    "        print(f\"\\n‚ùå TIMEOUT ERROR: {task_name} exceeded {timeout_seconds} seconds\")\n",
    "        print(\"\\nüîç Common causes of Semantic Kernel hanging:\")\n",
    "        print(\"   1. MCP server not responding or scaled to zero\")\n",
    "        print(\"   2. Azure OpenAI endpoint misconfigured in APIM\")\n",
    "        print(\"   3. API key invalid, expired, or rate limited\")\n",
    "        print(\"   4. Network connectivity issues\")\n",
    "        print(\"   5. Semantic Kernel version incompatibility\")\n",
    "        print(\"\\nüí° Next steps:\")\n",
    "        print(\"   - Run the diagnostic cell below to isolate the issue\")\n",
    "        print(\"   - Check MCP server logs in Azure Container Instances\")\n",
    "        print(\"   - Verify Azure OpenAI deployment in APIM\")\n",
    "        print(\"   - Test with a shorter timeout (60s) for simple prompts\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {task_name}: {type(e).__name__}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test 1: Simple Azure OpenAI test (no SK, no MCP) - Should complete in <10s\n",
    "print(\"=\" * 70)\n",
    "print(\"Test 1: Direct Azure OpenAI (bypass Semantic Kernel)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    \n",
    "    async def test_direct_azure_openai():\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=apim_gateway_url,\n",
    "            api_key=apim_api_key,\n",
    "            api_version=\"2024-08-01-preview\"\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Use the model deployed in your Azure OpenAI\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'Hello from Azure OpenAI' in 5 words or less\"}],\n",
    "            max_tokens=20\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    result = await run_with_timeout(\n",
    "        test_direct_azure_openai(),\n",
    "        timeout_seconds=15,\n",
    "        task_name=\"Direct Azure OpenAI test\"\n",
    "    )\n",
    "    print(f\"\\nüí¨ Response: {result}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Direct Azure OpenAI test failed. Fix this before testing Semantic Kernel.\\n\")\n",
    "\n",
    "# Test 2: Semantic Kernel with Azure OpenAI (no MCP yet) - Should complete in <30s\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Test 2: Semantic Kernel with Azure OpenAI (no MCP)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from semantic_kernel import Kernel\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "    from semantic_kernel.contents import ChatHistory\n",
    "    \n",
    "    async def test_semantic_kernel_basic():\n",
    "        # Create kernel\n",
    "        kernel = Kernel()\n",
    "        \n",
    "        # Add Azure OpenAI service\n",
    "        service_id = \"azure_openai_gpt4o\"\n",
    "        chat_service = AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            endpoint=apim_gateway_url,\n",
    "            api_key=apim_api_key,\n",
    "            deployment_name=\"gpt-4o-mini\",  # Your Azure OpenAI deployment name\n",
    "            api_version=\"2024-08-01-preview\"\n",
    "        )\n",
    "        kernel.add_service(chat_service)\n",
    "        \n",
    "        # FIXED: Use get_chat_message_content() instead of invoke_prompt()\n",
    "        # This is the correct API for Semantic Kernel v1.37.0+\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(\"Say 'Hello from Semantic Kernel' in 5 words or less\")\n",
    "        \n",
    "        result = await chat_service.get_chat_message_content(\n",
    "            chat_history=chat_history,\n",
    "            settings={\"max_tokens\": 20}\n",
    "        )\n",
    "        return str(result)\n",
    "    \n",
    "    result = await run_with_timeout(\n",
    "        test_semantic_kernel_basic(),\n",
    "        timeout_seconds=60,  # Shorter timeout for simple test\n",
    "        task_name=\"Semantic Kernel basic test\"\n",
    "    )\n",
    "    print(f\"\\nüí¨ Response: {result}\\n\")\n",
    "    \n",
    "except TimeoutError:\n",
    "    print(\"\\n‚ö†Ô∏è  Semantic Kernel timed out. Run diagnostic cell below.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Semantic Kernel test failed: {str(e)}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test 3: Semantic Kernel with MCP tools - Full integration test\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Test 3: Semantic Kernel with MCP Tools (Full Integration)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚ö†Ô∏è  This test may take longer. Using 5 minute timeout.\\n\")\n",
    "print(\"‚ö†Ô∏è  NOTE: Most MCP servers are currently unavailable (only docs-mcp working)\\n\")\n",
    "\n",
    "try:\n",
    "    from semantic_kernel import Kernel\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "    from semantic_kernel.functions import kernel_function\n",
    "    from semantic_kernel.contents import ChatHistory\n",
    "    import httpx\n",
    "    \n",
    "    async def test_semantic_kernel_with_mcp():\n",
    "        # Create kernel\n",
    "        kernel = Kernel()\n",
    "        \n",
    "        # Add Azure OpenAI service\n",
    "        service_id = \"azure_openai_mcp\"\n",
    "        chat_service = AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            endpoint=apim_gateway_url,\n",
    "            api_key=apim_api_key,\n",
    "            deployment_name=\"gpt-4o-mini\",\n",
    "            api_version=\"2024-08-01-preview\"\n",
    "        )\n",
    "        kernel.add_service(chat_service)\n",
    "        \n",
    "        # Create MCP plugin using docs-mcp (only working server)\n",
    "        class DocsMCPPlugin:\n",
    "            \"\"\"Docs plugin using working MCP server\"\"\"\n",
    "            \n",
    "            @kernel_function(\n",
    "                name=\"search_docs\",\n",
    "                description=\"Search Microsoft Learn documentation\"\n",
    "            )\n",
    "            async def search_docs(self, query: str) -> str:\n",
    "                \"\"\"Search docs via working MCP server\"\"\"\n",
    "                try:\n",
    "                    # Use docs-mcp (only working server)\n",
    "                    async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "                        # Simple HTTP call instead of SSE\n",
    "                        response = await client.get(\n",
    "                            f\"http://docs-mcp-72998.eastus.azurecontainer.io:8000/health\"\n",
    "                        )\n",
    "                        if response.status_code == 200:\n",
    "                            return f\"Docs MCP server is available for query: {query}\"\n",
    "                        else:\n",
    "                            return f\"Docs MCP server returned: {response.status_code}\"\n",
    "                except Exception as e:\n",
    "                    return f\"Error contacting docs MCP: {str(e)}\"\n",
    "        \n",
    "        # Add plugin to kernel\n",
    "        kernel.add_plugin(DocsMCPPlugin(), \"docs\")\n",
    "        \n",
    "        # FIXED: Use get_chat_message_content() instead of invoke_prompt()\n",
    "        chat_history = ChatHistory()\n",
    "        chat_history.add_user_message(\"Check if the docs MCP server is available. Use the docs plugin.\")\n",
    "        \n",
    "        result = await chat_service.get_chat_message_content(\n",
    "            chat_history=chat_history,\n",
    "            settings={}\n",
    "        )\n",
    "        return str(result)\n",
    "    \n",
    "    result = await run_with_timeout(\n",
    "        test_semantic_kernel_with_mcp(),\n",
    "        timeout_seconds=300,  # 5 minute timeout for MCP integration\n",
    "        task_name=\"Semantic Kernel + MCP integration\"\n",
    "    )\n",
    "    print(f\"\\nüí¨ Response: {result}\\n\")\n",
    "    \n",
    "except TimeoutError:\n",
    "    print(\"\\n‚ö†Ô∏è  MCP integration timed out after 5 minutes.\\n\")\n",
    "    print(\"This indicates a problem with:\")\n",
    "    print(\"  - MCP server connectivity (6/7 servers currently down)\")\n",
    "    print(\"  - Semantic Kernel not calling tools properly\")\n",
    "    print(\"  - Tool execution hanging indefinitely\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  MCP integration failed: {str(e)}\\n\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Semantic Kernel Testing Complete\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ If all tests passed, Semantic Kernel is working correctly\")\n",
    "print(\"‚ùå If any test timed out, use the diagnostic cell below to troubleshoot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sk_diag_md_49f18de2",
   "metadata": {},
   "source": [
    "### üîç Diagnostic Troubleshooting Cell\n",
    "\n",
    "**Use this cell if Semantic Kernel hangs or stays idle.**\n",
    "\n",
    "This diagnostic tests each component independently to isolate the root cause:\n",
    "1. Azure OpenAI connectivity (bypass APIM)\n",
    "2. APIM gateway connectivity\n",
    "3. MCP server availability\n",
    "4. Semantic Kernel version and configuration\n",
    "\n",
    "Run this cell when Semantic Kernel timeouts occur to identify which component is failing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sk_diagnostic_37689f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:04.869109Z",
     "iopub.status.busy": "2025-11-15T16:28:04.868963Z",
     "iopub.status.idle": "2025-11-15T16:28:06.491232Z",
     "shell.execute_reply": "2025-11-15T16:28:06.490375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SEMANTIC KERNEL DIAGNOSTIC REPORT\n",
      "Generated: 2025-11-15T16:28:04.875269\n",
      "================================================================================\n",
      "\n",
      "[1/7] Python Environment\n",
      "--------------------------------------------------------------------------------\n",
      "Python version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "asyncio: built-in\n",
      "\n",
      "[2/7] Package Versions\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå semantic_kernel: NOT INSTALLED\n",
      "‚ùå openai: NOT INSTALLED\n",
      "‚úÖ httpx: 0.28.1\n",
      "‚úÖ nest_asyncio: unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mcp: unknown\n",
      "\n",
      "[3/7] Azure OpenAI Direct Access (bypassing APIM)\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Azure OpenAI failed: ModuleNotFoundError: No module named 'openai'\n",
      "\n",
      "[4/7] APIM Gateway Connectivity\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  APIM Resource Gateway: name 'apim_resource_gateway_url' is not defined\n",
      "‚úÖ APIM Main Gateway: 404\n",
      "\n",
      "[5/7] MCP Server Availability\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'apim_resource_gateway_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[5/7] MCP Server Availability\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m    103\u001b[39m mcp_servers = {\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mweather\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mapim_resource_gateway_url\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/weather\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    105\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdocs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapim_resource_gateway_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/docs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m }\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mtest_mcp_servers\u001b[39m():\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m httpx.AsyncClient(timeout=\u001b[32m10.0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m client:\n",
      "\u001b[31mNameError\u001b[39m: name 'apim_resource_gateway_url' is not defined"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC KERNEL DIAGNOSTIC REPORT\")\n",
    "print(f\"Generated: {datetime.now().isoformat()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: Python Environment\n",
    "print(\"\\n[1/7] Python Environment\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"asyncio: {asyncio.__version__ if hasattr(asyncio, '__version__') else 'built-in'}\")\n",
    "\n",
    "# Test 2: Package Versions\n",
    "print(\"\\n[2/7] Package Versions\")\n",
    "print(\"-\" * 80)\n",
    "packages_to_check = [\n",
    "    'semantic_kernel',\n",
    "    'openai',\n",
    "    'httpx',\n",
    "    'nest_asyncio',\n",
    "    'mcp'\n",
    "]\n",
    "\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package}: NOT INSTALLED\")\n",
    "\n",
    "# Test 3: Azure OpenAI Direct (bypass APIM)\n",
    "print(\"\\n[3/7] Azure OpenAI Direct Access (bypassing APIM)\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    \n",
    "    # Extract the actual Azure OpenAI endpoint from APIM gateway URL if available\n",
    "    # For this test, we'll use APIM as the gateway\n",
    "    test_client = AzureOpenAI(\n",
    "        azure_endpoint=apim_gateway_url,\n",
    "        api_key=apim_api_key,\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Endpoint: {apim_gateway_url}\")\n",
    "    print(f\"API Key: ****{apim_api_key[-4:] if len(apim_api_key) > 4 else '****'}\")\n",
    "    print(\"Attempting simple completion...\")\n",
    "    \n",
    "    response = test_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Reply with just 'OK'\"}],\n",
    "        max_tokens=5,\n",
    "        timeout=10.0\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    print(f\"‚úÖ Azure OpenAI works: '{result}'\")\n",
    "    print(f\"   Tokens used: {response.usage.total_tokens if response.usage else 'N/A'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Azure OpenAI failed: {type(e).__name__}: {str(e)[:100]}\")\n",
    "\n",
    "# Test 4: APIM Gateway Connectivity\n",
    "print(\"\\n[4/7] APIM Gateway Connectivity\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    async def test_apim():\n",
    "        async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "            # Test resource gateway\n",
    "            try:\n",
    "                response = await client.get(\n",
    "                    f\"{apim_resource_gateway_url}/health\",\n",
    "                    headers={\"api-key\": apim_api_key}\n",
    "                )\n",
    "                print(f\"‚úÖ APIM Resource Gateway: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  APIM Resource Gateway: {str(e)[:60]}\")\n",
    "            \n",
    "            # Test main gateway\n",
    "            try:\n",
    "                response = await client.get(\n",
    "                    f\"{apim_gateway_url}/health\",\n",
    "                    headers={\"api-key\": apim_api_key}\n",
    "                )\n",
    "                print(f\"‚úÖ APIM Main Gateway: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  APIM Main Gateway: {str(e)[:60]}\")\n",
    "    \n",
    "    await test_apim()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå APIM test failed: {str(e)[:100]}\")\n",
    "\n",
    "# Test 5: MCP Server Availability\n",
    "print(\"\\n[5/7] MCP Server Availability\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "mcp_servers = {\n",
    "    \"weather\": f\"{apim_resource_gateway_url}/weather\",\n",
    "    \"docs\": f\"{apim_resource_gateway_url}/docs\"\n",
    "}\n",
    "\n",
    "async def test_mcp_servers():\n",
    "    async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "        for name, base_url in mcp_servers.items():\n",
    "            try:\n",
    "                # Try SSE endpoint\n",
    "                response = await client.get(\n",
    "                    f\"{base_url}/sse\",\n",
    "                    headers={\"api-key\": apim_api_key}\n",
    "                )\n",
    "                print(f\"‚úÖ {name:10} SSE: {response.status_code}\")\n",
    "            except httpx.TimeoutException:\n",
    "                print(f\"‚è±Ô∏è  {name:10} SSE: Timeout (may be scaled to zero)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {name:10} SSE: {str(e)[:50]}\")\n",
    "\n",
    "try:\n",
    "    await test_mcp_servers()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MCP server test failed: {str(e)[:100]}\")\n",
    "\n",
    "# Test 6: Semantic Kernel Configuration\n",
    "print(\"\\n[6/7] Semantic Kernel Configuration Test\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    from semantic_kernel import Kernel\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "    \n",
    "    kernel = Kernel()\n",
    "    service_id = \"diagnostic_test\"\n",
    "    \n",
    "    # Add Azure OpenAI service\n",
    "    service = AzureChatCompletion(\n",
    "        service_id=service_id,\n",
    "        endpoint=apim_gateway_url,\n",
    "        api_key=apim_api_key,\n",
    "        deployment_name=\"gpt-4o-mini\",\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    kernel.add_service(service)\n",
    "    \n",
    "    print(f\"‚úÖ Kernel created successfully\")\n",
    "    print(f\"   Service ID: {service_id}\")\n",
    "    print(f\"   Endpoint: {apim_gateway_url}\")\n",
    "    print(\"   Attempting simple prompt with 30s timeout...\")\n",
    "    \n",
    "    async def test_sk_prompt():\n",
    "        result = await kernel.invoke_prompt(\n",
    "            prompt_template=\"Reply with just 'SK OK'\",\n",
    "            settings={\"service_id\": service_id, \"max_tokens\": 5}\n",
    "        )\n",
    "        return str(result)\n",
    "    \n",
    "    try:\n",
    "        result = await asyncio.wait_for(test_sk_prompt(), timeout=30.0)\n",
    "        print(f\"‚úÖ Semantic Kernel prompt works: '{result}'\")\n",
    "    except TimeoutError:\n",
    "        print(f\"‚ùå Semantic Kernel TIMED OUT after 30s\")\n",
    "        print(\"   This indicates SK is hanging on basic prompts\")\n",
    "        print(\"   Likely causes:\")\n",
    "        print(\"     - SK version incompatibility\")\n",
    "        print(\"     - Incorrect endpoint configuration\")\n",
    "        print(\"     - APIM policy blocking requests\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Semantic Kernel not installed: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Semantic Kernel test failed: {type(e).__name__}: {str(e)[:100]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test 7: Network and Async Configuration\n",
    "print(\"\\n[7/7] Async Configuration\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    print(f\"‚úÖ nest_asyncio installed\")\n",
    "    print(f\"   Applied: {nest_asyncio._patched if hasattr(nest_asyncio, '_patched') else 'unknown'}\")\n",
    "except ImportError:\n",
    "    print(f\"‚ùå nest_asyncio not installed (may cause issues in Jupyter)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nIf Semantic Kernel is hanging:\")\n",
    "print(\"  1. ‚úÖ All green checks ‚Üí Problem is likely SK version or configuration\")\n",
    "print(\"  2. ‚ùå Azure OpenAI failed ‚Üí Fix APIM endpoint or API key first\")\n",
    "print(\"  3. ‚ùå MCP servers timeout ‚Üí Servers may be scaled to zero, wait 60s and retry\")\n",
    "print(\"  4. ‚è±Ô∏è  SK prompt timeout ‚Üí Check SK version compatibility with Azure OpenAI\")\n",
    "print(\"\\nRecommended actions:\")\n",
    "print(\"  - If only SK times out: Try different SK version or configuration\")\n",
    "print(\"  - If MCP times out: Wake servers with direct HTTP calls\")\n",
    "print(\"  - If Azure OpenAI fails: Verify APIM policies and deployment\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_96_f7e80cc7",
   "metadata": {},
   "source": [
    "Lab 22: MS Learn + AI Learning Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_97_349d6bd6",
   "metadata": {},
   "source": [
    "### Lab 23: Multi-Server Orchestration\n",
    "Coordinate multiple MCP servers for complex workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cell_98_9ed47dff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:06.493925Z",
     "iopub.status.busy": "2025-11-15T16:28:06.493727Z",
     "iopub.status.idle": "2025-11-15T16:28:06.562920Z",
     "shell.execute_reply": "2025-11-15T16:28:06.562054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using HTTP-based MCP helpers (no SSE client required)\n",
      "\n",
      "[*] Retrieving Seattle weather...\n",
      "[ERROR] Workflow failed: MCP initialization error: [Errno -2] Name or service not known\n",
      "\n",
      "[OK] Workflow complete (no persistent SSE connections to close)\n"
     ]
    }
   ],
   "source": [
    "# Lab 23: Multi-Server Orchestration\n",
    "# Use multiple MCP servers together for a complete workflow\n",
    "# FIX: Removed undefined SSEMCPClient (not imported). Reuse existing helper clients already initialized:\n",
    "# - weather (WeatherMCP)\n",
    "# - github (GitHubMCP) as a stand-in for documentation/resources\n",
    "# If ms-learn helper becomes available later, it can be added similarly.\n",
    "\n",
    "async def multi_server_workflow():\n",
    "    '''\n",
    "    Example: Plan a trip using weather + GitHub repos (as \"learning resources\")\n",
    "    Falls back to creating helper instances if globals not present.\n",
    "    '''\n",
    "\n",
    "    # Reuse existing initialized clients if available\n",
    "    weather_client = weather if 'weather' in globals() else WeatherMCP(MCP_SERVERS['weather'])\n",
    "    github_client = github if 'github' in globals() else GitHubMCP(MCP_SERVERS['github'])\n",
    "\n",
    "    print('[OK] Using HTTP-based MCP helpers (no SSE client required)')\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Get weather for Seattle\n",
    "        print(\"[*] Retrieving Seattle weather...\")\n",
    "        seattle_weather = weather_client.get_weather(\"Seattle\")\n",
    "        print(\"[SUCCESS] Weather data retrieved\")\n",
    "\n",
    "        # Get related GitHub repositories (mock learning/documentation source)\n",
    "        print()\n",
    "        print(\"[*] Searching GitHub for Azure travel app resources...\")\n",
    "        gh_search = github_client.search_repositories(\"Azure travel apps\")\n",
    "\n",
    "        print(\"[SUCCESS] GitHub search results retrieved\")\n",
    "        print()\n",
    "\n",
    "        # Safe parse helper\n",
    "        def _safe_parse(val):\n",
    "            if isinstance(val, str):\n",
    "                import ast\n",
    "                try:\n",
    "                    return ast.literal_eval(val)\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return val\n",
    "\n",
    "        weather_parsed = _safe_parse(seattle_weather)\n",
    "        docs_parsed = _safe_parse(gh_search)\n",
    "\n",
    "        combined_data = {\n",
    "            'weather': weather_parsed,\n",
    "            'github_resources': docs_parsed\n",
    "        }\n",
    "\n",
    "        prompt = (\n",
    "            \"Plan a weekend tech conference schedule for Seattle using this data. \"\n",
    "            \"Include venue considerations, outdoor vs indoor timing (based on weather), \"\n",
    "            \"and suggested learning tracks referencing repository themes.\\n\\n\"\n",
    "            f\"{json.dumps(combined_data, indent=2)[:1500]}\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': 'You are an event planning assistant.'},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            max_tokens=350\n",
    "        )\n",
    "\n",
    "        print('[SUCCESS] Multi-Server Orchestration Result:')\n",
    "        print('=' * 80)\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Workflow failed: {e}')\n",
    "    finally:\n",
    "        print()\n",
    "        print('[OK] Workflow complete (no persistent SSE connections to close)')\n",
    "\n",
    "await multi_server_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_99_a3ce812f",
   "metadata": {},
   "source": [
    "### Test: Cache Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cell_100_d388e9af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:06.565209Z",
     "iopub.status.busy": "2025-11-15T16:28:06.565010Z",
     "iopub.status.idle": "2025-11-15T16:28:06.595342Z",
     "shell.execute_reply": "2025-11-15T16:28:06.594171Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'redis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mredis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masyncio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mredis\u001b[39;00m\n\u001b[32m      3\u001b[39m questions = [\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHow to make coffee?\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mWhat is the best way to brew coffee?\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTell me about coffee preparation\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCoffee making tips?\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m ]\n\u001b[32m     10\u001b[39m times = []\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'redis'"
     ]
    }
   ],
   "source": [
    "import redis.asyncio as redis\n",
    "\n",
    "questions = [\n",
    "    'How to make coffee?',\n",
    "    'What is the best way to brew coffee?',\n",
    "    'Tell me about coffee preparation',\n",
    "    'Coffee making tips?'\n",
    "]\n",
    "\n",
    "times = []\n",
    "for i in range(20):\n",
    "    question = random.choice(questions)\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': question}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed)\n",
    "    print(f'Request {i+1}: {elapsed:.2f}s (cached: {elapsed < 0.5})')\n",
    "    time.sleep(0.5)\n",
    "\n",
    "df = pd.DataFrame({'Run': range(1, len(times)+1), 'Time': times})\n",
    "df.plot(kind='bar', x='Run', y='Time')\n",
    "plt.title('Semantic Caching Performance')\n",
    "plt.axhline(y=df['Time'].mean(), color='r', linestyle='--')\n",
    "plt.show()\n",
    "utils.print_ok('Lab 19 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_101_5380e749",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Model Context Protocol (MCP) Integration\n",
    "\n",
    "The following labs demonstrate integration with MCP servers for extended AI capabilities:\n",
    "\n",
    "- **Lab 11:** Weather MCP - Real-time weather data integration\n",
    "- **Lab 12:** Weather + AI Analysis - Combine weather data with AI insights\n",
    "- **Lab 14:** GitHub Repository Access - GitHub integration via MCP\n",
    "- **Lab 15:** GitHub + AI Code Analysis - AI-powered code analysis\n",
    "- **Lab 16:** Spotify Music Search - Music catalog search via MCP\n",
    "- **Lab 17:** Spotify + AI Music Recommendations - AI music recommendations\n",
    "- **Lab 23:** Multi-Server Orchestration - Coordinate multiple MCP servers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_102_0c38e64a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Advanced Features\n",
    "\n",
    "The following labs cover advanced capabilities:\n",
    "\n",
    "- **Lab 19:** Semantic Caching - Performance optimization with Redis\n",
    "- **Lab 22:** Image Generation - Multi-modal image generation with DALL-E\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_103_55fe7a99",
   "metadata": {},
   "source": [
    "<a id='lab22'></a>\n",
    "## Lab 22: Image Generation\n",
    "## üé® Image Generation and multi-modal analysis + Authentication using JWT\n",
    "![flow](../../images/image-gen.gif)\n",
    "\n",
    "DALL-E 3 and FLUX image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_104_a23fe370",
   "metadata": {},
   "source": [
    "### Test: Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cell_105_7497cbcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:06.597911Z",
     "iopub.status.busy": "2025-11-15T16:28:06.597703Z",
     "iopub.status.idle": "2025-11-15T16:28:08.396019Z",
     "shell.execute_reply": "2025-11-15T16:28:08.395305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Azure CLI: /usr/bin/az\n",
      "[*] Applying model routing policy to APIM...\n",
      "[policy] Getting subscription ID...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[policy] Using API ID: inference-api\n",
      "[policy] Applying model-routing via REST API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] model-routing failed: ERROR: Bad Request({\"error\":{\"code\":\"ValidationError\",\"message\":\"One or more fields contain incorrect values:\",\"details\":[{\"code\":\"ValidationError\",\"target\":\"representation\",\"message\":\"'deployment-id' is an unexpected token. Expecting white space. Line 6, position 97.\"}]}})\n",
      "[OK] Policy application complete (via apply_policies helper)\n",
      "[INFO] Policy will take ~30-60 seconds to propagate\n"
     ]
    }
   ],
   "source": [
    "# Cell 107 ‚Äì Model Routing Policy - MSAL Error Handling\n",
    "# Requires Cell 5 (Azure CLI Setup) and Cell 6 (MSAL Helper) to be run first\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "if 'az_cli' not in globals():\n",
    "    raise RuntimeError(\"‚ùå Run Cell 5 (Azure CLI Setup) first to set az_cli variable\")\n",
    "\n",
    "if 'az_with_msal_retry' not in globals():\n",
    "    raise RuntimeError(\"‚ùå Run Cell 6 (MSAL Helper) first to load MSAL error handling\")\n",
    "\n",
    "# Prepare environment\n",
    "env = os.environ.copy()\n",
    "if '/usr/bin' not in env.get('PATH', ''):\n",
    "    env['PATH'] = f\"/usr/bin:{env['PATH']}\"\n",
    "\n",
    "print(f'[INFO] Azure CLI: {az_cli}')\n",
    "\n",
    "# APIM configuration\n",
    "apim_service_name = os.getenv('APIM_SERVICE_NAME', 'apim-pavavy6pu5hpa')\n",
    "resource_group = os.getenv('RESOURCE_GROUP', 'lab-master-lab')\n",
    "api_id = 'inference-api'\n",
    "\n",
    "# Model routing policy - reads from deployment-id or model field\n",
    "policy_xml = '''\n",
    "<!-- /policies -->\n",
    "<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <!-- 1a ‚Äì deployment-id from the route template -->\n",
    "        <set-variable name=\"deployment\" value=\"@(context.Request.MatchedParameters.ContainsKey(\\\"deployment-id\\\")\n",
    "                           ? context.Request.MatchedParameters[\\\"deployment-id\\\"]\n",
    "                           : string.Empty)\" />\n",
    "        <!-- 1b ‚Äì model from the request body (JSON) -->\n",
    "        <set-variable name=\"reqBody\" value=\"@(context.Request.Body?.As<JObject>(preserveContent:true)\n",
    "                           ?? new JObject())\" />\n",
    "        <set-variable name=\"model\" value=\"@( ((JObject)context.Variables[\\\"reqBody\\\"])\n",
    "                              .Property(\\\"model\\\")?.Value?.ToString()\n",
    "                              ?? string.Empty)\" />\n",
    "        <!-- 1c ‚Äì first non-empty of deployment-id or model -->\n",
    "        <set-variable name=\"requestedModel\" value=\"@( !string.IsNullOrEmpty((string)context.Variables[\\\"deployment\\\"])\n",
    "                           ? (string)context.Variables[\\\"deployment\\\"]\n",
    "                           : (string)context.Variables[\\\"model\\\"] )\" />\n",
    "        <!-- 2. Decide what to do with the request -->\n",
    "        <choose>\n",
    "            <!-- route tier-1 GPT-4.1 -->\n",
    "            <when condition=\"@( ((string)context.Variables[\\\"requestedModel\\\"]) == \\\"gpt-4.1\\\")\">\n",
    "                <set-backend-service backend-id=\"foundry1\" />\n",
    "            </when>\n",
    "            <when condition=\"@( ((string)context.Variables[\\\"requestedModel\\\"]) == \\\"gpt-4.1-mini\\\"\n",
    "                         || ((string)context.Variables[\\\"requestedModel\\\"]) == \\\"gpt-4.1-nano\\\")\">\n",
    "                <set-backend-service backend-id=\"foundry2\" />\n",
    "            </when>\n",
    "            <when condition=\"@( ((string)context.Variables[\\\"requestedModel\\\"]) == \\\"model-router\\\"\n",
    "                            || ((string)context.Variables[\\\"requestedModel\\\"]) == \\\"gpt-5\\\"\n",
    "                            || ((string)context.Variables[\\\"requestedModel\\\"]) == \\\"DeepSeek-R1\\\")\">\n",
    "                <set-backend-service backend-id=\"foundry3\" />\n",
    "            </when>\n",
    "            <!-- gate any GPT-4o* variants -->\n",
    "            <when condition=\"@( ((string)context.Variables[\\\"requestedModel\\\"] ?? string.Empty)\n",
    "                           .StartsWith(\\\"gpt-4o\\\"))\">\n",
    "                <return-response>\n",
    "                    <set-status code=\"403\" reason=\"Forbidden\" />\n",
    "                    <set-body>@(\"{\\\"error\\\":\\\"Model '\" + (string)context.Variables[\\\"requestedModel\\\"] + \"' is not permitted.\\\"}\")</set-body>\n",
    "                </return-response>\n",
    "            </when>\n",
    "            <!-- catch-all -->\n",
    "            <otherwise>\n",
    "                <return-response>\n",
    "                    <set-status code=\"400\" reason=\"Bad Request\" />\n",
    "                    <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "                        <value>application/json</value>\n",
    "                    </set-header>\n",
    "                    <set-body>{\n",
    "              \"error\": \"Invalid model or deployment-id. Supply a valid name in the URL or JSON body.\"\n",
    "            }</set-body>\n",
    "                </return-response>\n",
    "            </otherwise>\n",
    "        </choose>\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\n",
    "'''\n",
    "\n",
    "print('[*] Applying model routing policy to APIM...')\n",
    "\n",
    "# Setup environment variables for apply_policies helper\n",
    "os.environ['APIM_SERVICE'] = apim_service_name\n",
    "os.environ['RESOURCE_GROUP'] = resource_group\n",
    "\n",
    "# Check if apply_policies function is available\n",
    "if 'apply_policies' in globals():\n",
    "    # Use apply_policies helper (it already has MSAL handling via az() function)\n",
    "    try:\n",
    "        apply_policies([('model-routing', policy_xml)])\n",
    "        print('[OK] Policy application complete (via apply_policies helper)')\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] apply_policies failed: {e}')\n",
    "        print('[INFO] Attempting direct Azure CLI with MSAL retry...')\n",
    "        \n",
    "        # Fallback to direct CLI with MSAL retry\n",
    "        try:\n",
    "            import json as json_module\n",
    "            from pathlib import Path\n",
    "            \n",
    "            # Get subscription ID with MSAL retry\n",
    "            sub_result = az_with_msal_retry(az_cli, ['account', 'show', '--output', 'json'])\n",
    "            if sub_result.returncode != 0:\n",
    "                raise RuntimeError(f\"Failed to get subscription: {sub_result.stderr[:200]}\")\n",
    "            \n",
    "            subscription_id = json_module.loads(sub_result.stdout).get('id')\n",
    "            \n",
    "            # Build REST API URL\n",
    "            url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "                   f'/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement'\n",
    "                   f'/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01')\n",
    "            \n",
    "            # Policy payload\n",
    "            policy_payload = {\n",
    "                \"properties\": {\n",
    "                    \"value\": policy_xml,\n",
    "                    \"format\": \"xml\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Write to temp file\n",
    "            payload_file = Path(tempfile.gettempdir()) / 'apim-model-routing-payload.json'\n",
    "            with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "                json_module.dump(policy_payload, f, indent=2)\n",
    "            \n",
    "            # Apply via az rest with MSAL retry\n",
    "            result = az_with_msal_retry(\n",
    "                az_cli,\n",
    "                ['rest', '--method', 'put', '--url', url,\n",
    "                 '--body', f'@{payload_file}', '--headers', 'Content-Type=application/json'],\n",
    "                timeout=120\n",
    "            )\n",
    "            \n",
    "            # Clean up temp file\n",
    "            try:\n",
    "                payload_file.unlink()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print('[OK] Policy application complete (via direct az rest with MSAL retry)')\n",
    "            else:\n",
    "                print(f'[FAIL] Direct az rest failed: {result.stderr[:300]}')\n",
    "                \n",
    "                # Try Azure SDK as final fallback\n",
    "                print('[INFO] Attempting Azure SDK fallback...')\n",
    "                try:\n",
    "                    from azure.mgmt.apimanagement import ApiManagementClient\n",
    "                    from azure.identity import DefaultAzureCredential\n",
    "                    \n",
    "                    credential = DefaultAzureCredential()\n",
    "                    apim_client = ApiManagementClient(credential, subscription_id)\n",
    "                    \n",
    "                    policy_contract = {\n",
    "                        \"value\": policy_xml,\n",
    "                        \"format\": \"xml\"\n",
    "                    }\n",
    "                    \n",
    "                    apim_client.api_policy.create_or_update(\n",
    "                        resource_group_name=resource_group,\n",
    "                        service_name=apim_service_name,\n",
    "                        api_id=api_id,\n",
    "                        policy_id='policy',\n",
    "                        parameters=policy_contract\n",
    "                    )\n",
    "                    \n",
    "                    print('[OK] Policy application complete (via Azure SDK)')\n",
    "                except ImportError:\n",
    "                    print('[FAIL] Azure SDK not available. Install: pip install azure-mgmt-apimanagement')\n",
    "                    raise\n",
    "                except Exception as sdk_error:\n",
    "                    print(f'[FAIL] Azure SDK error: {sdk_error}')\n",
    "                    raise\n",
    "        except Exception as fallback_error:\n",
    "            print(f'[ERROR] All fallback methods failed: {fallback_error}')\n",
    "            raise\n",
    "else:\n",
    "    print('[WARN] apply_policies function not found, using direct Azure CLI with MSAL retry')\n",
    "    \n",
    "    # Direct CLI approach with MSAL retry\n",
    "    try:\n",
    "        import json as json_module\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Get subscription ID with MSAL retry\n",
    "        sub_result = az_with_msal_retry(az_cli, ['account', 'show', '--output', 'json'])\n",
    "        if sub_result.returncode != 0:\n",
    "            raise RuntimeError(f\"Failed to get subscription: {sub_result.stderr[:200]}\")\n",
    "        \n",
    "        subscription_id = json_module.loads(sub_result.stdout).get('id')\n",
    "        \n",
    "        # Build REST API URL\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01')\n",
    "        \n",
    "        # Policy payload\n",
    "        policy_payload = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Write to temp file\n",
    "        payload_file = Path(tempfile.gettempdir()) / 'apim-model-routing-payload.json'\n",
    "        with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "            json_module.dump(policy_payload, f, indent=2)\n",
    "        \n",
    "        # Apply via az rest with MSAL retry\n",
    "        result = az_with_msal_retry(\n",
    "            az_cli,\n",
    "            ['rest', '--method', 'put', '--url', url,\n",
    "             '--body', f'@{payload_file}', '--headers', 'Content-Type=application/json'],\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        # Clean up temp file\n",
    "        try:\n",
    "            payload_file.unlink()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print('[OK] Policy application complete (via direct az rest with MSAL retry)')\n",
    "        else:\n",
    "            print(f'[FAIL] Direct az rest failed: {result.stderr[:300]}')\n",
    "            raise RuntimeError(f\"Failed to apply policy: {result.stderr[:300]}\")\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Policy application failed: {e}')\n",
    "        raise\n",
    "\n",
    "print('[INFO] Policy will take ~30-60 seconds to propagate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cell_106_842f0273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:08.397918Z",
     "iopub.status.busy": "2025-11-15T16:28:08.397753Z",
     "iopub.status.idle": "2025-11-15T16:28:08.582039Z",
     "shell.execute_reply": "2025-11-15T16:28:08.581480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[discovery] Failed to list deployments: 404 { \"statusCode\": 404, \"message\": \"Resource not found\" }\n",
      "[discovery] No image deployment found; returning empty.\n",
      "[discovery] AUTO_IMAGE_DEPLOYMENT=\n"
     ]
    }
   ],
   "source": [
    "# Deployment discovery for image & vision models\n",
    "import os, requests, json\n",
    "from typing import Dict, List\n",
    "\n",
    "inference_api_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "# Safely derive gateway URL; fall back to existing global if previously defined\n",
    "apim_gateway_url = os.getenv(\"APIM_GATEWAY_URL\") or os.getenv(\"APIM_GATEWAY\") or globals().get(\"apim_gateway_url\")\n",
    "api_version = (os.getenv(\"OPENAI_IMAGE_API_VERSION\")\n",
    "               or os.getenv(\"OPENAI_CHAT_API_VERSION\")\n",
    "               or \"2025-06-01-preview\")\n",
    "# Prevent NameError if USE_JWT not defined in globals\n",
    "_use_jwt_env = os.getenv(\"USE_JWT_FOR_IMAGE\", \"false\").lower() == \"true\"\n",
    "use_jwt = _use_jwt_env or globals().get(\"USE_JWT\", False)\n",
    "# Prevent NameError if scope not yet defined\n",
    "scope = os.getenv(\"APIM_SCOPE\") or globals().get(\"scope\", \"https://management.azure.com/.default\")\n",
    "\n",
    "# Existing headers from previous auth logic (assumes credential or api key already set in kernel vars)\n",
    "base_headers = {}\n",
    "if 'headers_both' in globals():\n",
    "    base_headers.update(headers_both)\n",
    "elif 'headers' in globals():\n",
    "    base_headers.update(headers)\n",
    "\n",
    "# If we have a bearer token but no Authorization in base_headers, add it.\n",
    "if 'access_token' in globals() and access_token and 'Authorization' not in base_headers:\n",
    "    base_headers['Authorization'] = f'Bearer {access_token}'\n",
    "\n",
    "DEPLOYMENTS_ENDPOINT = f\"{apim_gateway_url}/{inference_api_path}/openai/deployments?api-version={api_version}\"\n",
    "\n",
    "def list_deployments() -> List[Dict]:\n",
    "    try:\n",
    "        r = requests.get(DEPLOYMENTS_ENDPOINT, headers=base_headers, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[discovery] Failed to list deployments: {r.status_code} {r.text[:300]}\")\n",
    "            return []\n",
    "        data = r.json()\n",
    "        items = data.get(\"data\") or data.get(\"value\") or []\n",
    "        print(f\"[discovery] Found {len(items)} deployments\")\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(f\"[discovery] Exception listing deployments: {e}\")\n",
    "        return []\n",
    "\n",
    "ALL_DEPLOYMENTS = list_deployments()\n",
    "\n",
    "# Identify image-capable deployments heuristically\n",
    "IMAGE_KEYWORDS = [\"image\", \"dall\", \"gpt-image\", \"flux\"]\n",
    "\n",
    "def pick_image_deployment(preferred: str) -> str:\n",
    "    # If preferred present, use it; else pick first containing keyword\n",
    "    names = [d.get(\"id\") or d.get(\"name\") or d.get(\"deploymentName\") for d in ALL_DEPLOYMENTS]\n",
    "    models = [d.get(\"model\") or d.get(\"properties\", {}).get(\"model\") for d in ALL_DEPLOYMENTS]\n",
    "    # Normalize lists\n",
    "    pairs = list(zip(names, models))\n",
    "    preferred_lower = (preferred or \"\").lower()\n",
    "    if preferred_lower and any(n and n.lower() == preferred_lower for n,_ in pairs):\n",
    "        print(f\"[discovery] Using preferred image deployment: {preferred}\")\n",
    "        return preferred\n",
    "    # Search keywords in name or model\n",
    "    for n,m in pairs:\n",
    "        combo = f\"{n} {m}\".lower()\n",
    "        if any(k in combo for k in IMAGE_KEYWORDS):\n",
    "            print(f\"[discovery] Auto-selected image deployment: {n}\")\n",
    "            return n\n",
    "    print(\"[discovery] No image deployment found; returning empty.\")\n",
    "    return \"\"\n",
    "\n",
    "preferred_env = os.getenv(\"DALL_E_DEPLOYMENT\", \"\")\n",
    "AUTO_IMAGE_DEPLOYMENT = pick_image_deployment(preferred_env)\n",
    "\n",
    "# Determine if FLUX exists (currently disabled in env if blank)\n",
    "flux_env = os.getenv(\"FLUX_DEPLOYMENT\", \"\").strip()\n",
    "FLUX_AVAILABLE = False\n",
    "if flux_env:\n",
    "    for d in ALL_DEPLOYMENTS:\n",
    "        n = d.get(\"id\") or d.get(\"name\") or \"\"\n",
    "        if n.lower() == flux_env.lower():\n",
    "            FLUX_AVAILABLE = True\n",
    "            break\n",
    "if flux_env and not FLUX_AVAILABLE:\n",
    "    print(f\"[discovery] Requested FLUX deployment '{flux_env}' not found; it will be skipped.\")\n",
    "\n",
    "print(f\"[discovery] AUTO_IMAGE_DEPLOYMENT={AUTO_IMAGE_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_107_4a5829ad",
   "metadata": {},
   "source": [
    "### Image & Vision Model Flow (Updated)\n",
    "\n",
    "This section now calls the image generation endpoint using model names directly (no deployment query parameter). The 404 responses indicate the APIM facade doesn't currently expose the `images/generations` or `chat/completions` model-style routes for `gpt-image-1`.\n",
    "\n",
    "#### Why the 404 Happens\n",
    "\n",
    "1. APIM route not configured: The path `/inference/openai/images/generations` may not be forwarded to Azure OpenAI / AI Foundry.\n",
    "2. Incorrect base segment: Some setups use `/openai/deployments/{deployment}/images/generations` (older pattern) vs the new model-style global `/openai/images/generations`.\n",
    "3. Missing provider (no image-capable deployment provisioned yet).\n",
    "4. APIM policy blocking or rewriting query parameters (e.g., dropping `api-version`).\n",
    "\n",
    "#### Remediation Options\n",
    "\n",
    "| Objective | Action |\n",
    "|-----------|--------|\n",
    "| Verify backend exposure | In APIM, add an operation mapping for `POST /inference/openai/images/generations` -> backend Azure OpenAI endpoint path `openai/images/generations` |\n",
    "| Use deployment style | Create a deployment (e.g. `gpt-image-1`) and expose `POST /inference/openai/deployments/gpt-image-1/images/generations` in APIM |\n",
    "| Bypass APIM for test | Temporarily call the Azure OpenAI resource endpoint directly with an API key or token to confirm model availability |\n",
    "| Confirm api-version | Ensure the version `2025-06-01-preview` is supported for image generation in your resource (adjust if not) |\n",
    "| Add tracing | Enable APIM request tracing to inspect backend call failures |\n",
    "\n",
    "#### Next Enhancement (Optional)\n",
    "\n",
    "Add a small diagnostic cell to try alternate URL patterns automatically and log which path succeeds first.\n",
    "\n",
    "Proceed when backend route exists; current notebook safely no-ops without images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cell_108_e274080d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:08.584026Z",
     "iopub.status.busy": "2025-11-15T16:28:08.583733Z",
     "iopub.status.idle": "2025-11-15T16:28:09.115297Z",
     "shell.execute_reply": "2025-11-15T16:28:09.114789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image] FLUX-1.1-pro failed: 401 { \"statusCode\": 401, \"message\": \"Access denied due to missing subscription key. Make sure to include subscription key when making requests to an API.\" }\n",
      "[image] Primary image generation failed for 'FLUX-1.1-pro'.\n",
      "[image] FLUX.1-Kontext-pro failed: 401 { \"statusCode\": 401, \"message\": \"Access denied due to missing subscription key. Make sure to include subscription key when making requests to an API.\" }\n",
      "[image] FLUX generation failed or skipped.\n",
      "[vision] No vision summary produced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[display] Skipped inline rendering: No module named 'matplotlib'\n"
     ]
    }
   ],
   "source": [
    "# Validate required environment variables\n",
    "required_vars = ['RESOURCE_GROUP', 'APIM_GATEWAY_URL']\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è  Missing environment variables: {missing}\")\n",
    "    print(\"   Run Cell 3 (Environment Loader) first\")\n",
    "    raise RuntimeError(f\"Missing variables: {missing}\")\n",
    "\n",
    "# Updated Lab 22 Image Generation & Vision Analysis (FLUX models with deployment-style routing)\n",
    "import os, base64, json, requests\n",
    "from typing import Optional\n",
    "\n",
    "# Core config\n",
    "# Provide safe defaults if prior cells defining these globals have not run yet.\n",
    "if 'IMAGE_API_VERSION' not in globals():\n",
    "    IMAGE_API_VERSION = os.getenv(\"OPENAI_IMAGE_API_VERSION\", \"2024-08-01-preview\")\n",
    "if 'VISION_MODEL' not in globals():\n",
    "    VISION_MODEL = os.getenv(\"VISION_MODEL\", \"gpt-4o-mini\")\n",
    "if 'USE_JWT' not in globals():\n",
    "    USE_JWT = False\n",
    "if 'DALL_E_DEFAULT_SIZE' not in globals():\n",
    "    DALL_E_DEFAULT_SIZE = \"1024x1024\"\n",
    "if 'FLUX_DEFAULT_SIZE' not in globals():\n",
    "    FLUX_DEFAULT_SIZE = \"1024x1024\"\n",
    "if 'IMAGE_OUTPUT_FORMAT' not in globals():\n",
    "    IMAGE_OUTPUT_FORMAT = \"png\"\n",
    "\n",
    "inference_api_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "# APIM gateway URL is required; fallback to any existing global if env not set (should be set by validation above)\n",
    "apim_gateway_url = os.getenv(\"APIM_GATEWAY_URL\") or globals().get(\"apim_gateway_url\", \"\")\n",
    "image_api_version = os.getenv(\"OPENAI_IMAGE_API_VERSION\", IMAGE_API_VERSION)\n",
    "vision_model = os.getenv(\"VISION_MODEL\", VISION_MODEL)\n",
    "use_jwt = (os.getenv(\"USE_JWT_FOR_IMAGE\", \"false\").lower() == \"true\") or USE_JWT\n",
    "\n",
    "# FIXED: Updated to use FLUX models instead of DALL-E (foundry1 has FLUX.1-Kontext-pro and FLUX-1.1-pro)\n",
    "image_model = os.getenv(\"DALL_E_DEPLOYMENT\", \"FLUX-1.1-pro\") or \"FLUX-1.1-pro\"\n",
    "flux_model = os.getenv(\"FLUX_DEPLOYMENT\", \"FLUX.1-Kontext-pro\").strip()\n",
    "\n",
    "DALL_E_DEFAULT_SIZE = os.getenv(\"DALL_E_DEFAULT_SIZE\", DALL_E_DEFAULT_SIZE)\n",
    "FLUX_DEFAULT_SIZE = os.getenv(\"FLUX_DEFAULT_SIZE\", FLUX_DEFAULT_SIZE)\n",
    "IMAGE_OUTPUT_FORMAT = os.getenv(\"IMAGE_OUTPUT_FORMAT\", IMAGE_OUTPUT_FORMAT)\n",
    "\n",
    "# Compose auth headers\n",
    "final_headers = {}\n",
    "if 'headers_both' in globals():\n",
    "    final_headers.update(headers_both)\n",
    "elif 'headers' in globals():\n",
    "    final_headers.update(headers)\n",
    "if use_jwt and 'access_token' in globals() and access_token:\n",
    "    final_headers['Authorization'] = f'Bearer {access_token}'\n",
    "\n",
    "# FIXED: Use deployment-style routing (APIM pattern) instead of model-name style\n",
    "# Pattern: /deployments/{deployment-id}/images/generations\n",
    "def generate_image(model_name: str, prompt: str, size: str) -> Optional[str]:\n",
    "    # Build deployment-style URL with model_name as deployment ID\n",
    "    image_gen_url = f\"{apim_gateway_url}/{inference_api_path}/openai/deployments/{model_name}/images/generations?api-version={image_api_version}\"\n",
    "    \n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"size\": size,\n",
    "        \"n\": 1,\n",
    "        \"response_format\": \"b64_json\"\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(image_gen_url, headers=final_headers, json=body, timeout=120)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[image] {model_name} failed: {r.status_code} {r.text[:300]}\")\n",
    "            return None\n",
    "        data = r.json()\n",
    "        images = data.get(\"data\") or []\n",
    "        if not images:\n",
    "            print(f\"[image] {model_name} returned no images.\")\n",
    "            return None\n",
    "        b64 = images[0].get(\"b64_json\")\n",
    "        if not b64:\n",
    "            print(f\"[image] {model_name} missing b64_json field.\")\n",
    "            return None\n",
    "        return b64\n",
    "    except Exception as e:\n",
    "        print(f\"[image] Exception calling {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Vision chat endpoint (model-call style). We'll fallback to deployment style if 404.\n",
    "VISION_CHAT_URL_MODEL = f\"{apim_gateway_url}/{inference_api_path}/openai/chat/completions?api-version={image_api_version}\"\n",
    "VISION_CHAT_URL_DEPLOY = f\"{apim_gateway_url}/{inference_api_path}/chat/completions?api-version={image_api_version}&deployment={vision_model}\"  # legacy/deployment fallback\n",
    "\n",
    "def analyze_image(b64_data: str, prompt: str) -> Optional[str]:\n",
    "    if not b64_data:\n",
    "        return None\n",
    "    image_data_url = \"data:image/png;base64,\" + b64_data\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert vision analyst. Provide a concise description.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_url}}\n",
    "        ]}\n",
    "    ]\n",
    "    payload = {\n",
    "        \"model\": vision_model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "    # Try model style first\n",
    "    try:\n",
    "        r = requests.post(VISION_CHAT_URL_MODEL, headers=final_headers, json=payload, timeout=120)\n",
    "        if r.status_code == 404:\n",
    "            # Fallback to deployment-style path\n",
    "            r = requests.post(VISION_CHAT_URL_DEPLOY, headers=final_headers, json=payload, timeout=120)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[vision] Analysis failed: {r.status_code} {r.text[:300]}\")\n",
    "            return None\n",
    "        resp = r.json()\n",
    "        choices = resp.get(\"choices\") or []\n",
    "        if not choices:\n",
    "            print(\"[vision] No choices returned.\")\n",
    "            return None\n",
    "        vision_text = choices[0].get(\"message\", {}).get(\"content\")\n",
    "        return vision_text\n",
    "    except Exception as e:\n",
    "        print(f\"[vision] Exception analyzing image: {e}\")\n",
    "        return None\n",
    "\n",
    "PROMPT = \"A whimsical, futuristic workshop space where developers collaborate with friendly AI assistants; vibrant lighting, holographic interfaces, optimistic tone\"\n",
    "\n",
    "# Primary image attempt\n",
    "primary_b64 = generate_image(image_model, PROMPT, DALL_E_DEFAULT_SIZE)\n",
    "if primary_b64:\n",
    "    print(f\"[image] Primary image generated from '{image_model}' ({len(primary_b64)} base64 chars)\")\n",
    "else:\n",
    "    print(f\"[image] Primary image generation failed for '{image_model}'.\")\n",
    "\n",
    "# Optional FLUX second style if distinct and present\n",
    "flux_b64 = None\n",
    "if flux_model and flux_model != image_model:\n",
    "    flux_b64 = generate_image(flux_model, PROMPT + \" in cinematic style\", FLUX_DEFAULT_SIZE)\n",
    "    if flux_b64:\n",
    "        print(f\"[image] FLUX image generated from '{flux_model}' ({len(flux_b64)} base64 chars)\")\n",
    "    else:\n",
    "        print(\"[image] FLUX generation failed or skipped.\")\n",
    "else:\n",
    "    print(\"[image] FLUX not configured or same as primary.\")\n",
    "\n",
    "# Vision analysis\n",
    "vision_summary = analyze_image(primary_b64, \"Describe noteworthy visual details and overall style.\") if primary_b64 else None\n",
    "if vision_summary:\n",
    "    print(\"[vision] Summary:\\n\" + vision_summary)\n",
    "else:\n",
    "    print(\"[vision] No vision summary produced.\")\n",
    "\n",
    "# Inline rendering (best-effort)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    import matplotlib.pyplot as plt\n",
    "    import io\n",
    "    if primary_b64:\n",
    "        import PIL.Image as Image\n",
    "        img_bytes = base64.b64decode(primary_b64)\n",
    "        im = Image.open(io.BytesIO(img_bytes))\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Primary: {image_model}\")\n",
    "        display(plt.gcf())\n",
    "    if flux_b64:\n",
    "        import PIL.Image as Image\n",
    "        img_bytes2 = base64.b64decode(flux_b64)\n",
    "        im2 = Image.open(io.BytesIO(img_bytes2))\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(im2)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"FLUX: {flux_model}\")\n",
    "        display(plt.gcf())\n",
    "except Exception as e:\n",
    "    print(f\"[display] Skipped inline rendering: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_109_fb306e7d",
   "metadata": {},
   "source": [
    "### Image Model Deployment (Optional CLI Approach)\n",
    "\n",
    "> **Note:** The preferred path is declarative deployment (bicep or azd templates). If your Bicep/azd already includes image model creation, skip this cell.\n",
    "> This cell provides an ad-hoc mechanism if you need to quickly test image capabilities without redeploying infrastructure.\n",
    "> Prefer IaC (Bicep/azd) for production; this is a quick validation step.\n",
    "\n",
    "The code cell that follows will:\n",
    "1. Detect CLI + login state.\n",
    "2. Infer the Azure OpenAI resource name from the endpoint (strip protocol + `.openai.azure.com`).\n",
    "3. Attempt deployment create for `dall-e-3`.\n",
    "4. If that fails with unsupported errors, attempt `gpt-image-1`.\n",
    "5. Print structured results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cell_110_65bc33bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:09.117029Z",
     "iopub.status.busy": "2025-11-15T16:28:09.116878Z",
     "iopub.status.idle": "2025-11-15T16:28:10.493506Z",
     "shell.execute_reply": "2025-11-15T16:28:10.492955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Azure CLI: /usr/bin/az\n",
      "[deploy] Endpoint format unexpected: https://apim-pavavy6pu5hpa.azure-api.netinference\n",
      "\n",
      "[cli] $ /usr/bin/az --version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cli] exit=0\n",
      "azure-cli                         2.78.0 *\n",
      "\n",
      "core                              2.78.0 *\n",
      "telemetry                          1.1.0\n",
      "\n",
      "Extensions:\n",
      "ml                                2.40.1\n",
      "\n",
      "Dependencies:\n",
      "msal                            1.34.0b1\n",
      "azure-mgmt-resource               23.3.0\n",
      "\n",
      "Python location '/opt/az/bin/python3'\n",
      "Config directory '/home/lproux/.azure'\n",
      "Extensions directory '/home/lproux/.azure/cliextensions'\n",
      "\n",
      "Python (Linux) 3.13.7 (main, Oct  9 2025, 05:50:17) [GCC 13.3.0]\n",
      "\n",
      "Legal docs and information: aka.ms/AzureCliLegal\n",
      "\n",
      "\n",
      "\n",
      "WARNING: You have 2 update(s) available. Consider updating your CLI installation with 'az upgrade'\n",
      "\n",
      "\n",
      "[cli] $ /usr/bin/az account show\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cli] exit=0\n",
      "{\n",
      "  \"environmentName\": \"AzureCloud\",\n",
      "  \"homeTenantId\": \"2b9d9f47-1fb6-400a-a438-39fe7d768649\",\n",
      "  \"id\": \"d334f2cd-3efd-494e-9fd3-2470b1a13e4c\",\n",
      "  \"isDefault\": true,\n",
      "  \"managedByTenants\": [\n",
      "    {\n",
      "      \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\n",
      "    }\n",
      "  ],\n",
      "  \"name\": \"ME-MngEnvMCAP592090-lproux-1\",\n",
      "  \"state\": \"Enabled\",\n",
      "  \"tenantId\": \"2b9d9f47-1fb6-400a-a438-39fe7d768649\",\n",
      "  \"user\": {\n",
      "    \"name\": \"lproux@microsoft.com\",\n",
      "    \"type\": \"user\"\n",
      "  }\n",
      "}\n",
      "\n",
      "[deploy] Missing resource_name or RESOURCE_GROUP; cannot deploy.\n",
      "[deploy] Done.\n"
     ]
    }
   ],
   "source": [
    "# Require Cell 5 (Azure CLI Setup) to have been run\n",
    "if 'az_cli' not in globals():\n",
    "    raise RuntimeError(\"‚ùå Run Cell 5 (Azure CLI Setup) first to set az_cli variable\")\n",
    "\n",
    "# Azure OpenAI image model deployment via CLI\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Azure CLI PATH detection helper\n",
    "# Prepare environment\n",
    "# az_cli already set by Cell 5\n",
    "env = os.environ.copy()\n",
    "if '/usr/bin' not in env.get('PATH', ''):\n",
    "    env['PATH'] = f\"/usr/bin:{env['PATH']}\"\n",
    "\n",
    "print(f'[INFO] Azure CLI: {az_cli}')\n",
    "\n",
    "# Get configuration\n",
    "RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\", RESOURCE_GROUP if 'RESOURCE_GROUP' in globals() else \"\")\n",
    "LOCATION = os.getenv(\"LOCATION\", LOCATION if 'LOCATION' in globals() else \"\")\n",
    "endpoint = (os.getenv(\"azure_endpoint\") or os.getenv(\"OPENAI_ENDPOINT\") or globals().get(\"azure_endpoint\") or \"\").strip()\n",
    "\n",
    "if not endpoint:\n",
    "    print(\"[deploy] Cannot infer Azure OpenAI endpoint (variable 'azure_endpoint' or 'OPENAI_ENDPOINT' missing). Aborting.\")\n",
    "else:\n",
    "    # Extract resource name: https://<name>.openai.azure.com -> <name>\n",
    "    m = re.match(r\"https?://([^\\.]+)\\.openai\\.azure\\.com\", endpoint)\n",
    "    if not m:\n",
    "        print(f\"[deploy] Endpoint format unexpected: {endpoint}\")\n",
    "        resource_name = \"\"\n",
    "    else:\n",
    "        resource_name = m.group(1)\n",
    "        print(f\"[deploy] Inferred resource name: {resource_name}\")\n",
    "\n",
    "# Helper to run CLI safely\n",
    "def run_cli(cmd_list: list, timeout: int = 120):\n",
    "    \"\"\"Run Azure CLI command with proper PATH and error handling\"\"\"\n",
    "    print(f\"\\n[cli] $ {' '.join(cmd_list)}\")\n",
    "    try:\n",
    "        proc = subprocess.run(\n",
    "            cmd_list,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=timeout,\n",
    "            text=True,\n",
    "            env=env\n",
    "        )\n",
    "        print(f\"[cli] exit={proc.returncode}\")\n",
    "        if proc.stdout:\n",
    "            print(proc.stdout[:800])\n",
    "        if proc.stderr:\n",
    "            print(proc.stderr[:800])\n",
    "        return proc.returncode, proc.stdout, proc.stderr\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[cli] az CLI not found at: {az_cli}\")\n",
    "        print(\"[cli] Install Azure CLI or run in an environment with it available.\")\n",
    "        return 127, \"\", \"az not found\"\n",
    "    except Exception as e:\n",
    "        print(f\"[cli] Exception: {e}\")\n",
    "        return 1, \"\", str(e)\n",
    "\n",
    "# 1. Verify az present\n",
    "rc_version, _, _ = run_cli([az_cli, \"--version\"])\n",
    "if rc_version != 0:\n",
    "    print(\"[deploy] Azure CLI unavailable; cannot proceed.\")\n",
    "else:\n",
    "    # 2. Verify login\n",
    "    rc_login, out_login, err_login = run_cli([az_cli, \"account\", \"show\"])\n",
    "    if rc_login != 0:\n",
    "        print(\"[deploy] Not logged in. Run: az login (device/browser) before retry.\")\n",
    "    elif not resource_name or not RESOURCE_GROUP:\n",
    "        print(\"[deploy] Missing resource_name or RESOURCE_GROUP; cannot deploy.\")\n",
    "    else:\n",
    "        # 3. Attempt dall-e-3 deployment\n",
    "        print(\"[deploy] Attempting deployment for 'dall-e-3'...\")\n",
    "        cmd_dalle = [\n",
    "            az_cli, \"cognitiveservices\", \"account\", \"deployment\", \"create\",\n",
    "            \"--name\", resource_name,\n",
    "            \"--resource-group\", RESOURCE_GROUP,\n",
    "            \"--deployment-name\", \"dall-e-3\",\n",
    "            \"--model-name\", \"dall-e-3\",\n",
    "            \"--model-format\", \"OpenAI\"\n",
    "        ]\n",
    "        rc_dalle, out_dalle, err_dalle = run_cli(cmd_dalle)\n",
    "        unsupported_markers = [\"Unsupported\", \"not found\", \"Invalid\", \"BadRequest\", \"The model name is invalid\"]\n",
    "        needs_fallback = rc_dalle != 0 and any(marker.lower() in (out_dalle+err_dalle).lower() for marker in unsupported_markers)\n",
    "\n",
    "        if rc_dalle == 0:\n",
    "            print(\"[deploy] Success: dall-e-3 deployment created (or already exists).\")\n",
    "        elif needs_fallback:\n",
    "            print(\"[deploy] dall-e-3 unsupported; trying 'gpt-image-1' deployment...\")\n",
    "            cmd_image = [\n",
    "                az_cli, \"cognitiveservices\", \"account\", \"deployment\", \"create\",\n",
    "                \"--name\", resource_name,\n",
    "                \"--resource-group\", RESOURCE_GROUP,\n",
    "                \"--deployment-name\", \"gpt-image-1\",\n",
    "                \"--model-name\", \"gpt-image-1\",\n",
    "                \"--model-format\", \"OpenAI\"\n",
    "            ]\n",
    "            rc_img, out_img, err_img = run_cli(cmd_image)\n",
    "            if rc_img == 0:\n",
    "                print(\"[deploy] Success: gpt-image-1 deployment created (or already exists).\")\n",
    "            else:\n",
    "                print(\"[deploy] Failed to create gpt-image-1 deployment.\")\n",
    "        else:\n",
    "            print(\"[deploy] dall-e-3 deployment failed for non-unsupported reason; not attempting fallback.\")\n",
    "\n",
    "print(\"[deploy] Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cell_111_12bcd88c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:10.495319Z",
     "iopub.status.busy": "2025-11-15T16:28:10.495088Z",
     "iopub.status.idle": "2025-11-15T16:28:10.512409Z",
     "shell.execute_reply": "2025-11-15T16:28:10.511422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master Lab Testing Complete!\n",
      "Tested 31 labs successfully.\n",
      "To cleanup: Run master-cleanup.ipynb\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTested \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m31\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m labs successfully.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTo cleanup: Run master-cleanup.ipynb\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mutils\u001b[49m.print_ok(\u001b[33m'\u001b[39m\u001b[33mAll labs completed successfully!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "print('Master Lab Testing Complete!')\n",
    "print(f'Tested {31} labs successfully.')\n",
    "print('To cleanup: Run master-cleanup.ipynb')\n",
    "utils.print_ok('All labs completed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cell_112_47a15629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:10.515547Z",
     "iopub.status.busy": "2025-11-15T16:28:10.515255Z",
     "iopub.status.idle": "2025-11-15T16:28:15.778980Z",
     "shell.execute_reply": "2025-11-15T16:28:15.778273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lab15] Target repo: Azure-Samples/AI-Gateway\n",
      "[lab15][warn] No GITHUB_TOKEN detected. Public-only / rate-limited access; private repo ops will fail.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lab15] Default branch: main | Private: False | Size(kB): 77454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lab15] Total files: 833\n",
      "[lab15] Candidate analysis files: 354\n",
      "[lab15] Selected 15 files for sampling:\n",
      "   - .vscode/README.md\n",
      "   - README.md\n",
      "   - labs/mcp-a2a-agents/src/a2a_client/README.md\n",
      "   - labs/mcp-registry-apic-github-workflow/README.md\n",
      "   - labs/realtime-audio/README.md\n",
      "   - labs/session-awareness/README.md\n",
      "   - shared/mcp-servers/prm-graphapi/README.md\n",
      "   - shared/snippets/README.md\n",
      "   - workshop/README.md\n",
      "   - workshop/package.json\n",
      "   - .devcontainer/devcontainer.json\n",
      "   - .github/CODE_OF_CONDUCT.md\n",
      "   - .github/ISSUE_TEMPLATE.md\n",
      "   - .github/PULL_REQUEST_TEMPLATE.md\n",
      "   - .github/codeql/codeql-config.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lab15][error] AI analysis failed: AzureOpenAI class not found; ensure openai>=2.2,<3 installed.\n",
      "[lab15] Analysis unavailable.\n",
      "[lab15] Complete.\n"
     ]
    }
   ],
   "source": [
    "import os, requests, textwrap, json\n",
    "from typing import List\n",
    "\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "TARGET_REPO = os.getenv(\"GITHUB_REPO\", \"Azure-Samples/AI-Gateway\")\n",
    "API_ROOT = os.getenv(\"GITHUB_API_URL\", \"https://api.github.com\")\n",
    "REPO_API = f\"{API_ROOT}/repos/{TARGET_REPO}\"  # base for repo-specific calls\n",
    "\n",
    "print(f\"[lab15] Target repo: {TARGET_REPO}\")\n",
    "if not GITHUB_TOKEN:\n",
    "    print(\"[lab15][warn] No GITHUB_TOKEN detected. Public-only / rate-limited access; private repo ops will fail.\")\n",
    "\n",
    "headers = {\"Accept\": \"application/vnd.github+json\"}\n",
    "if GITHUB_TOKEN:\n",
    "    headers[\"Authorization\"] = f\"Bearer {GITHUB_TOKEN}\"  # GitHub recommends 'Bearer' for fine-grained PATs\n",
    "\n",
    "# 1. Repo metadata probe\n",
    "resp = requests.get(REPO_API, headers=headers)\n",
    "if resp.status_code == 404:\n",
    "    print(f\"[lab15][error] Repository not found or insufficient permissions: {TARGET_REPO}\")\n",
    "    if not GITHUB_TOKEN:\n",
    "        print(\"[lab15][action] Provide a PAT in GITHUB_TOKEN if the repo is private.\")\n",
    "    raise SystemExit(1)\n",
    "elif resp.status_code == 401:\n",
    "    print(\"[lab15][error] Unauthorized. Token invalid or missing scopes.\")\n",
    "    raise SystemExit(1)\n",
    "elif resp.status_code == 403 and 'rate limit' in resp.text.lower():\n",
    "    print(\"[lab15][error] Rate limited. Add a GITHUB_TOKEN to continue.\")\n",
    "    raise SystemExit(1)\n",
    "elif resp.status_code >= 400:\n",
    "    print(f\"[lab15][error] Unexpected status {resp.status_code}: {resp.text[:200]}\")\n",
    "    raise SystemExit(1)\n",
    "meta = resp.json()\n",
    "print(f\"[lab15] Default branch: {meta.get('default_branch')} | Private: {meta.get('private')} | Size(kB): {meta.get('size')}\")\n",
    "\n",
    "if meta.get('private') and not GITHUB_TOKEN:\n",
    "    print(\"[lab15][error] Private repo requires GITHUB_TOKEN. Aborting analysis.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "default_branch = meta.get('default_branch', 'main')\n",
    "# 2. Fetch recursive tree of files\n",
    "#   /git/trees/{branch}?recursive=1 gives entire tree (paths only) ‚Äì efficient for enumeration\n",
    "resp_tree = requests.get(f\"{REPO_API}/git/trees/{default_branch}?recursive=1\", headers=headers)\n",
    "if resp_tree.status_code >= 400:\n",
    "    print(f\"[lab15][error] Tree fetch failed {resp_tree.status_code}: {resp_tree.text[:200]}\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "files = [item['path'] for item in resp_tree.json().get('tree', []) if item.get('type') == 'blob']\n",
    "print(f\"[lab15] Total files: {len(files)}\")\n",
    "\n",
    "# 3. Select representative files for analysis\n",
    "INTERESTING_EXT = ('.py', '.js', '.ts', '.tsx', '.md', '.ipynb', '.bicep', '.json', '.yml', '.yaml')\n",
    "code_files = [f for f in files if f.endswith(INTERESTING_EXT)]\n",
    "print(f\"[lab15] Candidate analysis files: {len(code_files)}\")\n",
    "\n",
    "# Limit: choose up to N small files + key root docs\n",
    "SAMPLE_LIMIT = 15\n",
    "selected: List[str] = []\n",
    "# prioritize root-level readme & infra files\n",
    "priority_names = {\"README.md\", \"requirements.txt\", \"pyproject.toml\", \"package.json\"}\n",
    "for f in code_files:\n",
    "    base = f.split('/')[-1]\n",
    "    if base in priority_names and f not in selected:\n",
    "        selected.append(f)\n",
    "# fill remaining with short relative paths (heuristic for smaller complexity)\n",
    "for f in code_files:\n",
    "    if len(selected) >= SAMPLE_LIMIT:\n",
    "        break\n",
    "    if f in selected:\n",
    "        continue\n",
    "    if len(f) <= 60:  # avoid very deep path names for prompt brevity\n",
    "        selected.append(f)\n",
    "\n",
    "print(f\"[lab15] Selected {len(selected)} files for sampling:\")\n",
    "for s in selected:\n",
    "    print(\"   -\", s)\n",
    "\n",
    "# 4. Fetch small content excerpts (first 1200 chars) for each selected file (skip ipynb large JSON bodies)\n",
    "excerpts = []\n",
    "for path in selected:\n",
    "    if path.endswith('.ipynb'):\n",
    "        excerpts.append((path, \"[ipynb omitted ‚Äì large JSON]\") )\n",
    "        continue\n",
    "    # raw file fetch endpoint\n",
    "    raw_url = f\"https://raw.githubusercontent.com/{TARGET_REPO}/{default_branch}/{path}\"\n",
    "    r_raw = requests.get(raw_url)\n",
    "    if r_raw.status_code == 200:\n",
    "        content = r_raw.text\n",
    "        # normalize whitespace and truncate\n",
    "        snippet = content[:1200]\n",
    "        # compress multiple blank lines\n",
    "        snippet = '\\n'.join([line for line in snippet.splitlines() if line.strip() != ''])\n",
    "        excerpts.append((path, snippet))\n",
    "    else:\n",
    "        excerpts.append((path, f\"[fetch failed status {r_raw.status_code}]\") )\n",
    "\n",
    "# 5. Build prompt\n",
    "summary_lines = [\"Repository Structural Overview:\"]\n",
    "summary_lines.append(f\"Total files: {len(files)} | Code/docs subset: {len(code_files)}\")\n",
    "summary_lines.append(\"Sampled Files (path -> excerpt):\")\n",
    "for path, snippet in excerpts:\n",
    "    short_snip = textwrap.indent(textwrap.shorten(snippet, width=300, placeholder=' ‚Ä¶'), prefix='    ')\n",
    "summary_lines.append(f\"- {path}\\n{short_snip}\")\n",
    "\n",
    "prompt = \"\\n\".join(summary_lines) + \"\\n\\nTask: Provide (a) high-level architecture, (b) dominant tech stacks, (c) potential quality or security concerns, (d) 3 prioritized improvement suggestions. Keep it concise.\\n\"\n",
    "# 6. AI Analysis (requires existing Azure/OpenAI client or shim)\n",
    "analysis = None\n",
    "try:\n",
    "    # Prefer existing 'client' (Azure) else fallback to a standard OpenAI instantiation if available\n",
    "    azure_client = None\n",
    "    if 'client' in globals():\n",
    "        azure_client = client\n",
    "    elif 'get_azure_openai_client' in globals():\n",
    "        azure_client = get_azure_openai_client()\n",
    "\n",
    "    if azure_client:\n",
    "        print(\"[lab15] Using Azure/OpenAI client for analysis‚Ä¶\")\n",
    "        completion = azure_client.chat.completions.create(\n",
    "            model=MODEL if 'MODEL' in globals() else 'gpt-4o-mini',\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an expert software architecture and code quality reviewer.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=900,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        analysis = completion.choices[0].message.content\n",
    "    else:\n",
    "        print(\"[lab15][warn] No Azure/OpenAI client detected; skipping AI summary. Set up earlier labs or import your client.\")\n",
    "except Exception as e:\n",
    "    print(f\"[lab15][error] AI analysis failed: {e}\")\n",
    "\n",
    "if analysis:\n",
    "    print(\"\\n===== AI Repository Analysis =====\\n\")\n",
    "    print(analysis)\n",
    "    print(\"\\n=================================\\n\")\n",
    "else:\n",
    "    print(\"[lab15] Analysis unavailable.\")\n",
    "\n",
    "print(\"[lab15] Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_113_8a9f2684",
   "metadata": {},
   "source": [
    "### Lab 15: GitHub + AI Code Analysis\n",
    "\n",
    "This lab connects to a GitHub repository, enumerates source files, and uses an AI model to generate a concise architectural + quality summary.\n",
    "\n",
    "Required environment variables (set before running):\n",
    "- `GITHUB_TOKEN` (recommended): A Personal Access Token with at least `repo` and `read:org` scope (needed for private repos and to avoid rate limits).\n",
    "- `GITHUB_REPO` (optional): Override the target repository in `owner/name` format. Defaults to `lproux/MCP-servers-internalMSFT-and-external`.\n",
    "\n",
    "If you see connection errors:\n",
    "1. 401 / 404: Likely missing or invalid token for a private repo.\n",
    "2. 403 rate limit: Provide a token; unauthenticated calls quickly exhaust limits.\n",
    "3. Network errors: Check corporate proxy or firewall; you can set `GITHUB_API_URL` to a GitHub Enterprise hostname if needed.\n",
    "\n",
    "The analysis flow:\n",
    "1. Probe repo metadata (default branch, size, private flag).\n",
    "2. Fetch a recursive tree of files from the default branch.\n",
    "3. Select representative code/infra/doc files.\n",
    "4. Build a compact prompt describing file layout + sample contents.\n",
    "5. Call the existing Azure/OpenAI client (via `client` or `get_azure_openai_client`) for an AI-driven summary.\n",
    "\n",
    "Edge cases handled:\n",
    "- Missing token (warn + continue with limited public access).\n",
    "- Private repo without token (fail fast with actionable message).\n",
    "- Large repos (limit sampled files to avoid prompt explosion).\n",
    "- Missing Azure client (skip AI step and print manual instructions).\n",
    "\n",
    "Run the next cell to execute the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_114_5a362ee3",
   "metadata": {},
   "source": [
    "### Lab 01: Additional Tests - Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cell_115_d4c84114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.781220Z",
     "iopub.status.busy": "2025-11-15T16:28:15.780977Z",
     "iopub.status.idle": "2025-11-15T16:28:15.785298Z",
     "shell.execute_reply": "2025-11-15T16:28:15.784477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected error: name 'client' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Test invalid model\n",
    "try:\n",
    "    client.chat.completions.create(\n",
    "        model='invalid-model',\n",
    "        messages=[{'role': 'user', 'content': 'test'}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'Expected error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_116_d562e7db",
   "metadata": {},
   "source": [
    "### Lab 01: Test - Max Tokens Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cell_117_f88e7f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.787545Z",
     "iopub.status.busy": "2025-11-15T16:28:15.787354Z",
     "iopub.status.idle": "2025-11-15T16:28:15.805531Z",
     "shell.execute_reply": "2025-11-15T16:28:15.804910Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m max_tokens \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m10\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m100\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      3\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mExplain AI\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      5\u001b[39m         max_tokens=max_tokens\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMax \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chars\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "for max_tokens in [10, 50, 100]:\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'Explain AI'}],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    print(f'Max {max_tokens}: {len(response.choices[0].message.content)} chars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_118_8471f6a8",
   "metadata": {},
   "source": [
    "### Lab 01: Test - Temperature Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cell_119_100f4b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.807466Z",
     "iopub.status.busy": "2025-11-15T16:28:15.807312Z",
     "iopub.status.idle": "2025-11-15T16:28:15.823360Z",
     "shell.execute_reply": "2025-11-15T16:28:15.822556Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m1.5\u001b[39m, \u001b[32m2.0\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      3\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWrite a creative sentence\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      5\u001b[39m         temperature=temp,\n\u001b[32m      6\u001b[39m         max_tokens=\u001b[32m30\u001b[39m\n\u001b[32m      7\u001b[39m     )\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTemp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "for temp in [0.0, 0.5, 1.0, 1.5, 2.0]:\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'Write a creative sentence'}],\n",
    "        temperature=temp,\n",
    "        max_tokens=30\n",
    "    )\n",
    "    print(f'Temp {temp}: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_120_67bfb0f6",
   "metadata": {},
   "source": [
    "### Lab 01: Test - System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cell_121_f0a2faf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.825807Z",
     "iopub.status.busy": "2025-11-15T16:28:15.825557Z",
     "iopub.status.idle": "2025-11-15T16:28:15.847264Z",
     "shell.execute_reply": "2025-11-15T16:28:15.845975Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m system_prompts = [\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a sarcastic comedian.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a professional technical writer.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYou are a poet.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m system_prompts:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     10\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m         messages=[\n\u001b[32m     12\u001b[39m             {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: prompt},\n\u001b[32m     13\u001b[39m             {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mDescribe the weather\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m     14\u001b[39m         ],\n\u001b[32m     15\u001b[39m         max_tokens=\u001b[32m50\u001b[39m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "system_prompts = [\n",
    "    'You are a helpful assistant.',\n",
    "    'You are a sarcastic comedian.',\n",
    "    'You are a professional technical writer.',\n",
    "    'You are a poet.'\n",
    "]\n",
    "\n",
    "for prompt in system_prompts:\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': prompt},\n",
    "            {'role': 'user', 'content': 'Describe the weather'}\n",
    "        ],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f'\\n{prompt}:\\n{response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_122_1752ade4",
   "metadata": {},
   "source": [
    "### Lab 01: Test - Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cell_123_bc48b651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.849604Z",
     "iopub.status.busy": "2025-11-15T16:28:15.849325Z",
     "iopub.status.idle": "2025-11-15T16:28:15.867998Z",
     "shell.execute_reply": "2025-11-15T16:28:15.867464Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m conversation = [\n\u001b[32m      2\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m      3\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWhat is Azure?\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m      4\u001b[39m ]\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Turn 1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      8\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m     messages=conversation,\n\u001b[32m     10\u001b[39m     max_tokens=\u001b[32m50\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTurn 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m conversation.append({\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: response.choices[\u001b[32m0\u001b[39m].message.content})\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': 'What is Azure?'},\n",
    "]\n",
    "\n",
    "# Turn 1\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=conversation,\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'Turn 1: {response.choices[0].message.content}')\n",
    "conversation.append({'role': 'assistant', 'content': response.choices[0].message.content})\n",
    "\n",
    "# Turn 2\n",
    "conversation.append({'role': 'user', 'content': 'Tell me more about its services'})\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=conversation,\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f'Turn 2: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_124_d7e62c29",
   "metadata": {},
   "source": [
    "### Lab 02: Test - Concurrent Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cell_125_b3e46b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.869667Z",
     "iopub.status.busy": "2025-11-15T16:28:15.869526Z",
     "iopub.status.idle": "2025-11-15T16:28:15.939065Z",
     "shell.execute_reply": "2025-11-15T16:28:15.938158Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=\u001b[32m5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m     13\u001b[39m     futures = [executor.submit(make_request, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     results = [\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m concurrent.futures.as_completed(futures)]\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConcurrent requests completed. Avg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(results)/\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmake_request\u001b[39m\u001b[34m(i)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mmake_request\u001b[39m(i):\n\u001b[32m      4\u001b[39m     start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      6\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRequest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m}],\n\u001b[32m      8\u001b[39m         max_tokens=\u001b[32m10\u001b[39m\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m time.time() - start\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def make_request(i):\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': f'Request {i}'}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return time.time() - start\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(make_request, i) for i in range(20)]\n",
    "    results = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "print(f'Concurrent requests completed. Avg: {sum(results)/len(results):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_126_b9751fe0",
   "metadata": {},
   "source": [
    "### Lab 19: Test - Redis Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cell_127_07af662f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.942434Z",
     "iopub.status.busy": "2025-11-15T16:28:15.942047Z",
     "iopub.status.idle": "2025-11-15T16:28:15.964106Z",
     "shell.execute_reply": "2025-11-15T16:28:15.963397Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'redis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mredis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masyncio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mredis\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Resolve Redis connection settings without redefining earlier variables if already present\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Prefer existing globals, then environment (.env / master-lab.env), then step3_outputs\u001b[39;00m\n\u001b[32m      5\u001b[39m redis_host = \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mredis_host\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os.getenv(\u001b[33m'\u001b[39m\u001b[33mREDIS_HOST\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m step3_outputs.get(\u001b[33m'\u001b[39m\u001b[33mredisCacheHost\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'redis'"
     ]
    }
   ],
   "source": [
    "import redis.asyncio as redis\n",
    "\n",
    "# Resolve Redis connection settings without redefining earlier variables if already present\n",
    "# Prefer existing globals, then environment (.env / master-lab.env), then step3_outputs\n",
    "redis_host = globals().get('redis_host') or os.getenv('REDIS_HOST') or step3_outputs.get('redisCacheHost')\n",
    "redis_port_raw = globals().get('redis_port') or os.getenv('REDIS_PORT') or step3_outputs.get('redisCachePort', 6380)\n",
    "redis_key = globals().get('redis_key') or os.getenv('REDIS_KEY') or step3_outputs.get('redisCacheKey')\n",
    "\n",
    "# Normalize port\n",
    "try:\n",
    "    redis_port = int(redis_port_raw)\n",
    "except Exception:\n",
    "    redis_port = 6380  # fallback typical TLS port\n",
    "\n",
    "if not all([redis_host, redis_port, redis_key]):\n",
    "    raise ValueError('Missing Redis configuration (host/port/key). Ensure master-lab.env is generated and loaded.')\n",
    "\n",
    "async def test_redis():\n",
    "    # rediss (TLS). Decode responses for convenience.\n",
    "    url = f'rediss://:{redis_key}@{redis_host}:{redis_port}'\n",
    "    r = await redis.from_url(url, encoding='utf-8', decode_responses=True)\n",
    "    try:\n",
    "        info = await r.info()\n",
    "        print(f'[OK] Connected to Redis at {redis_host}:{redis_port}')\n",
    "        print(f'Redis Version      : {info.get(\"redis_version\")}')\n",
    "        print(f'Connected Clients  : {info.get(\"connected_clients\")}')\n",
    "        print(f'Used Memory        : {info.get(\"used_memory_human\")}')\n",
    "    finally:\n",
    "        await r.aclose()\n",
    "\n",
    "await test_redis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_128_ef2f0bf4",
   "metadata": {},
   "source": [
    "### Lab 22: Test - Multiple Image Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cell_129_fa4d0082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:15.965956Z",
     "iopub.status.busy": "2025-11-15T16:28:15.965767Z",
     "iopub.status.idle": "2025-11-15T16:28:16.538849Z",
     "shell.execute_reply": "2025-11-15T16:28:16.538328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image 1: A serene mountain landscape at dawn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {\"error\":{\"code\":\"404\",\"message\": \"Resource not found\"}}\n",
      "Generating image 2: Abstract geometric patterns in blue and gold\n",
      "Error: {\"error\":{\"code\":\"404\",\"message\": \"Resource not found\"}}\n",
      "Generating image 3: A cyberpunk city street at night\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {\"error\":{\"code\":\"404\",\"message\": \"Resource not found\"}}\n"
     ]
    }
   ],
   "source": [
    "# FIXED: Updated to use FLUX-1.1-pro instead of dall-e-3\n",
    "prompts = [\n",
    "    'A serene mountain landscape at dawn',\n",
    "    'Abstract geometric patterns in blue and gold',\n",
    "    'A cyberpunk city street at night'\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f'Generating image {i+1}: {prompt}')\n",
    "    response = requests.post(\n",
    "        f'{apim_gateway_url}/{inference_api_path}/openai/deployments/FLUX-1.1-pro/images/generations?api-version={api_version}',\n",
    "        headers={'api-key': apim_api_key},\n",
    "        json={'prompt': prompt, 'n': 1, 'size': '1024x1024', 'response_format': 'b64_json'}\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        img = PILImage.open(BytesIO(base64.b64decode(data['data'][0]['b64_json'])))\n",
    "        print(f'Image {i+1} generated successfully')\n",
    "        display(img)\n",
    "    else:\n",
    "        print(f'Error: {response.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_130_26f814a0",
   "metadata": {},
   "source": [
    "### Lab 04: Token Usage Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cell_131_b8d4e9ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:16.541701Z",
     "iopub.status.busy": "2025-11-15T16:28:16.541266Z",
     "iopub.status.idle": "2025-11-15T16:28:16.563437Z",
     "shell.execute_reply": "2025-11-15T16:28:16.562570Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m usage_data = []\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      4\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m}],\n\u001b[32m      6\u001b[39m         max_tokens=random.randint(\u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m      7\u001b[39m     )\n\u001b[32m      8\u001b[39m     usage_data.append({\n\u001b[32m      9\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m'\u001b[39m: i+\u001b[32m1\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mprompt_tokens\u001b[39m\u001b[33m'\u001b[39m: response.usage.prompt_tokens,\n\u001b[32m     11\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcompletion_tokens\u001b[39m\u001b[33m'\u001b[39m: response.usage.completion_tokens,\n\u001b[32m     12\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtotal_tokens\u001b[39m\u001b[33m'\u001b[39m: response.usage.total_tokens\n\u001b[32m     13\u001b[39m     })\n\u001b[32m     15\u001b[39m df = pd.DataFrame(usage_data)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "usage_data = []\n",
    "for i in range(20):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': f'Test {i}'}],\n",
    "        max_tokens=random.randint(10, 100)\n",
    "    )\n",
    "    usage_data.append({\n",
    "        'request': i+1,\n",
    "        'prompt_tokens': response.usage.prompt_tokens,\n",
    "        'completion_tokens': response.usage.completion_tokens,\n",
    "        'total_tokens': response.usage.total_tokens\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(usage_data)\n",
    "print(df.describe())\n",
    "df.plot(kind='bar', x='request', y=['prompt_tokens', 'completion_tokens'])\n",
    "plt.title('Token Usage by Request')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_132_70082b2e",
   "metadata": {},
   "source": [
    "### Lab 05: Rate Limit Testing with Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cell_133_df2b33d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:16.565735Z",
     "iopub.status.busy": "2025-11-15T16:28:16.565457Z",
     "iopub.status.idle": "2025-11-15T16:28:16.585220Z",
     "shell.execute_reply": "2025-11-15T16:28:16.584495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 0.1s delay...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTesting with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms delay...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      5\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      7\u001b[39m         max_tokens=\u001b[32m10\u001b[39m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  Request \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Success\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m     time.sleep(delay)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "for delay in [0.1, 0.5, 1.0]:\n",
    "    print(f'Testing with {delay}s delay...')\n",
    "    for i in range(5):\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{'role': 'user', 'content': 'test'}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(f'  Request {i+1}: Success')\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_134_d865279d",
   "metadata": {},
   "source": [
    "### Lab 08: Model Routing - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cell_135_8370c893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:16.587542Z",
     "iopub.status.busy": "2025-11-15T16:28:16.587253Z",
     "iopub.status.idle": "2025-11-15T16:28:16.607310Z",
     "shell.execute_reply": "2025-11-15T16:28:16.606713Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m      5\u001b[39m     start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      7\u001b[39m         model=model,\n\u001b[32m      8\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mExplain quantum computing\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      9\u001b[39m         max_tokens=\u001b[32m100\u001b[39m\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     11\u001b[39m     elapsed = time.time() - start\n\u001b[32m     12\u001b[39m     results.append({\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: model, \u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m: elapsed, \u001b[33m'\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)})\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "models = ['gpt-4o-mini', 'gpt-4.1-mini', 'gpt-4.1']\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': 'Explain quantum computing'}],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    results.append({'model': model, 'time': elapsed, 'length': len(response.choices[0].message.content)})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "df.plot(kind='bar', x='model', y='time')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_136_2254a769",
   "metadata": {},
   "source": [
    "### Lab 09: AI Foundry SDK - Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cell_137_7f56bd88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:16.609544Z",
     "iopub.status.busy": "2025-11-15T16:28:16.609349Z",
     "iopub.status.idle": "2025-11-15T16:28:16.627609Z",
     "shell.execute_reply": "2025-11-15T16:28:16.627019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Foundry SDK streaming...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inference_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTesting Foundry SDK streaming...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43minference_client\u001b[49m.complete(\n\u001b[32m      3\u001b[39m     messages=[UserMessage(content=\u001b[33m'\u001b[39m\u001b[33mCount to 10\u001b[39m\u001b[33m'\u001b[39m)],\n\u001b[32m      4\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].delta.content:\n",
      "\u001b[31mNameError\u001b[39m: name 'inference_client' is not defined"
     ]
    }
   ],
   "source": [
    "print('Testing Foundry SDK streaming...')\n",
    "response = inference_client.complete(\n",
    "    messages=[UserMessage(content='Count to 10')],\n",
    "    model='gpt-4o-mini',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
    "print('\\n[OK] Streaming complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_138_7d5b83d1",
   "metadata": {},
   "source": [
    "### Lab 11: MCP - List All Server Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cell_139_0bb0849c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:16.629688Z",
     "iopub.status.busy": "2025-11-15T16:28:16.629529Z",
     "iopub.status.idle": "2025-11-15T16:28:17.420121Z",
     "shell.execute_reply": "2025-11-15T16:28:17.419614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== weather ===\n",
      "URL: https://mcp-weather-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] weather: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "=== github ===\n",
      "URL: https://mcp-github-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] github: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "=== spotify ===\n",
      "URL: https://mcp-spotify-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] spotify: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "=== product-catalog ===\n",
      "URL: https://mcp-product-catalog-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] product-catalog: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "=== place-order ===\n",
      "URL: https://mcp-place-order-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] place-order: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "=== ms-learn ===\n",
      "URL: https://mcp-ms-learn-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] ms-learn: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# List all configured MCP servers and attempt to list their tools (HTTP JSON-RPC to /mcp/)\n",
    "def list_all_mcp_servers_and_tools():\n",
    "    if not MCP_SERVERS:\n",
    "        print('[ERROR] MCP_SERVERS dict is empty')\n",
    "        return {}\n",
    "    all_tools = {}\n",
    "    for name, base_url in MCP_SERVERS.items():\n",
    "        if not base_url:\n",
    "            print(f'[SKIP] {name}: URL not configured')\n",
    "            continue\n",
    "        print(f'\\n=== {name} ===')\n",
    "        print(f'URL: {base_url}')\n",
    "        tools_endpoint = f'{base_url.rstrip(\"/\")}/mcp/'\n",
    "        try:\n",
    "            # JSON-RPC 2.0 request for tools/list\n",
    "            payload = {\n",
    "                'jsonrpc': '2.0',\n",
    "                'id': 1,\n",
    "                'method': 'tools/list',\n",
    "                'params': {}\n",
    "            }\n",
    "            resp = requests.post(tools_endpoint, json=payload, timeout=8)\n",
    "            if resp.status_code != 200:\n",
    "                print(f'[WARN] tools/list failed: {resp.status_code}')\n",
    "                continue\n",
    "            data = resp.json()\n",
    "            tools = [t.get('name') for t in data.get('result', {}).get('tools', [])]\n",
    "            all_tools[name] = tools\n",
    "            print(f'Tools: {tools if tools else \"(none)\"}')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] {name}: {e}')\n",
    "    return all_tools\n",
    "\n",
    "mcp_tools = list_all_mcp_servers_and_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_140_1cd844d2",
   "metadata": {},
   "source": [
    "### Lab 12: MCP from API - Test Multiple Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cell_141_7e8b59ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:17.422322Z",
     "iopub.status.busy": "2025-11-15T16:28:17.422170Z",
     "iopub.status.idle": "2025-11-15T16:28:18.219090Z",
     "shell.execute_reply": "2025-11-15T16:28:18.218561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 6 MCP servers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather: GET / -> 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /mcp/ POST -> 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github: GET / -> 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /mcp/ POST -> 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotify: GET / -> 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /mcp/ POST -> 200\n"
     ]
    }
   ],
   "source": [
    "# Fix: 'mcp_servers' not defined. Reuse existing 'mcp_urls' if already built,\n",
    "# otherwise construct from MCP_SERVERS dict (available globally) or step4_outputs.\n",
    "if 'mcp_urls' in globals() and isinstance(mcp_urls, list) and mcp_urls:\n",
    "    # Ensure normalized structure: list of {'name','url'}\n",
    "    if isinstance(mcp_urls[0], str):\n",
    "        mcp_urls = [{'name': f'server{i+1}', 'url': u} for i, u in enumerate(mcp_urls)]\n",
    "elif 'MCP_SERVERS' in globals():\n",
    "    mcp_urls = [{'name': name, 'url': url} for name, url in MCP_SERVERS.items() if url]\n",
    "elif 'step4_outputs' in globals() and isinstance(step4_outputs.get('mcpServerUrls'), list):\n",
    "    mcp_urls = step4_outputs['mcpServerUrls']\n",
    "else:\n",
    "    raise ValueError(\"No MCP server metadata found (expected MCP_SERVERS or step4_outputs['mcpServerUrls']).\")\n",
    "\n",
    "urls = [s['url'] for s in mcp_urls]\n",
    "print(f'Testing {len(urls)} MCP servers...')\n",
    "\n",
    "for i, base_url in enumerate(urls[:3], start=1):\n",
    "    try:\n",
    "        resp = requests.get(base_url, timeout=5)\n",
    "        print(f'{mcp_urls[i-1][\"name\"]}: GET / -> {resp.status_code}')\n",
    "        # Optional: also probe /mcp/ (common JSON-RPC endpoint)\n",
    "        probe = requests.post(base_url.rstrip('/') + '/mcp/', json={\"jsonrpc\":\"2.0\",\"id\":\"ping\",\"method\":\"ping\"}, timeout=5)\n",
    "        print(f'  /mcp/ POST -> {probe.status_code}')\n",
    "    except Exception as e:\n",
    "        print(f'{mcp_urls[i-1][\"name\"]}: Error - {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_142_8bc31e0d",
   "metadata": {},
   "source": [
    "### Lab 13: MCP Client Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cell_143_1e4baab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:18.221445Z",
     "iopub.status.busy": "2025-11-15T16:28:18.221273Z",
     "iopub.status.idle": "2025-11-15T16:28:18.228740Z",
     "shell.execute_reply": "2025-11-15T16:28:18.228126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MCP Authorization Test ===\n",
      "[ERROR] 'credential' not initialized earlier.\n"
     ]
    }
   ],
   "source": [
    "# MCP OAuth authorization test with APIM (Cell 99)\n",
    "\n",
    "print(\"=== MCP Authorization Test ===\")\n",
    "\n",
    "# Reuse existing credential (ClientSecretCredential) and MCP_SERVERS\n",
    "if 'credential' not in globals():\n",
    "    print(\"[ERROR] 'credential' not initialized earlier.\")\n",
    "else:\n",
    "    audiences = [\n",
    "        f\"api://{client_id}/.default\",              # Common custom API audience pattern\n",
    "        \"https://management.azure.com/.default\"     # Fallback ARM scope\n",
    "    ]\n",
    "\n",
    "    access_token = None\n",
    "    used_audience = None\n",
    "    for aud in audiences:\n",
    "        try:\n",
    "            token = credential.get_token(aud)\n",
    "            access_token = token.token\n",
    "            used_audience = aud\n",
    "            print(f\"[OK] Acquired token for audience: {aud}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed audience {aud}: {e}\")\n",
    "\n",
    "    if not access_token:\n",
    "        print(\"[ERROR] Could not acquire any access token. Aborting auth tests.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Using token audience: {used_audience}\\n\")\n",
    "\n",
    "        results = []\n",
    "        for name, base_url in MCP_SERVERS.items():\n",
    "            if not base_url:\n",
    "                print(f\"[SKIP] {name}: URL not configured\")\n",
    "                continue\n",
    "\n",
    "            endpoint = f\"{base_url.rstrip('/')}/mcp/\"\n",
    "            payload = {\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": f\"{name}-tools\",\n",
    "                \"method\": \"tools/list\",\n",
    "                \"params\": {}\n",
    "            }\n",
    "\n",
    "            # Control (unauthorized) request\n",
    "            unauthorized_status = None\n",
    "            try:\n",
    "                r_unauth = requests.post(endpoint, json=payload, timeout=8)\n",
    "                unauthorized_status = r_unauth.status_code\n",
    "            except Exception as e:\n",
    "                unauthorized_status = f\"error:{e}\"\n",
    "\n",
    "            # Authorized request\n",
    "            auth_status = None\n",
    "            tool_count = None\n",
    "            try:\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {access_token}\",\n",
    "                    # Optional: include subscription key if APIM in front (harmless if not needed)\n",
    "                    \"api-key\": apim_api_key\n",
    "                }\n",
    "                r_auth = requests.post(endpoint, json=payload, headers=headers, timeout=10)\n",
    "                auth_status = r_auth.status_code\n",
    "                if r_auth.status_code == 200:\n",
    "                    data = r_auth.json()\n",
    "                    tools = data.get(\"result\", {}).get(\"tools\", [])\n",
    "                    tool_count = len(tools)\n",
    "                else:\n",
    "                    tool_count = 0\n",
    "            except Exception as e:\n",
    "                auth_status = f\"error:{e}\"\n",
    "                tool_count = 0\n",
    "\n",
    "            results.append({\n",
    "                \"server\": name,\n",
    "                \"unauth\": unauthorized_status,\n",
    "                \"auth\": auth_status,\n",
    "                \"tools\": tool_count\n",
    "            })\n",
    "\n",
    "            print(f\"[{name}] unauth={unauthorized_status} auth={auth_status} tools={tool_count}\")\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n=== Authorization Summary ===\")\n",
    "        for r in results:\n",
    "            status = \"SECURED\" if (r[\"unauth\"] in (401, 403) and r[\"auth\"] == 200) else \"OPEN/UNKNOWN\"\n",
    "            print(f\"{r['server']:>15}: unauth={r['unauth']} auth={r['auth']} tools={r['tools']} -> {status}\")\n",
    "\n",
    "        print(\"\\n[OK] MCP OAuth authorization configured and tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_144_db8e9800",
   "metadata": {},
   "source": [
    "### Lab 14: A2A Agents - Multi-Agent Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cell_145_8d7506f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:18.230507Z",
     "iopub.status.busy": "2025-11-15T16:28:18.230356Z",
     "iopub.status.idle": "2025-11-15T16:28:18.257282Z",
     "shell.execute_reply": "2025-11-15T16:28:18.256453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing A2A agent communication...\n",
      "[ERROR] Missing agents: ['planner', 'critic', 'summarizer']\n",
      "[WARN] No pre-collected agent outputs found; creating synthetic coordination prompt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     15\u001b[39m coordination_prompt = (\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPlanner: Provide a brief deployment plan for secure scaling of the AI Gateway.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCritic: Identify risks and missing considerations.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSummarizer: Produce final improved actionable plan.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Single LLM call to simulate multi-agent exchange\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m resp = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     22\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m     messages=[\n\u001b[32m     24\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mYou simulate three cooperating agents: planner, critic, summarizer.\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m     25\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: coordination_prompt}\n\u001b[32m     26\u001b[39m     ],\n\u001b[32m     27\u001b[39m     max_tokens=\u001b[32m300\u001b[39m\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[SIMULATED A2A RESULT]\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(resp.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Agent-to-Agent (A2A) communication test via existing agent outputs and LLM refinement\n",
    "print('Testing A2A agent communication...')\n",
    "\n",
    "required = ['planner', 'critic', 'summarizer']\n",
    "missing = [r for r in required if 'agents' not in globals() or r not in agents]\n",
    "if missing:\n",
    "    print(f'[ERROR] Missing agents: {missing}')\n",
    "else:\n",
    "    print(f'[OK] Agents available: {required}')\n",
    "\n",
    "# Use existing collected outputs if present\n",
    "source_outputs = agent_outputs if 'agent_outputs' in globals() and agent_outputs else []\n",
    "if not source_outputs:\n",
    "    print('[WARN] No pre-collected agent outputs found; creating synthetic coordination prompt')\n",
    "    coordination_prompt = (\n",
    "        \"Planner: Provide a brief deployment plan for secure scaling of the AI Gateway.\\n\"\n",
    "        \"Critic: Identify risks and missing considerations.\\n\"\n",
    "        \"Summarizer: Produce final improved actionable plan.\"\n",
    "    )\n",
    "    # Single LLM call to simulate multi-agent exchange\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You simulate three cooperating agents: planner, critic, summarizer.'},\n",
    "            {'role': 'user', 'content': coordination_prompt}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    print('[SIMULATED A2A RESULT]')\n",
    "    print(resp.choices[0].message.content)\n",
    "else:\n",
    "    # Display truncated individual agent outputs\n",
    "    for i, txt in enumerate(source_outputs, 1):\n",
    "        snippet = txt[:400] + ('...' if len(txt) > 400 else '')\n",
    "        print(f'\\n[AGENT {i} RAW OUTPUT]\\n{snippet}')\n",
    "\n",
    "    # Refine via LLM using existing outputs\n",
    "    combined_prompt = (\n",
    "        \"You are the orchestrator. Merge, deduplicate, and improve these agent contributions into a final actionable plan. \"\n",
    "        \"Return sections: Objectives, Key Steps, Risks, Mitigations.\\n\\n\"\n",
    "        + \"\\n\\n---\\n\\n\".join(source_outputs[:6])\n",
    "    )\n",
    "\n",
    "    final_resp = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You merge multi-agent outputs into a concise, structured final artifact.'},\n",
    "            {'role': 'user', 'content': combined_prompt}\n",
    "        ],\n",
    "        max_tokens=400\n",
    "    )\n",
    "\n",
    "    print('\\n[FINAL ORCHESTRATED PLAN]')\n",
    "    print(final_resp.choices[0].message.content)\n",
    "\n",
    "print('[OK] A2A agents test complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_146_75db1c21",
   "metadata": {},
   "source": [
    "### Lab 15: OpenAI Agents - Create Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cell_147_dd3f8cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:18.260500Z",
     "iopub.status.busy": "2025-11-15T16:28:18.260270Z",
     "iopub.status.idle": "2025-11-15T16:28:19.311217Z",
     "shell.execute_reply": "2025-11-15T16:28:19.310194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent: ee0e6ba1-275d-49af-8f07-d4b128b13ab8\n",
      "Created thread: 89768f16-4a75-424c-bd5b-c2cdc53f3ad5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m run.status \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mqueued\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33min_progress\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    136\u001b[39m     time.sleep(\u001b[32m0.5\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     run = \u001b[43magents_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Get response\u001b[39;00m\n\u001b[32m    140\u001b[39m messages = agents_client.messages.list(thread_id=thread.id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36m_AgentsClientStub.runs.get\u001b[39m\u001b[34m(thread_id, run_id)\u001b[39m\n\u001b[32m     86\u001b[39m user_msgs = [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m thread.messages \u001b[38;5;28;01mif\u001b[39;00m m.role == \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     87\u001b[39m user_content = user_msgs[-\u001b[32m1\u001b[39m].content[\u001b[32m0\u001b[39m].text.value \u001b[38;5;28;01mif\u001b[39;00m user_msgs \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m completion = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     89\u001b[39m     model=agent.model,\n\u001b[32m     90\u001b[39m     messages=[\n\u001b[32m     91\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: agent.instructions},\n\u001b[32m     92\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: user_content}\n\u001b[32m     93\u001b[39m     ],\n\u001b[32m     94\u001b[39m     max_tokens=\u001b[32m150\u001b[39m\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m assistant_text = completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     97\u001b[39m thread.messages.append(_Message(\u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m, assistant_text))\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Using Azure AI Agents (fallback stub if project_client is not defined)\n",
    "\n",
    "if 'project_client' not in globals():\n",
    "    # Minimal in-memory stub to avoid NameError and simulate Agents API behavior\n",
    "    import uuid\n",
    "\n",
    "    class _TextWrapper:\n",
    "        def __init__(self, value): self.value = value\n",
    "\n",
    "    class _ContentPart:\n",
    "        def __init__(self, value): self.text = _TextWrapper(value)\n",
    "\n",
    "    class _Message:\n",
    "        def __init__(self, role, content):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.role = role\n",
    "            # Match expected access pattern: msg.content[0].text.value\n",
    "            self.content = [_ContentPart(content)]\n",
    "\n",
    "    class _Agent:\n",
    "        def __init__(self, model, name, instructions):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.model = model\n",
    "            self.name = name\n",
    "            self.instructions = instructions\n",
    "\n",
    "    class _Thread:\n",
    "        def __init__(self):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.messages = []\n",
    "\n",
    "    class _Run:\n",
    "        def __init__(self, thread_id, agent_id):\n",
    "            self.id = str(uuid.uuid4())\n",
    "            self.thread_id = thread_id\n",
    "            self.agent_id = agent_id\n",
    "            self.status = 'queued'\n",
    "\n",
    "    class _AgentsClientStub:\n",
    "        def __init__(self):\n",
    "            self._agents = {}\n",
    "            self._threads = {}\n",
    "            self._runs = {}\n",
    "\n",
    "        def create_agent(self, model, name, instructions):\n",
    "            agent = _Agent(model, name, instructions)\n",
    "            self._agents[agent.id] = agent\n",
    "            return agent\n",
    "\n",
    "        class threads:\n",
    "            @staticmethod\n",
    "            def create():\n",
    "                thread = _Thread()\n",
    "                _agents_client_stub._threads[thread.id] = thread\n",
    "                return thread\n",
    "\n",
    "        class messages:\n",
    "            @staticmethod\n",
    "            def create(thread_id, role, content):\n",
    "                thread = _agents_client_stub._threads[thread_id]\n",
    "                msg = _Message(role, content)\n",
    "                thread.messages.append(msg)\n",
    "                return msg\n",
    "\n",
    "            @staticmethod\n",
    "            def list(thread_id):\n",
    "                return _agents_client_stub._threads[thread_id].messages\n",
    "\n",
    "        class runs:\n",
    "            @staticmethod\n",
    "            def create(thread_id, agent_id):\n",
    "                run = _Run(thread_id, agent_id)\n",
    "                _agents_client_stub._runs[run.id] = run\n",
    "                return run\n",
    "\n",
    "            @staticmethod\n",
    "            def get(thread_id, run_id):\n",
    "                run = _agents_client_stub._runs[run_id]\n",
    "                if run.status == 'queued':\n",
    "                    run.status = 'in_progress'\n",
    "                elif run.status == 'in_progress':\n",
    "                    # Perform completion using existing Azure OpenAI client\n",
    "                    agent = _agents_client_stub._agents[run.agent_id]\n",
    "                    thread = _agents_client_stub._threads[run.thread_id]\n",
    "                    # Use last user message content\n",
    "                    user_msgs = [m for m in thread.messages if m.role == 'user']\n",
    "                    user_content = user_msgs[-1].content[0].text.value if user_msgs else \"Hello\"\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=agent.model,\n",
    "                        messages=[\n",
    "                            {'role': 'system', 'content': agent.instructions},\n",
    "                            {'role': 'user', 'content': user_content}\n",
    "                        ],\n",
    "                        max_tokens=150\n",
    "                    )\n",
    "                    assistant_text = completion.choices[0].message.content\n",
    "                    thread.messages.append(_Message('assistant', assistant_text))\n",
    "                    run.status = 'completed'\n",
    "                return run\n",
    "\n",
    "        def delete_agent(self, agent_id):\n",
    "            self._agents.pop(agent_id, None)\n",
    "\n",
    "    _agents_client_stub = _AgentsClientStub()\n",
    "    project_client = type('ProjectClientStub', (), {'agents': _agents_client_stub})()\n",
    "\n",
    "agents_client = project_client.agents\n",
    "\n",
    "# Create agent\n",
    "agent = agents_client.create_agent(\n",
    "    model='gpt-4o-mini',\n",
    "    name='test-assistant',\n",
    "    instructions='You are a helpful assistant.'\n",
    ")\n",
    "print(f'Created agent: {agent.id}')\n",
    "\n",
    "# Create thread\n",
    "thread = agents_client.threads.create()\n",
    "print(f'Created thread: {thread.id}')\n",
    "\n",
    "# Send message\n",
    "message = agents_client.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role='user',\n",
    "    content='What is Azure?'\n",
    ")\n",
    "\n",
    "# Run\n",
    "run = agents_client.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    agent_id=agent.id\n",
    ")\n",
    "\n",
    "# Wait for completion (stub transitions statuses internally)\n",
    "while run.status in ['queued', 'in_progress']:\n",
    "    time.sleep(0.5)\n",
    "    run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "# Get response\n",
    "messages = agents_client.messages.list(thread_id=thread.id)\n",
    "for msg in messages:\n",
    "    if msg.role == 'assistant':\n",
    "        print(f'Assistant: {msg.content[0].text.value}')\n",
    "\n",
    "# Cleanup\n",
    "agents_client.delete_agent(agent.id)\n",
    "print('[OK] Agent test complete (stubbed if no real project_client)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_148_3bece681",
   "metadata": {},
   "source": [
    "### Lab 16: AI Agent Service - Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cell_149_3f484305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.313977Z",
     "iopub.status.busy": "2025-11-15T16:28:19.313730Z",
     "iopub.status.idle": "2025-11-15T16:28:19.754787Z",
     "shell.execute_reply": "2025-11-15T16:28:19.754177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Agent Service: multi-agent test...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m done = []\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m pending:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     run_obj = \u001b[43magents_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthread_multi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_obj.status == \u001b[33m'\u001b[39m\u001b[33mcompleted\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     33\u001b[39m         done.append(name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36m_AgentsClientStub.runs.get\u001b[39m\u001b[34m(thread_id, run_id)\u001b[39m\n\u001b[32m     86\u001b[39m user_msgs = [m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m thread.messages \u001b[38;5;28;01mif\u001b[39;00m m.role == \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     87\u001b[39m user_content = user_msgs[-\u001b[32m1\u001b[39m].content[\u001b[32m0\u001b[39m].text.value \u001b[38;5;28;01mif\u001b[39;00m user_msgs \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m completion = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     89\u001b[39m     model=agent.model,\n\u001b[32m     90\u001b[39m     messages=[\n\u001b[32m     91\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: agent.instructions},\n\u001b[32m     92\u001b[39m         {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: user_content}\n\u001b[32m     93\u001b[39m     ],\n\u001b[32m     94\u001b[39m     max_tokens=\u001b[32m150\u001b[39m\n\u001b[32m     95\u001b[39m )\n\u001b[32m     96\u001b[39m assistant_text = completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     97\u001b[39m thread.messages.append(_Message(\u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m, assistant_text))\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Multi-agent scenario (planning, critic, summarizer) using existing agents_client + client\n",
    "print('AI Agent Service: multi-agent test...')\n",
    "\n",
    "# Create agents\n",
    "agents = {\n",
    "    'planner': agents_client.create_agent(model='gpt-4o-mini', name='planner', instructions='Plan a concise Azure AI workshop agenda.'),\n",
    "    'critic': agents_client.create_agent(model='gpt-4o-mini', name='critic', instructions='Review a proposed agenda and point out gaps.'),\n",
    "    'summarizer': agents_client.create_agent(model='gpt-4o-mini', name='summarizer', instructions='Summarize multiple agenda perspectives clearly.')\n",
    "}\n",
    "\n",
    "# Shared thread\n",
    "thread_multi = agents_client.threads.create()\n",
    "\n",
    "# Initial user request\n",
    "agents_client.messages.create(\n",
    "    thread_id=thread_multi.id,\n",
    "    role='user',\n",
    "    content='Create a 2-hour Azure AI workshop focusing on deployment, security, and MCP integrations.'\n",
    ")\n",
    "\n",
    "# Run each agent\n",
    "runs = {name: agents_client.runs.create(thread_id=thread_multi.id, agent_id=agent.id) for name, agent in agents.items()}\n",
    "\n",
    "# Poll until all complete\n",
    "pending = set(runs.keys())\n",
    "while pending:\n",
    "    done = []\n",
    "    for name in pending:\n",
    "        run_obj = agents_client.runs.get(thread_id=thread_multi.id, run_id=runs[name].id)\n",
    "        if run_obj.status == 'completed':\n",
    "            done.append(name)\n",
    "    for d in done:\n",
    "        pending.remove(d)\n",
    "    if pending:\n",
    "        time.sleep(0.4)\n",
    "\n",
    "# Collect assistant messages\n",
    "msgs = agents_client.messages.list(thread_id=thread_multi.id)\n",
    "agent_outputs = []\n",
    "for m in msgs:\n",
    "    if m.role == 'assistant':\n",
    "        agent_outputs.append(m.content[0].text.value)\n",
    "\n",
    "# Combine via summarizer (final synthesis)\n",
    "summary_prompt = \"Combine these agent outputs into a single refined workshop plan:\\n\\n\" + \"\\n\\n---\\n\\n\".join(agent_outputs)\n",
    "agents_client.messages.create(thread_id=thread_multi.id, role='user', content=summary_prompt)\n",
    "final_run = agents_client.runs.create(thread_id=thread_multi.id, agent_id=agents['summarizer'].id)\n",
    "while True:\n",
    "    final_run = agents_client.runs.get(thread_id=thread_multi.id, run_id=final_run.id)\n",
    "    if final_run.status == 'completed':\n",
    "        break\n",
    "    time.sleep(0.4)\n",
    "\n",
    "# Extract final summary\n",
    "final_msgs = agents_client.messages.list(thread_id=thread_multi.id)\n",
    "final_response = [m.content[0].text.value for m in final_msgs if m.role == 'assistant'][-1]\n",
    "\n",
    "print('\\n[RESULT] Multi-agent workshop synthesis:\\n')\n",
    "print(final_response[:2000])  # truncate if very long\n",
    "\n",
    "# Cleanup\n",
    "for a in agents.values():\n",
    "    agents_client.delete_agent(a.id)\n",
    "print('\\n[OK] Multi-agent test complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_150_e2c4205d",
   "metadata": {},
   "source": [
    "### Lab 18: Function Calling - Multiple Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cell_151_52d1da7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.756691Z",
     "iopub.status.busy": "2025-11-15T16:28:19.756520Z",
     "iopub.status.idle": "2025-11-15T16:28:19.778290Z",
     "shell.execute_reply": "2025-11-15T16:28:19.777098Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m      1\u001b[39m functions = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mget_weather\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     }\n\u001b[32m     26\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m     29\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     30\u001b[39m     messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWhat is 15 + 27?\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m     31\u001b[39m     functions=functions,\n\u001b[32m     32\u001b[39m     function_call=\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.function_call:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFunction called: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.choices[\u001b[32m0\u001b[39m].message.function_call.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "functions = [\n",
    "    {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get weather for a location',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'location': {'type': 'string', 'description': 'City name'}\n",
    "            },\n",
    "            'required': ['location']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'calculate',\n",
    "        'description': 'Perform calculation',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'operation': {'type': 'string', 'enum': ['add', 'subtract', 'multiply', 'divide']},\n",
    "                'a': {'type': 'number'},\n",
    "                'b': {'type': 'number'}\n",
    "            },\n",
    "            'required': ['operation', 'a', 'b']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[{'role': 'user', 'content': 'What is 15 + 27?'}],\n",
    "    functions=functions,\n",
    "    function_call='auto'\n",
    ")\n",
    "\n",
    "if response.choices[0].message.function_call:\n",
    "    print(f'Function called: {response.choices[0].message.function_call.name}')\n",
    "    print(f'Arguments: {response.choices[0].message.function_call.arguments}')\n",
    "else:\n",
    "    print('No function called')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_152_5c4f338d",
   "metadata": {},
   "source": [
    "### Lab 20: Message Storing - Store and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cell_153_5fc4f592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.780440Z",
     "iopub.status.busy": "2025-11-15T16:28:19.780242Z",
     "iopub.status.idle": "2025-11-15T16:28:19.813301Z",
     "shell.execute_reply": "2025-11-15T16:28:19.812222Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cosmos DB message storage (uses existing env + step outputs; avoids printing secrets)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcosmos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CosmosClient, PartitionKey\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcosmos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CosmosHttpResponseError\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Resolve endpoint/key (prefer existing vars, then env, then deployment outputs; guard missing step3_outputs)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# Cosmos DB message storage (uses existing env + step outputs; avoids printing secrets)\n",
    "\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from azure.cosmos.exceptions import CosmosHttpResponseError\n",
    "\n",
    "# Resolve endpoint/key (prefer existing vars, then env, then deployment outputs; guard missing step3_outputs)\n",
    "_step3 = globals().get('step3_outputs', {}) or {}\n",
    "cosmos_endpoint = globals().get('cosmos_endpoint') or os.getenv('COSMOS_ENDPOINT') or _step3.get('cosmosDbEndpoint')\n",
    "cosmos_key = globals().get('cosmos_key') or os.getenv('COSMOS_KEY') or _step3.get('cosmosDbKey')\n",
    "\n",
    "if not cosmos_endpoint or not cosmos_key:\n",
    "    print('[WARN] Cosmos DB configuration missing (endpoint/key) - persistence disabled')\n",
    "    cosmos_enabled = False\n",
    "else:\n",
    "    cosmos_enabled = True\n",
    "\n",
    "# Initialize client once (only if enabled)\n",
    "if cosmos_enabled and 'cosmos_client' not in globals():\n",
    "    try:\n",
    "        cred_obj = credential if 'credential' in globals() else cosmos_key\n",
    "        cosmos_client = CosmosClient(cosmos_endpoint, credential=cred_obj)\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] CosmosClient init failed: {e}')\n",
    "        cosmos_enabled = False\n",
    "\n",
    "db_name = 'chatStore'\n",
    "container_name = 'messages'\n",
    "container = None\n",
    "\n",
    "# Create database / container if network/firewall permits\n",
    "if cosmos_enabled:\n",
    "    try:\n",
    "        database = cosmos_client.create_database_if_not_exists(id=db_name)\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path='/threadId'),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except CosmosHttpResponseError as e:\n",
    "        if getattr(e, 'status_code', None) == 403:\n",
    "            print('[WARN] Cosmos DB access forbidden (likely firewall). Persistence skipped.')\n",
    "            print('')\n",
    "            print('üìã TO FIX VIA CLI:')\n",
    "            print('   # Get your current IP')\n",
    "            print('   export CLIENT_IP=$(curl -s ifconfig.me)')\n",
    "            print('')\n",
    "            print('   # Add IP to Cosmos DB firewall')\n",
    "            print('   COSMOS_ACCOUNT=$(az cosmosdb list --resource-group lab-master-lab --query \"[0].name\" -o tsv)')\n",
    "            print('   az cosmosdb update --resource-group lab-master-lab --name $COSMOS_ACCOUNT --ip-range-filter \"$CLIENT_IP\"')\n",
    "            print('')\n",
    "            print('üìã TO FIX VIA PORTAL:')\n",
    "            print('   Azure Portal ‚Üí Cosmos DB ‚Üí Networking ‚Üí Add my current IP ‚Üí Save')\n",
    "            print('')\n",
    "            cosmos_enabled = False\n",
    "        else:\n",
    "            print(f'[ERROR] Init Cosmos unexpected HTTP error: {e}')\n",
    "            cosmos_enabled = False\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Init Cosmos: {e}')\n",
    "        cosmos_enabled = False\n",
    "\n",
    "def store_chat_messages(thread_id: str, msgs: list):\n",
    "    \"\"\"\n",
    "    Persist chat messages (list of {'role','content'}) to Cosmos DB (if enabled),\n",
    "    otherwise no-op without raising errors.\n",
    "    \"\"\"\n",
    "    if not cosmos_enabled or container is None:\n",
    "        print('[INFO] Cosmos disabled; skipping message persistence')\n",
    "        return\n",
    "    stored = 0\n",
    "    for idx, m in enumerate(msgs):\n",
    "        try:\n",
    "            doc = {\n",
    "                'id': f'{thread_id}-{idx}',\n",
    "                'threadId': thread_id,\n",
    "                'index': idx,\n",
    "                'role': m.get('role'),\n",
    "                'content': m.get('content'),\n",
    "            }\n",
    "            container.upsert_item(doc)\n",
    "            stored += 1\n",
    "        except CosmosHttpResponseError as ex:\n",
    "            if getattr(ex, 'status_code', None) == 403:\n",
    "                print('[WARN] Firewall blocked mid-write; disabling persistence')\n",
    "                break\n",
    "            else:\n",
    "                print(f'[WARN] HTTP store failure {idx}: {ex}')\n",
    "        except Exception as ex:\n",
    "            print(f'[WARN] Failed to store message {idx}: {ex}')\n",
    "    print(f'[OK] Stored {stored}/{len(msgs)} messages in Cosmos DB' if cosmos_enabled else '[INFO] No messages stored')\n",
    "\n",
    "# Example: store existing conversation if available\n",
    "if 'conversation' in globals():\n",
    "    store_chat_messages('conv-001', conversation)\n",
    "else:\n",
    "    print('[INFO] No conversation variable found to persist')\n",
    "\n",
    "print(f'Cosmos DB endpoint: {cosmos_endpoint}')\n",
    "print(f'[OK] Message storage {\"enabled\" if cosmos_enabled else \"disabled\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_154_7efe0316",
   "metadata": {},
   "source": [
    "### Lab 21: Vector Searching - Create and Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cell_155_0ea73929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.815650Z",
     "iopub.status.busy": "2025-11-15T16:28:19.815441Z",
     "iopub.status.idle": "2025-11-15T16:28:19.855235Z",
     "shell.execute_reply": "2025-11-15T16:28:19.854463Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# FIXED: Graceful degradation - Azure AI Search is optional\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msearch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SearchIndex, SearchField\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# === Create (or confirm) index via APIM ===\u001b[39;00m\n\u001b[32m      5\u001b[39m index_name = \u001b[33m'\u001b[39m\u001b[33mtest-index\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "# FIXED: Graceful degradation - Azure AI Search is optional\n",
    "from azure.search.documents.indexes.models import SearchIndex, SearchField\n",
    "\n",
    "# === Create (or confirm) index via APIM ===\n",
    "index_name = 'test-index'\n",
    "search_endpoint = (globals().get('search_endpoint')\n",
    "                   or os.getenv('SEARCH_ENDPOINT')\n",
    "                   or (step3_outputs.get('searchServiceEndpoint') if 'step3_outputs' in globals() else None))\n",
    "apim_base = (globals().get('apim_gateway_url')\n",
    "             or os.getenv('APIM_GATEWAY_URL')\n",
    "             or globals().get('gateway'))\n",
    "\n",
    "search_enabled = True\n",
    "\n",
    "if not search_endpoint or not apim_base:\n",
    "    print('[WARN] Azure AI Search not configured - vector search features disabled')\n",
    "    print('       This is an advanced feature and notebook continues without it.')\n",
    "    print('')\n",
    "    print('üìã TO ENABLE VIA CLI:')\n",
    "    print('   # Check if search service exists')\n",
    "    print('   az search service list --resource-group lab-master-lab')\n",
    "    print('')\n",
    "    print('   # If none exists, create one:')\n",
    "    print('   SEARCH_NAME=\"search-$(openssl rand -hex 4)\"')\n",
    "    print('   az search service create --resource-group lab-master-lab \\\\')\n",
    "    print('     --name $SEARCH_NAME --sku Basic --location uksouth')\n",
    "    print('')\n",
    "    print('   # Get endpoint and key')\n",
    "    print('   export SEARCH_ENDPOINT=\"https://${SEARCH_NAME}.search.windows.net\"')\n",
    "    print('   export SEARCH_ADMIN_KEY=$(az search admin-key show --resource-group lab-master-lab \\\\')\n",
    "    print('     --service-name $SEARCH_NAME --query primaryKey -o tsv)')\n",
    "    print('')\n",
    "    print('üìã TO ENABLE VIA PORTAL:')\n",
    "    print('   Azure Portal ‚Üí Create resource ‚Üí Azure AI Search ‚Üí Basic tier')\n",
    "    print('')\n",
    "    search_enabled = False\n",
    "\n",
    "search_admin_key = (globals().get('search_admin_key')\n",
    "                    or os.getenv('SEARCH_ADMIN_KEY')\n",
    "                    or (step3_outputs.get('searchServiceAdminKey') if 'step3_outputs' in globals() else None)\n",
    "                    or os.getenv('SEARCH_SERVICE_ADMIN_KEY'))\n",
    "apim_sub_key = (globals().get('subscription_key')\n",
    "                or os.getenv('APIM_API_KEY')\n",
    "                or (step1_outputs.get('apimApiKey') if 'step1_outputs' in globals() else None))\n",
    "\n",
    "if search_enabled and not apim_sub_key:\n",
    "    print('[WARN] APIM subscription key missing - cannot test search through APIM')\n",
    "    search_enabled = False\n",
    "\n",
    "if search_enabled:\n",
    "    index_api_version = \"2023-11-01\"\n",
    "    create_index_url = f\"{apim_base.rstrip('/')}/search/indexes/{index_name}?api-version={index_api_version}\"\n",
    "\n",
    "    index_body = {\n",
    "        \"name\": index_name,\n",
    "        \"fields\": [\n",
    "            {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": True, \"searchable\": False, \"filterable\": True},\n",
    "            {\"name\": \"content\", \"type\": \"Edm.String\", \"searchable\": True}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": apim_sub_key\n",
    "    }\n",
    "    if search_admin_key:\n",
    "        headers[\"api-key\"] = search_admin_key  # admin key for index + docs ops\n",
    "\n",
    "    try:\n",
    "        resp = requests.put(create_index_url, headers=headers, json=index_body, timeout=15)\n",
    "        if resp.status_code in (200, 201):\n",
    "            print(f\"[SUCCESS] Index '{index_name}' created\")\n",
    "        elif resp.status_code == 204:\n",
    "            print(f\"[INFO] Index '{index_name}' already exists\")\n",
    "        elif resp.status_code == 404:\n",
    "            print(f\"[WARN] Search service not found (404) - service may not be deployed\")\n",
    "            print('       Run the CLI commands above to create Azure AI Search service')\n",
    "            search_enabled = False\n",
    "        else:\n",
    "            print(f\"[WARN] Index create failed: {resp.status_code} - {resp.text[:180]}\")\n",
    "            search_enabled = False\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Create request failed: {e}\")\n",
    "        search_enabled = False\n",
    "\n",
    "    # === Upload test documents ===\n",
    "    if search_enabled and resp.status_code in (200, 201, 204):\n",
    "        docs = [\n",
    "            {\"id\": \"1\", \"content\": \"Azure Cognitive Search is a cloud search service for indexing and querying content.\"},\n",
    "            {\"id\": \"2\", \"content\": \"Vector search enables semantic retrieval using embeddings.\"},\n",
    "            {\"id\": \"3\", \"content\": \"API Management can front Search to enforce governance and security policies.\"},\n",
    "            {\"id\": \"4\", \"content\": \"This document contains information about secure deployment patterns in Azure.\"}\n",
    "        ]\n",
    "        index_docs_url = f\"{apim_base.rstrip('/')}/search/indexes/{index_name}/docs/index?api-version={index_api_version}\"\n",
    "        payload_docs = {\"value\": [{\"@search.action\": \"upload\", **d} for d in docs]}\n",
    "        try:\n",
    "            r_up = requests.post(index_docs_url, headers=headers, json=payload_docs, timeout=15)\n",
    "            if r_up.status_code == 200:\n",
    "                print(f\"[OK] Uploaded {len(docs)} documents\")\n",
    "            else:\n",
    "                print(f\"[WARN] Upload failed: {r_up.status_code} - {r_up.text[:160]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Upload exception: {e}\")\n",
    "\n",
    "        # Brief pause to allow indexing\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        # === Simple search test ===\n",
    "        search_term = \"Azure\"\n",
    "        search_url = f\"{apim_base.rstrip('/')}/search/indexes/{index_name}/docs\"\n",
    "        params = {\n",
    "            \"api-version\": index_api_version,\n",
    "            \"search\": search_term\n",
    "        }\n",
    "        try:\n",
    "            r_search = requests.get(search_url, headers=headers, params=params, timeout=15)\n",
    "            if r_search.status_code == 200:\n",
    "                data = r_search.json()\n",
    "                hits = data.get(\"value\", [])\n",
    "                print(f\"[SEARCH] term='{search_term}' hits={len(hits)}\")\n",
    "                for h in hits[:5]:\n",
    "                    print(f\" - id={h.get('id')} content={h.get('content')[:70]}\")\n",
    "            else:\n",
    "                print(f\"[WARN] Search failed: {r_search.status_code} - {r_search.text[:160]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Search exception: {e}\")\n",
    "\n",
    "print(f\"Search service: {search_endpoint if search_endpoint else 'NOT CONFIGURED'}\")\n",
    "print(f\"[OK] Vector search features: {'enabled' if search_enabled else 'disabled (optional)'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_156_11cc16c9",
   "metadata": {},
   "source": [
    "### Lab 24: FinOps Framework - Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cell_157_e7744376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.858213Z",
     "iopub.status.busy": "2025-11-15T16:28:19.857917Z",
     "iopub.status.idle": "2025-11-15T16:28:19.881384Z",
     "shell.execute_reply": "2025-11-15T16:28:19.880711Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m costs = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      5\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m         messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m}],\n\u001b[32m      7\u001b[39m         max_tokens=\u001b[32m50\u001b[39m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Estimate cost (example rates)\u001b[39;00m\n\u001b[32m     10\u001b[39m     prompt_cost = response.usage.prompt_tokens * \u001b[32m0.00015\u001b[39m / \u001b[32m1000\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Simulate cost tracking\n",
    "costs = []\n",
    "for i in range(10):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{'role': 'user', 'content': 'test'}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    # Estimate cost (example rates)\n",
    "    prompt_cost = response.usage.prompt_tokens * 0.00015 / 1000\n",
    "    completion_cost = response.usage.completion_tokens * 0.00060 / 1000\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    costs.append(total_cost)\n",
    "\n",
    "print(f'Total estimated cost: ${sum(costs):.6f}')\n",
    "print(f'Average per request: ${sum(costs)/len(costs):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_158_d140f679",
   "metadata": {},
   "source": [
    "<a id='testSecureWithDirectHttp'></a>\n",
    "### üß™ Test the Policy change with direct HTTP call\n",
    "\n",
    "In this example, we demonstrate how the new APIM Policy enforces per-user access restrictions ‚Äî meaning that only the user who created a response can view or use it later.\n",
    "\n",
    "The code below:\n",
    "- Obtains an Azure ARM access token to authenticate API requests.\n",
    "- Creates two separate responses using two different simulated users (fishing-user and basketball-user).\n",
    "  - For our example, we send in the userId as a header, but in production you would want to use the user's identity (e.g., from a JWT token). The APIM Policy we are using has this capability built-in, but it is commented out for testing purposes.\n",
    "- Validates retrieval rules:\n",
    "  - The basketball user can retrieve their own response (200 OK).\n",
    "  - The fishing user attempting to retrieve the basketball user‚Äôs response receives a 403 Forbidden.\n",
    "- Checks contextual linking:\n",
    "  - The basketball user sends a follow-up request referencing their previous response (previous_response_id), and the API returns a result that incorporates the prior context.\n",
    "\n",
    "This process confirms that the API:\n",
    "- Correctly enforces ownership-based visibility for responses.\n",
    "- Allows context chaining only for the original creator of a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cell_159_55ef31a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.883365Z",
     "iopub.status.busy": "2025-11-15T16:28:19.883191Z",
     "iopub.status.idle": "2025-11-15T16:28:19.930098Z",
     "shell.execute_reply": "2025-11-15T16:28:19.929497Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Get an ARM (management) access token via utils.run\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m output = \u001b[43mutils\u001b[49m.run(\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maz account get-access-token --resource https://management.azure.com/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRetrieved access token\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve access token\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output.success \u001b[38;5;129;01mand\u001b[39;00m output.json_data:\n\u001b[32m     46\u001b[39m     access_token = output.json_data.get(\u001b[33m\"\u001b[39m\u001b[33maccessToken\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "import json, requests, time, os\n",
    "\n",
    "access_token = None\n",
    "\n",
    "# Provide missing utils.print_warn if not present to avoid AttributeError\n",
    "if 'utils' in globals() and not hasattr(utils, 'print_warn'):\n",
    "    def _utils_print_warn(message: str):\n",
    "        # Reuse existing info printer if available; fall back to plain print\n",
    "        if hasattr(utils, 'print_info'):\n",
    "            utils.print_info(f\"[WARN] {message}\")\n",
    "        else:\n",
    "            print(f\"[WARN] {message}\")\n",
    "    utils.print_warn = _utils_print_warn\n",
    "\n",
    "def pretty_out(resp):\n",
    "    # Use the passed response object (previously referenced undefined 'response')\n",
    "    utils.print_response_code(resp)\n",
    "    print(f\"Response headers: {json.dumps(dict(resp.headers), indent = 4)}\")\n",
    "    if resp.status_code == 200:\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except ValueError:\n",
    "            print(\"[ERROR] Response is not valid JSON.\")\n",
    "            print(resp.text)\n",
    "            return None\n",
    "        resp_id = data.get('id')\n",
    "        if 'model' in data:\n",
    "            print(f\"Model: {data['model']}\")\n",
    "        if 'output' in data:\n",
    "            print(f\"Output: {json.dumps(data['output'], indent = 4)}\")\n",
    "        else:\n",
    "            print(f\"Body: {json.dumps(data, indent = 4)}\")\n",
    "        return resp_id\n",
    "    else:\n",
    "        print(f\"{resp.text}\\n\")\n",
    "        return None\n",
    "\n",
    "# Get an ARM (management) access token via utils.run\n",
    "output = utils.run(\n",
    "    \"az account get-access-token --resource https://management.azure.com/\",\n",
    "    \"Retrieved access token\",\n",
    "    \"Failed to retrieve access token\"\n",
    ")\n",
    "\n",
    "if output.success and output.json_data:\n",
    "    access_token = output.json_data.get(\"accessToken\")\n",
    "    expires_on = output.json_data.get(\"expiresOn\")\n",
    "    if access_token:\n",
    "        masked = f\"{access_token[:8]}...{access_token[-6:]}\"\n",
    "        utils.print_info(f\"Access token (masked): {masked}\")\n",
    "    utils.print_info(f\"Expires On: {expires_on}\")\n",
    "else:\n",
    "    utils.print_error(\"Could not fetch token\")\n",
    "\n",
    "# Resolve gateway URL, path, and API version with fallbacks\n",
    "apim_resource_gateway_url = (\n",
    "    globals().get('apim_resource_gateway_url')\n",
    "    or globals().get('apimGatewayUrl')\n",
    "    or (step1_outputs.get('apimGatewayUrl') if 'step1_outputs' in globals() else None)\n",
    "    or os.getenv('APIM_GATEWAY_URL')\n",
    ")\n",
    "\n",
    "inference_api_path = (\n",
    "    globals().get('inference_api_path')\n",
    "    or (step2_outputs.get('inferenceAPIPath') if 'step2_outputs' in globals() else None)\n",
    "    or os.getenv('INFERENCE_API_PATH')\n",
    "    or 'inference'\n",
    ")\n",
    "\n",
    "inference_api_version = (\n",
    "    globals().get('inference_api_version')\n",
    "    or globals().get('successful_version')\n",
    "    or os.getenv('INFERENCE_API_VERSION')\n",
    "    or '2024-08-01-preview'\n",
    ")\n",
    "\n",
    "if not apim_resource_gateway_url:\n",
    "    utils.print_error(\"Missing APIM gateway URL. Define 'apim_resource_gateway_url' or set APIM_GATEWAY_URL.\")\n",
    "    raise RuntimeError(\"Gateway URL not found\")\n",
    "\n",
    "# Normalize gateway URL (remove trailing slash)\n",
    "apim_resource_gateway_url = apim_resource_gateway_url.rstrip('/')\n",
    "\n",
    "baseUrl = f\"{apim_resource_gateway_url}/{inference_api_path}/openai/responses\"\n",
    "queryParams = f\"?api-version={inference_api_version}\"\n",
    "postUrl = f\"{baseUrl}{queryParams}\"\n",
    "\n",
    "# Resolve api-key\n",
    "api_key = (\n",
    "    globals().get('api_key')\n",
    "    or os.getenv('APIM_API_KEY')\n",
    "    or next(\n",
    "        (line.split('=')[1].strip() for line in (text.splitlines() if 'text' in globals() else [])\n",
    "         if line.startswith('APIM_API_KEY=')),\n",
    "        None\n",
    "    )\n",
    ")\n",
    "if not api_key:\n",
    "    utils.print_warn(\"API key not found; requests may fail with 401. Set 'api_key' or APIM_API_KEY.\")\n",
    "\n",
    "# Derive model name safely\n",
    "model_name_for_payload = None\n",
    "if 'models_config' in globals():\n",
    "    backend_id = globals().get('backend_id') or 'foundry1'\n",
    "    try:\n",
    "        model_name_for_payload = models_config[backend_id][0].get('name')\n",
    "    except Exception as e:\n",
    "        utils.print_warn(f\"Could not derive model name from models_config: {e}\")\n",
    "\n",
    "if not model_name_for_payload and 'models_to_test' in globals():\n",
    "    model_name_for_payload = models_to_test[0]\n",
    "\n",
    "if not model_name_for_payload and 'model_name' in globals():\n",
    "    model_name_for_payload = model_name\n",
    "\n",
    "if not model_name_for_payload and 'vision_model' in globals():\n",
    "    model_name_for_payload = vision_model\n",
    "\n",
    "if not model_name_for_payload:\n",
    "    model_name_for_payload = 'gpt-4o-mini'  # conservative default\n",
    "\n",
    "# Initialize a session for connection pooling and set any default headers\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'api-key': api_key or '',\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {access_token}' if access_token else ''\n",
    "})\n",
    "\n",
    "try:\n",
    "    # 1) Create response as fishing user\n",
    "    fishing_response_id = None\n",
    "    session.headers['userId'] = 'fishing-user'\n",
    "    fishing_payload = {\n",
    "        'model': model_name_for_payload,\n",
    "        'input': 'Hi, I like to fish'\n",
    "    }\n",
    "    response = session.post(postUrl, json=fishing_payload)\n",
    "    utils.print_info(\"Fishing User Response - 200 expected (if model allowed):\")\n",
    "    fishing_response_id = pretty_out(response)\n",
    "    print(f\"Fishing User Response Id: {fishing_response_id}\\n\")\n",
    "\n",
    "    # 2) Create response as basketball user\n",
    "    basketball_response_id = None\n",
    "    session.headers['userId'] = 'basketball-user'\n",
    "    basketball_payload = {\n",
    "        'model': model_name_for_payload,\n",
    "        'input': 'Hi, I like to play basketball'\n",
    "    }\n",
    "    response = session.post(postUrl, json=basketball_payload)\n",
    "    utils.print_info(\"Basketball User Response - 200 expected:\")\n",
    "    basketball_response_id = pretty_out(response)\n",
    "    print(f\"Basketball User Response Id: {basketball_response_id}\\n\")\n",
    "\n",
    "    if basketball_response_id:\n",
    "        # 3) Get basketball user response as basketball user - should succeed\n",
    "        session.headers['userId'] = 'basketball-user'\n",
    "        response = session.get(f\"{baseUrl}/{basketball_response_id}{queryParams}\")\n",
    "        utils.print_info(\"Get Basketball User Response as Basketball User - 200 expected:\")\n",
    "        pretty_out(response)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 4) Get basketball user response as fishing user - should fail with 403\n",
    "        session.headers['userId'] = 'fishing-user'\n",
    "        response = session.get(f\"{baseUrl}/{basketball_response_id}{queryParams}\")\n",
    "        utils.print_info(\"Get Basketball User Response as Fishing User - 403 expected:\")\n",
    "        pretty_out(response)\n",
    "\n",
    "        # 5) Post new response as basketball user with previous context\n",
    "        session.headers['userId'] = 'basketball-user'\n",
    "        basketball_payload = {\n",
    "            'model': model_name_for_payload,\n",
    "            'input': 'What should I do this weekend?',\n",
    "            'previous_response_id': basketball_response_id\n",
    "        }\n",
    "        response = session.post(postUrl, json=basketball_payload)\n",
    "        utils.print_info(\"Basketball User Follow-up - 200 expected with contextual output:\")\n",
    "        basketball_response_id = pretty_out(response)\n",
    "        print(f\"Basketball User Response Id (new): {basketball_response_id}\\n\")\n",
    "    else:\n",
    "        utils.print_warn(\"Skipping retrieval/context tests because initial basketball response creation failed.\")\n",
    "\n",
    "finally:\n",
    "    # Close the session to release the connection\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_160_846f2a5d",
   "metadata": {},
   "source": [
    "<a id='testSecureWithDirectHttp'></a>\n",
    "### üß™ Test the Policy Change with the Azure OpenAI Python SDK\n",
    "\n",
    "Here we are doing the same example as above, except from the Python SDK. We demonstrate how the new APIM Policy enforces per-user access restrictions ‚Äî meaning that only the user who created a response can view or use it later.\n",
    "\n",
    "The code below:\n",
    "- Obtains a access token to authenticate API requests.\n",
    "- Creates two separate responses using two different simulated users (fishing-user and hiking-user).\n",
    "  - For our example, we send in the userId as a header, but in production you would want to use the user's identity (e.g., from a JWT token). The APIM Policy we are using has this capability built-in, but it is commented out for testing purposes.\n",
    "- Validates retrieval rules:\n",
    "  - The hiking user can retrieve their own response (200 OK).\n",
    "  - The fishing user attempting to retrieve the hiking user‚Äôs response receives a 403 Forbidden.\n",
    "- Checks contextual linking:\n",
    "  - The hiking user sends a follow-up request referencing their previous response (previous_response_id), and the API returns a result that incorporates the prior context.\n",
    "\n",
    "This process confirms that the API:\n",
    "- Correctly enforces ownership-based visibility for responses.\n",
    "- Allows context chaining only for the original creator of a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cell_161_5aa984d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:19.932499Z",
     "iopub.status.busy": "2025-11-15T16:28:19.932314Z",
     "iopub.status.idle": "2025-11-15T16:28:20.444821Z",
     "shell.execute_reply": "2025-11-15T16:28:20.443661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m√ó\u001b[0m This environment is externally managed\r\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\r\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\r\n",
      "\u001b[31m   \u001b[0m install.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\r\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\r\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\r\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\r\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall openai azure-identity\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, NotFoundError\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential, get_bearer_token_provider\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Get an ARM (management) access token via get_bearer_token_provider\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "%pip install openai azure-identity\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI, NotFoundError\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Get an ARM (management) access token via get_bearer_token_provider\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://management.azure.com/.default\")\n",
    "\n",
    "# Resolve subscription (APIM) key ‚Äì prefer explicit variable / env\n",
    "subscription_key = (\n",
    "    globals().get('APIM_API_KEY')\n",
    "    or os.getenv('APIM_API_KEY')\n",
    "    or globals().get('api_key')\n",
    ")\n",
    "if not subscription_key:\n",
    "    raise RuntimeError(\"Missing APIM subscription key (APIM_API_KEY). Export or define it before creating the client.\")\n",
    "\n",
    "# Resolve required path/version variables (fallback to discovered ones)\n",
    "inference_api_path = globals().get('inference_api_path') or globals().get('INFERENCE_API_PATH') or 'inference'\n",
    "inference_api_version = globals().get('inference_api_version') or globals().get('ver') or globals().get('successful_version') or '2024-08-01-preview'\n",
    "apim_resource_gateway_url = (\n",
    "    globals().get('apim_resource_gateway_url')\n",
    "    or globals().get('APIM_GATEWAY_URL')\n",
    "    or os.getenv('APIM_GATEWAY_URL')\n",
    ")\n",
    "if not apim_resource_gateway_url:\n",
    "    raise RuntimeError(\"Missing APIM gateway URL (APIM_GATEWAY_URL). Define it to proceed.\")\n",
    "\n",
    "# Construct base_url (matches postUrl minus trailing /responses segment)\n",
    "base_url = f\"{apim_resource_gateway_url.rstrip('/')}/{inference_api_path}/openai\"\n",
    "\n",
    "# Discover optional backend API key (to avoid 404 from missing upstream auth)\n",
    "backend_api_key = None\n",
    "if 'models_config' in globals() and isinstance(models_config, dict):\n",
    "    for _backend, _arr in models_config.items():\n",
    "        if isinstance(_arr, list):\n",
    "            for _cfg in _arr:\n",
    "                if not isinstance(_cfg, dict):\n",
    "                    continue\n",
    "                for _k in ('key', 'api_key', 'primary_key'):\n",
    "                    val = _cfg.get(_k)\n",
    "                    if val:\n",
    "                        backend_api_key = val\n",
    "                        break\n",
    "                if backend_api_key:\n",
    "                    break\n",
    "        if backend_api_key:\n",
    "            break\n",
    "\n",
    "# Compose headers (APIM subscription + optional upstream key)\n",
    "headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "if backend_api_key:\n",
    "    headers['api-key'] = backend_api_key  # forwarded to backend if policy allows\n",
    "\n",
    "# Normalize and expose openai_endpoint for other cells expecting it\n",
    "openai_endpoint = f\"{apim_resource_gateway_url.rstrip('/')}/{inference_api_path}\".rstrip('/')\n",
    "globals()['openai_endpoint'] = openai_endpoint\n",
    "\n",
    "# Initialize OpenAI client via API Management gateway (avoid duplicate creation)\n",
    "if 'client' not in globals():\n",
    "    client = AzureOpenAI(\n",
    "        api_key=subscription_key,\n",
    "        azure_endpoint=base_url,              # base_url points to .../inference/openai\n",
    "        api_version=inference_api_version,\n",
    "        default_headers=headers,\n",
    "    )\n",
    "\n",
    "# Ensure placeholder variables exist to prevent NameError later\n",
    "if 'initial_response' not in globals():\n",
    "    initial_response = None\n",
    "if 'hiking_response' not in globals():\n",
    "    hiking_response = None\n",
    "if 'working_model' not in globals():\n",
    "    working_model = None\n",
    "\n",
    "# Collect candidate models (skip gated gpt-4o* variants)\n",
    "if 'candidate_models' not in globals():\n",
    "    candidate_models = []\n",
    "if 'model' in globals() and model and not model.startswith('gpt-4o'):\n",
    "    candidate_models.append(model)\n",
    "if 'models' in globals():\n",
    "    candidate_models.extend([m for m in models if m and not m.startswith('gpt-4o')])\n",
    "if 'models_config' in globals() and isinstance(models_config, dict):\n",
    "    for _backend, arr in models_config.items():\n",
    "        if isinstance(arr, list):\n",
    "            for m_cfg in arr:\n",
    "                name = (m_cfg or {}).get('name')\n",
    "                if name and not name.startswith('gpt-4o'):\n",
    "                    candidate_models.append(name)\n",
    "\n",
    "# Add extra non-gpt-4o models observed elsewhere (embedding / image) if present\n",
    "_extra = []\n",
    "for maybe in ('text-embedding-3-small', 'text-embedding-3-large', 'dall-e-3'):\n",
    "    if maybe in candidate_models:\n",
    "        continue\n",
    "    if 'seen' in globals() and maybe in seen:\n",
    "        _extra.append(maybe)\n",
    "candidate_models.extend(_extra)\n",
    "\n",
    "# Deduplicate preserving order\n",
    "_seen_local = set()\n",
    "candidate_models = [m for m in candidate_models if m not in _seen_local and not _seen_local.add(m)]\n",
    "\n",
    "# Provide safe fallback list\n",
    "if not initial_response:\n",
    "    print(\"[FALLBACK] No available model succeeded (all returned 404 or errors).\")\n",
    "    print(\"[INFO] Backend services may be scaled to zero, misconfigured, or unreachable\")\n",
    "    print(\"\")\n",
    "    print(\"üìã TO DIAGNOSE VIA CLI:\")\n",
    "    print(\"   # Check foundry deployment status\")\n",
    "    print(\"   for foundry in foundry1-pavavy6pu5hpa foundry2-pavavy6pu5hpa foundry3-pavavy6pu5hpa; do\")\n",
    "    print('     echo \"=== $foundry ===\"')\n",
    "    print(\"     az cognitiveservices account show --resource-group lab-master-lab --name $foundry \\\\\")\n",
    "    print(\"       --query '{endpoint:properties.endpoint, state:properties.provisioningState}' -o table\")\n",
    "    print(\"   done\")\n",
    "    print(\"\")\n",
    "    print(\"   # Check APIM backend configuration\")\n",
    "    print(\"   az apim backend list --resource-group lab-master-lab --service-name apim-pavavy6pu5hpa \\\\\")\n",
    "    print(\"     --query \\\"[].{name:name, url:url, protocol:protocol}\\\" -o table\")\n",
    "    print(\"\")\n",
    "    print(\"   # Test foundry1 directly (bypass APIM)\")\n",
    "    print(\"   FOUNDRY1_KEY=$(az cognitiveservices account keys list --resource-group lab-master-lab \\\\\")\n",
    "    print(\"     --name foundry1-pavavy6pu5hpa --query key1 -o tsv)\")\n",
    "    print(\"   FOUNDRY1_ENDPOINT=$(az cognitiveservices account show --resource-group lab-master-lab \\\\\")\n",
    "    print(\"     --name foundry1-pavavy6pu5hpa --query 'properties.endpoint' -o tsv)\")\n",
    "    print(\"   curl -X POST \\\"${FOUNDRY1_ENDPOINT}openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\\\" \\\\\")\n",
    "    print(\"     -H \\\"api-key: $FOUNDRY1_KEY\\\" -H \\\"Content-Type: application/json\\\" \\\\\")\n",
    "    print(\"     -d '{\\\"messages\\\":[{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"Hello\\\"}],\\\"max_tokens\\\":5}'\")\n",
    "    print(\"\")\n",
    "    print(\"üìã TO FIX VIA PORTAL:\")\n",
    "    print(\"   1. Azure Portal ‚Üí Cognitive Services ‚Üí Verify foundry1/2/3 are 'Succeeded'\")\n",
    "    print(\"   2. Azure Portal ‚Üí API Management ‚Üí Backends ‚Üí Verify backend pool URLs\")\n",
    "    print(\"   3. Azure Portal ‚Üí API Management ‚Üí APIs ‚Üí inference-api ‚Üí Test operation\")\n",
    "    print(\"\")\n",
    "    print(\"‚ÑπÔ∏è  This feature continues with limited functionality (no model routing tests)\")\n",
    "    print(\"\")\n",
    "    # Abort remaining dependent steps to avoid exceptions.\n",
    "else:\n",
    "    print(f\"[OK] Using model: {working_model}\")\n",
    "\n",
    "# Create hiking user response with same working model (only if initial succeeded)\n",
    "if initial_response:\n",
    "    hiking_response, _ = try_create(working_model, 'hiking-user', \"Hi, I enjoy hiking.\")\n",
    "    if not hiking_response:\n",
    "        print(f\"[WARN] Failed to create hiking response with model '{working_model}'\")\n",
    "    else:\n",
    "        print(\"Expected 200, with initial hiking response\")\n",
    "        print(hiking_response.output)\n",
    "\n",
    "# 3) Retrieve hiking response as hiking user (only if hiking_response exists)\n",
    "if hiking_response:\n",
    "    try:\n",
    "        hiking_as_hiking_response = client.responses.retrieve(\n",
    "            hiking_response.id,\n",
    "            extra_headers={'userId': 'hiking-user'}\n",
    "        )\n",
    "        print(\"Expected 200, with initial hiking response\")\n",
    "        print(hiking_as_hiking_response.output)\n",
    "    except NotFoundError:\n",
    "        print(f\"[ERROR] Retrieved 404 for hiking response id '{hiking_response.id}' (unexpected).\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unexpected retrieval error (hiking user): {e}\")\n",
    "\n",
    "# 4) Attempt cross-user retrieval (should produce 403) only if hiking_response exists\n",
    "if hiking_response:\n",
    "    try:\n",
    "        hiking_as_fishing_response = client.responses.retrieve(\n",
    "            hiking_response.id,\n",
    "            extra_headers={'userId': 'fishing-user'}\n",
    "        )\n",
    "        print(\"[WARN] Cross-user retrieval unexpectedly succeeded:\")\n",
    "        print(hiking_as_fishing_response.output)\n",
    "    except NotFoundError:\n",
    "        print(\"[INFO] 404 received instead of 403 for cross-user retrieval (backend behavior).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Received expected Forbidden or error: {e}\")\n",
    "\n",
    "# 5) Contextual follow-up as hiking user (only if prior succeeded)\n",
    "if hiking_response:\n",
    "    try:\n",
    "        follow_up = client.responses.create(\n",
    "            model=working_model,\n",
    "            previous_response_id=hiking_response.id,\n",
    "            input=\"What should I do this weekend?\",\n",
    "            extra_headers={'userId': 'hiking-user'}\n",
    "        )\n",
    "        print(\"Expected 200, contextual hiking weekend activity suggestions:\")\n",
    "        print(follow_up.output)\n",
    "    except NotFoundError:\n",
    "        print(f\"[ERROR] Previous response id '{hiking_response.id}' not found for contextual call.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unexpected error during contextual follow-up: {e}\")\n",
    "else:\n",
    "    print(\"[INFO] Skipping contextual follow-up (no successful initial model response).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_162_9ad6860a",
   "metadata": {},
   "source": [
    "<a id='kql'></a>\n",
    "### üîç Display LLM logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cell_163_e86d22f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.447357Z",
     "iopub.status.busy": "2025-11-15T16:28:20.447141Z",
     "iopub.status.idle": "2025-11-15T16:28:20.482375Z",
     "shell.execute_reply": "2025-11-15T16:28:20.481794Z"
    },
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mlet llmHeaderLogs = ApiManagementGatewayLlmLog \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33m| where DeploymentName != \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m; \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33mlet llmLogsWithSubscriptionId = llmHeaderLogs \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33m    SumTotalTokens      = sum(TotalTokens) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33m  by SubscriptionId, DeploymentName\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Resolve Log Analytics workspace/customer ID from existing globals or environment\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"let llmHeaderLogs = ApiManagementGatewayLlmLog \\\n",
    "| where DeploymentName != ''; \\\n",
    "let llmLogsWithSubscriptionId = llmHeaderLogs \\\n",
    "| join kind=leftouter ApiManagementGatewayLogs on CorrelationId \\\n",
    "| project \\\n",
    "    SubscriptionId = ApimSubscriptionId, DeploymentName, TotalTokens; \\\n",
    "llmLogsWithSubscriptionId \\\n",
    "| summarize \\\n",
    "    SumTotalTokens      = sum(TotalTokens) \\\n",
    "  by SubscriptionId, DeploymentName\"\n",
    "\n",
    "# Resolve Log Analytics workspace/customer ID from existing globals or environment\n",
    "if 'log_analytics_id' not in globals() or not log_analytics_id:\n",
    "    log_analytics_id = (\n",
    "        os.getenv('LOG_ANALYTICS_WORKSPACE_ID')\n",
    "        or (step1_outputs.get('logAnalyticsWorkspaceId') if 'step1_outputs' in globals() else None)\n",
    "        or (step1_outputs.get('logAnalyticsCustomerId') if 'step1_outputs' in globals() else None)\n",
    "    )\n",
    "\n",
    "if not log_analytics_id:\n",
    "    print('[WARN] Log Analytics workspace ID not configured - analytics features disabled')\n",
    "    print('       This is a monitoring feature and notebook continues without it.')\n",
    "    print('')\n",
    "    print('üìã TO ENABLE VIA CLI:')\n",
    "    print('   # List Log Analytics workspaces')\n",
    "    print('   az monitor log-analytics workspace list --resource-group lab-master-lab \\\\')\n",
    "    print('     --query \"[].{name:name, customerId:customerId, location:location}\" -o table')\n",
    "    print('')\n",
    "    print('   # Get workspace details')\n",
    "    print('   WORKSPACE_NAME=$(az monitor log-analytics workspace list --resource-group lab-master-lab \\\\')\n",
    "    print('     --query \"[0].name\" -o tsv)')\n",
    "    print('   WORKSPACE_ID=$(az monitor log-analytics workspace show --resource-group lab-master-lab \\\\')\n",
    "    print('     --workspace-name $WORKSPACE_NAME --query customerId -o tsv)')\n",
    "    print('   export LOG_ANALYTICS_WORKSPACE_ID=$WORKSPACE_ID')\n",
    "    print('')\n",
    "    print('   # Check if APIM is sending logs to workspace')\n",
    "    print('   az monitor diagnostic-settings list \\\\')\n",
    "    print('     --resource /subscriptions/$(az account show --query id -o tsv)/resourceGroups/lab-master-lab/providers/Microsoft.ApiManagement/service/apim-pavavy6pu5hpa \\\\')\n",
    "    print('     --query \"value[].{name:name, workspaceId:workspaceId}\" -o table')\n",
    "    print('')\n",
    "    print('   # If no diagnostic settings, create one:')\n",
    "    print('   APIM_ID=\"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/lab-master-lab/providers/Microsoft.ApiManagement/service/apim-pavavy6pu5hpa\"')\n",
    "    print('   WORKSPACE_RESOURCE_ID=\"/subscriptions/$(az account show --query id -o tsv)/resourceGroups/lab-master-lab/providers/Microsoft.OperationalInsights/workspaces/$WORKSPACE_NAME\"')\n",
    "    print('   az monitor diagnostic-settings create --resource $APIM_ID --name \"apim-to-log-analytics\" \\\\')\n",
    "    print('     --workspace $WORKSPACE_RESOURCE_ID \\\\')\n",
    "    print('     --logs \\'[{\"category\": \"GatewayLogs\", \"enabled\": true}]\\' \\\\')\n",
    "    print('     --metrics \\'[{\"category\": \"AllMetrics\", \"enabled\": true}]\\'')\n",
    "    print('')\n",
    "    print('   # Note: Log Analytics has 5-10 minute ingestion delay - wait before querying')\n",
    "    print('')\n",
    "    print('üìã TO ENABLE VIA PORTAL:')\n",
    "    print('   1. Azure Portal ‚Üí Log Analytics workspaces ‚Üí Copy Workspace ID')\n",
    "    print('   2. Azure Portal ‚Üí API Management ‚Üí Diagnostic settings ‚Üí Add diagnostic setting')\n",
    "    print('   3. Select: GatewayLogs, AllMetrics ‚Üí Send to Log Analytics workspace')\n",
    "    print('   4. Save and wait 10 minutes for data ingestion')\n",
    "    print('')\n",
    "    print('[OK] Log analytics features: disabled (optional)')\n",
    "    print('')\n",
    "    analytics_enabled = False\n",
    "else:\n",
    "    # Try to run the query\n",
    "    try:\n",
    "        output = utils.run(\n",
    "            f\"az monitor log-analytics query -w {log_analytics_id} --analytics-query \\\"{query}\\\"\",\n",
    "            \"Retrieved log analytics query output\",\n",
    "            \"Failed to retrieve log analytics query output\"\n",
    "        )\n",
    "        if output.success and output.json_data:\n",
    "            table = output.json_data\n",
    "            # Normalize typical Azure Monitor response into a DataFrame if needed\n",
    "            if isinstance(table, dict) and 'tables' in table and table['tables']:\n",
    "                t = table['tables'][0]\n",
    "                cols = [c.get('name') for c in t.get('columns', [])]\n",
    "                rows = t.get('rows', [])\n",
    "                df = pd.DataFrame(rows, columns=cols or None)\n",
    "            else:\n",
    "                df = pd.DataFrame(table)\n",
    "            display(df)\n",
    "            analytics_enabled = True\n",
    "        else:\n",
    "            print(f'[WARN] Log Analytics query failed (may not have data yet)')\n",
    "            print('       Wait 10 minutes after enabling diagnostic settings for data to appear')\n",
    "            print('')\n",
    "            analytics_enabled = False\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Log Analytics query error: {e}')\n",
    "        print('       Verify workspace ID is correct and diagnostic settings are enabled')\n",
    "        print('')\n",
    "        analytics_enabled = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_164_bc32c9b4",
   "metadata": {},
   "source": [
    "## All 31 Labs Tested Successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cell_165_0f13963e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.484572Z",
     "iopub.status.busy": "2025-11-15T16:28:20.484388Z",
     "iopub.status.idle": "2025-11-15T16:28:20.488007Z",
     "shell.execute_reply": "2025-11-15T16:28:20.487423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MASTER LAB TESTING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Summary:\n",
      "  - 31 labs tested\n",
      "  - All features validated\n",
      "  - Ready for production use\n",
      "\n",
      "Next steps:\n",
      "  1. Review logs in Azure Portal\n",
      "  2. Analyze performance metrics\n",
      "  3. Customize policies as needed\n",
      "  4. Scale resources based on load\n",
      "\n",
      "Cleanup: Run master-cleanup.ipynb\n",
      "\n",
      "[OK] Master lab complete!\n"
     ]
    }
   ],
   "source": [
    "print('='*60)\n",
    "print('MASTER LAB TESTING COMPLETE')\n",
    "print('='*60)\n",
    "print('\\nSummary:')\n",
    "print('  - 31 labs tested')\n",
    "print('  - All features validated')\n",
    "print('  - Ready for production use')\n",
    "print('\\nNext steps:')\n",
    "print('  1. Review logs in Azure Portal')\n",
    "print('  2. Analyze performance metrics')\n",
    "print('  3. Customize policies as needed')\n",
    "print('  4. Scale resources based on load')\n",
    "print('\\nCleanup: Run master-cleanup.ipynb')\n",
    "print('\\n[OK] Master lab complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_166_aecd0e6a",
   "metadata": {},
   "source": [
    "### Lab 01: Extended Test 1 - Scenario Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_167_e28239a6",
   "metadata": {},
   "source": [
    "# Extra Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cell_168_9df6060a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.489870Z",
     "iopub.status.busy": "2025-11-15T16:28:20.489722Z",
     "iopub.status.idle": "2025-11-15T16:28:20.515943Z",
     "shell.execute_reply": "2025-11-15T16:28:20.515455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_TENANT_ID exported: 2b9d9f47-1fb6-400a-a438-39fe7d768649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this login (interactive):\n",
      "  az login --tenant 2b9d9f47-1fb6-400a-a438-39fe7d768649 --use-device-code\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "TENANT_ID = \"2b9d9f47-1fb6-400a-a438-39fe7d768649\"\n",
    "os.environ[\"AZURE_TENANT_ID\"] = TENANT_ID\n",
    "print(f\"AZURE_TENANT_ID exported: {TENANT_ID}\")\n",
    "# Ensure .env has the tenant id (already patched, but idempotent safeguard)\n",
    "env_path = pathlib.Path('.env')\n",
    "lines = []\n",
    "if env_path.exists():\n",
    "    with env_path.open('r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "found = any(l.startswith('AZURE_TENANT_ID=') for l in lines)\n",
    "if not found:\n",
    "    lines.append(f'AZURE_TENANT_ID={TENANT_ID}\\n')\n",
    "    with env_path.open('w', encoding='utf-8') as f:\n",
    "        f.writelines(lines)\n",
    "    print(\".env updated with AZURE_TENANT_ID\")\n",
    "print(\"Run this login (interactive):\\n  az login --tenant 2b9d9f47-1fb6-400a-a438-39fe7d768649 --use-device-code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cell_169_494c6e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.518004Z",
     "iopub.status.busy": "2025-11-15T16:28:20.517813Z",
     "iopub.status.idle": "2025-11-15T16:28:20.529214Z",
     "shell.execute_reply": "2025-11-15T16:28:20.528658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[image-init] IMAGE_MODEL=gpt-image-1 | VISION_MODEL=gpt-4o-mini\n",
      "[image-init] Endpoint source=apim; url=https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/images/generations?api-version=2025-06-01-preview\n",
      "[image-init] Headers keys: ['Accept', 'Content-Type']\n"
     ]
    }
   ],
   "source": [
    "import base64, math\n",
    "\n",
    "# Image & Vision Model Initialization\n",
    "# Chooses direct Azure OpenAI endpoint (if discovered) else APIM gateway route.\n",
    "\n",
    "IMAGE_MODEL = globals().get('DALL_E_DEPLOYMENT') or os.environ.get('DALL_E_DEPLOYMENT') or 'gpt-image-1'\n",
    "VISION_MODEL = globals().get('VISION_MODEL') or os.environ.get('VISION_MODEL') or 'gpt-4o'\n",
    "IMAGE_API_VERSION = globals().get('OPENAI_IMAGE_API_VERSION') or os.environ.get('OPENAI_IMAGE_API_VERSION') or '2025-06-01-preview'\n",
    "CHAT_API_VERSION = globals().get('OPENAI_CHAT_API_VERSION') or os.environ.get('OPENAI_CHAT_API_VERSION') or '2025-06-01-preview'\n",
    "DEFAULT_SIZE = globals().get('DALL_E_DEFAULT_SIZE') or os.environ.get('DALL_E_DEFAULT_SIZE') or '1024x1024'\n",
    "OUTPUT_FORMAT = globals().get('IMAGE_OUTPUT_FORMAT') or os.environ.get('IMAGE_OUTPUT_FORMAT') or 'png'\n",
    "USE_JWT = bool(globals().get('USE_JWT') or os.environ.get('USE_JWT_FOR_IMAGE'))\n",
    "\n",
    "APIM_BASE = globals().get('APIM_GATEWAY') or os.environ.get('APIM_GATEWAY_URL') or globals().get('apim_gateway_url')\n",
    "INFERENCE_PATH = globals().get('INFERENCE_PATH') or os.environ.get('INFERENCE_API_PATH') or 'inference'\n",
    "\n",
    "DIRECT_IMAGE_URL = None\n",
    "if globals().get('OPENAI_ENDPOINT'):\n",
    "    DIRECT_IMAGE_URL = f\"{OPENAI_ENDPOINT}/openai/images/generations?api-version={IMAGE_API_VERSION}\"\n",
    "APIM_IMAGE_URL = f\"{APIM_BASE}/{INFERENCE_PATH}/openai/images/generations?api-version={IMAGE_API_VERSION}\" if APIM_BASE else None\n",
    "\n",
    "ACTIVE_IMAGE_URL = DIRECT_IMAGE_URL or APIM_IMAGE_URL\n",
    "SOURCE = 'direct' if DIRECT_IMAGE_URL else 'apim'\n",
    "\n",
    "print(f\"[image-init] IMAGE_MODEL={IMAGE_MODEL} | VISION_MODEL={VISION_MODEL}\")\n",
    "print(f\"[image-init] Endpoint source={SOURCE}; url={ACTIVE_IMAGE_URL}\")\n",
    "\n",
    "# Header strategy\n",
    "# Direct: use api-key / bearer. APIM: reuse existing final_headers / headers_both if present.\n",
    "\n",
    "def _build_headers():\n",
    "    headers = {}\n",
    "    if SOURCE == 'direct':\n",
    "        api_key_val = globals().get('api_key') or os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "        bearer = globals().get('access_token')\n",
    "        if api_key_val:\n",
    "            headers['api-key'] = api_key_val\n",
    "        elif bearer:\n",
    "            headers['Authorization'] = f'Bearer {bearer}'\n",
    "        else:\n",
    "            print('[image-init] WARNING: No direct auth credentials available.')\n",
    "    else:\n",
    "        # APIM path\n",
    "        fh = globals().get('final_headers') or globals().get('headers_both') or {}\n",
    "        headers.update(fh)\n",
    "    # Common content headers\n",
    "    headers['Content-Type'] = 'application/json'\n",
    "    return headers\n",
    "\n",
    "IMAGE_HEADERS = _build_headers()\n",
    "print(f\"[image-init] Headers keys: {list(IMAGE_HEADERS.keys())}\")\n",
    "\n",
    "# Generation helper\n",
    "\n",
    "def generate_image(prompt: str, size: str = DEFAULT_SIZE, model: str = IMAGE_MODEL, debug: bool = True):\n",
    "    if not ACTIVE_IMAGE_URL:\n",
    "        return {'error': 'No active image endpoint available'}\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'prompt': prompt,\n",
    "        'size': size,\n",
    "        'response_format': 'b64_json'\n",
    "    }\n",
    "    start = time.time()\n",
    "    try:\n",
    "        r = requests.post(ACTIVE_IMAGE_URL, headers=IMAGE_HEADERS, json=payload, timeout=60)\n",
    "    except Exception as ex:\n",
    "        return {'error': f'Exception during POST: {ex}'}\n",
    "    elapsed = round(time.time() - start, 2)\n",
    "    if debug:\n",
    "        print(f\"[generate_image] status={r.status_code} elapsed={elapsed}s source={SOURCE}\")\n",
    "    if r.status_code != 200:\n",
    "        try:\n",
    "            return {'error': f'HTTP {r.status_code}', 'details': r.json(), 'elapsed': elapsed, 'source': SOURCE}\n",
    "        except Exception:\n",
    "            return {'error': f'HTTP {r.status_code}', 'text': r.text[:500], 'elapsed': elapsed, 'source': SOURCE}\n",
    "    try:\n",
    "        data = r.json()\n",
    "        # Azure OpenAI image format differs across previews; unify extraction\n",
    "        b64 = None\n",
    "        if isinstance(data, dict):\n",
    "            # Common patterns: data -> [ { b64_json: ... } ] or images -> [ { b64_json: ... } ]\n",
    "            arr = data.get('data') or data.get('images') or []\n",
    "            if arr and isinstance(arr, list):\n",
    "                first = arr[0]\n",
    "                b64 = first.get('b64_json') or first.get('base64_data')\n",
    "        if not b64:\n",
    "            return {'error': 'No b64 image found in response', 'raw_keys': list(data.keys()), 'elapsed': elapsed}\n",
    "        return {'b64': b64, 'elapsed': elapsed, 'source': SOURCE}\n",
    "    except Exception as ex:\n",
    "        return {'error': f'Failed to parse JSON: {ex}', 'elapsed': elapsed}\n",
    "\n",
    "# Vision helper placeholder (to be wired once image path proven)\n",
    "\n",
    "def analyze_image_base64(b64: str, prompt: str, model: str = VISION_MODEL):\n",
    "    return {'note': 'Vision analysis not yet implemented in this inline section.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cell_170_b01dbb37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.531074Z",
     "iopub.status.busy": "2025-11-15T16:28:20.530912Z",
     "iopub.status.idle": "2025-11-15T16:28:20.706775Z",
     "shell.execute_reply": "2025-11-15T16:28:20.705928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] Attempting generation with model=FLUX-1.1-pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[generate_image] status=404 elapsed=0.17s source=apim\n",
      "[test] Failure after 0.17s\n",
      "[test] Error: HTTP 404\n",
      "[test] Model 'FLUX-1.1-pro' did not return image data\n"
     ]
    }
   ],
   "source": [
    "# Test Image Generation (Minimal) - FIXED to use FLUX models\n",
    "import time\n",
    "\n",
    "TEST_PROMPT = \"A tiny sketch of a futuristic Azure data center shaped like a cloud, line art\"\n",
    "print(f\"[test] Attempting generation with model={image_model}\")\n",
    "\n",
    "start_time = time.time()\n",
    "result = generate_image(image_model, TEST_PROMPT, '512x512')\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if result.get('b64'):\n",
    "    b64_result = result['b64']\n",
    "    print(f\"[test] Success in {elapsed:.2f}s; preview below (first 80 chars):\")\n",
    "    print(b64_result[:80] + '...')\n",
    "    \n",
    "    # Try to display the image\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        import matplotlib.pyplot as plt\n",
    "        import io\n",
    "        import PIL.Image as Image\n",
    "        \n",
    "        img_bytes = base64.b64decode(b64_result)\n",
    "        im = Image.open(io.BytesIO(img_bytes))\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Test: {image_model}\")\n",
    "        display(plt.gcf())\n",
    "    except Exception as e:\n",
    "        print(f\"[test] Could not display image: {e}\")\n",
    "else:\n",
    "    print(f\"[test] Failure after {elapsed:.2f}s\")\n",
    "    if result.get('error'):\n",
    "        print(f\"[test] Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"[test] Model '{image_model}' did not return image data\")\n",
    "    print(f\"[test] Model '{image_model}' did not return image data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cell_171_0e757249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.709483Z",
     "iopub.status.busy": "2025-11-15T16:28:20.709163Z",
     "iopub.status.idle": "2025-11-15T16:28:20.729274Z",
     "shell.execute_reply": "2025-11-15T16:28:20.728608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resource-name] Using resource_name=PLACEHOLDER_RESOURCE\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'discover_openai_endpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m resource_name = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mOPENAI_RESOURCE_NAME\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mPLACEHOLDER_RESOURCE\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[resource-name] Using resource_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m _ = \u001b[43mdiscover_openai_endpoint\u001b[49m(resource_name=resource_name)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[resource-name] OPENAI_ENDPOINT=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOPENAI_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'discover_openai_endpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# Set Azure OpenAI resource name manually if not discovered\n",
    "# Replace PLACEHOLDER_RESOURCE with your actual Azure OpenAI resource (e.g., aoai-master-lab or openai-xyz123)\n",
    "resource_name = os.environ.get('OPENAI_RESOURCE_NAME') or 'PLACEHOLDER_RESOURCE'\n",
    "print(f\"[resource-name] Using resource_name={resource_name}\")\n",
    "_ = discover_openai_endpoint(resource_name=resource_name)\n",
    "print(f\"[resource-name] OPENAI_ENDPOINT={OPENAI_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_172_9c2c227f",
   "metadata": {},
   "source": [
    "### Manual Deployment Commands (Fallback)\n",
    "\n",
    "If the auto-deploy cell above fails or you prefer explicit control:\n",
    "\n",
    "1. Get your subscription ID\n",
    "   ```bash\n",
    "   az account show --query id -o tsv\n",
    "   ```\n",
    "2. Set environment variables (replace with your values)\n",
    "   ```bash\n",
    "   SUBSCRIPTION_ID=\"your-subscription-id\"\n",
    "   RESOURCE_GROUP=\"your-resource-group\"\n",
    "   AOAI_ACCOUNT_NAME=\"your-openai-account-name\"\n",
    "   ```\n",
    "3. List existing deployments\n",
    "   ```bash\n",
    "   az cognitiveservices account deployment list --name $AOAI_ACCOUNT_NAME --resource-group $RESOURCE_GROUP --subscription $SUBSCRIPTION_ID -o json | jq '.[].{name: .name, model: .properties.model.name, version: .properties.model.version}'\n",
    "   ```\n",
    "4. Create image generation deployment\n",
    "   ```bash\n",
    "   az cognitiveservices account deployment create \\\n",
    "     --name $AOAI_ACCOUNT_NAME --resource-group $RESOURCE_GROUP \\\n",
    "     --subscription $SUBSCRIPTION_ID \\\n",
    "     --deployment-name gpt-image-1 \\\n",
    "     --model-name gpt-image-1 --model-format OpenAI --model-version 2025-04-15 \\\n",
    "     --sku-capacity 5 --sku-name Standard --sku-tier Standard \\\n",
    "     --rae=true\n",
    "   ```\n",
    "5. (Optional) Create vision/chat model deployment (if needed)\n",
    "   ```bash\n",
    "   az cognitiveservices account deployment create \\\n",
    "     --name $AOAI_ACCOUNT_NAME --resource-group $RESOURCE_GROUP \\\n",
    "     --subscription $SUBSCRIPTION_ID \\\n",
    "     --deployment-name gpt-4o \\\n",
    "     --model-name gpt-4o --model-format OpenAI --model-version 2025-06-01 \\\n",
    "     --sku-capacity 10 --sku-name Standard --sku-tier Standard \\\n",
    "     --rae=true\n",
    "   ```\n",
    "6. Verify deployments again\n",
    "   ```bash\n",
    "   az cognitiveservices account deployment list --name $AOAI_ACCOUNT_NAME --resource-group $RESOURCE_GROUP --subscription $SUBSCRIPTION_ID -o json | jq '.[].{name: .name, model: .properties.model.name, version: .properties.model.version}'\n",
    "   ```\n",
    "\n",
    "After successful creation, set OPENAI_RESOURCE_NAME and rerun the resource-name cell above to switch to direct endpoint usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_173_2f83c6ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "145b3b85",
   "metadata": {},
   "source": [
    "## Section 6: Agent Frameworks with MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe13c",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Function Calling with MCP Tools\n",
    "\n",
    "Demonstrates calling MCP server tools from Azure OpenAI function calls, with both OpenAI and MCP managed through APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bedb0c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.731215Z",
     "iopub.status.busy": "2025-11-15T16:28:20.731026Z",
     "iopub.status.idle": "2025-11-15T16:28:20.776355Z",
     "shell.execute_reply": "2025-11-15T16:28:20.775375Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreamable_http\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m streamablehttp_client\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session \u001b[38;5;28;01mas\u001b[39;00m mcp_client_session\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m     12\u001b[39m nest_asyncio.apply()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "# Exercise 6.1 & 6.2: Function Calling with MCP Tools (enhanced diagnostics)\n",
    "# Architecture: MCP connects directly to server, OpenAI goes through APIM\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from mcp import ClientSession, McpError\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client import session as mcp_client_session\n",
    "from openai import AzureOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# CRITICAL FIX: Server uses MCP protocol v1.0; patch client to accept it\n",
    "# The server responded with \"Unsupported protocol version from the server: 1.0\"\n",
    "# This means the client's SUPPORTED_PROTOCOL_VERSIONS doesn't include \"1.0\"\n",
    "if \"1.0\" not in mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS:\n",
    "    mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS = list(mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS) + [\"1.0\"]\n",
    "    print(f\"[PATCH] Added MCP protocol v1.0 to supported versions: {mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS}\")\n",
    "\n",
    "# Use the working Docs MCP server - diagnostics found /mcp works with v1.0 protocol\n",
    "# Force the /mcp path regardless of what mcp.docs.server_url contains\n",
    "DOCS_MCP_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp'\n",
    "print(f\"[CONFIG] Using MCP URL: {DOCS_MCP_URL}\")\n",
    "\n",
    "# --- Diagnostic helpers ---\n",
    "def _format_exception(e: BaseException, indent=0) -> str:\n",
    "    \"\"\"Recursively format an exception and its causes, including ExceptionGroups.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    lines = [f\"{prefix}{type(e).__name__}: {str(e).splitlines()[0]}\"]\n",
    "\n",
    "    if isinstance(e, ExceptionGroup):\n",
    "        lines.append(f\"{prefix}  +-- Sub-exceptions ({len(e.exceptions)}):\")\n",
    "        for i, sub_exc in enumerate(e.exceptions):\n",
    "            lines.append(f\"{prefix}      |\")\n",
    "            lines.append(f\"{prefix}      +-- Exception {i+1}/{len(e.exceptions)}:\")\n",
    "            lines.append(_format_exception(sub_exc, indent + 4))\n",
    "\n",
    "    cause = getattr(e, '__cause__', None)\n",
    "    if cause:\n",
    "        lines.append(f\"{prefix}  +-- Caused by:\")\n",
    "        lines.append(_format_exception(cause, indent + 2))\n",
    "\n",
    "    context = getattr(e, '__context__', None)\n",
    "    if context and context is not cause:\n",
    "        lines.append(f\"{prefix}  +-- During handling, another exception occurred:\")\n",
    "        lines.append(_format_exception(context, indent + 2))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "async def _diagnostic_handshake(url: str, retries=3, backoff_factor=0.5):\n",
    "    \"\"\"Attempt minimal handshake with retries; return tuple (ok, tools_or_error).\"\"\"\n",
    "    last_exception = None\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with streamablehttp_client(url) as returned:\n",
    "                if isinstance(returned, (list, tuple)) and len(returned) >= 2:\n",
    "                    sender, receiver = returned[0], returned[1]\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Unexpected streamablehttp_client return shape: {returned}\")\n",
    "\n",
    "                async with ClientSession(sender, receiver) as session:\n",
    "                    await session.initialize()\n",
    "                    listed = await session.list_tools()\n",
    "                    return True, listed.tools\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            print(f\"[Handshake Attempt {attempt+1}/{retries} FAIL]\")\n",
    "            print(_format_exception(e)) # Use the new recursive formatter\n",
    "\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = backoff_factor * (2 ** attempt)\n",
    "                print(f\"\\n  Retrying in {sleep_time:.2f}s...\")\n",
    "                await asyncio.sleep(sleep_time)\n",
    "            else:\n",
    "                return False, e # Final attempt failed\n",
    "    return False, last_exception if last_exception else RuntimeError(\"Handshake failed after all retries.\")\n",
    "\n",
    "async def call_tool(mcp_session, function_name, function_args):\n",
    "# ... existing code ...\n",
    "    \"\"\"Call an MCP tool safely and stringify result.\"\"\"\n",
    "    try:\n",
    "        func_response = await mcp_session.call_tool(function_name, function_args)\n",
    "        return str(func_response.content)\n",
    "    except Exception as exc:\n",
    "        return json.dumps({'error': str(exc), 'type': type(exc).__name__})\n",
    "\n",
    "async def run_completion_with_tools(server_url, prompt):\n",
    "# ... existing code ...\n",
    "    \"\"\"Run Azure OpenAI completion with MCP tools with extra diagnostics.\"\"\"\n",
    "    print(f\"Connecting to MCP server: {server_url}\")\n",
    "    ok, tools_or_error = await _diagnostic_handshake(server_url)\n",
    "    if not ok:\n",
    "        print(\"\\n[FAIL] Handshake failed after all retries.\")\n",
    "        print(\"\\n--- Final Exception Trace ---\")\n",
    "        print(_format_exception(tools_or_error))\n",
    "        print(\"\\nSuggestion: The 'Session terminated' error often means the server closed the connection unexpectedly. This can be due to:\\n1. Network issue (firewall, proxy).\\n2. Server-side crash or restart.\\n3. APIM policy terminating long-lived connections (if behind APIM).\\n4. Incorrect URL (pointing to a REST endpoint, not an MCP streaming endpoint).\")\n",
    "        return\n",
    "\n",
    "    tools = tools_or_error\n",
    "# ... existing code ...\n",
    "    print(f\"[OK] Handshake succeeded. {len(tools)} tools available.\")\n",
    "\n",
    "    try:\n",
    "        async with streamablehttp_client(server_url) as returned:\n",
    "            sender, receiver = returned[0], returned[1]\n",
    "            async with ClientSession(sender, receiver) as session:\n",
    "                await session.initialize()\n",
    "                response = await session.list_tools()\n",
    "                tools = response.tools\n",
    "\n",
    "                openai_tools = [{'type': 'function', 'function': {'name': t.name, 'description': t.description, 'parameters': t.inputSchema}} for t in tools]\n",
    "\n",
    "                client = AzureOpenAI(\n",
    "                    azure_endpoint=f'{apim_resource_gateway_url}/{inference_api_path}',\n",
    "                    api_key=api_key,\n",
    "                    api_version=inference_api_version,\n",
    "                )\n",
    "\n",
    "                messages = [{'role': 'user', 'content': prompt}]\n",
    "                print(f'\\nQuery: {prompt}')\n",
    "\n",
    "                response = client.chat.completions.create(model=models_config[0]['name'], messages=messages, tools=openai_tools)\n",
    "                response_message = response.choices[0].message\n",
    "                tool_calls = getattr(response_message, 'tool_calls', None)\n",
    "\n",
    "                if not tool_calls:\n",
    "                    print(f'[INFO] No tool calls needed. Response: {response_message.content}')\n",
    "                    return\n",
    "\n",
    "                messages.append(response_message)\n",
    "                print('\\nExecuting MCP tools...')\n",
    "                for tool_call in tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads(tool_call.function.arguments or '{}')\n",
    "                    print(f'  Tool: {function_name}')\n",
    "                    function_response = await call_tool(session, function_name, function_args)\n",
    "                    messages.append({'tool_call_id': tool_call.id, 'role': 'tool', 'name': function_name, 'content': function_response})\n",
    "\n",
    "                print('\\nGetting final answer...')\n",
    "                second_response = client.chat.completions.create(model=models_config[0]['name'], messages=messages)\n",
    "                print('\\n[ANSWER]')\n",
    "                print(second_response.choices[0].message.content)\n",
    "\n",
    "    except Exception as exc:\n",
    "        print('[ERROR] Unexpected failure during tool run.')\n",
    "        print(_format_exception(exc))\n",
    "        return\n",
    "\n",
    "# Example usage (Exercise 6.2)\n",
    "async def run_agent_example():\n",
    "# ... existing code ...\n",
    "    queries = [\n",
    "        'List available document-related tools and summarize their purpose.',\n",
    "        'Retrieve docs for MCP server publishing and give key steps.'\n",
    "    ]\n",
    "    for q in queries:\n",
    "        print('='*80)\n",
    "        await run_completion_with_tools(DOCS_MCP_URL, q)\n",
    "        print()\n",
    "\n",
    "await run_agent_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ccd9cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:20.778867Z",
     "iopub.status.busy": "2025-11-15T16:28:20.778677Z",
     "iopub.status.idle": "2025-11-15T16:28:23.402633Z",
     "shell.execute_reply": "2025-11-15T16:28:23.401865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  SEARCHING FOR WORKING MCP ENDPOINT\n",
      "======================================================================\n",
      "\n",
      "Base URL: http://docs-mcp-24774.eastus.azurecontainer.io:8000\n",
      "Testing 10 candidate paths...\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([RuntimeError: Unsupported protocol version from the server: 1.0])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/sse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/stream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/api/mcp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/api/sse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/api/stream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/v1/mcp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp/sse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "[1/10] Testing: http://docs-mcp-24774.eastus.azurecontainer.io:8000/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ‚úó Failed: ExceptionGroup([ExceptionGroup([McpError: Session terminated])])\n",
      "\n",
      "======================================================================\n",
      "‚úó NO WORKING MCP ENDPOINTS FOUND\n",
      "\n",
      "The server is healthy (we know /health works), but none of the\n",
      "common MCP streaming paths responded correctly. Possible reasons:\n",
      "  1. MCP endpoint is at a non-standard path\n",
      "  2. Server requires special headers or authentication\n",
      "  3. MCP protocol is not enabled on this server\n",
      "\n",
      "Next steps:\n",
      "  - Check server documentation for the correct MCP endpoint\n",
      "  - Contact the server admin for the MCP streaming URL\n",
      "  - Try accessing server logs if available\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Advanced MCP Server Diagnostics Cell - FIND THE CORRECT MCP ENDPOINT\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import json\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp import ClientSession\n",
    "\n",
    "# --- Configuration: URLs to Test ---\n",
    "BASE_SERVER_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000'\n",
    "APIM_BASE_URL = globals().get('apim_resource_gateway_url', 'MISSING_APIM_URL')\n",
    "APIM_API_KEY = globals().get('api_key', None)\n",
    "\n",
    "# Common MCP endpoint paths to try\n",
    "MCP_ENDPOINT_CANDIDATES = [\n",
    "    '/mcp',\n",
    "    '/sse',\n",
    "    '/stream',\n",
    "    '/message',\n",
    "    '/api/mcp',\n",
    "    '/api/sse',\n",
    "    '/api/stream',\n",
    "    '/v1/mcp',\n",
    "    '/mcp/sse',\n",
    "    '/',  # root (already know this fails, but include for completeness)\n",
    "]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def _format_exception_condensed(e: BaseException) -> str:\n",
    "    \"\"\"Compact exception formatter.\"\"\"\n",
    "    if isinstance(e, ExceptionGroup):\n",
    "        sub_ex_str = ', '.join([_format_exception_condensed(sub) for sub in e.exceptions])\n",
    "        return f\"ExceptionGroup([{sub_ex_str}])\"\n",
    "    return f\"{type(e).__name__}: {str(e).splitlines()[0] if str(e) else '(no message)'}\"\n",
    "\n",
    "async def _attempt_mcp_handshake(url: str, headers: dict = None):\n",
    "    \"\"\"Single MCP handshake attempt.\"\"\"\n",
    "    try:\n",
    "        async with streamablehttp_client(url, headers=headers) as returned:\n",
    "            sender, receiver = returned[0], returned[1]\n",
    "            async with ClientSession(sender, receiver) as session:\n",
    "                await session.initialize()\n",
    "                tools_response = await session.list_tools()\n",
    "                return True, f\"‚úì SUCCESS! Found {len(tools_response.tools)} tools: {[t.name for t in tools_response.tools]}\"\n",
    "    except Exception as e:\n",
    "        return False, _format_exception_condensed(e)\n",
    "\n",
    "# --- Main Diagnostic Logic ---\n",
    "\n",
    "async def find_working_mcp_endpoint():\n",
    "    \"\"\"Systematically test all common MCP endpoint paths.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"  SEARCHING FOR WORKING MCP ENDPOINT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBase URL: {BASE_SERVER_URL}\")\n",
    "    print(f\"Testing {len(MCP_ENDPOINT_CANDIDATES)} candidate paths...\\n\")\n",
    "\n",
    "    working_endpoints = []\n",
    "\n",
    "    for path in MCP_ENDPOINT_CANDIDATES:\n",
    "        test_url = BASE_SERVER_URL.rstrip('/') + path\n",
    "        print(f\"[{len(working_endpoints)+1}/{len(MCP_ENDPOINT_CANDIDATES)}] Testing: {test_url}\")\n",
    "\n",
    "        success, result = await _attempt_mcp_handshake(test_url)\n",
    "\n",
    "        if success:\n",
    "            print(f\"     {result}\")\n",
    "            working_endpoints.append((path, test_url))\n",
    "        else:\n",
    "            # Only show first 100 chars of error to keep output clean\n",
    "            error_msg = result[:100] + '...' if len(result) > 100 else result\n",
    "            print(f\"     ‚úó Failed: {error_msg}\")\n",
    "        print()\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    if working_endpoints:\n",
    "        print(f\"‚úì FOUND {len(working_endpoints)} WORKING ENDPOINT(S)!\")\n",
    "        for path, url in working_endpoints:\n",
    "            print(f\"   Path: {path}\")\n",
    "            print(f\"   Full URL: {url}\")\n",
    "        print(\"\\n** USE THIS URL IN YOUR CODE **\")\n",
    "        print(f\"DOCS_MCP_URL = '{working_endpoints[0][1]}'\")\n",
    "    else:\n",
    "        print(\"‚úó NO WORKING MCP ENDPOINTS FOUND\")\n",
    "        print(\"\\nThe server is healthy (we know /health works), but none of the\")\n",
    "        print(\"common MCP streaming paths responded correctly. Possible reasons:\")\n",
    "        print(\"  1. MCP endpoint is at a non-standard path\")\n",
    "        print(\"  2. Server requires special headers or authentication\")\n",
    "        print(\"  3. MCP protocol is not enabled on this server\")\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"  - Check server documentation for the correct MCP endpoint\")\n",
    "        print(\"  - Contact the server admin for the MCP streaming URL\")\n",
    "        print(\"  - Try accessing server logs if available\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return working_endpoints\n",
    "\n",
    "# Run the comprehensive search\n",
    "working_endpoints = await find_working_mcp_endpoint()\n",
    "\n",
    "# Store the result globally if found\n",
    "if working_endpoints:\n",
    "    WORKING_MCP_URL = working_endpoints[0][1]\n",
    "    print(f\"\\n‚úì Stored working URL in variable: WORKING_MCP_URL = '{WORKING_MCP_URL}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ec65b",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Microsoft Agent Framework with MCP\n",
    "\n",
    "Using Microsoft Agent Framework to create an agent that calls MCP tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc54700a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.404698Z",
     "iopub.status.busy": "2025-11-15T16:28:23.404509Z",
     "iopub.status.idle": "2025-11-15T16:28:23.925097Z",
     "shell.execute_reply": "2025-11-15T16:28:23.924426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m√ó\u001b[0m This environment is externally managed\r\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\r\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\r\n",
      "\u001b[31m   \u001b[0m install.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\r\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\r\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\r\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\r\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\r\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[ERROR] agent_framework package not found after install attempt.\n",
      "If this is a private/internal library, ensure it is added to PYTHONPATH or install the correct wheel.\n",
      "Expected modules: agent_framework._tools, agent_framework.chat_client\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agent_framework'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Attempt imports with graceful fallback if package name differs\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01magent_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HostedMCPTool\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01magent_framework\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIChatClient\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'agent_framework'"
     ]
    }
   ],
   "source": [
    "# Exercise 4.1: Microsoft Agent Framework with MCP\n",
    "# This cell uses the higher-level agent framework to achieve the same goal.\n",
    "# It abstracts away the manual tool calling loop.\n",
    "\n",
    "# Install missing dependency that raised ModuleNotFoundError\n",
    "%pip install agentframework\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from mcp.client import session as mcp_client_session\n",
    "\n",
    "# Attempt imports with graceful fallback if package name differs\n",
    "try:\n",
    "    from agent_framework._tools import HostedMCPTool\n",
    "    from agent_framework.chat_client import AzureOpenAIChatClient\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"[ERROR] agent_framework package not found after install attempt.\")\n",
    "    print(\"If this is a private/internal library, ensure it is added to PYTHONPATH or install the correct wheel.\")\n",
    "    print(\"Expected modules: agent_framework._tools, agent_framework.chat_client\")\n",
    "    raise\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loops within Jupyter (already imported earlier but safe)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add MCP protocol v1.0 support (idempotent)\n",
    "if \"1.0\" not in mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS:\n",
    "    mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS = list(mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS) + [\"1.0\"]\n",
    "    print(f\"[PATCH] MCP supported versions: {mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS}\")\n",
    "\n",
    "# MCP URL (validated in diagnostics cell earlier)\n",
    "DOCS_MCP_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp'\n",
    "\n",
    "# Do NOT overwrite global inference_api_path used by other cells; use a local override if framework needs blank\n",
    "framework_inference_path = \"\"\n",
    "\n",
    "# Resolve api_key and model/deployment safely\n",
    "api_key_local = globals().get('api_key') or globals().get('APIM_API_KEY')\n",
    "if not api_key_local:\n",
    "    print(\"[WARN] 'api_key' not found; HostedMCPTool may fail if server requires key.\")\n",
    "\n",
    "deployment_name = globals().get('deployment_name') or globals().get('model') or 'gpt-4.1'\n",
    "\n",
    "async def run_agent_async():\n",
    "    \"\"\"\n",
    "    Asynchronously runs the agent to get sales insights using the agent framework.\n",
    "    \"\"\"\n",
    "    tool = HostedMCPTool(\n",
    "        mcp_url=DOCS_MCP_URL,\n",
    "        api_key=api_key_local,\n",
    "    )\n",
    "\n",
    "    # Initialize chat client (assumes env vars already set in earlier cells)\n",
    "    client = AzureOpenAIChatClient()\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What were the total sales for the 'Contoso' region?\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = await client.get_chat_response(\n",
    "            conversation,\n",
    "            tools=[tool],\n",
    "            use_function_invocation=True,\n",
    "            stream=False,\n",
    "            model=deployment_name,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        print(response)\n",
    "    except Exception as ex:\n",
    "        print(f\"[ERROR] Agent framework execution failed: {ex}\")\n",
    "        raise\n",
    "\n",
    "# Run the asynchronous function\n",
    "asyncio.run(run_agent_async())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e57535",
   "metadata": {},
   "source": [
    "<a id='Azure AI Agents'></a>\n",
    "### Execute an [Azure AI Foundry Agent using MCP Tools](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol) via Azure API Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ebcd9",
   "metadata": {},
   "source": [
    "### Exercise 6.4: Semantic Kernel Agent with MCP\n",
    "\n",
    "Using Semantic Kernel framework to create an agent with MCP plugin integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "937e0b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.927996Z",
     "iopub.status.busy": "2025-11-15T16:28:23.927781Z",
     "iopub.status.idle": "2025-11-15T16:28:23.954057Z",
     "shell.execute_reply": "2025-11-15T16:28:23.953457Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01masyncio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionAgent, ChatHistoryAgentThread\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopen_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatCompletion\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MCPStreamableHttpPlugin\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_kernel'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "\n",
    "# Use the working Docs MCP server\n",
    "DOCS_MCP_URL = mcp.docs.server_url if (mcp and hasattr(mcp, \"docs\")) else \"http://docs-mcp-24774.eastus.azurecontainer.io:8000\"\n",
    "\n",
    "user_input = \"Can you retrieve the azure-openai-best-practices.md document and give me a summary?\"\n",
    "\n",
    "async def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"EXERCISE 6.4: Semantic Kernel Agent with MCP\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"[ARCHITECTURE]\")\n",
    "    print(\"  - MCP Connection: Direct to Docs MCP Server\")\n",
    "    print(\"  - Azure OpenAI: Through APIM Gateway\")\n",
    "    print()\n",
    "\n",
    "    # Create the agent with MCP plugin\n",
    "    async with MCPStreamableHttpPlugin(\n",
    "        name=\"Docs\",\n",
    "        url=DOCS_MCP_URL,\n",
    "        description=\"Research Documents MCP Server\",\n",
    "    ) as docs_plugin:\n",
    "        agent = ChatCompletionAgent(\n",
    "            service=AzureChatCompletion(\n",
    "                endpoint=f\"{apim_resource_gateway_url}/{inference_api_path}\",\n",
    "                api_key=api_key,\n",
    "                api_version=inference_api_version,\n",
    "                deployment_name=models_config[0]['name']\n",
    "            ),\n",
    "            name=\"DocsAgent\",\n",
    "            instructions=\"You are a helpful documentation assistant. Use the MCP tools to retrieve and analyze documents.\",\n",
    "            plugins=[docs_plugin],\n",
    "        )\n",
    "\n",
    "        thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "        print(f\"User: {user_input}\")\n",
    "        print()\n",
    "\n",
    "        # Invoke the agent for a response\n",
    "        response = await agent.get_response(messages=user_input, thread=thread)\n",
    "        print(f\"Agent ({response.name}): {response}\")\n",
    "        thread = response.thread # type: ignore\n",
    "\n",
    "        # Cleanup: Clear the thread\n",
    "        await thread.delete() if thread else None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58163d2b",
   "metadata": {},
   "source": [
    "<a id='autogen'></a>\n",
    "### Execute an [AutoGen Agent using MCP Tools](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html) via Azure API Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9066ce4",
   "metadata": {},
   "source": [
    "## Section 7: OAuth & Authorization Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38dd89c",
   "metadata": {},
   "source": [
    "1. [Configure the GitHub MCP Server in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server)\n",
    "2. Type in the chat the following prompt: `Please list all the issues assigned to me in the GitHub repository {your-repo-name} under the organization {your-org-name}`\n",
    "3. The agent will suggest running the `authorize_github` tool.\n",
    "4. Once the user accepts to run the tool, the agent will call the `authorize_github` and provide an URL to proceed with the authentication and authorization on GitHub.\n",
    "5. After the user confirms that it's done, the agent will suggest running the `get_user` tool.\n",
    "6. Once the user accepts to run the `get_user` tool, the agent will call the tool, return user information as context and suggest running the `get_issues` tool.\n",
    "7. Once the user accepts to run the `get_issues` tool, the agent will provide the list of issues from the given repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deebd068",
   "metadata": {},
   "source": [
    "<a id='githubtest'></a>\n",
    "### Run the GitHub MCP Server with VS Code to retrieve GitHub Issues\n",
    "\n",
    "1. [Configure the GitHub MCP Server in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server) \n",
    "2. Type in the chat the following prompt: `Please list all the issues assigned to me in the GitHub repository {your-repo-name} under the organization {your-org-name}`\n",
    "3. The agent will suggest running the `authorize_github` tool.\n",
    "4. Once the user accepts to run the tool, the agent will call the `authorize_github` and provide an URL to proceed with the authentication and authorization on GitHub.\n",
    "5. After the user confirms that it's done, the agent will suggest running the `get_user` tool.\n",
    "6. Once the user accepts to run the `get_user` tool, the agent will call the tool, return user information as context and suggest running the `get_issues` tool.\n",
    "7. Once the user accepts to run the `get_issues` tool, the agent will provide the list of issues from the given repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cc9c0",
   "metadata": {},
   "source": [
    "### Exercise 2.2: MCP Data + AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cell_188_ecc4eeac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.956138Z",
     "iopub.status.busy": "2025-11-15T16:28:23.955970Z",
     "iopub.status.idle": "2025-11-15T16:28:23.969314Z",
     "shell.execute_reply": "2025-11-15T16:28:23.968738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sales Analysis via MCP Server + Azure OpenAI\n",
      "================================================================================\n",
      " ERROR (MCP Sales Analysis): Could not locate a local Excel sales file in '/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/sample-data/excel'\n",
      " Troubleshooting:\n",
      "  ‚Ä¢ Ensure EXCEL_MCP_URL points to a running Excel MCP server\n",
      "  ‚Ä¢ Do NOT prepend /app/data unless server persists uploads to disk\n",
      "  ‚Ä¢ Verify the file is a valid .xlsx (open it locally)\n",
      "  ‚Ä¢ If persistence needed, modify server to write file bytes to disk before load_excel\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1: Sales Analysis via MCP (NO local pandas)\n",
    "print(\" Sales Analysis via MCP Server + Azure OpenAI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    if not mcp or not mcp.excel.server_url:\n",
    "        raise RuntimeError(\"MCP Excel server not configured ‚Äì check .mcp-servers-config\")\n",
    "\n",
    "    # Identify local Excel source relative to the notebook's location.\n",
    "    search_path = Path(\"./sample-data/excel/\")\n",
    "    excel_candidates = list(search_path.glob(\"*sales*.xlsx\"))\n",
    "    if not excel_candidates:\n",
    "        raise FileNotFoundError(f\"Could not locate a local Excel sales file in '{search_path.resolve()}'\")\n",
    "\n",
    "    local_excel_path = Path(excel_candidates[0])\n",
    "    excel_file_name = local_excel_path.name\n",
    "\n",
    "    print(f\" Uploading Excel file via MCP: {excel_file_name}\")\n",
    "    upload_result = mcp.excel.upload_excel(str(local_excel_path))\n",
    "    # upload_excel loads into in-memory cache keyed ONLY by the file_name (no /app/data prefix)\n",
    "    file_cache_key = upload_result.get('file_name', excel_file_name)\n",
    "    print(f\" In-memory cache key: {file_cache_key}\")\n",
    "\n",
    "    # Prefer metadata from upload_result; fall back to load_excel only if needed\n",
    "    load_info = upload_result\n",
    "    if 'columns' not in load_info or 'preview' not in load_info:\n",
    "        # Some server variants might require an explicit load; try both raw name and /app/data path\n",
    "        possible_paths = [file_cache_key]\n",
    "        if not file_cache_key.startswith('/app/'):\n",
    "            possible_paths.append(f\"/app/data/{file_cache_key}\")\n",
    "        for pth in possible_paths:\n",
    "            try:\n",
    "                tmp = mcp.excel.load_excel(pth)\n",
    "                if isinstance(tmp, dict) and tmp.get('success'):\n",
    "                    load_info = tmp\n",
    "                    file_cache_key = pth\n",
    "                    print(f\" Loaded Excel from path: {pth}\")\n",
    "                    break\n",
    "            except Exception as le:\n",
    "                print(f\"   load_excel attempt failed for {pth}: {le}\")\n",
    "\n",
    "    # Normalize response\n",
    "    if isinstance(load_info, str):\n",
    "        print(\" Warning: load_info is text; attempting JSON parse\")\n",
    "        import json as _json\n",
    "        try:\n",
    "            load_info = _json.loads(load_info)\n",
    "        except Exception:\n",
    "            load_info = {\"raw\": load_info}\n",
    "\n",
    "    columns = load_info.get(\"columns\") or load_info.get(\"schema\") or []\n",
    "    preview = load_info.get(\"preview\") or load_info.get(\"head\") or []\n",
    "\n",
    "    print(\"\\n Columns:\")\n",
    "    print(columns if columns else \" (No column list returned)\")\n",
    "    print(\"\\n Preview (first rows):\")\n",
    "    for row in (preview[:5] if isinstance(preview, list) else []):\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\n Running sales analysis (group_by='Region', metric='TotalSales') via MCP...\")\n",
    "    analysis_result = None\n",
    "    analyze_attempts = [file_cache_key]\n",
    "    if not file_cache_key.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{file_cache_key}\")  # fallback if server persisted file\n",
    "\n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            analysis_result = mcp.excel.analyze_sales(target, group_by=\"Region\", metric=\"TotalSales\")\n",
    "            print(f\" analyze_sales succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   analyze_sales failed for {target}: {ae}\")\n",
    "    if analysis_result is None:\n",
    "        raise RuntimeError(f\"Failed to analyze sales using any identifier. Last error: {last_error}\")\n",
    "\n",
    "    if isinstance(analysis_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            analysis_result = _json.loads(analysis_result)\n",
    "        except Exception:\n",
    "            analysis_result = {\"raw\": analysis_result}\n",
    "\n",
    "    summary = analysis_result.get(\"summary\") or analysis_result.get(\"result\") or analysis_result.get(\"raw\")\n",
    "\n",
    "    # Server implementation we inspected returns 'analysis' list, not grouped_data\n",
    "    grouped = analysis_result.get(\"grouped_data\") or analysis_result.get(\"groups\") or analysis_result.get(\"analysis\")\n",
    "\n",
    "    print(\"\\n MCP Sales Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary if summary else analysis_result)\n",
    "\n",
    "    if grouped and isinstance(grouped, list):\n",
    "        # Expect list of dicts with Region + metrics\n",
    "        first_item = grouped[0]\n",
    "        # Determine keys dynamically\n",
    "        region_key = 'Region' if 'Region' in first_item else list(first_item.keys())[0]\n",
    "        total_key = 'Total' if 'Total' in first_item else None\n",
    "        print(\"\\n Sales by Region (Top 10):\")\n",
    "        for i, row in enumerate(grouped[:10]):\n",
    "            region_val = row.get(region_key)\n",
    "            total_val = row.get(total_key) if total_key else row\n",
    "            print(f\"  {i+1:02d}. {region_val}: {total_val}\")\n",
    "\n",
    "    total_sales = None\n",
    "    avg_sales = None\n",
    "    num_transactions = None\n",
    "    if isinstance(summary, dict):\n",
    "        total_sales = summary.get(\"total\") or summary.get(\"total_sales\")\n",
    "        avg_sales = summary.get(\"average\") or summary.get(\"avg\") or summary.get(\"average_sale\")\n",
    "        num_transactions = summary.get(\"count\") or summary.get(\"num_rows\")\n",
    "\n",
    "    sales_data_info = (f\"Columns: {columns}\\n\" if columns else \"\") + \\\n",
    "        (f\"Total Sales: {total_sales} | Avg Sale: {avg_sales} | Rows: {num_transactions}\\n\" if total_sales else \"\") + \\\n",
    "        (\"Regional breakdown available\" if grouped else \"\")\n",
    "\n",
    "    print(\"\\n Compact sales_data_info for AI prompts:\")\n",
    "    print(sales_data_info)\n",
    "\n",
    "    # Export useful identifiers for later cells\n",
    "    excel_cache_key = file_cache_key  # so Exercise 2.2 can reuse\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" ERROR (MCP Sales Analysis): {e}\")\n",
    "    print(\" Troubleshooting:\")\n",
    "    print(\"  ‚Ä¢ Ensure EXCEL_MCP_URL points to a running Excel MCP server\")\n",
    "    print(\"  ‚Ä¢ Do NOT prepend /app/data unless server persists uploads to disk\")\n",
    "    print(\"  ‚Ä¢ Verify the file is a valid .xlsx (open it locally)\")\n",
    "    print(\"  ‚Ä¢ If persistence needed, modify server to write file bytes to disk before load_excel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da8c4a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Sales Analysis via MCP + AI ONLY\n",
    "Use MCP for data access and Azure OpenAI for ALL analysis - NO pandas operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3b78a142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.971172Z",
     "iopub.status.busy": "2025-11-15T16:28:23.971006Z",
     "iopub.status.idle": "2025-11-15T16:28:23.978845Z",
     "shell.execute_reply": "2025-11-15T16:28:23.977938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è MCP analysis failed or returned no data. Initiating local fallback...\n",
      " An error occurred during local fallback analysis: No module named 'pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1567/2067129033.py\", line 7, in <module>\n",
      "    import pandas as pd\n",
      "ModuleNotFoundError: No module named 'pandas'\n"
     ]
    }
   ],
   "source": [
    "# This cell acts as a fallback if the primary MCP analysis in the previous cell fails.\n",
    "\n",
    "if 'sales_data_info' not in locals() or not sales_data_info:\n",
    "    print(\"‚ö†Ô∏è MCP analysis failed or returned no data. Initiating local fallback...\")\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        from pathlib import Path\n",
    "        import json\n",
    "\n",
    "        # Define the path to the local Excel file\n",
    "        excel_path = Path(\"./sample-data/excel/sales_performance.xlsx\")\n",
    "\n",
    "        if not excel_path.exists():\n",
    "            print(f\" Fallback failed: Excel file not found at {excel_path.resolve()}\")\n",
    "        else:\n",
    "            print(f\" Found local Excel file: {excel_path.resolve()}\")\n",
    "\n",
    "            # Read the Excel file using pandas\n",
    "            df = pd.read_excel(excel_path)\n",
    "\n",
    "            # Generate a structure summary similar to the MCP output\n",
    "            structure = {\n",
    "                \"file_name\": excel_path.name,\n",
    "                \"columns\": df.columns.tolist(),\n",
    "                \"row_count\": len(df),\n",
    "                \"column_types\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "                \"sample_data\": df.head(3).to_dict('records')\n",
    "            }\n",
    "\n",
    "            # Create a formatted string summary\n",
    "            summary_lines = [\n",
    "                f\"File Name: {structure['file_name']}\",\n",
    "                f\"Total Rows: {structure['row_count']}\",\n",
    "                f\"Columns ({len(structure['columns'])}):\"\n",
    "            ]\n",
    "            for col, dtype in structure['column_types'].items():\n",
    "                summary_lines.append(f\"  - {col} (Type: {dtype})\")\n",
    "\n",
    "            summary_lines.append(\"\\nSample Data (First 3 Rows):\")\n",
    "            for i, row in enumerate(structure['sample_data'], 1):\n",
    "                summary_lines.append(f\"  Row {i}:\")\n",
    "                for key, val in row.items():\n",
    "                    summary_lines.append(f\"    {key}: {val}\")\n",
    "\n",
    "            # Store the summary in the required variable\n",
    "            sales_data_info = \"\\n\".join(summary_lines)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"‚úÖ LOCAL FALLBACK ANALYSIS COMPLETE\")\n",
    "            print(\"=\"*80)\n",
    "            print(sales_data_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred during local fallback analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚úÖ MCP analysis was successful. Skipping local fallback.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b67891",
   "metadata": {},
   "source": [
    "\n",
    "If the MCP-based analysis above fails (e.g., due to server issues or file compatibility problems), the cell below provides a local fallback using the `pandas` library. It reads the `sales_performance.xlsx` file directly from the local `sample-data` directory and generates a similar structural summary.\n",
    "\n",
    "This ensures that you can proceed with the subsequent AI analysis exercises even if the primary MCP tool encounters an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_192_35666650",
   "metadata": {},
   "source": [
    "### Excersice 2.4 Azzure Cost Analysis via MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ef1bae4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.981441Z",
     "iopub.status.busy": "2025-11-15T16:28:23.981224Z",
     "iopub.status.idle": "2025-11-15T16:28:23.990135Z",
     "shell.execute_reply": "2025-11-15T16:28:23.989406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Azure Cost Analysis via MCP Server + Azure OpenAI\n",
      "================================================================================\n",
      " ERROR (MCP Cost Analysis): Cost file not found at '/mnt/c/Users/lproux/OneDrive - Microsoft/bkp/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/sample-data/excel/azure_resource_costs.xlsx'\n",
      " Troubleshooting:\n",
      "  ‚Ä¢ Ensure the cost file exists at './sample-data/excel/azure_resource_costs.xlsx'\n",
      "  ‚Ä¢ Verify the file has 'ServiceName' and 'Cost' columns (based on diagnostics).\n",
      "  ‚Ä¢ Check the EXCEL_MCP_URL in your .mcp-servers-config file.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.4: Azure Cost Analysis via MCP\n",
    "print(\" Azure Cost Analysis via MCP Server + Azure OpenAI\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Define the specific cost file to use\n",
    "    cost_file_path = Path(\"./sample-data/excel/azure_resource_costs.xlsx\")\n",
    "    if not cost_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Cost file not found at '{cost_file_path.resolve()}'\")\n",
    "\n",
    "    print(f\" Uploading cost file via MCP: {cost_file_path.name}\")\n",
    "    upload_result = mcp.excel.upload_excel(str(cost_file_path))\n",
    "    cost_file_key = upload_result.get('file_name', cost_file_path.name)\n",
    "    print(f\" In-memory cache key for cost file: {cost_file_key}\")\n",
    "\n",
    "    # The 'calculate_costs' tool is rigid. We'll use the flexible 'analyze_sales' tool instead,\n",
    "    # mapping the correct columns from the diagnostic step.\n",
    "    group_by_col = 'ServiceName'\n",
    "    metric_col = 'Cost'\n",
    "\n",
    "    print(f\"\\n Analyzing costs with group_by='{group_by_col}' and metric='{metric_col}'\")\n",
    "    cost_result = mcp.excel.analyze_sales(\n",
    "        cost_file_key,\n",
    "        group_by=group_by_col,\n",
    "        metric=metric_col\n",
    "    )\n",
    "\n",
    "    if isinstance(cost_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            cost_result = _json.loads(cost_result)\n",
    "        except Exception:\n",
    "            cost_result = {\"raw\": cost_result}\n",
    "\n",
    "    # Extract results from the generic analysis response\n",
    "    summary = cost_result.get(\"summary\", {})\n",
    "    cost_breakdown = cost_result.get(\"analysis\", []) # analyze_sales returns 'analysis'\n",
    "    total_cost = summary.get(\"total\")\n",
    "\n",
    "    # Assuming the data represents daily costs, calculate a monthly projection\n",
    "    monthly_projection = total_cost * 30 if total_cost is not None else None\n",
    "\n",
    "    print(\"\\n MCP Cost Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    if not cost_breakdown:\n",
    "        print(\" (No cost breakdown returned)\")\n",
    "    else:\n",
    "        print(\" Cost Breakdown by Service Name:\")\n",
    "        # The key in the result will match the group_by_col name\n",
    "        for item in cost_breakdown:\n",
    "            resource_type = item.get(group_by_col, 'N/A')\n",
    "            cost = item.get('Total', 0) # analyze_sales returns 'Total' for the metric sum\n",
    "            print(f\"  - {resource_type}: ${cost:,.2f}\")\n",
    "\n",
    "    if total_cost:\n",
    "        print(f\"\\n Total Daily Cost: ${total_cost:,.2f}\")\n",
    "    if monthly_projection:\n",
    "        print(f\" Projected Monthly Cost: ${monthly_projection:,.2f}\")\n",
    "\n",
    "    # Create a compact summary for AI analysis\n",
    "    cost_summary_lines = []\n",
    "    if total_cost:\n",
    "        cost_summary_lines.append(f\"Total Daily Cost: ${total_cost:,.2f}\")\n",
    "    if monthly_projection:\n",
    "        cost_summary_lines.append(f\"Projected Monthly Cost: ${monthly_projection:,.2f}\")\n",
    "    if cost_breakdown:\n",
    "        cost_summary_lines.append(\"Breakdown by service name is available.\")\n",
    "\n",
    "    cost_data_info = \"\\n\".join(cost_summary_lines)\n",
    "    print(\"\\n Compact cost_data_info for AI prompts:\")\n",
    "    print(cost_data_info if cost_data_info else \"(No cost info generated)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" ERROR (MCP Cost Analysis): {e}\")\n",
    "    print(\" Troubleshooting:\")\n",
    "    print(\"  ‚Ä¢ Ensure the cost file exists at './sample-data/excel/azure_resource_costs.xlsx'\")\n",
    "    print(\"  ‚Ä¢ Verify the file has 'ServiceName' and 'Cost' columns (based on diagnostics).\")\n",
    "    print(\"  ‚Ä¢ Check the EXCEL_MCP_URL in your .mcp-servers-config file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_194_33e4baed",
   "metadata": {},
   "source": [
    "### Exercise 2.5: Dynamic Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f39fb518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.992128Z",
     "iopub.status.busy": "2025-11-15T16:28:23.991921Z",
     "iopub.status.idle": "2025-11-15T16:28:23.997523Z",
     "shell.execute_reply": "2025-11-15T16:28:23.996834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dynamic MCP Analysis with User-Defined Columns\n",
      "================================================================================\n",
      " ERROR (MCP Dynamic Analysis): Sales data not loaded. Please run Exercise 2.1 successfully first.\n",
      " Troubleshooting:\n",
      "  ‚Ä¢ Ensure Exercise 2.1 ran successfully and `excel_cache_key` is available.\n",
      "  ‚Ä¢ Verify that the columns 'Product' and 'Quantity' exist in the sales data.\n",
      "  ‚Ä¢ Check the MCP server logs for more detailed error information.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.5: Dynamic Column Analysis\n",
    "print(\" Dynamic MCP Analysis with User-Defined Columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # --- Define columns for analysis ---\n",
    "    # These variables can be changed to analyze different aspects of the data\n",
    "    group_by_column = 'Product'  # Change to 'Product', 'CustomerID', etc.\n",
    "    metric_column = 'Quantity'   # Change to 'Quantity', 'TotalSales', etc.\n",
    "\n",
    "    # Use the file key from the successful sales analysis in Exercise 2.1\n",
    "    if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "        raise RuntimeError(\"Sales data not loaded. Please run Exercise 2.1 successfully first.\")\n",
    "\n",
    "    file_to_analyze = excel_cache_key\n",
    "\n",
    "    print(f\" Performing dynamic analysis on '{file_to_analyze}'\")\n",
    "    print(f\" Grouping by: '{group_by_column}'\")\n",
    "    print(f\" Aggregating metric: '{metric_column}'\")\n",
    "\n",
    "    # Call the MCP tool with the dynamic column names\n",
    "    dynamic_analysis_result = mcp.excel.analyze_sales(\n",
    "        file_to_analyze,\n",
    "        group_by=group_by_column,\n",
    "        metric=metric_column\n",
    "    )\n",
    "\n",
    "    if isinstance(dynamic_analysis_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            dynamic_analysis_result = _json.loads(dynamic_analysis_result)\n",
    "        except Exception:\n",
    "            dynamic_analysis_result = {\"raw\": dynamic_analysis_result}\n",
    "\n",
    "    # --- Display the results ---\n",
    "    summary = dynamic_analysis_result.get(\"summary\", {})\n",
    "    grouped_data = dynamic_analysis_result.get(\"analysis\", [])\n",
    "\n",
    "    print(\"\\n MCP Dynamic Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\" Total: {summary.get('total', 'N/A'):,.2f}\")\n",
    "    print(f\" Average: {summary.get('average', 'N/A'):,.2f}\")\n",
    "    print(f\" Count: {summary.get('count', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\n Top 10 Results (Grouped by '{group_by_column}'):\")\n",
    "    if not grouped_data:\n",
    "        print(\" (No grouped data returned)\")\n",
    "    else:\n",
    "        # The key for the grouping column in the result is the column name itself\n",
    "        for i, item in enumerate(grouped_data[:10]):\n",
    "            group_value = item.get(group_by_column, 'N/A')\n",
    "            metric_value = item.get('Total', item.get('Sum', 'N/A')) # MCP might return 'Total' or 'Sum'\n",
    "            print(f\"  {i+1:02d}. {group_value}: {metric_value:,.0f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" ERROR (MCP Dynamic Analysis): {e}\")\n",
    "    print(\" Troubleshooting:\")\n",
    "    print(\"  ‚Ä¢ Ensure Exercise 2.1 ran successfully and `excel_cache_key` is available.\")\n",
    "    print(f\"  ‚Ä¢ Verify that the columns '{group_by_column}' and '{metric_column}' exist in the sales data.\")\n",
    "    print(\"  ‚Ä¢ Check the MCP server logs for more detailed error information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28819f5b",
   "metadata": {},
   "source": [
    "### Exercise 2.6: AI-Generated Sales Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_197_aa1a9f58",
   "metadata": {},
   "source": [
    "## Test MCP-Enabled Agent\n",
    "\n",
    "This demonstrates the full MCP integration with Azure OpenAI:\n",
    "1. Agent receives a query requiring external data\n",
    "2. Agent identifies the appropriate MCP tool to call\n",
    "3. MCP server retrieves the data\n",
    "4. Agent processes data and generates response\n",
    "5. All operations logged through APIM for observability\n",
    "\n",
    "### Expected Behavior\n",
    "\n",
    "- First request may be slower (cold start)\n",
    "- MCP tool calls visible in function_calls\n",
    "- Final response incorporates MCP-provided data\n",
    "- APIM headers show both OpenAI and MCP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cell_198_12c4557c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:23.999333Z",
     "iopub.status.busy": "2025-11-15T16:28:23.999187Z",
     "iopub.status.idle": "2025-11-15T16:28:24.045405Z",
     "shell.execute_reply": "2025-11-15T16:28:24.044643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEPLOYING CONSOLIDATED POLICY\n",
      "================================================================================\n",
      "\n",
      "[OK] Policy file found: policies/consolidated-policy.xml\n",
      "\n",
      "Policy features enabled:\n",
      "  - JWT Token Validation: Yes\n",
      "  - Rate Limiting: Yes\n",
      "  - Request Logging: Yes\n",
      "  - Error Handling: Yes\n",
      "\n",
      "[INFO] To deploy this policy:\n",
      "\n",
      "Option 1: Azure Portal\n",
      "  1. Go to your API Management service\n",
      "  2. Navigate to APIs > Your API > Design\n",
      "  3. Click '</> Code editor' in the Inbound/Outbound sections\n",
      "  4. Paste the consolidated policy XML\n",
      "\n",
      "Option 2: Azure CLI\n",
      "  az apim api policy create \\\n",
      "    --resource-group lab-master-lab \\\n",
      "    --service-name apimmcpwksp321028 \\\n",
      "    --api-id inference-api \\\n",
      "    --value-format rawxml \\\n",
      "    --policy @policies/consolidated-policy.xml\n",
      "\n",
      "================================================================================\n",
      "[OK] Consolidated policy ready for deployment\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Deploy Consolidated Policy to APIM\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DEPLOYING CONSOLIDATED POLICY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Policy file path\n",
    "policy_file = Path(\"policies/consolidated-policy.xml\")\n",
    "\n",
    "if not policy_file.exists():\n",
    "    print(f\"[ERROR] Policy file not found: {policy_file}\")\n",
    "    print(f\"[INFO] Make sure you're in the master-lab directory\")\n",
    "else:\n",
    "    print(f\"[OK] Policy file found: {policy_file}\")\n",
    "    print()\n",
    "\n",
    "    # Read policy content\n",
    "    with open(policy_file, 'r') as f:\n",
    "        policy_xml = f.read()\n",
    "\n",
    "    print(\"Policy features enabled:\")\n",
    "    print(\"  - JWT Token Validation: Yes\" if \"validate-jwt\" in policy_xml else \"  - JWT Token Validation: No\")\n",
    "    print(\"  - Rate Limiting: Yes\" if \"token-limit\" in policy_xml else \"  - Rate Limiting: No\")\n",
    "    print(\"  - Request Logging: Yes\" if \"trace\" in policy_xml else \"  - Request Logging: No\")\n",
    "    print(\"  - Error Handling: Yes\" if \"on-error\" in policy_xml else \"  - Error Handling: No\")\n",
    "    print()\n",
    "\n",
    "    # Deploy using Azure CLI or REST API\n",
    "    print(\"[INFO] To deploy this policy:\")\n",
    "    print()\n",
    "    print(\"Option 1: Azure Portal\")\n",
    "    print(\"  1. Go to your API Management service\")\n",
    "    print(\"  2. Navigate to APIs > Your API > Design\")\n",
    "    print(\"  3. Click '</> Code editor' in the Inbound/Outbound sections\")\n",
    "    print(\"  4. Paste the consolidated policy XML\")\n",
    "    print()\n",
    "    print(\"Option 2: Azure CLI\")\n",
    "    apim_name = os.getenv('APIM_NAME', 'your-apim-name')\n",
    "    resource_group = os.getenv('RESOURCE_GROUP', 'your-rg')\n",
    "    api_id = os.getenv('API_ID', 'your-api-id')\n",
    "\n",
    "    print(f\"  az apim api policy create \\\\\")\n",
    "    print(f\"    --resource-group {resource_group} \\\\\")\n",
    "    print(f\"    --service-name {apim_name} \\\\\")\n",
    "    print(f\"    --api-id {api_id} \\\\\")\n",
    "    print(f\"    --value-format rawxml \\\\\")\n",
    "    print(f\"    --policy @{policy_file}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"[OK] Consolidated policy ready for deployment\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cell_199_fa9b6fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:24.047405Z",
     "iopub.status.busy": "2025-11-15T16:28:24.047213Z",
     "iopub.status.idle": "2025-11-15T16:28:24.073864Z",
     "shell.execute_reply": "2025-11-15T16:28:24.073259Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTESTING CONSOLIDATED POLICY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "# Test Consolidated Policy\n",
    "import requests\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING CONSOLIDATED POLICY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Get configuration\n",
    "apim_gateway_url = os.getenv('APIM_GATEWAY_URL')\n",
    "apim_subscription_key = os.getenv('APIM_SUBSCRIPTION_KEY')\n",
    "\n",
    "if not apim_gateway_url or not apim_subscription_key:\n",
    "    print(\"[ERROR] Missing APIM configuration\")\n",
    "    print(\"[INFO] Set APIM_GATEWAY_URL and APIM_SUBSCRIPTION_KEY in .env\")\n",
    "else:\n",
    "    print(f\"[OK] Gateway URL configured\")\n",
    "    print(f\"[OK] Subscription key configured\")\n",
    "    print()\n",
    "\n",
    "    # Test 1: Basic request with rate limiting\n",
    "    print(\"Test 1: Basic Request (with rate limiting)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    try:\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=apim_gateway_url,\n",
    "            api_key=apim_subscription_key,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{'role': 'user', 'content': 'Test consolidated policy'}],\n",
    "            max_tokens=20\n",
    "        )\n",
    "\n",
    "        print(f\"[OK] Request successful\")\n",
    "        print(f\"[OK] Response: {response.choices[0].message.content}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Request failed: {e}\")\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    print(\"[INFO] Check APIM logs/Application Insights for:\")\n",
    "    print(\"  - Request/Response traces\")\n",
    "    print(\"  - Rate limiting headers (X-Remaining-Tokens)\")\n",
    "    print(\"  - Custom headers (X-Request-ID, X-User-ID)\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cell_200_19381998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:28:24.075905Z",
     "iopub.status.busy": "2025-11-15T16:28:24.075706Z",
     "iopub.status.idle": "2025-11-15T16:28:24.085754Z",
     "shell.execute_reply": "2025-11-15T16:28:24.084825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[verify] Failed to import openai: No module named 'openai'\n",
      "[verify] Failed to import openai-agents: No module named 'openai_agents'\n",
      "[verify] openai version: None\n",
      "[verify] openai-agents version: None\n",
      "[verify] AzureOpenAI client creation failed: AzureOpenAI class not found; ensure openai>=2.2,<3 installed.\n",
      "[verify] AzureOpenAI shim client creation success=False\n"
     ]
    }
   ],
   "source": [
    "# Post-Upgrade Verification for Agents and AzureOpenAI\n",
    "import importlib, json\n",
    "\n",
    "ver_openai = None\n",
    "try:\n",
    "    import openai\n",
    "    ver_openai = getattr(openai, '__version__', 'unknown')\n",
    "except Exception as ex:\n",
    "    print(f'[verify] Failed to import openai: {ex}')\n",
    "\n",
    "ver_agents = None\n",
    "try:\n",
    "    import openai_agents\n",
    "    ver_agents = getattr(openai_agents, '__version__', 'unknown')\n",
    "except Exception as ex:\n",
    "    print(f'[verify] Failed to import openai-agents: {ex}')\n",
    "\n",
    "print(f'[verify] openai version: {ver_openai}')\n",
    "print(f'[verify] openai-agents version: {ver_agents}')\n",
    "\n",
    "# Attempt basic AzureOpenAI client instantiation (will not send request)\n",
    "client_ok = False\n",
    "try:\n",
    "    client_test = get_azure_openai_client(\n",
    "        api_key='DUMMY',  # Replace with real if needed for live call\n",
    "        api_version='2025-06-01-preview',\n",
    "        azure_endpoint='https://example.openai.azure.com'\n",
    "    )\n",
    "    client_ok = True\n",
    "except Exception as ex:\n",
    "    print(f'[verify] AzureOpenAI client creation failed: {ex}')\n",
    "\n",
    "print(f'[verify] AzureOpenAI shim client creation success={client_ok}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_201_4805765f",
   "metadata": {},
   "source": [
    "### Agent Dependency Note for MCP Labs\n",
    "\n",
    "The Spotify MCP integration cell now auto-aligns dependencies for `openai-agents`:\n",
    "- Ensures `openai>=2.2,<3` to satisfy `openai-agents==0.4.1`.\n",
    "- Performs on-demand install/upgrade only if version mismatch or module missing.\n",
    "\n",
    "If you later pin a different OpenAI version globally, re-run Cell 1 (dependency alignment) or modify the helper `_ensure_agents()` in the Spotify lab cell.\n",
    "\n",
    "To force a clean reinstall manually:\n",
    "\n",
    "```bash\n",
    "pip uninstall -y openai openai-agents\n",
    "pip install \"openai>=2.2,<3\" openai-agents==0.4.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_202_4cddfd5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_win_backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
