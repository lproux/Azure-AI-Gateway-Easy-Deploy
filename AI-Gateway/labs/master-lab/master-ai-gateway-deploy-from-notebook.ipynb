{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master AI Gateway Workshop\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### [Section 0: Initialize and Deploy](#section0)\n",
    "- [0.1 Environment Detection](#env-detection)\n",
    "- [0.2 Bootstrap Configuration](#bootstrap)\n",
    "- [0.3 Dependencies Installation](#dependencies)\n",
    "- [0.4 Azure Authentication & Service Principal](#azure-auth)\n",
    "- [0.5 Core Helper Functions](#helpers)\n",
    "- [0.6 Deployment Configuration](#deploy-config)\n",
    "- [0.7 Deploy Infrastructure](#deploy-infra)\n",
    "- [0.8 Reload Complete Configuration](#reload-config)\n",
    "- [0.9 Access Controlling](#access-control)\n",
    "\n",
    "### [Section 1: Core AI Gateway Features](#section1)\n",
    "- [1.1 Advanced Caching & Storage](#lab1-1)\n",
    "  - [1.1.1 Configure Embeddings Backend](#lab1-1-1)\n",
    "  - [1.1.2 Apply Caching Policy](#lab1-1-2)\n",
    "  - [1.1.3 Performance Test](#lab1-1-3)\n",
    "  - [1.1.4 Visualize Performance](#lab1-1-4)\n",
    "  - [1.1.5 Redis Cache Statistics](#lab1-1-5)\n",
    "- [1.2 Message Storing with Cosmos DB](#lab1-2)\n",
    "  - [1.2.1 Generate Test Conversations](#lab1-2-1)\n",
    "  - [1.2.2 Query Stored Messages](#lab1-2-2)\n",
    "- [1.3 Vector Searching with RAG](#lab1-3)\n",
    "  - [1.3.1 Index Sample Documents](#lab1-3-1)\n",
    "  - [1.3.2 Test RAG Pattern](#lab1-3-2)\n",
    "- [1.4 Zero to Production](#lab1-4)\n",
    "  - [1.4.1 Basic Chat Completion](#lab1-4-1)\n",
    "  - [1.4.2 Streaming Response](#lab1-4-2)\n",
    "  - [1.4.3 Multiple Requests](#lab1-4-3)\n",
    "- [1.5 Backend Pool Load Balancing](#lab1-5)\n",
    "  - [1.5.1 Create Backend Pool](#lab1-5-1)\n",
    "  - [1.5.2 Verify Backend Pool](#lab1-5-2)\n",
    "  - [1.5.3 Load Distribution Test](#lab1-5-3)\n",
    "  - [1.5.4 Visualize Response Times](#lab1-5-4)\n",
    "- [1.6 Token Metrics Emitting](#lab1-6)\n",
    "- [1.7 Content Safety](#lab1-7)\n",
    "- [1.8 Model Routing](#lab1-8)\n",
    "- [1.9 AI Foundry SDK](#lab1-9)\n",
    "\n",
    "### [Section 2: MCP Fundamentals](#section2)\n",
    "- [2.1 Exercise: Sales Analysis via MCP + AI](#lab2-1)\n",
    "- [2.2 Exercise: Azure Cost Analysis via MCP](#lab2-2)\n",
    "- [2.3 Exercise: Dynamic Column Analysis](#lab2-3)\n",
    "- [2.4 Exercise: Function Calling with MCP Tools](#lab2-4)\n",
    "  - [2.4.1 Weather API via APIM](#lab2-4-1)\n",
    "  - [2.4.2 GitHub API via APIM](#lab2-4-2)\n",
    "- [2.5 GitHub Repository Access](#lab2-5)\n",
    "- [2.6 GitHub + AI Code Analysis](#lab2-6)\n",
    "- [2.7 Multi-MCP AI Aggregation](#lab2-7)\n",
    "\n",
    "### [Section 3: Semantic Kernel & AutoGen](#section3)\n",
    "- [3.1 SK Plugin for Gateway-Routed Function Calling](#lab3-1)\n",
    "- [3.2 SK Streaming Chat with Function Calling](#lab3-2)\n",
    "- [3.3 AutoGen Multi-Agent Conversation](#lab3-3)\n",
    "- [3.4 SK Agent with Custom Azure OpenAI Client](#lab3-4)\n",
    "- [3.5 Built-in LLM Logging](#lab3-5)\n",
    "  - [3.5.1 Generate Test Data](#lab3-5-1)\n",
    "  - [3.5.2 Query Token Usage by Model](#lab3-5-2)\n",
    "  - [3.5.3 Query Token Usage by Subscription](#lab3-5-3)\n",
    "  - [3.5.4 View Prompts and Completions](#lab3-5-4)\n",
    "- [3.6 Hybrid SK + AutoGen Orchestration](#lab3-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section0\"></a>\n",
    "\n",
    "# Section 0: Initialize and Deploy\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section bootstraps the entire AI Gateway workshop environment. These cells must be run FIRST before any lab exercises.\n",
    "\n",
    "**Important**: These cells run WITHOUT master-lab.env (it doesn't exist yet!) because they CREATE the configuration file.\n",
    "\n",
    "## What Gets Deployed\n",
    "\n",
    "This section deploys a complete AI Gateway infrastructure including:\n",
    "\n",
    "### Core Components (Step 1)\n",
    "- **Azure API Management (APIM)**: Central gateway for all AI and API traffic\n",
    "- **Log Analytics Workspace**: Centralized logging and monitoring\n",
    "- **Application Insights**: Application performance monitoring\n",
    "- **Estimated time**: ~10 minutes\n",
    "\n",
    "### AI Foundry (Step 2)\n",
    "- **3 Azure AI Services accounts** (East US, West US, Sweden Central)\n",
    "- **14 AI model deployments** across regions:\n",
    "  - GPT-4o-mini (all regions)\n",
    "  - GPT-4o (all regions)\n",
    "  - GPT-4 (all regions)\n",
    "  - text-embedding-ada-002 (all regions)\n",
    "  - Additional models as configured\n",
    "- **Estimated time**: ~15 minutes\n",
    "\n",
    "### Supporting Services (Step 3)\n",
    "- **Redis Cache**: For semantic caching and session storage\n",
    "- **Cosmos DB**: For message storing and conversation history\n",
    "- **Azure AI Search**: For vector search and RAG patterns\n",
    "- **Estimated time**: ~8 minutes\n",
    "\n",
    "### Configuration (Step 4)\n",
    "- Generates `master-lab.env` with all endpoints, keys, and settings\n",
    "- Configures APIM policies and backends\n",
    "- Sets up MCP server connections\n",
    "- **Estimated time**: ~2 minutes\n",
    "\n",
    "**Total deployment time**: ~35-40 minutes\n",
    "\n",
    "## Authentication Options\n",
    "\n",
    "This notebook supports three authentication methods:\n",
    "\n",
    "### Option 1: Service Principal (Recommended for Automation)\n",
    "Best for: CI/CD, automation, production deployments\n",
    "\n",
    "**Setup:**\n",
    "```bash\n",
    "# Create service principal\n",
    "az ad sp create-for-rbac --name \"ai-gateway-sp\" --role Contributor --scopes /subscriptions/{subscription-id}\n",
    "\n",
    "# Set environment variables (done automatically by Cell 9)\n",
    "export AZURE_CLIENT_ID=\"<client-id>\"\n",
    "export AZURE_CLIENT_SECRET=\"<client-secret>\"\n",
    "export AZURE_TENANT_ID=\"<tenant-id>\"\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Non-interactive (no browser login required)\n",
    "- Works in automated environments\n",
    "- Reproducible deployments\n",
    "- Can be used in CI/CD pipelines\n",
    "\n",
    "**Requirements:**\n",
    "- Service principal created with Contributor role\n",
    "- Three environment variables set\n",
    "\n",
    "### Option 2: Managed Identity (Recommended for Azure-Hosted Notebooks)\n",
    "Best for: Running notebooks on Azure VMs, AKS, Container Apps, Azure ML\n",
    "\n",
    "**Setup:**\n",
    "- No explicit setup required\n",
    "- Azure automatically provides identity to compute resources\n",
    "- Assign appropriate roles to the managed identity\n",
    "\n",
    "**Advantages:**\n",
    "- No secrets to manage\n",
    "- Automatic credential rotation\n",
    "- Highest security posture\n",
    "- Works seamlessly on Azure compute\n",
    "\n",
    "**Requirements:**\n",
    "- Notebook running on Azure compute resource\n",
    "- Managed identity assigned to the resource\n",
    "- Identity has necessary Azure roles\n",
    "\n",
    "### Option 3: Azure CLI Login (Recommended for Interactive Development)\n",
    "Best for: Local development, learning, testing\n",
    "\n",
    "**Setup:**\n",
    "```bash\n",
    "# Login interactively\n",
    "az login\n",
    "\n",
    "# Select subscription (if you have multiple)\n",
    "az account set --subscription \"subscription-name-or-id\"\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Simplest for local development\n",
    "- Uses your personal Azure credentials\n",
    "- No additional setup required\n",
    "\n",
    "**Requirements:**\n",
    "- Azure CLI installed locally\n",
    "- User account with sufficient permissions (Contributor role)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running Section 0, ensure you have:\n",
    "\n",
    "### Required\n",
    "1. **Azure Subscription** with sufficient quota:\n",
    "   - API Management: 1 instance (Developer or Premium tier)\n",
    "   - AI Services: 3 accounts (S0 tier)\n",
    "   - Redis Cache: 1 instance (Basic or Standard)\n",
    "   - Cosmos DB: 1 account (Serverless or Provisioned)\n",
    "   - AI Search: 1 service (Basic or Standard)\n",
    "\n",
    "2. **Azure Permissions**: Contributor role on subscription or resource group\n",
    "\n",
    "3. **Azure CLI**: Installed and configured (v2.50.0 or later)\n",
    "\n",
    "4. **Python Environment**:\n",
    "   - Python 3.8 or later\n",
    "   - pip package manager\n",
    "   - Virtual environment (recommended)\n",
    "\n",
    "### Optional but Recommended\n",
    "- **VS Code**: With Jupyter extension for notebook editing\n",
    "- **GitHub Codespaces**: Pre-configured environment with all tools\n",
    "- **Docker**: For running MCP servers locally\n",
    "\n",
    "## Environment Detection\n",
    "\n",
    "The notebook automatically detects your environment:\n",
    "- **GitHub Codespaces**: Auto-configures paths and settings\n",
    "- **Local (WSL/Linux)**: Uses local Azure CLI\n",
    "- **Local (Windows)**: Detects Windows paths\n",
    "\n",
    "## Quick Start Guide\n",
    "\n",
    "**For first-time users:**\n",
    "\n",
    "1. **Choose authentication method** (pick one):\n",
    "   - Option A: Service Principal \u2192 Run Cell 9, provide credentials when prompted\n",
    "   - Option B: Azure CLI \u2192 Run `az login` in terminal before Cell 1\n",
    "   - Option C: Managed Identity \u2192 No action needed if on Azure compute\n",
    "\n",
    "2. **Run cells in order**:\n",
    "   ```\n",
    "   Cell 3:  Environment Detection\n",
    "   Cell 5:  Bootstrap Configuration\n",
    "   Cell 7:  Install Dependencies (~5 min)\n",
    "   Cell 9:  Azure Authentication Setup\n",
    "   Cell 11: Load Helper Functions\n",
    "   Cell 14: Set Deployment Configuration\n",
    "   Cell 18: Deploy Infrastructure (~35 min)\n",
    "   Cell 21: Generate master-lab.env file\n",
    "   Cell 23: Reload Configuration\n",
    "   ```\n",
    "\n",
    "3. **Verify deployment**:\n",
    "   - Check Azure Portal for resources\n",
    "   - Ensure `master-lab.env` file exists\n",
    "   - Verify environment variables are loaded\n",
    "\n",
    "4. **Proceed to labs**: Jump to [Lab 1: Access Control](#lab1-4)\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "### \"Azure CLI not found\"\n",
    "- **Solution**: Install Azure CLI from https://aka.ms/installazurecli\n",
    "- **Or**: Set `AZURE_CLI_PATH` environment variable to CLI location\n",
    "\n",
    "### \"Please run 'az login'\"\n",
    "- **Solution**: Run `az login` in terminal or set Service Principal credentials\n",
    "- **Check**: Run `az account show` to verify authentication\n",
    "\n",
    "### \"Insufficient quota for deployment\"\n",
    "- **Solution**: Request quota increase in Azure Portal (Support \u2192 Quotas)\n",
    "- **Or**: Use different regions with available quota\n",
    "- **Note**: AI Services quota often needs increase for multiple deployments\n",
    "\n",
    "### \"Deployment timeout\"\n",
    "- **Solution**: Azure deployments can take time; wait for completion\n",
    "- **Check**: View deployment status in Azure Portal \u2192 Resource Group \u2192 Deployments\n",
    "- **Retry**: Re-run the deployment cell (it's idempotent)\n",
    "\n",
    "### \"Module not found\" errors\n",
    "- **Solution**: Run Cell 7 (Dependencies Install) completely\n",
    "- **Check**: Restart kernel after installing dependencies\n",
    "- **Verify**: Run `pip list | grep azure` to confirm packages installed\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After Section 0 completes successfully:\n",
    "\n",
    "1. **Verify deployment**: Check that all resources exist in Azure Portal\n",
    "2. **Review configuration**: Open `master-lab.env` to see all settings\n",
    "3. **Choose labs**: Pick from 12 comprehensive labs covering:\n",
    "   - Access Control (JWT and API keys)\n",
    "   - Load Balancing (multi-region)\n",
    "   - Semantic Caching (Redis + embeddings)\n",
    "   - Message Storing (Cosmos DB)\n",
    "   - Vector Search (AI Search + RAG)\n",
    "   - LLM Logging (Log Analytics)\n",
    "   - MCP Integration (Excel, Docs, GitHub, Weather)\n",
    "\n",
    "4. **Start learning**: Jump to any lab using the table of contents\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to begin?** Start with Cell 3 below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"env-detection\"></a>\n",
    "\n",
    "## 0.1 Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: GitHub Codespace\n",
      "Workspace: /workspaces/Azure-AI-Gateway-Easy-Deploy\n",
      "Python: 3.12.11\n"
     ]
    }
   ],
   "source": [
    "# Cell 003: Environment Detection\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_CODESPACE = bool(os.getenv('CODESPACE_NAME'))\n",
    "WORKSPACE_ROOT = Path.cwd()\n",
    "\n",
    "print(f\"Environment: {'GitHub Codespace' if IS_CODESPACE else 'Local'}\")\n",
    "print(f\"Workspace: {WORKSPACE_ROOT}\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bootstrap\"></a>\n",
    "\n",
    "## 0.2 Bootstrap Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Detecting notebook directory...\n",
      "    Current working directory: /workspaces/Azure-AI-Gateway-Easy-Deploy\n",
      "    Platform: linux\n",
      "    Environment: Native linux\n",
      "[*] Method 2: Checking known path: C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab\n",
      "    Path does not exist\n",
      "[*] Method 3: Searching parent directories...\n",
      "    Checking: /workspaces/Azure-AI-Gateway-Easy-Deploy\n",
      "    Checking: /workspaces\n",
      "    Checking: /\n",
      "    Checking: /\n",
      "    Checking: /\n",
      "[*] Method 4: Looking for AI-Gateway in current directory...\n",
      "    Found AI-Gateway, checking: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab\n",
      "[OK] Method 4: Found via AI-Gateway navigation\n",
      "\n",
      "[OK] Notebook directory: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab\n",
      "[OK] Changed working directory to: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab\n",
      "[WARN] bootstrap.env not found at: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/bootstrap.env\n",
      "[INFO] Using template: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/bootstrap.env.template\n",
      "[OK] Loading from: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/bootstrap.env.template\n",
      "\n",
      "Bootstrap Configuration:\n",
      "  Subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  Resource Group: lab-master-lab\n",
      "  Location: eastus2\n",
      "\n",
      "[OK] Bootstrap configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 005: Load Bootstrap Configuration (minimal)\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get notebook directory (works in WSL and Windows)\n",
    "NOTEBOOK_DIR = None\n",
    "\n",
    "print(\"[*] Detecting notebook directory...\")\n",
    "print(f\"    Current working directory: {Path.cwd()}\")\n",
    "print(f\"    Platform: {sys.platform}\")\n",
    "\n",
    "# Detect if running in WSL\n",
    "IS_WSL = 'microsoft' in str(Path('/proc/version').read_text()).lower() if Path('/proc/version').exists() else False\n",
    "if IS_WSL:\n",
    "    print(\"    Environment: WSL (Windows Subsystem for Linux)\")\n",
    "else:\n",
    "    print(f\"    Environment: Native {sys.platform}\")\n",
    "\n",
    "# Method 1: Check if we're in the right directory already\n",
    "if (Path.cwd() / 'bootstrap.env').exists() or (Path.cwd() / 'bootstrap.env.template').exists():\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "    print(f\"[OK] Method 1: Found in current directory\")\n",
    "\n",
    "# Method 2: Use known absolute path (WSL-aware)\n",
    "if NOTEBOOK_DIR is None:\n",
    "    if IS_WSL:\n",
    "        # WSL path format\n",
    "        known_path = Path('/mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab')\n",
    "    else:\n",
    "        # Windows path format\n",
    "        known_path = Path(r'C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab')\n",
    "\n",
    "    print(f\"[*] Method 2: Checking known path: {known_path}\")\n",
    "\n",
    "    if known_path.exists():\n",
    "        NOTEBOOK_DIR = known_path\n",
    "        print(f\"[OK] Method 2: Using known path\")\n",
    "    else:\n",
    "        print(f\"    Path does not exist\")\n",
    "\n",
    "# Method 3: Search parent directories\n",
    "if NOTEBOOK_DIR is None:\n",
    "    print(f\"[*] Method 3: Searching parent directories...\")\n",
    "    current = Path.cwd()\n",
    "    for level in range(5):\n",
    "        print(f\"    Checking: {current}\")\n",
    "        if (current / 'bootstrap.env').exists() or (current / 'bootstrap.env.template').exists():\n",
    "            NOTEBOOK_DIR = current\n",
    "            print(f\"[OK] Method 3: Found at level {level}\")\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "# Method 4: Navigate from current directory if we see AI-Gateway\n",
    "if NOTEBOOK_DIR is None:\n",
    "    print(f\"[*] Method 4: Looking for AI-Gateway in current directory...\")\n",
    "    current = Path.cwd()\n",
    "\n",
    "    # Check if AI-Gateway exists in current dir\n",
    "    ai_gateway = current / 'AI-Gateway'\n",
    "    if ai_gateway.exists() and ai_gateway.is_dir():\n",
    "        master_lab = ai_gateway / 'labs' / 'master-lab'\n",
    "        print(f\"    Found AI-Gateway, checking: {master_lab}\")\n",
    "        if master_lab.exists() and ((master_lab / 'bootstrap.env').exists() or (master_lab / 'bootstrap.env.template').exists()):\n",
    "            NOTEBOOK_DIR = master_lab\n",
    "            print(f\"[OK] Method 4: Found via AI-Gateway navigation\")\n",
    "\n",
    "# Method 5: Search for master-lab folder in tree\n",
    "if NOTEBOOK_DIR is None:\n",
    "    print(f\"[*] Method 5: Searching for master-lab folder...\")\n",
    "    current = Path.cwd()\n",
    "\n",
    "    # Check current and all parents\n",
    "    for parent in [current] + list(current.parents)[:5]:\n",
    "        if parent.name == 'master-lab':\n",
    "            if (parent / 'bootstrap.env').exists() or (parent / 'bootstrap.env.template').exists():\n",
    "                NOTEBOOK_DIR = parent\n",
    "                print(f\"[OK] Method 5: Found master-lab folder: {parent}\")\n",
    "                break\n",
    "\n",
    "        # Also check if master-lab exists as subdirectory\n",
    "        master_lab_candidates = list(parent.glob('**/master-lab'))\n",
    "        for candidate in master_lab_candidates[:3]:  # Check first 3 matches\n",
    "            if (candidate / 'bootstrap.env').exists() or (candidate / 'bootstrap.env.template').exists():\n",
    "                NOTEBOOK_DIR = candidate\n",
    "                print(f\"[OK] Method 5: Found master-lab via glob: {candidate}\")\n",
    "                break\n",
    "\n",
    "        if NOTEBOOK_DIR:\n",
    "            break\n",
    "\n",
    "if NOTEBOOK_DIR is None:\n",
    "    # Last resort: Show what's available\n",
    "    print(\"\\n[!] DEBUG: Current directory contents:\")\n",
    "    try:\n",
    "        items = list(Path.cwd().iterdir())\n",
    "        for item in items[:20]:\n",
    "            marker = \"DIR\" if item.is_dir() else \"   \"\n",
    "            print(f\"    [{marker}] {item.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error listing: {e}\")\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Cannot locate notebook directory.\\n\"\n",
    "        f\"Current directory: {Path.cwd()}\\n\"\n",
    "        f\"Platform: {sys.platform} ({'WSL' if IS_WSL else 'Native'})\\n\"\n",
    "        \"Expected to find: bootstrap.env or bootstrap.env.template\\n\"\n",
    "        \"\\n\"\n",
    "        \"Possible solutions:\\n\"\n",
    "        \"1. Change to the notebook directory first:\\n\"\n",
    "        \"   import os\\n\"\n",
    "        \"   os.chdir(r'C:\\\\Users\\\\lproux\\\\Documents\\\\GitHub\\\\MCP-servers-internalMSFT-and-external\\\\AI-Gateway\\\\labs\\\\master-lab')\\n\"\n",
    "        \"   # or in WSL:\\n\"\n",
    "        \"   os.chdir('/mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab')\\n\"\n",
    "        \"\\n\"\n",
    "        \"2. Or create bootstrap.env.template in the current directory\"\n",
    "    )\n",
    "\n",
    "# Change to notebook directory\n",
    "os.chdir(NOTEBOOK_DIR)\n",
    "print(f\"\\n[OK] Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"[OK] Changed working directory to: {Path.cwd()}\")\n",
    "\n",
    "@dataclass\n",
    "class BootstrapConfig:\n",
    "    subscription_id: str = \"\"\n",
    "    resource_group: str = \"ai-gateway-workshop\"\n",
    "    location: str = \"eastus2\"\n",
    "    deploy_suffix: str = \"\"\n",
    "\n",
    "# Use absolute path for bootstrap.env\n",
    "bootstrap_file = NOTEBOOK_DIR / 'bootstrap.env'\n",
    "if not bootstrap_file.exists():\n",
    "    print(f\"[WARN] bootstrap.env not found at: {bootstrap_file}\")\n",
    "    bootstrap_file = NOTEBOOK_DIR / 'bootstrap.env.template'\n",
    "    print(f\"[INFO] Using template: {bootstrap_file}\")\n",
    "\n",
    "# Load ONLY bootstrap values\n",
    "bootstrap = BootstrapConfig()\n",
    "if bootstrap_file.exists():\n",
    "    print(f\"[OK] Loading from: {bootstrap_file}\")\n",
    "    for line in bootstrap_file.read_text().splitlines():\n",
    "        if '=' in line and not line.strip().startswith('#'):\n",
    "            key, value = line.split('=', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            if hasattr(bootstrap, key.lower()):\n",
    "                setattr(bootstrap, key.lower(), value)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Bootstrap file not found at: {bootstrap_file}\\n\"\n",
    "        f\"Please create bootstrap.env\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nBootstrap Configuration:\")\n",
    "print(f\"  Subscription: {bootstrap.subscription_id or 'NOT SET'}\")\n",
    "print(f\"  Resource Group: {bootstrap.resource_group}\")\n",
    "print(f\"  Location: {bootstrap.location}\")\n",
    "\n",
    "# Validate\n",
    "if not bootstrap.subscription_id:\n",
    "    raise ValueError(\n",
    "        \"SUBSCRIPTION_ID must be set in bootstrap.env\\n\"\n",
    "        f\"File location: {bootstrap_file}\\n\"\n",
    "        \"Please edit the file and add your Azure subscription ID.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n[OK] Bootstrap configuration loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dependencies\"></a>\n",
    "\n",
    "## 0.3 Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell_3_41f69468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEPENDENCY INSTALLATION\n",
      "================================================================================\n",
      "\n",
      "Python: 3.12.11\n",
      "Path:   /usr/local/bin/python\n",
      "In virtual environment: False\n",
      "System Python: True\n",
      "Externally managed: True\n",
      "\n",
      "\u26a0\ufe0f  Externally-managed system Python detected\n",
      "   Using --user flag to install to user site-packages\n",
      "\n",
      "================================================================================\n",
      "[1/2] Installing python-dotenv (critical for environment loading)...\n",
      "      \u2705 python-dotenv installed\n",
      "\n",
      "[2/2] Installing from: requirements-py312.txt\n",
      "      (Python 3.12+ - no pyautogen)\n",
      "\n",
      "      Running pip install...\n",
      "      Command: /usr/local/bin/python -m pip install --user -r requirements-py312.txt\n",
      "\n",
      "      Requirement already satisfied: python-dotenv>=1.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 11)) (1.2.1)\n",
      "      Requirement already satisfied: azure-identity>=1.15.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 16)) (1.25.1)\n",
      "      Requirement already satisfied: azure-keyvault-secrets>=4.7.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 17)) (4.10.0)\n",
      "      Requirement already satisfied: azure-storage-blob>=12.19.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 18)) (12.27.1)\n",
      "      Requirement already satisfied: azure-mgmt-resource>=23.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 19)) (24.0.0)\n",
      "      Requirement already satisfied: azure-mgmt-apimanagement>=4.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 20)) (5.0.0)\n",
      "      Requirement already satisfied: azure-mgmt-cognitiveservices>=13.5.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 21)) (14.1.0)\n",
      "      Requirement already satisfied: azure-mgmt-cosmosdb>=9.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 22)) (9.9.0)\n",
      "      Requirement already satisfied: azure-mgmt-containerregistry>=10.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 23)) (14.0.0)\n",
      "      Requirement already satisfied: azure-core>=1.29.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 24)) (1.36.0)\n",
      "      Requirement already satisfied: azure-cosmos>=4.5.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 25)) (4.14.2)\n",
      "      Requirement already satisfied: azure-monitor-query>=1.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 26)) (2.0.0)\n",
      "      Requirement already satisfied: openai>=1.12.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 31)) (1.109.1)\n",
      "      Requirement already satisfied: azure-ai-inference>=1.0.0b1 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 32)) (1.0.0b9)\n",
      "      Requirement already satisfied: azure-search-documents>=11.4.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 33)) (11.6.0)\n",
      "      Requirement already satisfied: mcp>=0.9.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 38)) (1.22.0)\n",
      "      Requirement already satisfied: httpx>=0.27.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 39)) (0.28.1)\n",
      "      Requirement already satisfied: semantic-kernel>=1.0.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 45)) (1.39.0)\n",
      "      Requirement already satisfied: pyautogen~=0.2.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 49)) (0.2.35)\n",
      "      Requirement already satisfied: autogen-agentchat>=0.4.0 in /home/vscode/.local/lib/python3.12/site-packages (from -r requirements-py312.txt (line 52)) (0.7.5)\n",
      "      ... (truncating output) ...\n",
      "\n",
      "      \u2705 All dependencies installed successfully!\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\u2705 Packages installed to: /home/vscode/.local/lib/python3.12/site-packages\n",
      "   Using --user flag (externally-managed system)\n",
      "\n",
      "\u2139\ufe0f  Note: Python 3.12+ detected\n",
      "   - AutoGen 0.2.x skipped (not compatible)\n",
      "   - All other packages installed successfully\n",
      "   - Cells 8, 105, 111 can be skipped (AutoGen setup)\n",
      "\n",
      "Next steps:\n",
      "  1. Restart kernel if needed (Kernel \u2192 Restart Kernel)\n",
      "  2. Continue with the labs!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# (-1.2) Dependencies Install (Smart Multi-Environment)\n",
    "import sys\n",
    "import subprocess\n",
    "import pathlib\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEPENDENCY INSTALLATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check Python version\n",
    "py_version = sys.version_info\n",
    "print(f'\\nPython: {py_version.major}.{py_version.minor}.{py_version.micro}')\n",
    "print(f'Path:   {sys.executable}')\n",
    "\n",
    "# 2. Detect environment\n",
    "in_venv = sys.prefix != sys.base_prefix\n",
    "is_system_python = '/usr/bin/python' in sys.executable or '/usr/local/bin/python' in sys.executable\n",
    "externally_managed = is_system_python and py_version.major == 3 and py_version.minor >= 11\n",
    "\n",
    "print(f'In virtual environment: {in_venv}')\n",
    "print(f'System Python: {is_system_python}')\n",
    "print(f'Externally managed: {externally_managed}')\n",
    "\n",
    "# 3. Determine pip install strategy\n",
    "pip_args = [sys.executable, '-m', 'pip', 'install']\n",
    "\n",
    "if in_venv:\n",
    "    # In a virtual environment - install normally\n",
    "    print('\\n\u2705 Virtual environment detected - installing packages normally')\n",
    "    extra_args = []\n",
    "elif externally_managed:\n",
    "    # System Python with PEP 668 (externally-managed-environment)\n",
    "    print('\\n\u26a0\ufe0f  Externally-managed system Python detected')\n",
    "    print('   Using --user flag to install to user site-packages')\n",
    "    extra_args = ['--user']\n",
    "else:\n",
    "    # Other cases (older Python, non-Debian systems)\n",
    "    print('\\n\u2705 Installing packages normally')\n",
    "    extra_args = []\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 4. Install python-dotenv first (CRITICAL - needed by subsequent cells)\n",
    "print('[1/2] Installing python-dotenv (critical for environment loading)...')\n",
    "cmd_dotenv = pip_args + extra_args + ['-q', 'python-dotenv>=1.0.0']\n",
    "\n",
    "try:\n",
    "    r = subprocess.run(cmd_dotenv, capture_output=True, text=True, timeout=60)\n",
    "    if r.returncode == 0:\n",
    "        print('      \u2705 python-dotenv installed')\n",
    "    else:\n",
    "        print(f'      \u26a0\ufe0f  Warning: {r.stderr.strip()[:100]}')\n",
    "        # Try without -q for better error messages\n",
    "        if '--user' not in extra_args and not in_venv:\n",
    "            print('      Retrying with --user flag...')\n",
    "            cmd_dotenv_retry = pip_args + ['--user', 'python-dotenv>=1.0.0']\n",
    "            r2 = subprocess.run(cmd_dotenv_retry, capture_output=True, text=True, timeout=60)\n",
    "            if r2.returncode == 0:\n",
    "                print('      \u2705 python-dotenv installed with --user')\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('      \u26a0\ufe0f  Installation timeout (network issue?)')\n",
    "except Exception as e:\n",
    "    print(f'      \u26a0\ufe0f  Error: {e}')\n",
    "\n",
    "print()\n",
    "\n",
    "# 5. Determine which requirements file to use\n",
    "REQ_FILE = pathlib.Path('requirements.txt')\n",
    "REQ_FILE_PY312 = pathlib.Path('requirements-py312.txt')\n",
    "\n",
    "# Use Python 3.12-specific requirements if available and Python >= 3.12\n",
    "if py_version.minor >= 12 and REQ_FILE_PY312.exists():\n",
    "    install_file = REQ_FILE_PY312\n",
    "    print(f'[2/2] Installing from: {install_file}')\n",
    "    print('      (Python 3.12+ - no pyautogen)')\n",
    "elif REQ_FILE.exists():\n",
    "    req_content = REQ_FILE.read_text()\n",
    "\n",
    "    # If Python >= 3.12 but no py312 requirements, create temp file without pyautogen\n",
    "    if py_version.minor >= 12:\n",
    "        print('[2/2] Python 3.12+ detected - filtering out pyautogen...')\n",
    "\n",
    "        temp_req = pathlib.Path('.requirements-temp.txt')\n",
    "        lines = []\n",
    "        for line in req_content.splitlines():\n",
    "            # Skip pyautogen but keep comments\n",
    "            if 'pyautogen' not in line.lower() or line.strip().startswith('#'):\n",
    "                lines.append(line)\n",
    "        temp_req.write_text('\\n'.join(lines))\n",
    "        install_file = temp_req\n",
    "        print(f'      Installing from: {install_file} (filtered)')\n",
    "    else:\n",
    "        install_file = REQ_FILE\n",
    "        print(f'[2/2] Installing from: {install_file}')\n",
    "else:\n",
    "    print('[2/2] \u274c No requirements file found')\n",
    "    install_file = None\n",
    "\n",
    "# 6. Install all dependencies\n",
    "if install_file:\n",
    "    cmd = pip_args + extra_args + ['-r', str(install_file)]\n",
    "\n",
    "    print()\n",
    "    print('      Running pip install...')\n",
    "    print(f'      Command: {\" \".join(shlex.quote(str(c)) for c in cmd)}')\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Run with real-time output\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "\n",
    "        # Print output in real-time (truncated)\n",
    "        line_count = 0\n",
    "        for line in process.stdout:\n",
    "            line_count += 1\n",
    "            # Only print first 20 and last 10 lines to avoid flooding\n",
    "            if line_count <= 20:\n",
    "                print(f'      {line.rstrip()}')\n",
    "            elif line_count == 21:\n",
    "                print('      ... (truncating output) ...')\n",
    "\n",
    "        process.wait()\n",
    "\n",
    "        print()\n",
    "        if process.returncode == 0:\n",
    "            print('      \u2705 All dependencies installed successfully!')\n",
    "        else:\n",
    "            print(f'      \u26a0\ufe0f  pip exited with code {process.returncode}')\n",
    "            print('      Some packages may have failed - check output above')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'      \u274c Error during installation: {e}')\n",
    "\n",
    "    # Clean up temp file\n",
    "    if install_file.name == '.requirements-temp.txt' and install_file.exists():\n",
    "        install_file.unlink()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 7. Summary\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if in_venv:\n",
    "    print(f\"\u2705 Packages installed to: {sys.prefix}\")\n",
    "    print(\"   You're using a virtual environment (recommended!)\")\n",
    "elif extra_args and '--user' in extra_args:\n",
    "    import site\n",
    "    print(f\"\u2705 Packages installed to: {site.USER_SITE}\")\n",
    "    print(\"   Using --user flag (externally-managed system)\")\n",
    "else:\n",
    "    print(f\"\u2705 Packages installed to: {sys.prefix}\")\n",
    "\n",
    "if py_version.minor >= 12:\n",
    "    print()\n",
    "    print(\"\u2139\ufe0f  Note: Python 3.12+ detected\")\n",
    "    print(\"   - AutoGen 0.2.x skipped (not compatible)\")\n",
    "    print(\"   - All other packages installed successfully\")\n",
    "    print(\"   - Cells 8, 105, 111 can be skipped (AutoGen setup)\")\n",
    "\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Restart kernel if needed (Kernel \u2192 Restart Kernel)\")\n",
    "print(\"  2. Continue with the labs!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"azure-auth\"></a>\n",
    "\n",
    "## 0.4 Azure Authentication & Service Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az resolved: /usr/bin/az (reason=ranked selection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[azure] az version: azure-cli                         2.80.0\n",
      "[azure] Active subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[azure] Loading existing credentials file\n",
      "\n",
      "[azure] Credentials summary:\n",
      "  SUBSCRIPTION_ID=d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  AZURE_CLIENT_ID=a308304b-c1ff-40ed-a9d9-af22fd0bc464\n",
      "  AZURE_TENANT_ID=2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "  AZURE_CLIENT_SECRET=***\n",
      "\n",
      "[rbac] Function assign_rg_roles() ready - call after resource group creation\n",
      "[msal] MSAL cache helpers loaded\n",
      "[endpoint] Existing OPENAI_ENDPOINT found\n",
      "[endpoint] Derived convenience vars ready\n"
     ]
    }
   ],
   "source": [
    "# (-1.3) Azure CLI & Service Principal Setup with Full RBAC\n",
    "# Creates Service Principal with ALL required roles for the entire notebook:\n",
    "# - Subscription: Contributor, User Access Administrator\n",
    "# - Resource Group: Cognitive Services OpenAI User, Cosmos DB roles, Search roles, etc.\n",
    "import json, os, shutil, subprocess, sys, time\n",
    "from pathlib import Path\n",
    "AZ_CREDS_FILE=Path('.azure-credentials.env')\n",
    "\n",
    "OS_RELEASE = {}\n",
    "try:\n",
    "    if Path('/etc/os-release').exists():\n",
    "        for line in Path('/etc/os-release').read_text().splitlines():\n",
    "            if '=' in line:\n",
    "                k,v=line.split('=',1)\n",
    "                OS_RELEASE[k]=v.strip().strip('\"')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ARCH_LINUX = OS_RELEASE.get('ID') == 'arch'\n",
    "CODESPACES = bool(os.environ.get('CODESPACES')) or bool(os.environ.get('CODESPACE_NAME'))\n",
    "retry_delay_sec = float(os.environ.get('AZ_RETRY_DELAY_SEC', '3'))\n",
    "\n",
    "def resolve_az_cli():\n",
    "    override=os.environ.get('AZURE_CLI_PATH')\n",
    "    if override and Path(override).exists():\n",
    "        return override, 'env AZURE_CLI_PATH'\n",
    "    candidates = []\n",
    "    for name in ['az','az.cmd','az.exe']:\n",
    "        p=shutil.which(name)\n",
    "        if p: candidates.append(p)\n",
    "    candidates += ['/usr/bin/az', '/usr/local/bin/az', '/snap/bin/az', '/opt/homebrew/bin/az']\n",
    "    if CODESPACES:\n",
    "        candidates.append(str(Path.home()/'.local/bin/az'))\n",
    "    candidates += [\n",
    "        'C:/Program Files (x86)/Microsoft SDKs/Azure/CLI2/wbin/az.cmd',\n",
    "        'C:/Program Files/Microsoft SDKs/Azure/CLI2/wbin/az.cmd'\n",
    "    ]\n",
    "    home_cli = Path.home()/'.azure-cli/az'\n",
    "    candidates.append(str(home_cli))\n",
    "    existing=[c for c in candidates if c and Path(c).exists()]\n",
    "    if not existing:\n",
    "        venv_az = Path(sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "        if venv_az.exists():\n",
    "            return str(venv_az), 'venv fallback'\n",
    "        return None, 'not found'\n",
    "    def rank(p):\n",
    "        p_low=p.lower()\n",
    "        penalty = 1000 if ('.venv' in p_low or 'scripts' in p_low) else 0\n",
    "        return penalty, len(p)\n",
    "    existing.sort(key=rank)\n",
    "    return existing[0], 'ranked selection'\n",
    "\n",
    "az_cli, reason = resolve_az_cli()\n",
    "print(f'[azure] az resolved: {az_cli or \"NOT FOUND\"} (reason={reason})')\n",
    "if not az_cli:\n",
    "    if ARCH_LINUX:\n",
    "        print('[azure] Arch Linux detected. Install Azure CLI: sudo pacman -S azure-cli')\n",
    "    else:\n",
    "        print('[azure] Install Azure CLI: https://learn.microsoft.com/cli/azure/install-azure-cli')\n",
    "    raise SystemExit('Azure CLI not found.')\n",
    "\n",
    "os.environ['AZ_CLI']=az_cli\n",
    "try:\n",
    "    ver=subprocess.run([az_cli,'--version'],capture_output=True,text=True,timeout=4)\n",
    "    if ver.returncode==0:\n",
    "        first_line=ver.stdout.splitlines()[0] if ver.stdout else ''\n",
    "        print('[azure] az version:', first_line)\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('[azure] WARN: az version check timed out')\n",
    "except Exception as e:\n",
    "    print('[azure] WARN: az version check error:', e)\n",
    "\n",
    "# Subscription discovery\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "if not subscription_id:\n",
    "    for attempt in range(1, 3):\n",
    "        try:\n",
    "            timeout_sec = 8 if attempt == 1 else 20\n",
    "            sub_proc = subprocess.run(\n",
    "                [az_cli, 'account', 'show', '--output', 'json'],\n",
    "                capture_output=True, text=True, timeout=timeout_sec\n",
    "            )\n",
    "            if sub_proc.returncode == 0:\n",
    "                sub = json.loads(sub_proc.stdout)\n",
    "                subscription_id = sub.get('id')\n",
    "                print('[azure] Active subscription:', subscription_id)\n",
    "                if subscription_id:\n",
    "                    os.environ.setdefault('SUBSCRIPTION_ID', subscription_id)\n",
    "                break\n",
    "            else:\n",
    "                print(f'[azure] account show failed: {sub_proc.stderr[:200]}')\n",
    "                break\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f'[azure] account show timed out (attempt {attempt}/2)')\n",
    "            if attempt < 2:\n",
    "                time.sleep(retry_delay_sec)\n",
    "else:\n",
    "    print('[azure] Using existing SUBSCRIPTION_ID:', subscription_id)\n",
    "\n",
    "# Service Principal creation with comprehensive RBAC\n",
    "sp_env_keys=['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID']\n",
    "creds_present=all(os.environ.get(k) for k in sp_env_keys)\n",
    "if creds_present:\n",
    "    print('[azure] SP credentials already present; skipping creation')\n",
    "elif AZ_CREDS_FILE.exists():\n",
    "    print('[azure] Loading existing credentials file')\n",
    "    for line in AZ_CREDS_FILE.read_text().splitlines():\n",
    "        if line.strip() and '=' in line:\n",
    "            k,v=line.split('=',1); os.environ.setdefault(k.strip(),v.strip())\n",
    "else:\n",
    "    if not os.environ.get('SUBSCRIPTION_ID'):\n",
    "        print('[azure] Cannot create SP: missing SUBSCRIPTION_ID')\n",
    "    else:\n",
    "        sub_id = os.environ.get('SUBSCRIPTION_ID','')\n",
    "        sp_name = f'sp-master-lab-{sub_id[:8]}'\n",
    "        scope = f\"/subscriptions/{sub_id}\"\n",
    "        \n",
    "        print(f'[azure] Creating service principal: {sp_name}')\n",
    "        print('[azure] This SP will have roles for the entire notebook:')\n",
    "        print('        - Contributor (subscription)')\n",
    "        print('        - User Access Administrator (subscription)')\n",
    "        print()\n",
    "        \n",
    "        # Create SP with Contributor role first\n",
    "        sp_cmd=[az_cli,'ad','sp','create-for-rbac','--name',sp_name,'--role','Contributor','--scopes',scope,'--sdk-auth']\n",
    "        r=subprocess.run(sp_cmd,capture_output=True,text=True,timeout=60)\n",
    "        if r.returncode!=0:\n",
    "            print('[azure] SP creation failed:', r.stderr[:300])\n",
    "        else:\n",
    "            data=json.loads(r.stdout)\n",
    "            mapping={'clientId':'AZURE_CLIENT_ID','clientSecret':'AZURE_CLIENT_SECRET','tenantId':'AZURE_TENANT_ID','subscriptionId':'SUBSCRIPTION_ID'}\n",
    "            for src,dst in mapping.items():\n",
    "                if src in data:\n",
    "                    os.environ[dst]=data[src]\n",
    "            \n",
    "            client_id = data.get('clientId')\n",
    "            if client_id:\n",
    "                # Subscription-level roles\n",
    "                sub_roles = ['User Access Administrator']\n",
    "                for role in sub_roles:\n",
    "                    print(f'[azure] Adding {role} role (subscription)...')\n",
    "                    cmd=[az_cli,'role','assignment','create','--assignee',client_id,'--role',role,'--scope',scope]\n",
    "                    result=subprocess.run(cmd,capture_output=True,text=True,timeout=30)\n",
    "                    if result.returncode==0:\n",
    "                        print(f'[azure] {role} assigned')\n",
    "                    else:\n",
    "                        print(f'[azure] WARN: {role} failed (may already exist)')\n",
    "            \n",
    "            lines=[f'{k}={os.environ[k]}' for k in mapping.values() if k in os.environ]\n",
    "            AZ_CREDS_FILE.write_text('\\n'.join(lines))\n",
    "            print('[azure] SP created & credentials saved (.azure-credentials.env)')\n",
    "\n",
    "# Credentials summary\n",
    "print()\n",
    "print('[azure] Credentials summary:')\n",
    "for k in ['SUBSCRIPTION_ID','AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET']:\n",
    "    v=os.environ.get(k)\n",
    "    if not v: continue\n",
    "    masked='***' if 'SECRET' in k else v\n",
    "    print(f'  {k}={masked}')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RBAC ROLE ASSIGNMENT FOR RESOURCE GROUP\n",
    "# =============================================================================\n",
    "# These roles are assigned AFTER resource group creation (in deployment cell)\n",
    "# but we define the function here for reuse\n",
    "\n",
    "def assign_rg_roles(resource_group_name: str, principal_id: str = None):\n",
    "    \"\"\"Assign all required Resource Group level roles for the notebook.\n",
    "    \n",
    "    Required roles:\n",
    "    - Cognitive Services OpenAI User: JWT token auth, model inference\n",
    "    - Cognitive Services Contributor: Model deployments\n",
    "    - Cosmos DB Account Reader: Query stored messages\n",
    "    - Search Index Data Contributor: RAG pattern, vector search\n",
    "    - Redis Cache Contributor: Semantic caching\n",
    "    \"\"\"\n",
    "    if not principal_id:\n",
    "        # Use current user/SP\n",
    "        result = subprocess.run([az_cli,'ad','signed-in-user','show','--query','id','-o','tsv'],\n",
    "                               capture_output=True,text=True,timeout=15)\n",
    "        if result.returncode == 0:\n",
    "            principal_id = result.stdout.strip()\n",
    "        else:\n",
    "            # Try getting SP client ID\n",
    "            principal_id = os.environ.get('AZURE_CLIENT_ID')\n",
    "    \n",
    "    if not principal_id:\n",
    "        print('[rbac] WARNING: Could not determine principal ID for role assignment')\n",
    "        return\n",
    "    \n",
    "    sub_id = os.environ.get('SUBSCRIPTION_ID','')\n",
    "    rg_scope = f\"/subscriptions/{sub_id}/resourceGroups/{resource_group_name}\"\n",
    "    \n",
    "    # Resource Group level roles needed for notebook operations\n",
    "    rg_roles = [\n",
    "        'Cognitive Services OpenAI User',      # JWT auth, inference\n",
    "        'Cognitive Services Contributor',       # Model management\n",
    "        'Search Index Data Contributor',        # Vector search, RAG\n",
    "        'DocumentDB Account Contributor',       # Cosmos DB operations\n",
    "    ]\n",
    "    \n",
    "    print(f'[rbac] Assigning Resource Group roles to {principal_id[:8]}...')\n",
    "    for role in rg_roles:\n",
    "        cmd=[az_cli,'role','assignment','create','--assignee',principal_id,'--role',role,'--scope',rg_scope]\n",
    "        result=subprocess.run(cmd,capture_output=True,text=True,timeout=30)\n",
    "        if result.returncode==0:\n",
    "            print(f'  [OK] {role}')\n",
    "        elif 'already exists' in result.stderr.lower() or 'conflict' in result.stderr.lower():\n",
    "            print(f'  [OK] {role} (already assigned)')\n",
    "        else:\n",
    "            print(f'  [WARN] {role}: {result.stderr[:80]}')\n",
    "\n",
    "print()\n",
    "print('[rbac] Function assign_rg_roles() ready - call after resource group creation')\n",
    "\n",
    "\n",
    "# (-1.3b) MSAL Cache Flush Helper\n",
    "import os, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def flush_msal_cache():\n",
    "    msal_cache_dirs = [\n",
    "        Path.home() / '.azure' / 'msal_token_cache.bin',\n",
    "        Path.home() / '.azure' / 'msal_token_cache.json',\n",
    "        Path.home() / '.azure' / 'msal_http_cache',\n",
    "        Path.home() / '.azure' / 'service_principal_entries.bin',\n",
    "    ]\n",
    "    flushed = []\n",
    "    for cache_path in msal_cache_dirs:\n",
    "        try:\n",
    "            if cache_path.exists():\n",
    "                if cache_path.is_file():\n",
    "                    cache_path.unlink()\n",
    "                elif cache_path.is_dir():\n",
    "                    shutil.rmtree(cache_path)\n",
    "                flushed.append(str(cache_path))\n",
    "        except Exception as e:\n",
    "            print(f'[msal] Warning: Could not remove {cache_path}: {e}')\n",
    "    if flushed:\n",
    "        print(f'[msal] Flushed {len(flushed)} cache entries')\n",
    "    return bool(flushed)\n",
    "\n",
    "def az_with_msal_retry(az_cli, command_args, **kwargs):\n",
    "    kwargs.setdefault('capture_output', True)\n",
    "    kwargs.setdefault('text', True)\n",
    "    kwargs.setdefault('timeout', 30)\n",
    "    result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "    if result.returncode != 0 and 'NormalizedResponse' in result.stderr:\n",
    "        print('[msal] MSAL cache corruption detected, flushing...')\n",
    "        flush_msal_cache()\n",
    "        print('[msal] Re-authenticating...')\n",
    "        login_result = subprocess.run([az_cli, 'login'], capture_output=True, text=True, timeout=60)\n",
    "        if login_result.returncode == 0:\n",
    "            result = subprocess.run([az_cli] + command_args, **kwargs)\n",
    "    return result\n",
    "\n",
    "print('[msal] MSAL cache helpers loaded')\n",
    "\n",
    "\n",
    "# (-1.4) Endpoint Normalizer\n",
    "from pathlib import Path\n",
    "import os, re\n",
    "env_path=Path('master-lab.env')\n",
    "text=env_path.read_text() if env_path.exists() else ''\n",
    "get=lambda k: os.environ.get(k) or (re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE).group(1).strip() if re.search(fr'^\\s*{k}=(.*)$', text, re.MULTILINE) else '')\n",
    "openai_endpoint=get('OPENAI_ENDPOINT')\n",
    "modified=False\n",
    "if openai_endpoint:\n",
    "    print('[endpoint] Existing OPENAI_ENDPOINT found')\n",
    "else:\n",
    "    apim=get('APIM_GATEWAY_URL')\n",
    "    path_var=get('INFERENCE_API_PATH') or '/inference'\n",
    "    if apim:\n",
    "        openai_endpoint=apim.rstrip('/')+path_var\n",
    "        print('[endpoint] Derived from APIM_GATEWAY_URL')\n",
    "        modified=True\n",
    "    else:\n",
    "        fallback=get('AZURE_OPENAI_ENDPOINT') or get('AI_FOUNDRY_ENDPOINT')\n",
    "        if fallback:\n",
    "            openai_endpoint=fallback.rstrip('/')\n",
    "            print('[endpoint] Derived from Foundry endpoint')\n",
    "            modified=True\n",
    "        else:\n",
    "            print('[endpoint] Unable to derive endpoint; set OPENAI_ENDPOINT in master-lab.env')\n",
    "if openai_endpoint:\n",
    "    os.environ['OPENAI_ENDPOINT']=openai_endpoint\n",
    "    os.environ.setdefault('OPENAI_API_BASE', openai_endpoint)\n",
    "    os.environ.setdefault('OPENAI_MODELS_URL', openai_endpoint.rstrip('/') + '/models')\n",
    "print('[endpoint] Derived convenience vars ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"helpers\"></a>\n",
    "\n",
    "## 0.5 Core Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure CLI Helper Functions\n",
    "\n",
    "\n",
    "**Purpose**: Provides unified Azure CLI command execution with automatic authentication and error handling\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Azure CLI installed and accessible\n",
    "- Environment variable `AZ_CLI` or `AZURE_CLI_PATH` set\n",
    "- Optional: Service Principal credentials (`AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_CLIENT_SECRET`)\n",
    "- Optional: Azure CLI authenticated via `az login`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "This cell initializes three critical helper function sets:\n",
    "\n",
    "**1. az() Helper Function**\n",
    "- Executes Azure CLI commands programmatically\n",
    "- Automatically handles authentication (Service Principal or interactive)\n",
    "- Supports JSON output parsing for easy data extraction\n",
    "- Includes timeout controls and error handling\n",
    "- Auto-retries with login if authentication expires\n",
    "\n",
    "**2. Deployment Helpers**\n",
    "Functions for infrastructure deployment:\n",
    "- `compile_bicep(bicep_path)` - Compiles Bicep templates to ARM JSON\n",
    "- `deploy_template(rg, name, template_file, params)` - Deploys ARM templates\n",
    "- `get_deployment_outputs(rg, name)` - Retrieves deployment outputs\n",
    "- `ensure_deployment(rg, name, template, params)` - Smart deployment with skip-if-exists logic\n",
    "\n",
    "**3. Policy Application Helper**\n",
    "- `apply_policies(policies)` - Applies API Management policies via Azure REST API\n",
    "- Automatically discovers API IDs if not provided\n",
    "- Supports multiple policy applications in sequence\n",
    "- Uses REST API for broader CLI version compatibility\n",
    "\n",
    "**4. AzureOpenAI Client Shim**\n",
    "- Provides unified import for OpenAI SDK\n",
    "- Ensures compatibility across different SDK versions\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "You should see output indicating:\n",
    "- Azure CLI version detected\n",
    "- Current Azure account information (if logged in)\n",
    "- Confirmation that deployment helpers are ready\n",
    "- Confirmation that policy application helpers are ready\n",
    "- Confirmation that AzureOpenAI shim is ready\n",
    "\n",
    "If not logged in, you'll see instructions to run `az login`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[az] version: azure-cli                         2.80.0\n",
      "[az] account: ME-MngEnvMCAP592090-lproux-1 d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[shim] AzureOpenAI shim ready.\n",
      "[deploy] helpers ready\n",
      "[policy] Missing env vars; set: RESOURCE_GROUP, APIM_SERVICE\n",
      "\ud83d\udd04 Initializing MCP Client with 4 Data Sources...\n",
      "\n",
      "\u2705 MCP Client initialized successfully!\n",
      "\ud83d\udcca Available: 4/4 data sources\n",
      "\n",
      "\ud83d\udce1 Data Sources:\n",
      "  1. Excel Analytics MCP\n",
      "     URL: http://excel-mcp-master.eastus.azurecontainer.io:8000\n",
      "     Type: Direct MCP Protocol\n",
      "     Capabilities: Analytics, charts, calculations\n",
      "\n",
      "  2. Research Documents MCP\n",
      "     URL: https://mcp-ms-learn-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "     Type: Direct MCP Protocol\n",
      "     Capabilities: Document search, retrieval, comparison\n",
      "\n",
      "  3. GitHub REST API (via APIM)\n",
      "     URL: https://api.github.com\n",
      "     Type: APIM-Routed REST API\n",
      "     Capabilities: Repo search, code analysis, issues\n",
      "\n",
      "  4. OpenWeather API (via APIM)\n",
      "     URL: https://mcp-weather-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "     Type: APIM-Routed REST API\n",
      "     Capabilities: Real-time weather, forecasts\n",
      "\n",
      "\ud83d\udca1 Configuration loaded from .mcp-servers-config\n",
      "   Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\n",
      "[AzureOps] CLI: /usr/bin/az\n",
      "[AzureOps] login status: OK\n",
      "[AzureOps] version: azure-cli                         2.80.0\n",
      "[AzureOps] strategy: sdk\n"
     ]
    }
   ],
   "source": [
    "# (-1.5) Unified az() Helper & Login Check\n",
    "\"\"\"Provides a cached az CLI executor with:\n",
    "- Path reuse via AZ_CLI env (expects (-1.3) run first)\n",
    "- Automatic login prompt if account show fails and no service principal creds\n",
    "- Timeout controls & JSON parsing convenience\n",
    "Usage:\n",
    "    ok, data = az('account show', json_out=True)\n",
    "    ok, text = az('apim list --resource-group X')\n",
    "\"\"\"\n",
    "import os, subprocess, json, shlex\n",
    "from pathlib import Path\n",
    "AZ_CLI = os.environ.get('AZ_CLI') or os.environ.get('AZURE_CLI_PATH')\n",
    "_cached_version=None\n",
    "\n",
    "def az(cmd:str, json_out:bool=False, timeout:int=25, login_if_needed:bool=True):\n",
    "    global _cached_version\n",
    "    if not AZ_CLI:\n",
    "        return False, 'AZ_CLI not set; run (-1.3) first.'\n",
    "    parts=[AZ_CLI]+shlex.split(cmd)\n",
    "    try:\n",
    "        proc=subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, f'timeout after {timeout}s: {cmd}'\n",
    "    if proc.returncode!=0:\n",
    "        stderr=proc.stderr.strip()\n",
    "        if login_if_needed and 'az login' in stderr.lower():\n",
    "            # If SP creds exist, attempt non-interactive login; else instruct.\n",
    "            sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_TENANT_ID','AZURE_CLIENT_SECRET'])\n",
    "            if sp_ok:\n",
    "                sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} \"\n",
    "                        f\"-p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "                print('[az] Attempting SP login ...')\n",
    "                lp=subprocess.run([AZ_CLI]+shlex.split(sp_cmd),capture_output=True,text=True,timeout=40)\n",
    "                if lp.returncode==0:\n",
    "                    print('[az] SP login successful; retrying command')\n",
    "                    return az(cmd,json_out=json_out,timeout=timeout,login_if_needed=False)\n",
    "                else:\n",
    "                    print('[az] SP login failed:', lp.stderr[:180])\n",
    "            else:\n",
    "                print('[az] Interactive login required: run \"az login\" in a terminal.')\n",
    "        return False, stderr or proc.stdout\n",
    "    out=proc.stdout\n",
    "    if json_out:\n",
    "        try:\n",
    "            return True, json.loads(out or '{}')\n",
    "        except Exception as e:\n",
    "            return False, f'json parse error: {e}\\nRaw: {out[:200]}'\n",
    "    return True, out\n",
    "\n",
    "# Cache version lazily\n",
    "if not _cached_version:\n",
    "    ok, ver = az('--version', json_out=False, timeout=5, login_if_needed=False)\n",
    "    if ok:\n",
    "        _cached_version=ver.splitlines()[0] if ver else ''\n",
    "        print('[az] version:', _cached_version)\n",
    "    else:\n",
    "        print('[az] version check skipped:', ver[:120])\n",
    "\n",
    "# Quick account context (suppresses login if SP already authenticated)\n",
    "ok, acct = az('account show', json_out=True, timeout=10)\n",
    "if ok:\n",
    "    print('[az] account:', acct.get('name'), acct.get('id'))\n",
    "else:\n",
    "    print('[az] account show issue:', acct[:160])\n",
    "\n",
    "\n",
    "# (-1.6) Deployment Helpers (Consolidated)\n",
    "\"\"\"Utilities for ARM/Bicep deployments via az CLI.\n",
    "Depends on az() from (-1.5).\n",
    "Functions:\n",
    "  compile_bicep(bicep_path) -> str json_template_path\n",
    "  deploy_template(rg, name, template_file, params: dict) -> (ok, result_json)\n",
    "  get_deployment_outputs(rg, name) -> dict outputs or {}\n",
    "  ensure_deployment(rg, name, template, params, skip_if_exists=True)\n",
    "\"\"\"\n",
    "import os, json, tempfile, pathlib, shlex\n",
    "from pathlib import Path\n",
    "\n",
    "def compile_bicep(bicep_path:str):\n",
    "    b=Path(bicep_path)\n",
    "    if not b.exists():\n",
    "        raise FileNotFoundError(f'Bicep file not found: {bicep_path}')\n",
    "    out_json = b.with_suffix('.json')\n",
    "    ok, res = az(f'bicep build --file {shlex.quote(str(b))}')\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Failed bicep build: {res}')\n",
    "    if not out_json.exists():\n",
    "        raise RuntimeError(f'Expected compiled template missing: {out_json}')\n",
    "    print('[deploy] compiled', bicep_path, '->', out_json)\n",
    "    return str(out_json)\n",
    "\n",
    "def deploy_template(rg:str, name:str, template_file:str, params:dict):\n",
    "    param_args=[]\n",
    "    for k,v in params.items():\n",
    "        if isinstance(v, (dict,list)):\n",
    "            # Write complex params to temp file\n",
    "            tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "            tmp.write_text(json.dumps({\"value\": v}, indent=2))\n",
    "            param_args.append(f'{k}=@{tmp}')\n",
    "        else:\n",
    "            param_args.append(f'{k}={json.dumps(v)}')\n",
    "    params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "    cmd=f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}'\n",
    "    print('[deploy] running:', cmd)\n",
    "    ok, res = az(cmd, json_out=True, timeout=600)\n",
    "    return ok, res\n",
    "\n",
    "def get_deployment_outputs(rg:str, name:str):\n",
    "    ok,res = az(f'deployment group show --resource-group {rg} --name {name}', json_out=True)\n",
    "    if not ok:\n",
    "        print('[deploy] show failed:', res[:140])\n",
    "        return {}\n",
    "    outputs = res.get('properties',{}).get('outputs',{})\n",
    "    simplified={k: v.get('value') for k,v in outputs.items()} if isinstance(outputs, dict) else {}\n",
    "    print('[deploy] outputs keys:', ', '.join(simplified.keys()))\n",
    "    return simplified\n",
    "\n",
    "def check_deployment_exists(rg:str, name:str):\n",
    "    ok,res=az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=15)\n",
    "    return ok and res.get('name')==name\n",
    "\n",
    "def ensure_deployment(rg:str, name:str, bicep_file:str, params:dict, skip_if_exists:bool=True):\n",
    "    if skip_if_exists and check_deployment_exists(rg,name):\n",
    "        print('[deploy] existing deployment found:', name)\n",
    "        return get_deployment_outputs(rg,name)\n",
    "    template=compile_bicep(bicep_file) if bicep_file.endswith('.bicep') else bicep_file\n",
    "    ok,res=deploy_template(rg,name,template,params)\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Deployment {name} failed: {res}')\n",
    "    return get_deployment_outputs(rg,name)\n",
    "\n",
    "# AzureOpenAI Compatibility Import Shim\n",
    "# Some cells use: from openai import AzureOpenAI\n",
    "# Provide a unified accessor that can adapt if future SDK reorganizes paths.\n",
    "\n",
    "def get_azure_openai_client(**kwargs):\n",
    "    try:\n",
    "        from openai import AzureOpenAI  # standard location\n",
    "        return AzureOpenAI(**kwargs)\n",
    "    except ImportError as ex:\n",
    "        raise ImportError(\"AzureOpenAI class not found; ensure openai>=2.2,<3 installed.\") from ex\n",
    "\n",
    "print('[shim] AzureOpenAI shim ready.')\n",
    "\n",
    "print('[deploy] helpers ready')\n",
    "\n",
    "\n",
    "# (-1.7) Unified Policy Application with Auto-Discovery\n",
    "\n",
    "\"\"\"Applies one or more API Management policies to the target API using Azure REST API.\n",
    "\n",
    "Provide policies as a list of (policy_name, policy_xml_string).\n",
    "\n",
    "Automatically discovers the API ID if not set in environment.\n",
    "Creates policy payloads and invokes az rest to apply them.\n",
    "\n",
    "Requires ENV values: RESOURCE_GROUP, APIM_SERVICE (service name)\n",
    "Optional: API_ID (will be auto-discovered if not provided)\n",
    "\n",
    "Note: Uses Azure REST API because 'az apim api policy' command is not available in all CLI versions.\n",
    "\"\"\"\n",
    "\n",
    "import os, json as json_module, tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "REQUIRED_POLICY_ENV=['RESOURCE_GROUP','APIM_SERVICE']\n",
    "\n",
    "missing=[k for k in REQUIRED_POLICY_ENV if not os.environ.get(k)]\n",
    "\n",
    "if missing:\n",
    "    print('[policy] Missing env vars; set:', ', '.join(missing))\n",
    "else:\n",
    "    def discover_api_id():\n",
    "        \"\"\"Discover the API ID from APIM instance.\"\"\"\n",
    "        service = os.environ['APIM_SERVICE']\n",
    "        rg = os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get subscription ID\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print('[policy] Failed to get subscription ID')\n",
    "            return None\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "\n",
    "        # List APIs using REST API\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{service}/apis?api-version=2022-08-01')\n",
    "\n",
    "        print('[policy] Discovering APIs in APIM instance...')\n",
    "        ok, result = az(f'rest --method get --url \"{url}\"', json_out=True, timeout=60)\n",
    "\n",
    "        if not ok or not result:\n",
    "            print('[policy] Failed to list APIs')\n",
    "            return None\n",
    "\n",
    "        apis = result.get('value', [])\n",
    "\n",
    "        if not apis:\n",
    "            print('[policy] ERROR: No APIs found in APIM instance')\n",
    "            print('[policy] HINT: You may need to deploy the infrastructure first')\n",
    "            return None\n",
    "\n",
    "        # Prefer APIs with 'openai' in the name\n",
    "        openai_apis = [api for api in apis if 'openai' in api.get('name', '').lower()]\n",
    "\n",
    "        if openai_apis:\n",
    "            api_id = openai_apis[0]['name']\n",
    "            print(f'[policy] Found OpenAI API: {api_id}')\n",
    "        else:\n",
    "            api_id = apis[0]['name']\n",
    "            print(f'[policy] Using first available API: {api_id}')\n",
    "\n",
    "        return api_id\n",
    "\n",
    "    def apply_policies(policies):\n",
    "        service=os.environ['APIM_SERVICE']\n",
    "        rg=os.environ['RESOURCE_GROUP']\n",
    "\n",
    "        # Get or discover API_ID\n",
    "        api_id = os.environ.get('API_ID')\n",
    "\n",
    "        if not api_id:\n",
    "            print('[policy] API_ID not set in environment, discovering...')\n",
    "            api_id = discover_api_id()\n",
    "\n",
    "            if not api_id:\n",
    "                print('[policy] ERROR: Could not discover API ID')\n",
    "                print('[policy] HINT: Set API_ID environment variable or deploy infrastructure')\n",
    "                return\n",
    "\n",
    "            # Save for future use\n",
    "            os.environ['API_ID'] = api_id\n",
    "            print(f'[policy] Saved API_ID to environment: {api_id}')\n",
    "\n",
    "        # Get subscription ID\n",
    "        print('[policy] Getting subscription ID...')\n",
    "        ok_sub, sub_result = az('account show', json_out=True, timeout=30)\n",
    "        if not ok_sub:\n",
    "            print(f'[policy] Failed to get subscription ID: {sub_result}')\n",
    "            return\n",
    "\n",
    "        subscription_id = sub_result.get('id')\n",
    "        print(f'[policy] Subscription ID: {subscription_id}')\n",
    "        print(f'[policy] Using API ID: {api_id}')\n",
    "\n",
    "        for name, xml in policies:\n",
    "            xml = xml.strip()\n",
    "\n",
    "            # Azure REST API endpoint for APIM policy\n",
    "            url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "                   f'/resourceGroups/{rg}/providers/Microsoft.ApiManagement'\n",
    "                   f'/service/{service}/apis/{api_id}/policies/policy?api-version=2022-08-01')\n",
    "\n",
    "            # Policy payload in Azure format\n",
    "            policy_payload = {\n",
    "                \"properties\": {\n",
    "                    \"value\": xml,\n",
    "                    \"format\": \"xml\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write JSON payload to temp file (Windows-friendly)\n",
    "            payload_file = Path(tempfile.gettempdir()) / f'apim-{name}-payload.json'\n",
    "            with open(payload_file, 'w', encoding='utf-8') as f:\n",
    "                json_module.dump(policy_payload, f, indent=2)\n",
    "\n",
    "            print(f'[policy] Applying {name} via REST API...')\n",
    "\n",
    "            # Use az rest command with @file syntax for body\n",
    "            cmd = f'rest --method put --url \"{url}\" --body @\"{payload_file}\" --headers \"Content-Type=application/json\"'\n",
    "\n",
    "            ok, res = az(cmd, json_out=False, timeout=120)\n",
    "\n",
    "            # Clean up temp file\n",
    "            try:\n",
    "                payload_file.unlink()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if ok:\n",
    "                print(f'[policy] {name} applied successfully')\n",
    "            else:\n",
    "                error_msg = str(res)[:400] if res else 'Unknown error'\n",
    "                print(f'[policy] {name} failed: {error_msg}')\n",
    "\n",
    "    print('[policy] apply_policies(policies) ready with auto-discovery (using Azure REST API)')\n",
    "\n",
    "\n",
    "# (-1.8) Unified MCP Initialization (Updated for 4 Data Sources)\n",
    "\"\"\"Initializes MCP servers and APIM-routed APIs.\n",
    "\n",
    "Available Data Sources:\n",
    "  1. Excel MCP (direct) - Analytics, charts, data processing\n",
    "  2. Docs MCP (direct) - Document search, retrieval\n",
    "  3. GitHub API (APIM) - Code repos, search\n",
    "  4. Weather API (APIM) - Real-time weather data\n",
    "\n",
    "Reads configuration from .mcp-servers-config file.\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "# Check if already initialized\n",
    "if 'mcp' in globals() and hasattr(mcp, 'excel'):\n",
    "    print(\"\u26a0\ufe0f  MCP Client already initialized. Skipping re-initialization.\")\n",
    "    print()\n",
    "    print(\"Available Data Sources:\")\n",
    "    if mcp.excel:\n",
    "        print(f\"  \u2713 Excel MCP: {mcp.excel.server_url}\")\n",
    "    if mcp.docs:\n",
    "        print(f\"  \u2713 Docs MCP: {mcp.docs.server_url}\")\n",
    "    if mcp.github:\n",
    "        url = getattr(mcp.github, 'base_url', 'configured')\n",
    "        print(f\"  \u2713 GitHub API (APIM): {url}\")\n",
    "    if mcp.weather:\n",
    "        url = getattr(mcp.weather, 'base_url', 'configured')\n",
    "        print(f\"  \u2713 Weather API (APIM): {url}\")\n",
    "else:\n",
    "    print(\"\ud83d\udd04 Initializing MCP Client with 4 Data Sources...\")\n",
    "    print()\n",
    "    try:\n",
    "        mcp = MCPClient()\n",
    "\n",
    "        # Count available sources\n",
    "        available = []\n",
    "        if mcp.excel:\n",
    "            available.append(\"Excel MCP\")\n",
    "        if mcp.docs:\n",
    "            available.append(\"Docs MCP\")\n",
    "        if mcp.github:\n",
    "            available.append(\"GitHub API\")\n",
    "        if mcp.weather:\n",
    "            available.append(\"Weather API\")\n",
    "\n",
    "        print(f\"\u2705 MCP Client initialized successfully!\")\n",
    "        print(f\"\ud83d\udcca Available: {len(available)}/4 data sources\")\n",
    "        print()\n",
    "        print(f\"\ud83d\udce1 Data Sources:\")\n",
    "\n",
    "        if mcp.excel:\n",
    "            print(f\"  1. Excel Analytics MCP\")\n",
    "            print(f\"     URL: {mcp.excel.server_url}\")\n",
    "            print(f\"     Type: Direct MCP Protocol\")\n",
    "            print(f\"     Capabilities: Analytics, charts, calculations\")\n",
    "            print()\n",
    "\n",
    "        if mcp.docs:\n",
    "            print(f\"  2. Research Documents MCP\")\n",
    "            print(f\"     URL: {mcp.docs.server_url}\")\n",
    "            print(f\"     Type: Direct MCP Protocol\")\n",
    "            print(f\"     Capabilities: Document search, retrieval, comparison\")\n",
    "            print()\n",
    "\n",
    "        if mcp.github:\n",
    "            url = getattr(mcp.github, 'base_url', 'configured')\n",
    "            print(f\"  3. GitHub REST API (via APIM)\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     Type: APIM-Routed REST API\")\n",
    "            print(f\"     Capabilities: Repo search, code analysis, issues\")\n",
    "            print()\n",
    "\n",
    "        if mcp.weather:\n",
    "            url = getattr(mcp.weather, 'base_url', 'configured')\n",
    "            print(f\"  4. OpenWeather API (via APIM)\")\n",
    "            print(f\"     URL: {url}\")\n",
    "            print(f\"     Type: APIM-Routed REST API\")\n",
    "            print(f\"     Capabilities: Real-time weather, forecasts\")\n",
    "            print()\n",
    "\n",
    "        if len(available) < 4:\n",
    "            print(\"\u26a0\ufe0f  Some data sources not configured:\")\n",
    "            if not mcp.excel:\n",
    "                print(\"  - Excel MCP: Set EXCEL_MCP_URL\")\n",
    "            if not mcp.docs:\n",
    "                print(\"  - Docs MCP: Set DOCS_MCP_URL\")\n",
    "            if not mcp.github:\n",
    "                print(\"  - GitHub API: Set APIM_GITHUB_URL + APIM_SUBSCRIPTION_KEY\")\n",
    "            if not mcp.weather:\n",
    "                print(\"  - Weather API: Set APIM_WEATHER_URL + OPENWEATHER_API_KEY\")\n",
    "            print()\n",
    "\n",
    "        print(f\"\ud83d\udca1 Configuration loaded from .mcp-servers-config\")\n",
    "        print(f\"   Access via: mcp.excel, mcp.docs, mcp.github, mcp.weather\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to initialize MCP Client: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "## For backward compatibility\n",
    "#MCP_SERVERS = {}\n",
    "#if mcp.excel:\n",
    "#    MCP_SERVERS['excel'] = mcp.excel\n",
    "#if mcp.docs:\n",
    "#    MCP_SERVERS['docs'] = mcp.docs\n",
    "#if mcp.github:\n",
    "#    MCP_SERVERS['github'] = mcp.github\n",
    "#if mcp.weather:\n",
    "#    MCP_SERVERS['weather'] = mcp.weather\n",
    "\n",
    "\n",
    "# (-1.9) Unified AzureOps Wrapper (Enhanced SDK Strategy)\n",
    "\"\"\"High-level Azure operations wrapper consolidating:\n",
    "- CLI resolution & version\n",
    "- Service principal / interactive login fallback\n",
    "- Generic az() invocation (JSON/text)\n",
    "- Resource group ensure (CLI or SDK)\n",
    "- Bicep compile (CLI) + group deployment (CLI or SDK)\n",
    "- AI Foundry model deployments (SDK)\n",
    "- APIM policy fragments + API policy apply (with rollback)\n",
    "- Deployment outputs retrieval & simplification\n",
    "- MCP server health probing\n",
    "\n",
    "Strategy:\n",
    "    AzureOps(strategy='sdk' | 'cli')  # default 'sdk' to favor richer status & long-running handling.\n",
    "\n",
    "Example:\n",
    "    AZ_OPS = AzureOps(strategy='sdk')\n",
    "    AZ_OPS.ensure_login()\n",
    "    AZ_OPS.ensure_resource_group(rg, location)\n",
    "    tpl = AZ_OPS.compile_bicep('deploy-01-core.bicep')\n",
    "    ok, res = AZ_OPS.deploy_group(rg,'core',tpl, params={})\n",
    "    outputs = AZ_OPS.get_deployment_outputs(rg,'core')\n",
    "    AZ_OPS.ensure_policy_fragment(rg, service, 'semanticCacheFragment', xml)\n",
    "    AZ_OPS.apply_api_policy_with_fragments(rg, service, api_id, ['semanticCacheFragment','contentSafetyFragment'])\n",
    "\n",
    "NOTE: Legacy helper cells remain for reference; prefer AzureOps going forward.\n",
    "\"\"\"\n",
    "import os, shutil, subprocess, json, time, shlex, tempfile, sys, socket\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Optional Azure SDK imports (defer errors until used)\n",
    "try:\n",
    "    from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "    from azure.mgmt.resource import ResourceManagementClient\n",
    "    from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "    from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "except Exception as _sdk_err:\n",
    "    _AZURE_SDK_IMPORT_ERROR = _sdk_err\n",
    "else:\n",
    "    _AZURE_SDK_IMPORT_ERROR = None\n",
    "\n",
    "class DeploymentError(Exception):\n",
    "    pass\n",
    "class PolicyError(Exception):\n",
    "    pass\n",
    "class ModelDeploymentError(Exception):\n",
    "    pass\n",
    "\n",
    "class AzureOps:\n",
    "    def __init__(self, strategy: str = 'sdk'):\n",
    "        self.strategy = strategy.lower()\n",
    "        if self.strategy not in {'sdk','cli'}:\n",
    "            self.strategy = 'sdk'\n",
    "        self.az_cli = None\n",
    "        self.version = None\n",
    "        self.subscription_id = os.environ.get('SUBSCRIPTION_ID') or os.environ.get('AZURE_SUBSCRIPTION_ID') or ''\n",
    "        self.credential = None\n",
    "        self.resource_client: Optional[ResourceManagementClient] = None\n",
    "        self.cog_client: Optional[CognitiveServicesManagementClient] = None\n",
    "        self._resolve_cli()\n",
    "        self._init_credentials_if_possible()\n",
    "        self._cache_version()\n",
    "\n",
    "    # ---------- CLI RESOLUTION ----------\n",
    "    def _resolve_cli(self):\n",
    "        override = os.environ.get('AZURE_CLI_PATH')\n",
    "        if override and Path(override).exists():\n",
    "            self.az_cli = override\n",
    "        else:\n",
    "            candidates = []\n",
    "            for name in ['az','az.cmd','az.exe']:\n",
    "                p = shutil.which(name)\n",
    "                if p: candidates.append(p)\n",
    "            candidates += [ '/usr/bin/az','/usr/local/bin/az', str(Path.home()/'.local/bin/az'), str(Path.home()/'.azure-cli/az') ]\n",
    "            existing = [c for c in candidates if c and Path(c).exists()]\n",
    "            if not existing:\n",
    "                venv = Path(os.environ.get('VIRTUAL_ENV','') or sys.prefix)/('Scripts' if os.name=='nt' else 'bin')/'az'\n",
    "                if venv.exists(): existing=[str(venv)]\n",
    "            if existing:\n",
    "                def rank(p):\n",
    "                    pl=p.lower(); penalty=1000 if '.venv' in pl or 'scripts' in pl else 0\n",
    "                    return penalty, len(p)\n",
    "                existing.sort(key=rank)\n",
    "                self.az_cli = existing[0]\n",
    "            else:\n",
    "                self.az_cli = 'az'\n",
    "        os.environ['AZ_CLI'] = self.az_cli\n",
    "\n",
    "    # ---------- GENERIC az() INVOCATION ----------\n",
    "    def _run(self, parts, timeout=30):\n",
    "        try:\n",
    "            return subprocess.run(parts,capture_output=True,text=True,timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            class Dummy: returncode=1; stdout=''; stderr=f'timeout>{timeout}s'\n",
    "            return Dummy()\n",
    "\n",
    "    def az(self, cmd: str, json_out=False, timeout=30, login_retry=True) -> Tuple[bool, str | Dict]:\n",
    "        parts=[self.az_cli]+shlex.split(cmd)\n",
    "        proc=self._run(parts,timeout)\n",
    "        if proc.returncode!=0:\n",
    "            stderr=proc.stderr.strip()\n",
    "            if login_retry and 'az login' in stderr.lower():\n",
    "                if self.ensure_login(silent=True):\n",
    "                    return self.az(cmd,json_out=json_out,timeout=timeout,login_retry=False)\n",
    "            return False, stderr or proc.stdout\n",
    "        out=proc.stdout\n",
    "        if json_out:\n",
    "            try:\n",
    "                return True, json.loads(out or '{}')\n",
    "            except Exception as e:\n",
    "                return False, f'json parse error: {e}\\n{out[:200]}'\n",
    "        return True, out\n",
    "\n",
    "    def _cache_version(self):\n",
    "        ok, ver = self.az('--version', json_out=False, timeout=6, login_retry=False)\n",
    "        if ok:\n",
    "            self.version = ver.splitlines()[0] if ver else ''\n",
    "\n",
    "    # ---------- AUTHENTICATION ----------\n",
    "    def _init_credentials_if_possible(self):\n",
    "        # Service Principal first\n",
    "        sp_keys = ['AZURE_TENANT_ID','AZURE_CLIENT_ID','AZURE_CLIENT_SECRET']\n",
    "        if all(os.environ.get(k) for k in sp_keys):\n",
    "            try:\n",
    "                self.credential = ClientSecretCredential(\n",
    "                    tenant_id=os.environ['AZURE_TENANT_ID'],\n",
    "                    client_id=os.environ['AZURE_CLIENT_ID'],\n",
    "                    client_secret=os.environ['AZURE_CLIENT_SECRET']\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] SP credential init failed:', e)\n",
    "                self.credential=None\n",
    "        if self.credential is None:\n",
    "            try:\n",
    "                self.credential = AzureCliCredential()\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] AzureCliCredential failed (defer login):', e)\n",
    "                self.credential=None\n",
    "        # Resource client if SDK chosen\n",
    "        if self.strategy=='sdk' and self.credential and self.subscription_id:\n",
    "            if _AZURE_SDK_IMPORT_ERROR:\n",
    "                print('[AzureOps] SDK import error; fallback to CLI deployments:', _AZURE_SDK_IMPORT_ERROR)\n",
    "                self.strategy='cli'\n",
    "                return\n",
    "            try:\n",
    "                self.resource_client = ResourceManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] ResourceManagementClient init failed:', e)\n",
    "                self.resource_client=None\n",
    "            try:\n",
    "                self.cog_client = CognitiveServicesManagementClient(self.credential, self.subscription_id)\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] CognitiveServicesManagementClient init failed:', e)\n",
    "                self.cog_client=None\n",
    "\n",
    "    def ensure_login(self, silent=False):\n",
    "        ok,_ = self.az('account show', json_out=True, login_retry=False, timeout=8)\n",
    "        if ok:\n",
    "            acct_id = _.get('id') if isinstance(_,dict) else None\n",
    "            if acct_id and not self.subscription_id:\n",
    "                self.subscription_id = acct_id\n",
    "            return True\n",
    "        # Attempt SP non-interactive if creds exist\n",
    "        sp_ok = all(os.environ.get(k) for k in ['AZURE_CLIENT_ID','AZURE_CLIENT_SECRET','AZURE_TENANT_ID'])\n",
    "        if sp_ok:\n",
    "            sp_cmd=(f\"login --service-principal -u {os.environ['AZURE_CLIENT_ID']} -p {os.environ['AZURE_CLIENT_SECRET']} --tenant {os.environ['AZURE_TENANT_ID']}\")\n",
    "            proc=self._run([self.az_cli]+shlex.split(sp_cmd),timeout=40)\n",
    "            if proc.returncode==0:\n",
    "                if not silent: print('[AzureOps] SP login successful')\n",
    "                return True\n",
    "            else:\n",
    "                if not silent: print('[AzureOps] SP login failed:', proc.stderr[:160])\n",
    "        if not silent:\n",
    "            print('[AzureOps] Interactive login required: run \"az login\" in terminal')\n",
    "        return False\n",
    "\n",
    "    # ---------- RESOURCE GROUP ----------\n",
    "    def ensure_resource_group(self, rg: str, location: str) -> bool:\n",
    "        if self.strategy=='sdk' and self.resource_client:\n",
    "            try:\n",
    "                self.resource_client.resource_groups.create_or_update(rg, {'location': location})\n",
    "                print('[AzureOps] RG ensured (sdk):', rg)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] RG ensure failed (sdk):', e)\n",
    "        # CLI fallback\n",
    "        ok,res=self.az(f'group exists --name {rg}', json_out=False)\n",
    "        exists = ok and res.strip()=='true'\n",
    "        if exists:\n",
    "            print('[AzureOps] RG exists:', rg); return True\n",
    "        ok,_=self.az(f'group create --name {rg} --location {location}', json_out=True, timeout=120)\n",
    "        print('[AzureOps] RG created' if ok else '[AzureOps] RG create failed')\n",
    "        return ok\n",
    "\n",
    "    # ---------- BICEP COMPILE ----------\n",
    "    def compile_bicep(self, path: str) -> str:\n",
    "        b=Path(path); out=b.with_suffix('.json')\n",
    "        ok,res=self.az(f'bicep build --file {shlex.quote(str(b))}', json_out=False)\n",
    "        if not ok or not out.exists():\n",
    "            raise DeploymentError(f'Bicep compile failed: {res}')\n",
    "        print('[AzureOps] compiled', path, '->', out)\n",
    "        return str(out)\n",
    "\n",
    "    # ---------- DEPLOYMENT (CLI OR SDK) ----------\n",
    "    def _deploy_group_cli(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        param_args=[]\n",
    "        for k,v in params.items():\n",
    "            if isinstance(v,(dict,list)):\n",
    "                tmp=Path(tempfile.gettempdir())/f'param_{k}.json'\n",
    "                tmp.write_text(json.dumps({\"value\":v}))\n",
    "                param_args.append(f'{k}=@{tmp}')\n",
    "            else:\n",
    "                param_args.append(f'{k}={json.dumps(v)}')\n",
    "        params_str=' '.join(f'--parameters {p}' for p in param_args)\n",
    "        cmd=(f'deployment group create --resource-group {rg} --name {name} --template-file {template_file} {params_str}')\n",
    "        print('[AzureOps] deploy(cli):', cmd)\n",
    "        ok,res=self.az(cmd,json_out=True,timeout=timeout)\n",
    "        return ok,res\n",
    "\n",
    "    def _deploy_group_sdk(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if not self.resource_client:\n",
    "            print('[AzureOps] SDK resource_client missing; fallback to CLI')\n",
    "            return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "        template = json.loads(Path(template_file).read_text(encoding='utf-8'))\n",
    "        # Convert params to ARM expected {k:{\"value\":v}}\n",
    "        arm_params={k:{'value':v} for k,v in params.items()}\n",
    "        properties={'mode':'Incremental','template':template,'parameters':arm_params}\n",
    "        print('[AzureOps] deploy(sdk):', name)\n",
    "        poller = self.resource_client.deployments.begin_create_or_update(rg,name,{'properties':properties})\n",
    "        start=time.time();\n",
    "        while not poller.done():\n",
    "            time.sleep(30)\n",
    "            elapsed=int(time.time()-start)\n",
    "            if elapsed%120<30:  # periodic status\n",
    "                print(f'  [AzureOps] deploying... {elapsed}s')\n",
    "        result=poller.result()\n",
    "        state=getattr(result.properties,'provisioning_state',None)\n",
    "        ok = state=='Succeeded'\n",
    "        if ok:\n",
    "            print('[AzureOps] deployment succeeded:', name)\n",
    "        else:\n",
    "            print('[AzureOps] deployment state:', state)\n",
    "        return ok, {'properties':{'outputs': getattr(result.properties,'outputs',{})}}\n",
    "\n",
    "    def deploy_group(self, rg: str, name: str, template_file: str, params: dict, timeout=1800):\n",
    "        if self.strategy=='sdk':\n",
    "            return self._deploy_group_sdk(rg,name,template_file,params,timeout)\n",
    "        return self._deploy_group_cli(rg,name,template_file,params,timeout)\n",
    "\n",
    "    def get_deployment_outputs(self, rg: str, name: str) -> Dict[str,str]:\n",
    "        # Attempt CLI first for uniformity\n",
    "        ok,res=self.az(f'deployment group show --resource-group {rg} --name {name}', json_out=True, timeout=60)\n",
    "        if ok and isinstance(res,dict):\n",
    "            outputs=res.get('properties',{}).get('outputs',{})\n",
    "            return {k:v.get('value') for k,v in outputs.items()} if isinstance(outputs,dict) else {}\n",
    "        # SDK fallback if available\n",
    "        if self.resource_client:\n",
    "            try:\n",
    "                dep=self.resource_client.deployments.get(rg,name)\n",
    "                outs=getattr(dep.properties,'outputs',{})\n",
    "                return {k:v.get('value') for k,v in outs.items()} if isinstance(outs,dict) else {}\n",
    "            except Exception as e:\n",
    "                print('[AzureOps] outputs retrieval failed (sdk):', e)\n",
    "        return {}\n",
    "\n",
    "    # ---------- MODEL DEPLOYMENTS (AI Foundry) ----------\n",
    "    def deploy_models_via_sdk(self, rg: str, foundries: List[dict], models_config: Dict[str,List[dict]]):\n",
    "        if not self.cog_client:\n",
    "            print('[AzureOps] Cognitive Services client not initialized; skipping model deployments')\n",
    "            return {'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        existing_accounts={acc.name:acc for acc in self.cog_client.accounts.list_by_resource_group(rg)}\n",
    "        results={'succeeded':[], 'failed':[], 'skipped':[]}\n",
    "        # Ensure accounts\n",
    "        for f in foundries:\n",
    "            name=f['name']; location=f['location']\n",
    "            if name in existing_accounts:\n",
    "                print(f'  [AzureOps] foundry exists: {name}')\n",
    "            else:\n",
    "                print(f'  [AzureOps] creating foundry: {name}')\n",
    "                try:\n",
    "                    account_params=Account(location=location, sku=CogSku(name='S0'), kind='AIServices', properties={'customSubDomainName':name.lower(),'publicNetworkAccess':'Enabled','allowProjectManagement':True}, identity={'type':'SystemAssigned'})\n",
    "                    poll=self.cog_client.accounts.begin_create(rg,name,account_params)\n",
    "                    poll.result(timeout=600)\n",
    "                    print(f'    [AzureOps] created {name}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [AzureOps] create failed {name}: {e}'); continue\n",
    "        # Deploy models\n",
    "        for f in foundries:\n",
    "            name=f['name']; short=name.split('-')[0]\n",
    "            models=models_config.get(short,[])\n",
    "            print(f'  [AzureOps] models for {name}: {len(models)}')\n",
    "            for m in models:\n",
    "                mname=m['name']\n",
    "                try:\n",
    "                    # Exists check\n",
    "                    try:\n",
    "                        existing=self.cog_client.deployments.get(rg,name,mname)\n",
    "                        if existing.properties.provisioning_state=='Succeeded':\n",
    "                            print(f'    [skip] {mname} already')\n",
    "                            results['skipped'].append(f'{short}/{mname}')\n",
    "                            continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    dep_params=Deployment(sku=CogSku(name=m['sku'],capacity=m['capacity']), properties=DeploymentProperties(model=DeploymentModel(format=m['format'],name=m['name'],version=m['version'])))\n",
    "                    poll=self.cog_client.deployments.begin_create_or_update(rg,name,mname,dep_params)\n",
    "                    poll.result(timeout=900)\n",
    "                    print(f'    [ok] {mname}')\n",
    "                    results['succeeded'].append(f'{short}/{mname}')\n",
    "                except Exception as e:\n",
    "                    print(f'    [fail] {mname}: {e}')\n",
    "                    results['failed'].append({'model':f'{short}/{mname}','error':str(e)})\n",
    "        return results\n",
    "\n",
    "    # ---------- POLICY FRAGMENTS & API POLICY ----------\n",
    "    def ensure_policy_fragment(self, rg: str, service: str, fragment_name: str, xml_policy: str):\n",
    "        body={\"properties\":{\"format\":\"xml\",\"value\":xml_policy.strip()}}\n",
    "        url=(f'https://management.azure.com/subscriptions/{self.subscription_id}/resourceGroups/{rg}/providers/Microsoft.ApiManagement/service/{service}/policyFragments/{fragment_name}?api-version=2023-03-01-preview')\n",
    "        body_json=json.dumps(body)\n",
    "        ok,res=self.az(f\"rest --method put --url {shlex.quote(url)} --body {shlex.quote(body_json)} --headers Content-Type=application/json\", json_out=True, timeout=120)\n",
    "        if ok:\n",
    "            print(f'[AzureOps] fragment ensured: {fragment_name}')\n",
    "        else:\n",
    "            print(f'[AzureOps] fragment failed {fragment_name}: {str(res)[:160]}')\n",
    "        return ok\n",
    "\n",
    "    def backup_api_policy(self, rg: str, service: str, api_id: str):\n",
    "        ok,res=self.az(f'apim api policy show --resource-group {rg} --service-name {service} --api-id {api_id}', json_out=False, timeout=60)\n",
    "        if not ok:\n",
    "            print('[AzureOps] no existing policy (show failed)'); return None\n",
    "        backup_dir=Path('.apim-policy-backups'); backup_dir.mkdir(exist_ok=True)\n",
    "        ts=time.strftime('%Y%m%d-%H%M%S')\n",
    "        file=backup_dir/f'{api_id}-{ts}.xml'\n",
    "        file.write_text(res)\n",
    "        print('[AzureOps] policy backed up:', file)\n",
    "        return str(file)\n",
    "\n",
    "    def apply_api_policy_with_fragments(self, rg: str, service: str, api_id: str, fragments: List[str], extra_inbound: str=''):\n",
    "        self.backup_api_policy(rg,service,api_id)\n",
    "        fragment_tags='\\n'.join(f'        <fragment ref=\"{f}\" />' for f in fragments)\n",
    "        inbound = f\"<inbound>\\n        <base />\\n{fragment_tags}\\n{extra_inbound}\\n    </inbound>\".rstrip()\n",
    "        policy_xml=f\"<policies>\\n{inbound}\\n    <backend><base /></backend>\\n    <outbound><base /></outbound>\\n    <on-error><base /></on-error>\\n</policies>\"\n",
    "        tmp=Path(tempfile.gettempdir())/f'apim-{api_id}-policy.xml'\n",
    "        tmp.write_text(policy_xml)\n",
    "        ok,res=self.az(f'apim api policy create --resource-group {rg} --service-name {service} --api-id {api_id} --xml-path {tmp}', json_out=False, timeout=180)\n",
    "        if not ok:\n",
    "            raise PolicyError(f'Policy apply failed: {res}')\n",
    "        print('[AzureOps] API policy applied with fragments:', fragments)\n",
    "        return True\n",
    "\n",
    "    # ---------- MCP HEALTH ----------\n",
    "    def mcp_health(self, servers: Dict[str,object]) -> Dict[str,Dict[str,str]]:\n",
    "        summary={}\n",
    "        for name,client in servers.items():\n",
    "            url=getattr(client,'server_url',None) or getattr(client,'url',None) or ''\n",
    "            status='unknown'; latency_ms='-'\n",
    "            if url.startswith('http'):  # basic TCP connect\n",
    "                try:\n",
    "                    host=url.split('//',1)[1].split('/',1)[0].split(':')[0]\n",
    "                    port=443 if url.startswith('https') else (int(url.split(':')[2].split('/')[0]) if ':' in url[8:] else 80)\n",
    "                    s=socket.socket(); s.settimeout(3); start=time.time(); s.connect((host,port)); s.close(); latency_ms=int((time.time()-start)*1000); status='ok'\n",
    "                except Exception:\n",
    "                    status='unreachable'\n",
    "            summary[name]={'url':url,'status':status,'latency_ms':latency_ms}\n",
    "        return summary\n",
    "\n",
    "# Instantiate global wrapper (prefer sdk)\n",
    "AZ_OPS = AzureOps(strategy=os.environ.get('AZ_OPS_STRATEGY','sdk'))\n",
    "print('[AzureOps] CLI:', AZ_OPS.az_cli)\n",
    "az_ok = AZ_OPS.ensure_login(silent=True)\n",
    "print('[AzureOps] login status:', 'OK' if az_ok else 'AUTH REQUIRED')\n",
    "if AZ_OPS.version: print('[AzureOps] version:', AZ_OPS.version)\n",
    "print('[AzureOps] strategy:', AZ_OPS.strategy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- MERGED WITH SECTION 0 - Deployment continues in Section 0 above -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-config\"></a>\n",
    "\n",
    "## 0.6 Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell_26_13f05f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Configuration set\n",
      "  Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "  Resource Group: rg-master-lab-pavavy6pu5hpa\n",
      "  Location: uksouth\n",
      "  Deployment Prefix: master-lab\n",
      "  Resource Suffix: pavavy6pu5hpa\n"
     ]
    }
   ],
   "source": [
    "# Master Lab Configuration\n",
    "import random\n",
    "import string\n",
    "\n",
    "# IMPORTANT: Set your Azure subscription ID\n",
    "# Get this from: Azure Portal > Subscriptions > Copy Subscription ID\n",
    "subscription_id = bootstrap.subscription_id\n",
    "\n",
    "# Generate or use existing suffix\n",
    "if hasattr(bootstrap, 'deploy_suffix') and bootstrap.deploy_suffix:\n",
    "    foundry_suffix = bootstrap.deploy_suffix\n",
    "else:\n",
    "    # Auto-generate random suffix for unique resource names\n",
    "    foundry_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=13))\n",
    "    print(f'[*] Auto-generated suffix: {foundry_suffix}')\n",
    "\n",
    "deployment_name_prefix = 'master-lab'\n",
    "# Resource group uses format: rg-master-lab-{suffix}\n",
    "resource_group_name = f'rg-master-lab-{foundry_suffix}'\n",
    "location = 'uksouth'\n",
    "\n",
    "# Deployment names for each step\n",
    "deployment_step1 = f'{deployment_name_prefix}-01-core'\n",
    "deployment_step2 = f'{deployment_name_prefix}-02-ai-foundry'\n",
    "deployment_step2c = f'{deployment_name_prefix}-02c-apim-api'\n",
    "deployment_step3 = f'{deployment_name_prefix}-03-supporting'\n",
    "deployment_step4 = f'{deployment_name_prefix}-04-mcp'\n",
    "\n",
    "print('[OK] Configuration set')\n",
    "print(f'  Subscription ID: {subscription_id}')\n",
    "print(f'  Resource Group: {resource_group_name}')\n",
    "print(f'  Location: {location}')\n",
    "print(f'  Deployment Prefix: {deployment_name_prefix}')\n",
    "print(f'  Resource Suffix: {foundry_suffix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-infra\"></a>\n",
    "\n",
    "## 0.7 Deploy Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell_28_1ecfb480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing Azure authentication...\n",
      "\n",
      "[*] Found .azure-credentials.env, using Service Principal authentication\n",
      "[OK] Service Principal credentials loaded\n",
      "\n",
      "[OK] Using Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[*] Creating Azure Resource Management client...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Azure SDK initialized and connection verified\n",
      "\n",
      "[OK] Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "\n",
    "print('[*] Initializing Azure authentication...')\n",
    "print()\n",
    "\n",
    "# Try to load Service Principal credentials from .azure-credentials.env\n",
    "credentials_file = '.azure-credentials.env'\n",
    "credential = None\n",
    "\n",
    "if os.path.exists(credentials_file):\n",
    "    print(f'[*] Found {credentials_file}, using Service Principal authentication')\n",
    "    load_dotenv(credentials_file)\n",
    "\n",
    "    tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "    if tenant_id and client_id and client_secret:\n",
    "        try:\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id,\n",
    "                client_id=client_id,\n",
    "                client_secret=client_secret\n",
    "            )\n",
    "            print('[OK] Service Principal credentials loaded')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to create Service Principal credential: {e}')\n",
    "            credential = None\n",
    "    else:\n",
    "        print('[ERROR] Missing credentials in .azure-credentials.env')\n",
    "        print('[INFO] Required: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET')\n",
    "else:\n",
    "    print(f'[*] {credentials_file} not found')\n",
    "    print('[INFO] Run: create_service_principal.ps1 to create Service Principal')\n",
    "\n",
    "# Fallback to Azure CLI credential if Service Principal not available\n",
    "if credential is None:\n",
    "    print('[*] Falling back to Azure CLI authentication...')\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        print('[OK] Using Azure CLI credentials')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Azure CLI authentication failed: {e}')\n",
    "        print()\n",
    "        print('[ERROR] Authentication failed. Options:')\n",
    "        print('  1. Create Service Principal: run create_service_principal.ps1')\n",
    "        print('  2. Clear Azure CLI cache and re-login:')\n",
    "        print('     - Delete: %USERPROFILE%\\\\.azure')\n",
    "        print('     - Run: az login')\n",
    "        raise Exception('Authentication failed')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify subscription ID from config\n",
    "if not subscription_id or len(subscription_id) < 10:\n",
    "    raise Exception('Please set your subscription_id in Cell 11')\n",
    "\n",
    "print(f'[OK] Using Subscription ID: {subscription_id}')\n",
    "\n",
    "# Create Resource Management Client\n",
    "print('[*] Creating Azure Resource Management client...')\n",
    "try:\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "    # Test connection by listing resource groups\n",
    "    list(resource_client.resource_groups.list())\n",
    "    print('[OK] Azure SDK initialized and connection verified')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to initialize Resource Management client: {e}')\n",
    "    print()\n",
    "    print('[INFO] If you see MSAL or cache errors, try clearing Azure CLI cache:')\n",
    "    print('       rd /s /q \"%USERPROFILE%\\\\.azure\"')\n",
    "    print('       az login')\n",
    "    raise e\n",
    "\n",
    "print()\n",
    "\n",
    "def compile_bicep(bicep_file):\n",
    "    \"\"\"Compile Bicep to JSON - SIMPLIFIED: Just use existing JSON files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path if string\n",
    "    bicep_path = Path(bicep_file)\n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    print(f'[*] Looking for template: {json_path.name}...')\n",
    "    \n",
    "    # Check if JSON exists\n",
    "    if json_path.exists():\n",
    "        print(f'[OK] Found existing template: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    \n",
    "    # JSON doesn't exist - this is an error since we don't want to compile\n",
    "    print(f'[ERROR] Template not found: {json_path}')\n",
    "    print(f'[INFO] Expected location: {json_path.absolute()}')\n",
    "    print(f'[INFO] Please ensure Bicep templates are pre-compiled')\n",
    "    print(f'[INFO] Or set BICEP_DIR to the correct scripts directory')\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_resource_group_exists(rg_name):\n",
    "    \"\"\"Check if resource group exists\"\"\"\n",
    "    try:\n",
    "        resource_client.resource_groups.get(rg_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_deployment_exists(rg_name, deployment_name):\n",
    "    \"\"\"Check if deployment exists and succeeded - Fixed to return boolean only\"\"\"\n",
    "    try:\n",
    "        deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "        return deployment.properties.provisioning_state == 'Succeeded'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_template(rg_name, deployment_name, template_file, parameters_dict):\n",
    "    \"\"\"Deploy ARM template using Azure SDK\"\"\"\n",
    "    print(f'[*] Deploying {deployment_name}...')\n",
    "\n",
    "    # Read template\n",
    "    with open(template_file, 'r', encoding='utf-8') as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # Prepare deployment properties\n",
    "    deployment_properties = {\n",
    "        'mode': 'Incremental',\n",
    "        'template': template,\n",
    "        'parameters': parameters_dict\n",
    "    }\n",
    "\n",
    "    # Start deployment\n",
    "    print('[*] Starting deployment...')\n",
    "    deployment_async = resource_client.deployments.begin_create_or_update(\n",
    "        rg_name,\n",
    "        deployment_name,\n",
    "        {'properties': deployment_properties}\n",
    "    )\n",
    "\n",
    "    # Poll deployment status\n",
    "    print('[*] Deployment in progress. Polling status...')\n",
    "    start_time = time.time()\n",
    "    last_update = start_time\n",
    "\n",
    "    while not deployment_async.done():\n",
    "        time.sleep(30)\n",
    "        elapsed = time.time() - start_time\n",
    "        if time.time() - last_update >= 60:\n",
    "            mins = int(elapsed / 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f'[*] Still deploying... {mins}m {secs}s elapsed')\n",
    "            last_update = time.time()\n",
    "\n",
    "    # Get result\n",
    "    deployment_result = deployment_async.result()\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    if deployment_result.properties.provisioning_state == 'Succeeded':\n",
    "        print(f'[OK] Deployment succeeded in {mins}m {secs}s')\n",
    "        return True, deployment_result\n",
    "    else:\n",
    "        print(f'[ERROR] Deployment failed: {deployment_result.properties.provisioning_state}')\n",
    "        if deployment_result.properties.error:\n",
    "            print(f'[ERROR] Error: {deployment_result.properties.error.message}')\n",
    "        return False, deployment_result\n",
    "\n",
    "def get_deployment_outputs(rg_name, deployment_name):\n",
    "    \"\"\"Get deployment outputs\"\"\"\n",
    "    deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "    if deployment.properties.outputs:\n",
    "        return {k: v['value'] for k, v in deployment.properties.outputs.items()}\n",
    "    return {}\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_29_a7330fb3",
   "metadata": {},
   "source": [
    "### Main Deployment - All 4 Steps\n",
    "\n",
    "Deploys all infrastructure in sequence:\n",
    "1. Core (APIM, Log Analytics, App Insights) - ~10 min\n",
    "2. AI Foundry (3 hubs + 14 models) - ~15 min\n",
    "3. Supporting Services (Redis, Search, Cosmos, Content Safety) - ~10 min\n",
    "4. MCP Servers (Container Apps + 7 servers) - ~5 min\n",
    "\n",
    "**Total time: ~40 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell_28_1ecfb480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initializing Azure authentication...\n",
      "\n",
      "[*] Found .azure-credentials.env, using Service Principal authentication\n",
      "[OK] Service Principal credentials loaded\n",
      "\n",
      "[OK] Using Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[*] Creating Azure Resource Management client...\n",
      "[OK] Azure SDK initialized and connection verified\n",
      "\n",
      "[OK] Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import ClientSecretCredential, AzureCliCredential\n",
    "\n",
    "print('[*] Initializing Azure authentication...')\n",
    "print()\n",
    "\n",
    "# Try to load Service Principal credentials from .azure-credentials.env\n",
    "credentials_file = '.azure-credentials.env'\n",
    "credential = None\n",
    "\n",
    "if os.path.exists(credentials_file):\n",
    "    print(f'[*] Found {credentials_file}, using Service Principal authentication')\n",
    "    load_dotenv(credentials_file)\n",
    "\n",
    "    tenant_id = os.getenv('AZURE_TENANT_ID')\n",
    "    client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "    client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "    if tenant_id and client_id and client_secret:\n",
    "        try:\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id,\n",
    "                client_id=client_id,\n",
    "                client_secret=client_secret\n",
    "            )\n",
    "            print('[OK] Service Principal credentials loaded')\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to create Service Principal credential: {e}')\n",
    "            credential = None\n",
    "    else:\n",
    "        print('[ERROR] Missing credentials in .azure-credentials.env')\n",
    "        print('[INFO] Required: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET')\n",
    "else:\n",
    "    print(f'[*] {credentials_file} not found')\n",
    "    print('[INFO] Run: create_service_principal.ps1 to create Service Principal')\n",
    "\n",
    "# Fallback to Azure CLI credential if Service Principal not available\n",
    "if credential is None:\n",
    "    print('[*] Falling back to Azure CLI authentication...')\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        print('[OK] Using Azure CLI credentials')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Azure CLI authentication failed: {e}')\n",
    "        print()\n",
    "        print('[ERROR] Authentication failed. Options:')\n",
    "        print('  1. Create Service Principal: run create_service_principal.ps1')\n",
    "        print('  2. Clear Azure CLI cache and re-login:')\n",
    "        print('     - Delete: %USERPROFILE%\\\\.azure')\n",
    "        print('     - Run: az login')\n",
    "        raise Exception('Authentication failed')\n",
    "\n",
    "print()\n",
    "\n",
    "# Verify subscription ID from config\n",
    "if not subscription_id or len(subscription_id) < 10:\n",
    "    raise Exception('Please set your subscription_id in Cell 11')\n",
    "\n",
    "print(f'[OK] Using Subscription ID: {subscription_id}')\n",
    "\n",
    "# Create Resource Management Client\n",
    "print('[*] Creating Azure Resource Management client...')\n",
    "try:\n",
    "    resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "    # Test connection by listing resource groups\n",
    "    list(resource_client.resource_groups.list())\n",
    "    print('[OK] Azure SDK initialized and connection verified')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to initialize Resource Management client: {e}')\n",
    "    print()\n",
    "    print('[INFO] If you see MSAL or cache errors, try clearing Azure CLI cache:')\n",
    "    print('       rd /s /q \"%USERPROFILE%\\\\.azure\"')\n",
    "    print('       az login')\n",
    "    raise e\n",
    "\n",
    "print()\n",
    "\n",
    "def compile_bicep(bicep_file):\n",
    "    \"\"\"Compile Bicep to JSON - SIMPLIFIED: Just use existing JSON files\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Convert to Path if string\n",
    "    bicep_path = Path(bicep_file)\n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    print(f'[*] Looking for template: {json_path.name}...')\n",
    "    \n",
    "    # Check if JSON exists\n",
    "    if json_path.exists():\n",
    "        print(f'[OK] Found existing template: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    \n",
    "    # JSON doesn't exist - this is an error since we don't want to compile\n",
    "    print(f'[ERROR] Template not found: {json_path}')\n",
    "    print(f'[INFO] Expected location: {json_path.absolute()}')\n",
    "    print(f'[INFO] Please ensure Bicep templates are pre-compiled')\n",
    "    print(f'[INFO] Or set BICEP_DIR to the correct scripts directory')\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_resource_group_exists(rg_name):\n",
    "    \"\"\"Check if resource group exists\"\"\"\n",
    "    try:\n",
    "        resource_client.resource_groups.get(rg_name)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_deployment_exists(rg_name, deployment_name):\n",
    "    \"\"\"Check if deployment exists and succeeded - Fixed to return boolean only\"\"\"\n",
    "    try:\n",
    "        deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "        return deployment.properties.provisioning_state == 'Succeeded'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def deploy_template(rg_name, deployment_name, template_file, parameters_dict):\n",
    "    \"\"\"Deploy ARM template using Azure SDK\"\"\"\n",
    "    print(f'[*] Deploying {deployment_name}...')\n",
    "\n",
    "    # Read template\n",
    "    with open(template_file, 'r', encoding='utf-8') as f:\n",
    "        template = json.load(f)\n",
    "\n",
    "    # Prepare deployment properties\n",
    "    deployment_properties = {\n",
    "        'mode': 'Incremental',\n",
    "        'template': template,\n",
    "        'parameters': parameters_dict\n",
    "    }\n",
    "\n",
    "    # Start deployment\n",
    "    print('[*] Starting deployment...')\n",
    "    deployment_async = resource_client.deployments.begin_create_or_update(\n",
    "        rg_name,\n",
    "        deployment_name,\n",
    "        {'properties': deployment_properties}\n",
    "    )\n",
    "\n",
    "    # Poll deployment status\n",
    "    print('[*] Deployment in progress. Polling status...')\n",
    "    start_time = time.time()\n",
    "    last_update = start_time\n",
    "\n",
    "    while not deployment_async.done():\n",
    "        time.sleep(30)\n",
    "        elapsed = time.time() - start_time\n",
    "        if time.time() - last_update >= 60:\n",
    "            mins = int(elapsed / 60)\n",
    "            secs = int(elapsed % 60)\n",
    "            print(f'[*] Still deploying... {mins}m {secs}s elapsed')\n",
    "            last_update = time.time()\n",
    "\n",
    "    # Get result\n",
    "    deployment_result = deployment_async.result()\n",
    "    elapsed = time.time() - start_time\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed % 60)\n",
    "\n",
    "    if deployment_result.properties.provisioning_state == 'Succeeded':\n",
    "        print(f'[OK] Deployment succeeded in {mins}m {secs}s')\n",
    "        return True, deployment_result\n",
    "    else:\n",
    "        print(f'[ERROR] Deployment failed: {deployment_result.properties.provisioning_state}')\n",
    "        if deployment_result.properties.error:\n",
    "            print(f'[ERROR] Error: {deployment_result.properties.error.message}')\n",
    "        return False, deployment_result\n",
    "\n",
    "def get_deployment_outputs(rg_name, deployment_name):\n",
    "    \"\"\"Get deployment outputs\"\"\"\n",
    "    deployment = resource_client.deployments.get(rg_name, deployment_name)\n",
    "    if deployment.properties.outputs:\n",
    "        return {k: v['value'] for k, v in deployment.properties.outputs.items()}\n",
    "    return {}\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Deployment Paths\n",
    "\n",
    "\n",
    "**Purpose**: Sets up file system paths for Bicep template deployment\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- `NOTEBOOK_DIR` variable must be set (from Cell 5 - Bootstrap Configuration)\n",
    "- Bicep template files must exist in the `deploy/` subdirectory\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "1. Determines the absolute path to the `deploy/` directory containing Bicep templates\n",
    "2. Uses the `NOTEBOOK_DIR` variable established during bootstrap\n",
    "3. Falls back to current working directory if bootstrap wasn't run\n",
    "4. Prints separator for visual clarity in deployment logs\n",
    "\n",
    "This path configuration is used by subsequent deployment cells to locate:\n",
    "- Core infrastructure Bicep files (APIM, Log Analytics)\n",
    "- AI Foundry Bicep files (AI hubs and models)\n",
    "- Supporting services Bicep files (Redis, Cosmos DB, AI Search)\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "A separator line of '=' characters will be printed.\n",
    "The `BICEP_DIR` variable will be set to the absolute path of the deploy folder.\n",
    "No errors should occur if bootstrap cell was run successfully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell_30_1a0a2a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)\n",
      "======================================================================\n",
      "\n",
      "[*] Step 0: Ensuring resource group exists...\n",
      "[OK] Resource group already exists\n",
      "[*] Step 0b: Assigning RBAC roles to resource group...\n",
      "[rbac] Assigning Resource Group roles to 15495795...\n",
      "  [OK] Cognitive Services OpenAI User\n",
      "  [OK] Cognitive Services Contributor\n",
      "  [OK] Search Index Data Contributor\n",
      "  [OK] DocumentDB Account Contributor\n",
      "\n",
      "======================================================================\n",
      "STEP 1: CORE INFRASTRUCTURE\n",
      "======================================================================\n",
      "[*] Resources: Log Analytics, App Insights, API Management\n",
      "[*] Estimated time: ~10 minutes\n",
      "\n",
      "[OK] Step 1 already deployed. Skipping...\n",
      "\n",
      "[OK] Step 1 outputs retrieved from deployment\n",
      "  - APIM Gateway: https://apim-c7uj6vzppah74.azure-api.net\n",
      "  - Log Analytics: /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resource...\n",
      "\n",
      "======================================================================\n",
      "STEP 2: AI FOUNDRY (RESILIENT DEPLOYMENT)\n",
      "======================================================================\n",
      "[*] Resources: 3 Foundry hubs, 3 projects, AI models\n",
      "[*] Estimated time: ~15 minutes\n",
      "\n",
      "[*] Using resource suffix: c7uj6vzppah74\n",
      "[*] Phase 2a: AI Foundry Hubs\n",
      "  [OK] foundry1-c7uj6vzppah74 already exists\n",
      "  [OK] foundry2-c7uj6vzppah74 already exists\n",
      "  [OK] foundry3-c7uj6vzppah74 already exists\n",
      "\n",
      "[*] Phase 2b: AI Models (Resilient)\n",
      "  [*] foundry1-c7uj6vzppah74: 6 models\n",
      "    [OK] gpt-4o-mini already deployed\n",
      "    [*] Deploying gpt-4o...\n",
      "    [SKIP] gpt-4o failed: (InsufficientQuota) This operation require 100 new capacity in quota Tokens Per \n",
      "    [OK] text-embedding-3-small already deployed\n",
      "    [OK] text-embedding-3-large already deployed\n",
      "    [*] Deploying dall-e-3...\n",
      "    [SKIP] dall-e-3 failed: (InvalidResourceProperties) The specified SKU 'Standard' for model 'dall-e-3 3.0\n",
      "    [OK] gpt-4.1-nano already deployed\n",
      "  [*] foundry2-c7uj6vzppah74: 2 models\n",
      "    [OK] gpt-4o-mini already deployed\n",
      "    [*] Deploying gpt-4.1-nano...\n",
      "    [SKIP] gpt-4.1-nano failed: (SpecialFeatureOrQuotaIdRequired) The current subscription does not have feature\n",
      "  [*] foundry3-c7uj6vzppah74: 2 models\n",
      "    [OK] gpt-4o-mini already deployed\n",
      "    [OK] gpt-4.1-nano already deployed\n",
      "\n",
      "[OK] Models: 0 deployed, 7 skipped, 3 failed\n",
      "\n",
      "[*] Collecting foundry deployment outputs for env file...\n",
      "  [OK] Captured foundry1-c7uj6vzppah74: 6 models\n",
      "  [OK] Captured foundry2-c7uj6vzppah74: 2 models\n",
      "  [OK] Captured foundry3-c7uj6vzppah74: 2 models\n",
      "[OK] Captured 3 foundry outputs\n",
      "\n",
      "[*] Phase 2c: APIM Inference API\n",
      "[OK] APIM API already configured. Skipping...\n",
      "[OK] Step 2 complete\n",
      "\n",
      "======================================================================\n",
      "STEP 3: SUPPORTING SERVICES\n",
      "======================================================================\n",
      "[*] Resources: Redis, Search, Cosmos DB, Content Safety\n",
      "\n",
      "[*] Getting principal ID for Cosmos DB RBAC...\n",
      "[OK] Using signed-in user: 15495795...\n",
      "[OK] Step 3 already deployed. Skipping...\n",
      "\n",
      "[OK] Step 3 outputs retrieved\n",
      "======================================================================\n",
      "STEP 4: MCP SERVERS\n",
      "======================================================================\n",
      "[*] Resources: Container Registry + Container Apps + 7 MCP servers\n",
      "[*] Estimated time: ~8 minutes\n",
      "\n",
      "[*] Step 4a: Setting up Container Registry for MCP images...\n",
      "[OK] ACR already exists: acrc7uj6vzppah74.azurecr.io\n",
      "[*] Retrieving ACR admin credentials...\n",
      "[OK] ACR credentials retrieved (username: acrc7uj6vzppah74)\n",
      "\n",
      "[*] Step 4b: Checking MCP images in ACR...\n",
      "  [WARN] Could not check/import excel-analytics-mcp:v4: 'RegistryUsageListResult' object is not iterable\n",
      "  [WARN] Could not check/import research-docs-mcp:v2: 'RegistryUsageListResult' object is not iterable\n",
      "\n",
      "[*] Step 4c: Deploying MCP infrastructure...\n",
      "[OK] Step 4 already deployed. Skipping...\n",
      "\n",
      "[OK] Step 4 outputs retrieved\n",
      "\n",
      "======================================================================\n",
      "DEPLOYMENT COMPLETE\n",
      "======================================================================\n",
      "[OK] Total time: 0m 36s\n",
      "\n",
      "[OK] All 4 steps deployed successfully!\n",
      "[OK] Next: Run Cell 18-19 to generate master-lab.env\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=' * 70)\n",
    "# Load BICEP_DIR (set by Cell 3)\n",
    "# Use absolute path for BICEP_DIR\n",
    "if \"NOTEBOOK_DIR\" in globals():\n",
    "    BICEP_DIR = NOTEBOOK_DIR / \"deploy\"\n",
    "else:\n",
    "    # Auto-detect notebook directory from common locations\n",
    "    for candidate in [\n",
    "        Path.cwd() / \"deploy\",\n",
    "        Path.cwd() / \"AI-Gateway\" / \"labs\" / \"master-lab\" / \"deploy\",\n",
    "        Path(\"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/deploy\"),\n",
    "    ]:\n",
    "        if candidate.exists():\n",
    "            BICEP_DIR = candidate\n",
    "            break\n",
    "    else:\n",
    "        BICEP_DIR = Path(\"deploy\")  # Last resort\n",
    "if not BICEP_DIR.exists():\n",
    "    print(f\"[deploy] \u26a0\ufe0f  BICEP_DIR not found: {BICEP_DIR}\")\n",
    "    raise Exception(f\"Deploy directory not found. Run from master-lab folder or set NOTEBOOK_DIR.\")\n",
    "\n",
    "print('MASTER LAB DEPLOYMENT - 4 STEPS (RESILIENT)')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "# Ensure resource group exists\n",
    "print('[*] Step 0: Ensuring resource group exists...')\n",
    "if not check_resource_group_exists(resource_group_name):\n",
    "    print(f'[*] Creating resource group: {resource_group_name}')\n",
    "    resource_client.resource_groups.create_or_update(\n",
    "        resource_group_name,\n",
    "        {'location': location}\n",
    "    )\n",
    "    print('[OK] Resource group created')\n",
    "else:\n",
    "    print('[OK] Resource group already exists')\n",
    "\n",
    "# Assign RBAC roles to resource group for notebook operations\n",
    "print('[*] Step 0b: Assigning RBAC roles to resource group...')\n",
    "if 'assign_rg_roles' in dir():\n",
    "    assign_rg_roles(resource_group_name)\n",
    "else:\n",
    "    print('[WARN] assign_rg_roles not defined - run cell 9 first')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: CORE INFRASTRUCTURE (Bicep - as before)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 1: CORE INFRASTRUCTURE')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Log Analytics, App Insights, API Management')\n",
    "print('[*] Estimated time: ~10 minutes')\n",
    "print()\n",
    "\n",
    "deployment_step1 = 'master-lab-01-core'\n",
    "\n",
    "# Define compile_bicep_safe function (used by all steps)\n",
    "def compile_bicep_safe(bicep_path: Path):\n",
    "    \"\"\"Compile Bicep to ARM JSON template\"\"\"\n",
    "    import subprocess\n",
    "    if not bicep_path.exists():\n",
    "        print(f'[ERROR] Bicep file not found: {bicep_path}')\n",
    "        return None\n",
    "    \n",
    "    json_path = bicep_path.with_suffix('.json')\n",
    "    \n",
    "    # Check if JSON already exists and is newer than bicep\n",
    "    if json_path.exists():\n",
    "        bicep_mtime = bicep_path.stat().st_mtime\n",
    "        json_mtime = json_path.stat().st_mtime\n",
    "        if json_mtime >= bicep_mtime:\n",
    "            print(f'[OK] Using cached template: {json_path.name}')\n",
    "            return str(json_path)\n",
    "    \n",
    "    # Compile bicep to JSON\n",
    "    print(f'[*] Compiling {bicep_path.name}...')\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['az', 'bicep', 'build', '--file', str(bicep_path), '--outfile', str(json_path)],\n",
    "            capture_output=True, text=True, timeout=120\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(f'[ERROR] Bicep compilation failed: {result.stderr[:200]}')\n",
    "            return None\n",
    "        print(f'[OK] Compiled: {json_path.name}')\n",
    "        return str(json_path)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f'[ERROR] Bicep compilation timed out')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Compilation error: {str(e)[:100]}')\n",
    "        return None\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step1):\n",
    "    print('[OK] Step 1 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 1 not found. Deploying...')\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-01-core.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 1')\n",
    "\n",
    "    # Load parameters (optional - bicep has defaults)\n",
    "    params_file = BICEP_DIR / 'params-01-core.json'\n",
    "    if params_file.exists():\n",
    "        with open(params_file) as f:\n",
    "            params = json.load(f)\n",
    "        params_dict = params.get('parameters', {})\n",
    "    else:\n",
    "        # Use defaults from bicep\n",
    "        params_dict = {'apimSku': {'value': 'Standardv2'}}\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step1, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 1 deployment failed')\n",
    "\n",
    "    print('[OK] Step 1 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "# Get Step 1 outputs (with fallback to saved file)\n",
    "step1_outputs = None\n",
    "try:\n",
    "    step1_outputs = get_deployment_outputs(resource_group_name, deployment_step1)\n",
    "    print('[OK] Step 1 outputs retrieved from deployment')\n",
    "except Exception as e:\n",
    "    print(f'[WARN] Failed to retrieve Step 1 outputs from deployment: {str(e)}')\n",
    "    # Try loading from saved file\n",
    "    step1_output_file = BICEP_DIR / 'step1-outputs.json'\n",
    "    if step1_output_file.exists():\n",
    "        try:\n",
    "            with open(step1_output_file) as f:\n",
    "                step1_outputs = json.load(f)\n",
    "            print(f'[OK] Step 1 outputs loaded from {step1_output_file.name}')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Failed to load from file: {str(e2)}')\n",
    "    \n",
    "if not step1_outputs:\n",
    "    print('[ERROR] Cannot proceed without Step 1 outputs')\n",
    "    print('[INFO] Please ensure Step 1 deployment completed or step1-outputs.json exists')\n",
    "    raise Exception('Cannot proceed without Step 1 outputs')\n",
    "\n",
    "print(f\"  - APIM Gateway: {step1_outputs.get('apimGatewayUrl', 'N/A')}\")\n",
    "print(f\"  - Log Analytics: {step1_outputs.get('logAnalyticsWorkspaceId', 'N/A')[:60]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: AI FOUNDRY (RESILIENT PYTHON APPROACH)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 2: AI FOUNDRY (RESILIENT DEPLOYMENT)')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: 3 Foundry hubs, 3 projects, AI models')\n",
    "print('[*] Estimated time: ~15 minutes')\n",
    "print()\n",
    "\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "from azure.mgmt.cognitiveservices.models import Account, Sku as CogSku, Deployment, DeploymentModel, DeploymentProperties\n",
    "\n",
    "cog_client = CognitiveServicesManagementClient(credential, subscription_id)\n",
    "\n",
    "# Configuration\n",
    "# Extract suffix from Step 1 outputs (APIM uses uniqueString which differs from RG suffix)\n",
    "# APIM name format: apim-{uniqueString} e.g., apim-c7uj6vzppah74\n",
    "apim_name = step1_outputs.get('apimServiceName', '')\n",
    "if apim_name.startswith('apim-'):\n",
    "    resource_suffix = apim_name.split('-', 1)[1]  # Get everything after 'apim-'\n",
    "else:\n",
    "    # Fallback to resource group suffix\n",
    "    resource_suffix = resource_group_name.split('-')[-1] if resource_group_name.startswith('rg-master-lab-') else resource_group_name[-13:]\n",
    "print(f'[*] Using resource suffix: {resource_suffix}')\n",
    "foundries = [\n",
    "    {'name': f'foundry1-{resource_suffix}', 'location': 'uksouth', 'project': 'master-lab-foundry1'},\n",
    "    {'name': f'foundry2-{resource_suffix}', 'location': 'eastus', 'project': 'master-lab-foundry2'},\n",
    "    {'name': f'foundry3-{resource_suffix}', 'location': 'norwayeast', 'project': 'master-lab-foundry3'}\n",
    "]\n",
    "\n",
    "models_config = {\n",
    "    'foundry1': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4o', 'format': 'OpenAI', 'version': '2024-08-06', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'text-embedding-3-small', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "        {'name': 'text-embedding-3-large', 'format': 'OpenAI', 'version': '1', 'sku': 'GlobalStandard', 'capacity': 20},\n",
    "                {'name': 'dall-e-3', 'format': 'OpenAI', 'version': '3.0', 'sku': 'Standard', 'capacity': 1},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry2': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ],\n",
    "    'foundry3': [\n",
    "        {'name': 'gpt-4o-mini', 'format': 'OpenAI', 'version': '2024-07-18', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "        {'name': 'gpt-4.1-nano', 'format': 'OpenAI', 'version': '2025-04-14', 'sku': 'GlobalStandard', 'capacity': 100},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Phase 2a: Check/Create Foundry Hubs\n",
    "print('[*] Phase 2a: AI Foundry Hubs')\n",
    "existing_accounts = {acc.name: acc for acc in cog_client.accounts.list_by_resource_group(resource_group_name)}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    if foundry_name in existing_accounts:\n",
    "        print(f'  [OK] {foundry_name} already exists')\n",
    "    else:\n",
    "        print(f'  [*] Creating {foundry_name}...')\n",
    "        try:\n",
    "            account_params = Account(\n",
    "                location=foundry['location'],\n",
    "                sku=CogSku(name='S0'),\n",
    "                kind='AIServices',\n",
    "                properties={\n",
    "                    'customSubDomainName': foundry_name.lower(),\n",
    "                    'publicNetworkAccess': 'Enabled',\n",
    "                    'allowProjectManagement': True\n",
    "                },\n",
    "                identity={'type': 'SystemAssigned'}\n",
    "            )\n",
    "            poller = cog_client.accounts.begin_create(resource_group_name, foundry_name, account_params)\n",
    "            poller.result(timeout=300)\n",
    "            print(f'  [OK] {foundry_name} created')\n",
    "        except Exception as e:\n",
    "            print(f'  [ERROR] Failed: {str(e)[:100]}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Phase 2b: Deploy Models (Resilient)\n",
    "print('[*] Phase 2b: AI Models (Resilient)')\n",
    "deployment_results = {'succeeded': [], 'failed': [], 'skipped': []}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    short_name = foundry_name.split('-')[0]\n",
    "    models = models_config.get(short_name, [])\n",
    "\n",
    "    print(f'  [*] {foundry_name}: {len(models)} models')\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model['name']\n",
    "        try:\n",
    "            # Check if exists\n",
    "            existing = cog_client.deployments.get(resource_group_name, foundry_name, model_name)\n",
    "            if existing.properties.provisioning_state == 'Succeeded':\n",
    "                deployment_results['skipped'].append(f'{short_name}/{model_name}')\n",
    "                print(f'    [OK] {model_name} already deployed')\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            print(f'    [*] Deploying {model_name}...')\n",
    "            deployment_params = Deployment(\n",
    "                sku=CogSku(name=model['sku'], capacity=model['capacity']),\n",
    "                properties=DeploymentProperties(\n",
    "                    model=DeploymentModel(\n",
    "                        format=model['format'],\n",
    "                        name=model['name'],\n",
    "                        version=model['version']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            poller = cog_client.deployments.begin_create_or_update(\n",
    "                resource_group_name, foundry_name, model_name, deployment_params\n",
    "            )\n",
    "            poller.result(timeout=600)\n",
    "            deployment_results['succeeded'].append(f'{short_name}/{model_name}')\n",
    "            print(f'    [OK] {model_name} deployed')\n",
    "        except Exception as e:\n",
    "            deployment_results['failed'].append({'model': f'{short_name}/{model_name}', 'error': str(e)})\n",
    "            print(f'    [SKIP] {model_name} failed: {str(e)[:80]}')\n",
    "\n",
    "print()\n",
    "print(f'[OK] Models: {len(deployment_results[\"succeeded\"])} deployed, {len(deployment_results[\"skipped\"])} skipped, {len(deployment_results[\"failed\"])} failed')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Collect Foundry Deployment Outputs for Env File\n",
    "# ============================================================================\n",
    "print()\n",
    "print('[*] Collecting foundry deployment outputs for env file...')\n",
    "step2_outputs = {\n",
    "    'foundryProjectEndpoint': '',\n",
    "    'inferenceAPIPath': 'inference',\n",
    "    'foundries': []\n",
    "}\n",
    "\n",
    "for foundry in foundries:\n",
    "    foundry_name = foundry['name']\n",
    "    try:\n",
    "        # Get account details\n",
    "        account = cog_client.accounts.get(resource_group_name, foundry_name)\n",
    "        \n",
    "        # Get primary key\n",
    "        keys = cog_client.accounts.list_keys(resource_group_name, foundry_name)\n",
    "        primary_key = keys.key1\n",
    "        \n",
    "        # Build endpoint\n",
    "        endpoint = f\"https://{foundry_name}.openai.azure.com/\"\n",
    "        \n",
    "        # Get deployed model names for this foundry\n",
    "        short_name = foundry_name.split('-')[0]\n",
    "        model_names = [m['name'] for m in models_config.get(short_name, [])]\n",
    "        \n",
    "        foundry_output = {\n",
    "            'name': foundry_name,\n",
    "            'location': foundry['location'],\n",
    "            'endpoint': endpoint,\n",
    "            'key': primary_key,\n",
    "            'models': model_names\n",
    "        }\n",
    "        \n",
    "        step2_outputs['foundries'].append(foundry_output)\n",
    "        print(f\"  [OK] Captured {foundry_name}: {len(model_names)} models\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Could not capture {foundry_name} outputs: {str(e)[:80]}\")\n",
    "\n",
    "print(f'[OK] Captured {len(step2_outputs[\"foundries\"])} foundry outputs')\n",
    "print()\n",
    "\n",
    "print('[*] Phase 2c: APIM Inference API')\n",
    "\n",
    "deployment_step2c = 'master-lab-02c-apim-api'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step2c):\n",
    "    print('[OK] APIM API already configured. Skipping...')\n",
    "else:\n",
    "    print('[*] Configuring APIM Inference API...')\n",
    "\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-02c-apim-api.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 2c')\n",
    "\n",
    "    params_dict = {\n",
    "        'apimLoggerId': {'value': step1_outputs['apimLoggerId']},\n",
    "        'appInsightsId': {'value': step1_outputs['appInsightsId']},\n",
    "        'appInsightsInstrumentationKey': {'value': step1_outputs['appInsightsInstrumentationKey']},\n",
    "        'inferenceAPIPath': {'value': 'inference'},\n",
    "        'inferenceAPIType': {'value': 'AzureOpenAI'}\n",
    "    }\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step2c, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 2c deployment failed')\n",
    "\n",
    "    print('[OK] APIM API configured')\n",
    "\n",
    "print('[OK] Step 2 complete')\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SUPPORTING SERVICES (Bicep)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 3: SUPPORTING SERVICES')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Redis, Search, Cosmos DB, Content Safety')\n",
    "print()\n",
    "\n",
    "deployment_step3 = 'master-lab-03-supporting'\n",
    "\n",
    "# Get current user's principal ID for Cosmos DB data plane RBAC\n",
    "print('[*] Getting principal ID for Cosmos DB RBAC...')\n",
    "cosmos_principal_id = None\n",
    "try:\n",
    "    # Try signed-in user first\n",
    "    result = subprocess.run(\n",
    "        [os.environ.get('AZ_CLI', 'az'), 'ad', 'signed-in-user', 'show', '--query', 'id', '-o', 'tsv'],\n",
    "        capture_output=True, text=True, timeout=15\n",
    "    )\n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        cosmos_principal_id = result.stdout.strip()\n",
    "        print(f'[OK] Using signed-in user: {cosmos_principal_id[:8]}...')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not cosmos_principal_id:\n",
    "    # Fall back to service principal\n",
    "    cosmos_principal_id = os.environ.get('AZURE_CLIENT_ID', '')\n",
    "    if cosmos_principal_id:\n",
    "        print(f'[OK] Using service principal: {cosmos_principal_id[:8]}...')\n",
    "    else:\n",
    "        print('[WARN] Could not determine principal ID for Cosmos DB RBAC')\n",
    "\n",
    "if check_deployment_exists(resource_group_name, deployment_step3):\n",
    "    print('[OK] Step 3 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 3 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-03-supporting.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 3')\n",
    "\n",
    "    params_dict = {}\n",
    "    if os.path.exists(BICEP_DIR / 'params-03-supporting.json'):\n",
    "        with open(BICEP_DIR / 'params-03-supporting.json') as f:\n",
    "            params = json.load(f)\n",
    "        # Extract only the 'parameters' section from ARM parameter file\n",
    "        params_dict = params.get('parameters', {})\n",
    "    \n",
    "    # Add Cosmos DB principal ID for data plane RBAC\n",
    "    if cosmos_principal_id:\n",
    "        params_dict['cosmosDbPrincipalId'] = {'value': cosmos_principal_id}\n",
    "        print(f'[*] Will assign Cosmos DB data plane RBAC to {cosmos_principal_id[:8]}...')\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step3, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 3 deployment failed')\n",
    "    \n",
    "    # Wait for RBAC propagation (Cosmos DB data plane roles can take 5-10 minutes)\n",
    "    if cosmos_principal_id:\n",
    "        print('[*] Waiting 30s for Cosmos DB RBAC propagation...')\n",
    "        time.sleep(30)\n",
    "    \n",
    "    print('[OK] Step 3 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    print('[OK] Step 3 outputs retrieved')\n",
    "except Exception:\n",
    "    step3_outputs = {}\n",
    "    print('[*] No Step 3 outputs available')\n",
    "# =============================================================================\n",
    "# STEP 4: MCP SERVERS (with ACR Setup)\n",
    "# =============================================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('STEP 4: MCP SERVERS')\n",
    "print('=' * 70)\n",
    "print('[*] Resources: Container Registry + Container Apps + 7 MCP servers')\n",
    "print('[*] Estimated time: ~8 minutes')\n",
    "print()\n",
    "\n",
    "# Step 4a: Create Container Registry for MCP images\n",
    "print('[*] Step 4a: Setting up Container Registry for MCP images...')\n",
    "\n",
    "from azure.mgmt.containerregistry import ContainerRegistryManagementClient\n",
    "from azure.mgmt.containerregistry.models import Registry, Sku as AcrSku\n",
    "\n",
    "acr_client = ContainerRegistryManagementClient(credential, subscription_id)\n",
    "\n",
    "# Use the same suffix as APIM\n",
    "acr_name = f'acr{resource_suffix}'.replace('-', '')[:50]  # ACR names: alphanumeric only, max 50 chars\n",
    "\n",
    "# Check if ACR exists\n",
    "acr_exists = False\n",
    "acr_server = None\n",
    "acr_username = None\n",
    "acr_password = None\n",
    "\n",
    "try:\n",
    "    acr = acr_client.registries.get(resource_group_name, acr_name)\n",
    "    acr_exists = True\n",
    "    acr_server = acr.login_server\n",
    "    print(f'[OK] ACR already exists: {acr_server}')\n",
    "except Exception:\n",
    "    print(f'[*] Creating ACR: {acr_name}...')\n",
    "    try:\n",
    "        acr_params = Registry(\n",
    "            location=location,\n",
    "            sku=AcrSku(name='Basic'),\n",
    "            admin_user_enabled=True\n",
    "        )\n",
    "        poller = acr_client.registries.begin_create(resource_group_name, acr_name, acr_params)\n",
    "        acr = poller.result(timeout=300)\n",
    "        acr_server = acr.login_server\n",
    "        print(f'[OK] ACR created: {acr_server}')\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] ACR creation failed: {str(e)[:150]}')\n",
    "        raise\n",
    "\n",
    "# Get ACR admin credentials\n",
    "print('[*] Retrieving ACR admin credentials...')\n",
    "try:\n",
    "    creds = acr_client.registries.list_credentials(resource_group_name, acr_name)\n",
    "    acr_username = creds.username\n",
    "    acr_password = creds.passwords[0].value\n",
    "    print(f'[OK] ACR credentials retrieved (username: {acr_username})')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Failed to get ACR credentials: {str(e)[:100]}')\n",
    "    raise\n",
    "\n",
    "# Step 4b: Import MCP images into ACR (if not already present)\n",
    "print()\n",
    "print('[*] Step 4b: Checking MCP images in ACR...')\n",
    "\n",
    "# Source images (from external ACR or public registries)\n",
    "SOURCE_ACR = 'acrmcpwksp321028.azurecr.io'\n",
    "mcp_images = [\n",
    "    {'source': f'{SOURCE_ACR}/excel-analytics-mcp:v4', 'target': 'excel-analytics-mcp:v4'},\n",
    "    {'source': f'{SOURCE_ACR}/research-docs-mcp:v2', 'target': 'research-docs-mcp:v2'},\n",
    "]\n",
    "\n",
    "for img in mcp_images:\n",
    "    target_repo = img['target'].split(':')[0]\n",
    "    target_tag = img['target'].split(':')[1] if ':' in img['target'] else 'latest'\n",
    "    \n",
    "    # Check if image exists in target ACR\n",
    "    try:\n",
    "        tags = list(acr_client.registries.list_usages(resource_group_name, acr_name))\n",
    "        # Import image using az acr import\n",
    "        print(f'  [*] Importing {img[\"target\"]}...')\n",
    "        import_cmd = [\n",
    "            os.environ.get('AZ_CLI', 'az'), 'acr', 'import',\n",
    "            '--name', acr_name,\n",
    "            '--source', img['source'],\n",
    "            '--image', img['target'],\n",
    "            '--force'\n",
    "        ]\n",
    "        r = subprocess.run(import_cmd, capture_output=True, text=True, timeout=300)\n",
    "        if r.returncode == 0:\n",
    "            print(f'  [OK] Imported {img[\"target\"]}')\n",
    "        else:\n",
    "            # Image might already exist or source not accessible\n",
    "            if 'already exists' in r.stderr.lower():\n",
    "                print(f'  [OK] {img[\"target\"]} already exists')\n",
    "            else:\n",
    "                print(f'  [WARN] Import issue: {r.stderr[:100]}')\n",
    "                print(f'  [INFO] MCP image may need manual import from source')\n",
    "    except Exception as e:\n",
    "        print(f'  [WARN] Could not check/import {img[\"target\"]}: {str(e)[:80]}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 4b.1: Build and push weather-mcp image to ACR\n",
    "print('[*] Step 4b.1: Building weather-mcp HTTP wrapper image...')\n",
    "\n",
    "# Weather MCP wrapper is built from local source (mcp-http-wrappers/weather-mcp)\n",
    "weather_mcp_dir = NOTEBOOK_DIR / 'mcp-http-wrappers' / 'weather-mcp'\n",
    "weather_image_tag = f'{acr_server}/weather-mcp:latest'\n",
    "\n",
    "if weather_mcp_dir.exists():\n",
    "    # Login to ACR\n",
    "    print(f'  [*] Logging into ACR: {acr_name}...')\n",
    "    login_result = subprocess.run(\n",
    "        [os.environ.get('AZ_CLI', 'az'), 'acr', 'login', '--name', acr_name],\n",
    "        capture_output=True, text=True, timeout=60\n",
    "    )\n",
    "    if login_result.returncode != 0:\n",
    "        print(f'  [WARN] ACR login failed: {login_result.stderr[:100]}')\n",
    "    else:\n",
    "        print('  [OK] ACR login successful')\n",
    "\n",
    "    # Build docker image\n",
    "    print(f'  [*] Building image: {weather_image_tag}')\n",
    "    build_result = subprocess.run(\n",
    "        ['docker', 'build', '-t', weather_image_tag, str(weather_mcp_dir)],\n",
    "        capture_output=True, text=True, timeout=300\n",
    "    )\n",
    "    if build_result.returncode != 0:\n",
    "        print(f'  [ERROR] Docker build failed: {build_result.stderr[:200]}')\n",
    "        print('  [WARN] Weather MCP server will not be available')\n",
    "    else:\n",
    "        print('  [OK] Image built successfully')\n",
    "\n",
    "        # Push to ACR\n",
    "        print(f'  [*] Pushing image to ACR...')\n",
    "        push_result = subprocess.run(\n",
    "            ['docker', 'push', weather_image_tag],\n",
    "            capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "        if push_result.returncode != 0:\n",
    "            print(f'  [ERROR] Docker push failed: {push_result.stderr[:200]}')\n",
    "        else:\n",
    "            print(f'  [OK] Image pushed: {weather_image_tag}')\n",
    "else:\n",
    "    print(f'  [WARN] Weather MCP source not found: {weather_mcp_dir}')\n",
    "    print('  [INFO] Weather MCP server will use placeholder image')\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 4b.2: Build and push excel-mcp image to ACR\n",
    "print('[*] Step 4b.2: Building excel-mcp MCP server image...')\n",
    "\n",
    "excel_mcp_dir = NOTEBOOK_DIR / 'mcp-http-wrappers' / 'excel-mcp'\n",
    "excel_image_tag = f'{acr_server}/excel-mcp:latest'\n",
    "\n",
    "if excel_mcp_dir.exists():\n",
    "    # Build docker image\n",
    "    print(f'  [*] Building image: {excel_image_tag}')\n",
    "    build_result = subprocess.run(\n",
    "        ['docker', 'build', '-t', excel_image_tag, str(excel_mcp_dir)],\n",
    "        capture_output=True, text=True, timeout=300\n",
    "    )\n",
    "    if build_result.returncode != 0:\n",
    "        print(f'  [ERROR] Docker build failed: {build_result.stderr[:200]}')\n",
    "        print('  [WARN] Excel MCP server will not be available')\n",
    "    else:\n",
    "        print('  [OK] Image built successfully')\n",
    "\n",
    "        # Push to ACR\n",
    "        print(f'  [*] Pushing image to ACR...')\n",
    "        push_result = subprocess.run(\n",
    "            ['docker', 'push', excel_image_tag],\n",
    "            capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "        if push_result.returncode != 0:\n",
    "            print(f'  [ERROR] Docker push failed: {push_result.stderr[:200]}')\n",
    "        else:\n",
    "            print(f'  [OK] Image pushed: {excel_image_tag}')\n",
    "else:\n",
    "    print(f'  [WARN] Excel MCP source not found: {excel_mcp_dir}')\n",
    "    print('  [INFO] Excel MCP server will use placeholder image')\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 4c: Deploy MCP infrastructure\n",
    "print('[*] Step 4c: Deploying MCP infrastructure...')\n",
    "\n",
    "deployment_step4 = 'master-lab-04-mcp'\n",
    "if check_deployment_exists(resource_group_name, deployment_step4):\n",
    "    print('[OK] Step 4 already deployed. Skipping...')\n",
    "else:\n",
    "    print('[*] Step 4 not found. Deploying...')\n",
    "    json_file = compile_bicep_safe(BICEP_DIR / 'deploy-04-mcp.bicep')\n",
    "    if not json_file:\n",
    "        raise Exception('Bicep compilation failed for Step 4')\n",
    "\n",
    "    # Get OWM_API_KEY from env file if it exists\n",
    "    owm_api_key = os.environ.get('OWM_API_KEY', '')\n",
    "    if not owm_api_key:\n",
    "        env_file = NOTEBOOK_DIR / 'master-lab.env'\n",
    "        if env_file.exists():\n",
    "            with open(env_file) as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('OWM_API_KEY='):\n",
    "                        owm_api_key = line.split('=', 1)[1].strip()\n",
    "                        break\n",
    "\n",
    "    params_dict = {\n",
    "        'logAnalyticsCustomerId': {'value': step1_outputs.get('logAnalyticsCustomerId', '')},\n",
    "        'logAnalyticsPrimarySharedKey': {'value': step1_outputs.get('logAnalyticsPrimarySharedKey', '')},\n",
    "    }\n",
    "\n",
    "    # Add OWM_API_KEY if available (for weather MCP server)\n",
    "    if owm_api_key:\n",
    "        params_dict['owmApiKey'] = {'value': owm_api_key}\n",
    "        print(f'  [*] OWM_API_KEY configured for weather MCP server')\n",
    "\n",
    "    success, result = deploy_template(resource_group_name, deployment_step4, json_file, params_dict)\n",
    "    if not success:\n",
    "        raise Exception('Step 4 deployment failed')\n",
    "    print('[OK] Step 4 complete')\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    step4_outputs['acrServer'] = acr_server  # Add ACR server to outputs\n",
    "    print('[OK] Step 4 outputs retrieved')\n",
    "except Exception:\n",
    "    step4_outputs = {'acrServer': acr_server}\n",
    "    print('[*] Step 4 outputs: ACR info only')\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# DEPLOYMENT COMPLETE\n",
    "# =============================================================================\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "total_mins = int(total_elapsed / 60)\n",
    "total_secs = int(total_elapsed % 60)\n",
    "\n",
    "print('=' * 70)\n",
    "print('DEPLOYMENT COMPLETE')\n",
    "print('=' * 70)\n",
    "print(f'[OK] Total time: {total_mins}m {total_secs}s')\n",
    "print()\n",
    "print('[OK] All 4 steps deployed successfully!')\n",
    "print('[OK] Next: Run Cell 18-19 to generate master-lab.env')\n",
    "print()\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy AI Foundry Accounts (Optional Bicep Step)\n",
    "\n",
    "\n",
    "**Purpose**: Alternative method to create AI Foundry (Azure AI Services) accounts using Bicep templates\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Azure subscription with sufficient quota for AI Services\n",
    "- Resource group already created\n",
    "- Bicep templates compiled and ready\n",
    "- Service principal or Azure CLI authentication configured\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**This is an optional infrastructure-as-code deployment step.**\n",
    "\n",
    "1. Checks if Bicep Step 2 template exists (`deploy-02-foundries.bicep`)\n",
    "2. If found, deploys AI Foundry accounts for three regions:\n",
    "   - East US\n",
    "   - West US\n",
    "   - Sweden Central\n",
    "3. Each account supports multiple AI model deployments\n",
    "4. Uses ARM template deployment via Azure CLI or SDK\n",
    "5. Captures deployment outputs for subsequent configuration\n",
    "\n",
    "**Note**: This step can be skipped if AI Foundry accounts are created through other means (Portal, SDK, or the main deployment cell).\n",
    "\n",
    "The deployment typically takes 5-10 minutes per account.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "If Bicep file exists:\n",
    "- Deployment progress messages for each region\n",
    "- Confirmation of successful account creation\n",
    "- Output variables containing account endpoints and keys\n",
    "\n",
    "If Bicep file doesn't exist:\n",
    "- Message indicating Bicep-based foundry deployment is optional\n",
    "- Instruction to use SDK-based deployment or Azure Portal instead\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e183ba1f-b5db-4152-b5e6-009ad9f334b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BICEP STEP 2: AI Foundry Accounts (Infra-as-Code Option)\n",
      "======================================================================\n",
      "\n",
      "[*] Deploying foundry accounts via Bicep...\n",
      "[ERROR] Bicep file not found: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/deploy/deploy-02-foundry.bicep\n",
      "[WARN] Bicep template or precompiled JSON not found at: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/deploy/deploy-02-foundry.bicep\n",
      "[WARN] Skipping Bicep deployment and relying on previously created Python-based foundry resources.\n",
      "[WARN] Could not retrieve foundry outputs: (DeploymentNotFound) Deployment 'master-lab-02-foundry' could not be found.\n",
      "Code: DeploymentNotFound\n",
      "Message: Deployment 'master-lab-02-foundry' could not be fo\n",
      "[INFO] Falling back to existing_accounts already provisioned:\n",
      "  - foundry2-c7uj6vzppah74 @ eastus -> https://foundry2-c7uj6vzppah74.cognitiveservices.azure.com/\n",
      "  - foundry1-c7uj6vzppah74 @ uksouth -> https://foundry1-c7uj6vzppah74.cognitiveservices.azure.com/\n",
      "  - contentsafety-c7uj6vzppah74 @ uksouth -> https://contentsafety-c7uj6vzppah74.cognitiveservices.azure.com/\n",
      "  - foundry3-c7uj6vzppah74 @ norwayeast -> https://foundry3-c7uj6vzppah74.cognitiveservices.azure.com/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import string\n",
    "\n",
    "# --- OPTIONAL BICEP-BASED STEP 2 (AI FOUNDRY ACCOUNTS) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BICEP STEP 2: AI Foundry Accounts (Infra-as-Code Option)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Fallbacks if prior cell not executed\n",
    "if 'foundry_suffix' not in globals():\n",
    "    # Auto-generate random suffix for unique resource names\n",
    "    foundry_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=13))\n",
    "    print(f'[*] Auto-generated suffix: {foundry_suffix}')\n",
    "if 'resource_group_name' not in globals():\n",
    "    # Use environment variable or construct from suffix\n",
    "    env_rg = os.getenv('RESOURCE_GROUP')\n",
    "    if env_rg:\n",
    "        resource_group_name = env_rg\n",
    "    else:\n",
    "        resource_group_name = f'rg-master-lab-{foundry_suffix}'\n",
    "    print(f'[*] Using resource group: {resource_group_name}')\n",
    "if 'BICEP_DIR' not in globals():\n",
    "    # Use absolute path from NOTEBOOK_DIR\n",
    "    if \"NOTEBOOK_DIR\" in globals():\n",
    "        BICEP_DIR = NOTEBOOK_DIR / \"deploy\"\n",
    "    else:\n",
    "        # Codespaces path\n",
    "        BICEP_DIR = Path('/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/deploy')\n",
    "        if not BICEP_DIR.exists():\n",
    "            BICEP_DIR = Path.cwd() / 'AI-Gateway' / 'labs' / 'master-lab' / 'deploy'\n",
    "\n",
    "if 'compile_bicep_safe' not in globals():\n",
    "    def compile_bicep_safe(bicep_path):\n",
    "        b = Path(bicep_path)\n",
    "        if not b.exists():\n",
    "            print(f'[ERROR] Missing bicep: {b}')\n",
    "            return None\n",
    "        json_path = b.with_suffix('.json')\n",
    "        if json_path.exists():\n",
    "            print(f'[OK] Using precompiled: {json_path.name}')\n",
    "            return str(json_path)\n",
    "        if 'compile_bicep' in globals():\n",
    "            try:\n",
    "                print('[*] Precompiled JSON not found; fallback compile_bicep()')\n",
    "                return compile_bicep(str(b))\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] compile_bicep() failed: {e}')\n",
    "        print(f'[ERROR] No JSON + no fallback: {json_path}')\n",
    "        return None\n",
    "\n",
    "bicep_foundry_deployment = 'master-lab-02-foundry'\n",
    "\n",
    "if check_deployment_exists(resource_group_name, bicep_foundry_deployment):\n",
    "    print('[OK] Foundry Bicep deployment already exists \u2013 skipping.')\n",
    "else:\n",
    "    print('[*] Deploying foundry accounts via Bicep...')\n",
    "    template_candidate = BICEP_DIR / 'deploy-02-foundry.bicep'\n",
    "    template_file = compile_bicep_safe(template_candidate)\n",
    "    if not template_file:\n",
    "        print(f\"[WARN] Bicep template or precompiled JSON not found at: {template_candidate}\")\n",
    "        print(\"[WARN] Skipping Bicep deployment and relying on previously created Python-based foundry resources.\")\n",
    "    else:\n",
    "        params_dict = {\n",
    "            'resourceSuffix': {'value': foundry_suffix},\n",
    "        }\n",
    "        success, _ = deploy_template(resource_group_name, bicep_foundry_deployment, template_file, params_dict)\n",
    "        if not success:\n",
    "            print('[WARN] Foundry Bicep deployment failed \u2013 continuing without Bicep deployment.')\n",
    "        else:\n",
    "            print('[OK] Foundry accounts deployed via Bicep')\n",
    "\n",
    "# Outputs (graceful fallback to existing_accounts if Bicep outputs unavailable)\n",
    "try:\n",
    "    foundry_outputs = get_deployment_outputs(resource_group_name, bicep_foundry_deployment)\n",
    "    print('[OK] Foundry outputs retrieved')\n",
    "    accounts = foundry_outputs.get('foundryAccounts', [])\n",
    "    if isinstance(accounts, list):\n",
    "        print('\\n[Foundry Accounts]')\n",
    "        for a in accounts:\n",
    "            print(f\"  - {a.get('name')} @ {a.get('location')} -> {a.get('endpoint')}\")\n",
    "    else:\n",
    "        print('[WARN] foundryAccounts output missing or wrong type')\n",
    "except Exception as e:\n",
    "    print('[WARN] Could not retrieve foundry outputs:', str(e)[:160])\n",
    "    if 'existing_accounts' in globals() and existing_accounts:\n",
    "        print('[INFO] Falling back to existing_accounts already provisioned:')\n",
    "        for name, acct_obj in existing_accounts.items():\n",
    "            try:\n",
    "                loc = getattr(acct_obj, 'location', 'unknown')\n",
    "                endpoint = getattr(acct_obj.properties, 'endpoint', None) or getattr(acct_obj.properties, 'apiEndpoint', '')\n",
    "                print(f\"  - {name} @ {loc} -> {endpoint}\")\n",
    "            except Exception:\n",
    "                print(f\"  - {name}\")\n",
    "    else:\n",
    "        print('[INFO] No existing_accounts fallback available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Master Configuration File\n",
    "\n",
    "\n",
    "**Purpose**: Creates the `master-lab.env` file containing all environment variables needed for the labs\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Successful completion of all deployment steps (Steps 1-4)\n",
    "- Deployment outputs stored in `step2_outputs`, `step3_outputs`, and `step4_outputs` variables\n",
    "- Write permissions in the notebook directory\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "This cell consolidates all deployment outputs and configuration into a single `.env` file:\n",
    "\n",
    "**1. Collects Configuration From:**\n",
    "- Bootstrap configuration (subscription ID, resource group, location)\n",
    "- Step 1 outputs: APIM service name, gateway URL, subscription key\n",
    "- Step 2 outputs: AI Foundry account endpoints and keys (3 regions)\n",
    "- Step 3 outputs: Redis cache, Cosmos DB, AI Search endpoints\n",
    "- Step 4 outputs: Model deployment URLs and keys\n",
    "\n",
    "**2. Generates Environment Variables:**\n",
    "- `SUBSCRIPTION_ID`, `RESOURCE_GROUP`, `LOCATION`\n",
    "- `APIM_SERVICE`, `APIM_GATEWAY_URL`, `APIM_SUBSCRIPTION_KEY`\n",
    "- `AZURE_OPENAI_ENDPOINT_*` (for each region)\n",
    "- `AZURE_OPENAI_API_KEY_*` (for each region)\n",
    "- `REDIS_HOST`, `REDIS_PASSWORD`\n",
    "- `COSMOS_ENDPOINT`, `COSMOS_KEY`\n",
    "- `AI_SEARCH_ENDPOINT`, `AI_SEARCH_KEY`\n",
    "- `OPENAI_API_BASE`, `OPENAI_API_KEY` (pointing to APIM gateway)\n",
    "\n",
    "**3. Writes master-lab.env file:**\n",
    "- Creates/overwrites the file in the notebook directory\n",
    "- Includes timestamp and generation comments\n",
    "- Formats for easy reading and debugging\n",
    "\n",
    "**4. Safety Features:**\n",
    "- Uses safe fallback to empty dicts if outputs missing\n",
    "- Handles missing keys gracefully\n",
    "- Provides clear error messages if critical values are missing\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "- Success message: \"[*] Generating master-lab.env...\"\n",
    "- Confirmation message with file location\n",
    "- The `master-lab.env` file created in the notebook directory\n",
    "- File contains 30+ environment variables\n",
    "- All subsequent lab cells will load from this file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Generating master-lab.env...\n",
      "[*] Auto-discovering APIM_API_ID...\n",
      "[OK] Auto-discovered APIM_API_ID: inference-api\n",
      "[OK] Loaded environment variables into os.environ\n",
      "[OK] Created /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "[OK] File location: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "[*] Model Deployment Summary:\n",
      "  Region 1 (UK South): 6 models\n",
      "    - gpt-4o-mini\n",
      "    - gpt-4o\n",
      "    - text-embedding-3-small\n",
      "    - text-embedding-3-large\n",
      "    - dall-e-3\n",
      "    - gpt-4.1-nano\n",
      "  Region 2 (East US): 2 models\n",
      "    - gpt-4o-mini\n",
      "    - gpt-4.1-nano\n",
      "  Region 3 (Norway East): 2 models\n",
      "    - gpt-4o-mini\n",
      "    - gpt-4.1-nano\n",
      "\n",
      "[OK] Load Balancing: ENABLED (3 regions)\n",
      "[OK] LB Regions: uksouth, eastus, norwayeast\n",
      "\n",
      "[OK] You can now load this in all lab tests:\n",
      "  from dotenv import load_dotenv\n",
      "  load_dotenv(\"master-lab.env\")\n",
      "\n",
      "======================================================================\n",
      "SETUP COMPLETE - ALL LABS READY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print('[*] Generating master-lab.env...')\n",
    "\n",
    "# Ensure step2_outputs, step3_outputs and step4_outputs exist (safe fallback to empty dicts)\n",
    "try:\n",
    "    step2_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step2_outputs = get_deployment_outputs(resource_group_name, deployment_step2c)\n",
    "    except Exception:\n",
    "        step2_outputs = {}\n",
    "\n",
    "try:\n",
    "    step3_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step3_outputs = get_deployment_outputs(resource_group_name, deployment_step3)\n",
    "    except Exception:\n",
    "        step3_outputs = {}\n",
    "\n",
    "try:\n",
    "    step4_outputs\n",
    "except NameError:\n",
    "    try:\n",
    "        step4_outputs = get_deployment_outputs(resource_group_name, deployment_step4)\n",
    "    except Exception:\n",
    "        step4_outputs = {}\n",
    "\n",
    "# Get API key from APIM subscriptions (prefer step1 outputs)\n",
    "apim_subscriptions = step1_outputs.get('apimSubscriptions', []) if isinstance(step1_outputs, dict) else []\n",
    "api_key = apim_subscriptions[0]['key'] if apim_subscriptions else 'N/A'\n",
    "\n",
    "# Auto-discover APIM API_ID from deployed APIM service\n",
    "print('[*] Auto-discovering APIM_API_ID...')\n",
    "discovered_api_id = None\n",
    "\n",
    "try:\n",
    "    import subprocess\n",
    "    import json as json_module\n",
    "    import shutil\n",
    "    \n",
    "    # Get APIM service name from step1 outputs\n",
    "    apim_service_name = step1_outputs.get('apimServiceName', 'apim-pavavy6pu5hpa')\n",
    "    \n",
    "    # Find Azure CLI\n",
    "    az_cli = shutil.which(\"az\")\n",
    "    if az_cli and subscription_id:\n",
    "        # Query APIM for APIs\n",
    "        url = (f'https://management.azure.com/subscriptions/{subscription_id}'\n",
    "               f'/resourceGroups/{resource_group_name}/providers/Microsoft.ApiManagement'\n",
    "               f'/service/{apim_service_name}/apis?api-version=2022-08-01')\n",
    "        \n",
    "        result = subprocess.run([az_cli, \"rest\", \"--method\", \"get\", \"--url\", url], \n",
    "                               capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            apis_data = json_module.loads(result.stdout)\n",
    "            apis = apis_data.get('value', [])\n",
    "            \n",
    "            # Find inference API\n",
    "            for api in apis:\n",
    "                api_id = api.get('name', '')\n",
    "                api_props = api.get('properties', {})\n",
    "                api_name = api_props.get('displayName', '').lower()\n",
    "                api_path = api_props.get('path', '').lower()\n",
    "                \n",
    "                if 'inference' in api_id.lower() or 'inference' in api_name or 'inference' in api_path:\n",
    "                    discovered_api_id = api_id\n",
    "                    print(f'[OK] Auto-discovered APIM_API_ID: {discovered_api_id}')\n",
    "                    break\n",
    "            \n",
    "            if not discovered_api_id:\n",
    "                # Fallback to inference-api if exists\n",
    "                for api in apis:\n",
    "                    if api.get('name') == 'inference-api':\n",
    "                        discovered_api_id = 'inference-api'\n",
    "                        print(f'[OK] Found APIM_API_ID: {discovered_api_id}')\n",
    "                        break\n",
    "except Exception as e:\n",
    "    print(f'[!] Could not auto-discover APIM_API_ID: {e}')\n",
    "\n",
    "# Use discovered ID or fallback to default\n",
    "apim_api_id = discovered_api_id if discovered_api_id else 'inference-api'\n",
    "if not discovered_api_id:\n",
    "    print(f'[!] Using default APIM_API_ID: {apim_api_id}')\n",
    "\n",
    "# Set in environment for downstream use\n",
    "os.environ['APIM_API_ID'] = apim_api_id\n",
    "\n",
    "# Build .env content with grouped structure\n",
    "env_content = f\"\"\"# Master AI Gateway Lab - Deployment Outputs\n",
    "# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "# Resource Group: {resource_group_name}\n",
    "\n",
    "# ===========================================\n",
    "# APIM (API Management)\n",
    "# ===========================================\n",
    "APIM_GATEWAY_URL={step1_outputs.get('apimGatewayUrl', '')}\n",
    "APIM_SERVICE_ID={step1_outputs.get('apimServiceId', '')}\n",
    "APIM_SERVICE_NAME={step1_outputs.get('apimServiceName', '')}\n",
    "APIM_API_KEY={api_key}\n",
    "APIM_API_ID={apim_api_id}\n",
    "\n",
    "# ===========================================\n",
    "# OpenAI Endpoint (APIM Gateway + Inference Path)\n",
    "# ===========================================\n",
    "OPENAI_ENDPOINT={step1_outputs.get('apimGatewayUrl', '')}/{step2_outputs.get('inferenceAPIPath', 'inference')}/openai\n",
    "\n",
    "# ===========================================\n",
    "# AI Foundry\n",
    "# ===========================================\n",
    "FOUNDRY_PROJECT_ENDPOINT={step2_outputs.get('foundryProjectEndpoint', '')}\n",
    "INFERENCE_API_PATH={step2_outputs.get('inferenceAPIPath', 'inference')}/openai\n",
    "\"\"\"\n",
    "\n",
    "# ===========================================\n",
    "# AI Models (Multi-Region Load Balancing)\n",
    "# ===========================================\n",
    "# Extract foundry deployment information from step2_outputs\n",
    "foundries_data = step2_outputs.get('foundries', []) if isinstance(step2_outputs, dict) else []\n",
    "\n",
    "# Region mapping for display\n",
    "region_names = {\n",
    "    'uksouth': 'UK South',\n",
    "    'eastus': 'East US',\n",
    "    'norwayeast': 'Norway East'\n",
    "}\n",
    "\n",
    "# Track endpoints for load balancing\n",
    "lb_endpoints = []\n",
    "lb_regions = []\n",
    "\n",
    "env_content += \"\\n# ===========================================\\n\"\n",
    "env_content += \"# AI Models (Multi-Region Load Balancing)\\n\"\n",
    "env_content += \"# ===========================================\\n\\n\"\n",
    "\n",
    "# Process each foundry (region)\n",
    "for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "    if not isinstance(foundry_info, dict):\n",
    "        continue\n",
    "\n",
    "    foundry_name = foundry_info.get('name', '')\n",
    "    location = foundry_info.get('location', '')\n",
    "    endpoint = foundry_info.get('endpoint', '')\n",
    "    key = foundry_info.get('key', '')\n",
    "    models = foundry_info.get('models', [])\n",
    "\n",
    "    # Add region to load balancing config\n",
    "    if location:\n",
    "        lb_regions.append(location)\n",
    "\n",
    "    # Add comment for region\n",
    "    region_display = region_names.get(location, location)\n",
    "    env_content += f\"# Region {idx} ({region_display}) - {foundry_name}\\n\"\n",
    "\n",
    "    # Process each model in this foundry\n",
    "    for model_name in models:\n",
    "        # Normalize model name for env var (replace hyphens with underscores, uppercase)\n",
    "        model_var = model_name.upper().replace('-', '_').replace('.', '_')\n",
    "\n",
    "        # Add endpoint and key for this model in this region\n",
    "        env_content += f\"MODEL_{model_var}_ENDPOINT_R{idx}={endpoint}\\n\"\n",
    "        env_content += f\"MODEL_{model_var}_KEY_R{idx}={key}\\n\"\n",
    "\n",
    "        # Track gpt-4o-mini endpoints for load balancing\n",
    "        if model_name == 'gpt-4o-mini' and endpoint:\n",
    "            lb_endpoints.append(endpoint)\n",
    "\n",
    "    env_content += \"\\n\"\n",
    "\n",
    "# Add load balancing configuration\n",
    "env_content += \"# Load Balancing Configuration\\n\"\n",
    "env_content += f\"LB_REGIONS={','.join(lb_regions)}\\n\"\n",
    "env_content += f\"LB_GPT4O_MINI_ENDPOINTS={','.join(lb_endpoints)}\\n\"\n",
    "env_content += f\"LB_ENABLED={'true' if len(lb_endpoints) > 1 else 'false'}\\n\"\n",
    "env_content += \"\\n\"\n",
    "\n",
    "# Continue with supporting services\n",
    "env_content += f\"\"\"# ===========================================\n",
    "# Supporting Services\n",
    "# ===========================================\n",
    "\n",
    "# Redis (Semantic Caching)\n",
    "REDIS_HOST={step3_outputs.get('redisCacheHost', '')}\n",
    "REDIS_PORT={step3_outputs.get('redisCachePort', 10000)}\n",
    "REDIS_KEY={step3_outputs.get('redisCacheKey', '')}\n",
    "\n",
    "# Azure Cognitive Search\n",
    "SEARCH_SERVICE_NAME={step3_outputs.get('searchServiceName', '')}\n",
    "SEARCH_ENDPOINT={step3_outputs.get('searchServiceEndpoint', '')}\n",
    "SEARCH_ADMIN_KEY={step3_outputs.get('searchServiceAdminKey', '')}\n",
    "\n",
    "# Cosmos DB\n",
    "COSMOS_ACCOUNT_NAME={step3_outputs.get('cosmosDbAccountName', '')}\n",
    "COSMOS_ENDPOINT={step3_outputs.get('cosmosDbEndpoint', '')}\n",
    "COSMOS_KEY={step3_outputs.get('cosmosDbKey', '')}\n",
    "\n",
    "# Content Safety\n",
    "CONTENT_SAFETY_ENDPOINT={step3_outputs.get('contentSafetyEndpoint', '')}\n",
    "CONTENT_SAFETY_KEY={step3_outputs.get('contentSafetyKey', '')}\n",
    "\n",
    "# ===========================================\n",
    "# MCP Servers\n",
    "# ===========================================\n",
    "# OpenWeather API Key (free tier)\n",
    "# Get yours at: https://openweathermap.org/api (sign up for free, use \"Current Weather\" API)\n",
    "OWM_API_KEY=YOUR_OPENWEATHER_API_KEY_HERE\n",
    "\n",
    "CONTAINER_REGISTRY={step4_outputs.get('containerRegistryLoginServer', '')}\n",
    "CONTAINER_APP_ENV_ID={step4_outputs.get('containerAppEnvId', '')}\n",
    "\"\"\"\n",
    "\n",
    "# Add MCP server URLs (safe handling if not present)\n",
    "mcp_urls = step4_outputs.get('mcpServerUrls', []) if isinstance(step4_outputs, dict) else []\n",
    "for mcp_server in mcp_urls:  # FIXED: Changed from 'mcp' to 'mcp_server' to avoid overwriting global mcp variable\n",
    "    # Guard against missing fields\n",
    "    name = mcp_server.get('name') if isinstance(mcp_server, dict) else None\n",
    "    url = mcp_server.get('url') if isinstance(mcp_server, dict) else None\n",
    "    if name and url:\n",
    "        var_name = f\"MCP_SERVER_{name.upper().replace('-', '_')}_URL\"\n",
    "        env_content += f\"{var_name}={url}\\n\"\n",
    "\n",
    "env_content += f\"\"\"\n",
    "# ===========================================\n",
    "# Deployment Info\n",
    "# ===========================================\n",
    "RESOURCE_GROUP={resource_group_name}\n",
    "LOCATION={location}\n",
    "DEPLOYMENT_PREFIX={deployment_name_prefix}\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "env_file = NOTEBOOK_DIR / 'master-lab.env'  # Use absolute path from Cell 004\n",
    "with open(str(env_file), 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "# CRITICAL: Load the env file immediately into os.environ\n",
    "# This ensures subsequent cells can access the variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(str(env_file), override=True)\n",
    "print(f\"[OK] Loaded environment variables into os.environ\")\n",
    "\n",
    "print(f'[OK] Created {env_file}')\n",
    "print(f'[OK] File location: {os.path.abspath(env_file)}')\n",
    "\n",
    "# Display summary of model deployments\n",
    "if foundries_data:\n",
    "    print()\n",
    "    print('[*] Model Deployment Summary:')\n",
    "    for idx, foundry_info in enumerate(foundries_data, 1):\n",
    "        if isinstance(foundry_info, dict):\n",
    "            location = foundry_info.get('location', 'unknown')\n",
    "            models = foundry_info.get('models', [])\n",
    "            region_display = region_names.get(location, location)\n",
    "            print(f'  Region {idx} ({region_display}): {len(models)} models')\n",
    "            for model in models:\n",
    "                print(f'    - {model}')\n",
    "\n",
    "# Display load balancing info\n",
    "if len(lb_endpoints) > 1:\n",
    "    print()\n",
    "    print(f'[OK] Load Balancing: ENABLED ({len(lb_endpoints)} regions)')\n",
    "    print(f'[OK] LB Regions: {\", \".join(lb_regions)}')\n",
    "else:\n",
    "    print()\n",
    "    print('[!] Load Balancing: Disabled (requires 2+ regions with gpt-4o-mini)')\n",
    "\n",
    "print()\n",
    "print('[OK] You can now load this in all lab tests:')\n",
    "print('  from dotenv import load_dotenv')\n",
    "print('  load_dotenv(\"master-lab.env\")')\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('SETUP COMPLETE - ALL LABS READY')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reload-config\"></a>\n",
    "\n",
    "## 0.8 Reload Complete Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apim_vars_definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "[APIM & API Variables Defined]\n",
      "  apim_gateway_url: https://apim-c7uj6vzppah74.azure-api.net...\n",
      "  apim_api_key: ****3b91\n",
      "  inference_api_path: inference\n",
      "  inference_api_version: 2024-08-01-preview\n",
      "  deployment_name: gpt-4o-mini\n",
      "  api_key: ****3b91\n"
     ]
    }
   ],
   "source": [
    "# Load complete configuration from master-lab.env\n",
    "# This cell must be run AFTER Cell 021 generates the env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use NOTEBOOK_DIR from Cell 004 (or detect it again)\n",
    "if 'NOTEBOOK_DIR' not in globals():\n",
    "    # Fallback: detect notebook directory again\n",
    "    NOTEBOOK_DIR = None\n",
    "    if Path('bootstrap.env').exists() or Path('master-lab.env').exists():\n",
    "        NOTEBOOK_DIR = Path.cwd()\n",
    "    else:\n",
    "        known_path = Path(r'C:\\Users\\lproux\\Documents\\GitHub\\MCP-servers-internalMSFT-and-external\\AI-Gateway\\labs\\master-lab')\n",
    "        if known_path.exists():\n",
    "            NOTEBOOK_DIR = known_path\n",
    "            os.chdir(NOTEBOOK_DIR)\n",
    "    \n",
    "    if NOTEBOOK_DIR is None:\n",
    "        raise ValueError(\"Cannot locate notebook directory. Please run Cell 004 first.\")\n",
    "    \n",
    "    print(f\"[INFO] Detected notebook directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Load master-lab.env into environment variables using absolute path\n",
    "env_file = NOTEBOOK_DIR / 'master-lab.env'\n",
    "\n",
    "if env_file.exists():\n",
    "    load_dotenv(str(env_file), override=True)\n",
    "    print(f'[OK] Loaded: {env_file}')\n",
    "else:\n",
    "    print(f'[WARN] File not found: {env_file}')\n",
    "    print('       Run Cell 021 to generate master-lab.env')\n",
    "    print('       Some cells may fail without environment variables.')\n",
    "\n",
    "# APIM Variable Definitions (for cells that use lowercase names)\n",
    "# These map environment variables to lowercase snake_case for backwards compatibility\n",
    "\n",
    "# APIM Gateway URLs\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "apim_resource_gateway_url = apim_gateway_url  # Same as gateway URL\n",
    "apim_api_key = os.environ.get('APIM_API_KEY', '')\n",
    "\n",
    "# Azure OpenAI API Configuration\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "inference_api_version = '2024-08-01-preview'  # Azure OpenAI API version\n",
    "api_key = apim_api_key  # Alias for backward compatibility\n",
    "\n",
    "# Model deployment (default to gpt-4o-mini for cost efficiency)\n",
    "deployment_name = 'gpt-4o-mini'\n",
    "\n",
    "# Display for verification\n",
    "print('[APIM & API Variables Defined]')\n",
    "print(f'  apim_gateway_url: {apim_gateway_url[:50]}...' if apim_gateway_url else '  apim_gateway_url: NOT SET')\n",
    "print(f'  apim_api_key: ****{apim_api_key[-4:]}' if len(apim_api_key) > 4 else '  apim_api_key: NOT SET')\n",
    "print(f'  inference_api_path: {inference_api_path}')\n",
    "print(f'  inference_api_version: {inference_api_version}')\n",
    "print(f'  deployment_name: {deployment_name}')\n",
    "print(f'  api_key: ****{api_key[-4:]}' if len(str(api_key)) > 4 else '  api_key: NOT SET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d04e7",
   "metadata": {},
   "source": [
    "<a id=\"access-control\"></a>\n",
    "\n",
    "## 0.9 Access Controlling\n",
    "\n",
    "#### Objective\n",
    "Implement OAuth 2.0 based access control to restrict API access by user or client. This lab demonstrates how to use Azure AD (Entra ID) as an identity provider for fine-grained authorization on Azure OpenAI models through APIM.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **OAuth 2.0 Authorization:** Configure identity provider-based authentication\n",
    "- **Token Acquisition:** Request tokens from Azure AD for authenticated API calls\n",
    "- **Bearer Tokens:** Include tokens in API requests for authorization\n",
    "- **Access Scopes:** Define granular permissions for different API endpoints\n",
    "- **Token Expiration:** Handle token refresh and expiration scenarios\n",
    "- **Troubleshooting:** Debug 401/403 errors and policy propagation delays\n",
    "\n",
    "#### How It Works\n",
    "1. Client application requests OAuth token from Azure AD\n",
    "2. Azure AD validates credentials and returns access token\n",
    "3. Client includes token in Authorization header (Bearer token)\n",
    "4. APIM policy validates token with Azure AD\n",
    "5. Policy checks token scope against API requirements\n",
    "6. Authorized requests proceed to backend Azure OpenAI\n",
    "7. Unauthorized requests return 403 Forbidden\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "- Azure AD application registration (created during deployment)\n",
    "\n",
    "#### Expected Results\n",
    "- Successful authentication with valid OAuth token\n",
    "- Requests with invalid/missing tokens receive 401 Unauthorized\n",
    "- Token-based access control enforced at APIM level\n",
    "- Can observe policy evaluation in APIM tracing\n",
    "- Different users can have different access levels\n",
    "- Token expiration properly handled with refresh\n",
    "\n",
    "#### Common Issues & Solutions\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| 401 Unauthorized | Wait 30-60 seconds for policy to propagate |\n",
    "| 500 Internal Server Error | Check backend health with Azure CLI |\n",
    "| Token not found | Run `az login` to authenticate |\n",
    "| Missing API Key | Verify APIM_API_KEY in environment variables |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Requisite: Azure CLI Authentication\n",
    "Access Control workshop requires Azure CLI to be logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd10 Checking Azure CLI authentication...\n",
      "\u2705 Logged in as: lproux@microsoft.com\n",
      "   Tenant: 2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "   Subscription: ME-MngEnvMCAP592090-lproux-1\n"
     ]
    }
   ],
   "source": [
    "# Ensure Azure CLI is logged in (required for Access Control workshop)\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"\ud83d\udd10 Checking Azure CLI authentication...\")\n",
    "\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "\n",
    "# Try to get current account\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [az_cli, 'account', 'show'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        import json\n",
    "        account = json.loads(result.stdout)\n",
    "        print(f\"\u2705 Logged in as: {account.get('user', {}).get('name', 'Unknown')}\")\n",
    "        print(f\"   Tenant: {account.get('tenantId', 'Unknown')}\")\n",
    "        print(f\"   Subscription: {account.get('name', 'Unknown')}\")\n",
    "    else:\n",
    "        print(\"\u274c Azure CLI not logged in\")\n",
    "        print(\"\\nPlease run ONE of the following:\")\n",
    "        print(\"\\n1. In a terminal window:\")\n",
    "        print(\"   az login\")\n",
    "        print(\"\\n2. In a Jupyter cell:\")\n",
    "        print(\"   !az login\")\n",
    "        print(\"\\n3. Use device code (if browser not available):\")\n",
    "        print(\"   !az login --use-device-code\")\n",
    "        raise RuntimeError(\"Azure login required. Run 'az login' in a terminal.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"\u274c Azure CLI not found at: {az_cli}\")\n",
    "    print(\"\\nPlease install Azure CLI:\")\n",
    "    print(\"  https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\")\n",
    "    raise\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"\u26a0\ufe0f Azure CLI timeout - trying to continue anyway\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Could not verify Azure login: {e}\")\n",
    "    print(\"   Continuing anyway...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Control Tests\n",
    "\n",
    "The following cells demonstrate token acquisition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83e\uddea TEST 1: No Authentication\n",
      "================================================================================\n",
      "\n",
      "Attempting API call WITHOUT any authentication...\n",
      "Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\n",
      "\u2705 EXPECTED: Request failed with 401 Unauthorized\n",
      "Error: Error code: 401 - {'statusCode': 401, 'message': 'Access denied due to invalid subscription key. Make sure to provide a valid key for an active subscription.'}\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: No Authentication (should fail with 401)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83e\uddea TEST 1: No Authentication\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "print(f\"\\nAttempting API call WITHOUT any authentication...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy-key\",  # Not used\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    # Try to call WITHOUT auth headers\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        extra_headers={}  # NO auth headers\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\u274c UNEXPECTED: Request succeeded without auth!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if '401' in error_msg or 'Unauthorized' in error_msg or 'JWT' in error_msg:\n",
    "        print(\"\\n\u2705 EXPECTED: Request failed with 401 Unauthorized\")\n",
    "        print(f\"Error: {error_msg[:200]}\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f UNEXPECTED ERROR: {error_msg[:200]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: JWT Token Authentication (Azure AD)\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates OAuth 2.0 authentication using Azure Active Directory (Entra ID) bearer tokens\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Azure CLI authenticated (`az login` completed)\n",
    "- APIM service deployed with JWT validation policy active\n",
    "- Environment variables loaded from `master-lab.env`:\n",
    "  - `APIM_GATEWAY_URL`\n",
    "  - `RESOURCE_GROUP`\n",
    "  - `APIM_SERVICE`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**This test demonstrates enterprise-grade authentication using Azure AD:**\n",
    "\n",
    "1. **Acquires Azure AD Token:**\n",
    "   - Uses Azure CLI to obtain a bearer token\n",
    "   - Token is scoped to Azure Cognitive Services\n",
    "   - Token represents the authenticated user's identity\n",
    "   - Token has limited lifetime (typically 1 hour)\n",
    "\n",
    "2. **Makes Authenticated API Request:**\n",
    "   - Sends POST request to APIM gateway\n",
    "   - Includes JWT token in Authorization header: `Bearer <token>`\n",
    "   - Requests a simple chat completion from GPT-4\n",
    "   - Does NOT include API subscription key\n",
    "\n",
    "3. **APIM Policy Validation:**\n",
    "   - APIM validates the JWT token signature\n",
    "   - Checks token issuer, audience, and claims\n",
    "   - Ensures token hasn't expired\n",
    "   - Allows request if token is valid, returns 401 if invalid\n",
    "\n",
    "4. **Response Handling:**\n",
    "   - Displays the AI-generated response if successful\n",
    "   - Shows error details if authentication fails\n",
    "   - Measures and displays response time\n",
    "\n",
    "**Security Benefits:**\n",
    "- No static API keys transmitted\n",
    "- Tokens are short-lived and rotatable\n",
    "- User identity can be tracked for audit logs\n",
    "- Fine-grained access control via Azure AD roles\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "When JWT policy is active:\n",
    "\u2705 **SUCCESS** - Request succeeds with 200 status code\n",
    "- Bearer token successfully validated by APIM\n",
    "- AI model response returned\n",
    "- Response time displayed\n",
    "\n",
    "When JWT policy is NOT active or token is invalid:\n",
    "\u274c **FAIL** - Request fails with 401 Unauthorized\n",
    "- Error message indicating authentication failure\n",
    "- Details about token validation error\n",
    "\n",
    "**Note**: JWT validation requires specific APIM policy configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "\ud83d\udcdd APPLY: JWT Only Policy (disable subscriptionRequired)\n",
      "================================================================================\n",
      "[1] Current subscriptionRequired: True\n",
      "[2] \u2713 Disabled subscriptionRequired for 'inference-api'\n",
      "\n",
      "[3] Applying JWT policy...\n",
      "[4] Policy Status: 200 - \u2713 SUCCESS\n",
      "\n",
      "\u2713 JWT policy applied with multi-issuer support\n",
      "\u23f3 Waiting 60 seconds for propagation...\n",
      "\u2713 Ready for testing\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"\ud83d\udcdd APPLY: JWT Only Policy (disable subscriptionRequired)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# STEP 1: Disable subscription requirement for pure JWT auth\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        api_config = response.json()\n",
    "        current_subscription_required = api_config.get('properties', {}).get('subscriptionRequired', False)\n",
    "        \n",
    "        print(f\"[1] Current subscriptionRequired: {current_subscription_required}\")\n",
    "        \n",
    "        if current_subscription_required:\n",
    "            # Disable subscription requirement\n",
    "            api_config['properties']['subscriptionRequired'] = False\n",
    "            \n",
    "            update_response = requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "            \n",
    "            if update_response.status_code in [200, 201]:\n",
    "                print(f\"[2] \u2713 Disabled subscriptionRequired for '{api_id}'\")\n",
    "            else:\n",
    "                print(f\"[2] \u2717 Failed: {update_response.status_code}\")\n",
    "        else:\n",
    "            print(f\"[2] \u2713 subscriptionRequired already disabled\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] {str(e)}\")\n",
    "\n",
    "# STEP 2: Apply JWT policy with v1.0 + v2.0 issuer support\n",
    "print(f\"\\n[3] Applying JWT policy...\")\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID\")\n",
    "else:\n",
    "    # JWT policy - CRITICAL: correct element order (openid-config, audiences, issuers)\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "    \n",
    "    try:\n",
    "        policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "        \n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "        \n",
    "        print(f\"[4] Policy Status: {response.status_code} - {'\u2713 SUCCESS' if response.status_code in [200, 201] else '\u2717 FAILED'}\")\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"\\n\u2713 JWT policy applied with multi-issuer support\")\n",
    "            print(f\"\u23f3 Waiting 60 seconds for propagation...\")\n",
    "            time.sleep(60)\n",
    "            print(f\"\u2713 Ready for testing\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: JWT Token Only (Works When JWT Policy Active)\n",
    "\n",
    "\n",
    "**Purpose**: Tests pure JWT authentication without API key fallback\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Same as Test 2 (JWT Token Authentication)\n",
    "- JWT validation policy must be configured in APIM\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "This is a variant of Test 2 that explicitly tests JWT-only authentication:\n",
    "\n",
    "1. Acquires a fresh Azure AD token\n",
    "2. Sends request with ONLY the Authorization header (no API key)\n",
    "3. Validates that APIM properly enforces JWT authentication\n",
    "4. Useful for testing policy configurations\n",
    "\n",
    "**Difference from Test 2:**\n",
    "- Test 2 may include compatibility logic for dual auth\n",
    "- Test 3 explicitly tests JWT-only scenarios\n",
    "- Test 3 helps verify policy isolation\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Same as Test 2, but more strict:\n",
    "- \u2705 Succeeds ONLY when JWT policy is properly configured\n",
    "- \u274c Fails if JWT policy is missing or misconfigured\n",
    "- Helps diagnose policy application issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83e\uddea TEST 3: JWT Token Authentication\n",
      "================================================================================\n",
      "\n",
      "[1] Acquiring JWT token...\n",
      "\u2705 JWT Token: eyJ0eXAiOiJKV1QiLCJh...4QUgauaCAA\n",
      "\n",
      "[2] Calling API with JWT token only (no API key)...\n",
      "Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\n",
      "\u2705 SUCCESS: JWT Authentication Working!\n",
      "Response: JWT auth successful!\n",
      "Tokens: 18\n"
     ]
    }
   ],
   "source": [
    "# TEST 3: JWT Token Only (works when JWT policy active)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83e\uddea TEST 3: JWT Token Authentication\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Get JWT token\n",
    "print(\"\\n[1] Acquiring JWT token...\")\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(f\"\u2705 JWT Token: {jwt_token[:20]}...{jwt_token[-10:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to get JWT: {e}\")\n",
    "    print(\"   Run: az login\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n[2] Calling API with JWT token only (no API key)...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy-not-used\",  # Ignored when JWT provided\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'JWT auth successful!'\"}],\n",
    "        max_tokens=50,\n",
    "        extra_headers={\n",
    "            \"Authorization\": f\"Bearer {jwt_token}\"  # JWT only, no API key\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\u2705 SUCCESS: JWT Authentication Working!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if 'api-key' in error_msg.lower() or ('401' in error_msg and 'API' in error_msg):\n",
    "        print(\"\\n\u274c FAILED: API requires API Key in addition to JWT\")\n",
    "        print(\"   Current policy may be Dual Auth or API Key only\")\n",
    "        print(\"   Run Cell 028 to enable JWT-only mode\")\n",
    "    else:\n",
    "        print(f\"\\n\u274c ERROR: {error_msg[:300]}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Dual Authentication (JWT + API Key)\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates using both JWT token and API subscription key simultaneously\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Azure CLI authenticated\n",
    "- APIM subscription key available\n",
    "- APIM configured to accept both authentication methods\n",
    "- Environment variables from `master-lab.env`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Tests the most secure authentication pattern: defense in depth**\n",
    "\n",
    "1. **Acquires Both Credentials:**\n",
    "   - JWT bearer token from Azure AD\n",
    "   - API subscription key from environment\n",
    "\n",
    "2. **Sends Dual-Authenticated Request:**\n",
    "   - Authorization header: `Bearer <jwt_token>`\n",
    "   - Ocp-Apim-Subscription-Key header: `<api_key>`\n",
    "   - Both credentials included in same request\n",
    "\n",
    "3. **APIM Policy Behavior:**\n",
    "   - Can validate EITHER credential (OR logic)\n",
    "   - Can require BOTH credentials (AND logic)\n",
    "   - Configuration depends on policy design\n",
    "\n",
    "4. **Use Cases:**\n",
    "   - Migration scenarios (supporting both old and new auth)\n",
    "   - Defense in depth (multiple auth layers)\n",
    "   - Different auth for different operations\n",
    "   - Compliance requirements\n",
    "\n",
    "**Best Practice:**\n",
    "Dual authentication provides maximum flexibility during transitions but should eventually converge to JWT-only for enterprise scenarios.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "\u2705 **SUCCESS** - Request succeeds with both credentials\n",
    "- APIM validates both authentication methods\n",
    "- Response shows successful authorization\n",
    "- Useful for migration and compliance scenarios\n",
    "\n",
    "**Security Note:**\n",
    "In production, choose one primary authentication method for consistency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "\ud83d\udcdd APPLY: Dual Auth (JWT + API Key)\n",
      "================================================================================\n",
      "[auth] Resolved tenant_id: 2b9d9f47-1fb6-400a-a438-39fe7d768649\n",
      "\ud83d\udcdd Policy Applied: Dual Auth (JWT + API Key)\n",
      "Status: 200 - \u2713 SUCCESS\n",
      "Policy requires BOTH:\n",
      "  \u2022 Valid JWT token (Authorization header)\n",
      "  \u2022 Valid API key (api-key header)\n",
      "\u23f3 Waiting 60 seconds for policy to propagate...\n",
      "   60 seconds remaining...   59 seconds remaining...   58 seconds remaining...   57 seconds remaining...   56 seconds remaining...   55 seconds remaining...   54 seconds remaining...   53 seconds remaining...   52 seconds remaining...   51 seconds remaining...   50 seconds remaining...   49 seconds remaining...   48 seconds remaining...   47 seconds remaining...   46 seconds remaining...   45 seconds remaining...   44 seconds remaining...   43 seconds remaining...   42 seconds remaining...   41 seconds remaining...   40 seconds remaining...   39 seconds remaining...   38 seconds remaining...   37 seconds remaining...   36 seconds remaining...   35 seconds remaining...   34 seconds remaining...   33 seconds remaining...   32 seconds remaining...   31 seconds remaining...   30 seconds remaining...   29 seconds remaining...   28 seconds remaining...   27 seconds remaining...   26 seconds remaining...   25 seconds remaining...   24 seconds remaining...   23 seconds remaining...   22 seconds remaining...   21 seconds remaining...   20 seconds remaining...   19 seconds remaining...   18 seconds remaining...   17 seconds remaining...   16 seconds remaining...   15 seconds remaining...   14 seconds remaining...   13 seconds remaining...   12 seconds remaining...   11 seconds remaining...   10 seconds remaining...   9 seconds remaining...   8 seconds remaining...   7 seconds remaining...   6 seconds remaining...   5 seconds remaining...   4 seconds remaining...   3 seconds remaining...   2 seconds remaining...   1 seconds remaining...\u2713 Policy propagation complete!\n",
      "\ud83d\udca1 TIP: Run Cell 65 to test Dual Auth\n"
     ]
    }
   ],
   "source": [
    "import requests, os, subprocess, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"\ud83d\udcdd APPLY: Dual Auth (JWT + API Key)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get tenant ID\n",
    "az_cli = os.environ.get('AZ_CLI', 'az')\n",
    "result = subprocess.run(\n",
    "    [az_cli, 'account', 'show', '--query', 'tenantId', '-o', 'tsv'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "tenant_id = result.stdout.strip() if result.returncode == 0 else os.environ.get('AZURE_TENANT_ID', '')\n",
    "\n",
    "if not tenant_id:\n",
    "    print(\"[ERROR] Cannot resolve tenant ID. Ensure az login completed.\")\n",
    "else:\n",
    "    print(f\"[auth] Resolved tenant_id: {tenant_id}\")\n",
    "\n",
    "    # Dual Auth policy - BOTH JWT validation AND API key check\n",
    "    policy_xml = f\"\"\"<policies>\n",
    "        <inbound>\n",
    "            <base />\n",
    "            <validate-jwt header-name=\"Authorization\" failed-validation-httpcode=\"401\" require-expiration-time=\"true\" require-signed-tokens=\"true\">\n",
    "                <openid-config url=\"https://login.microsoftonline.com/{tenant_id}/v2.0/.well-known/openid-configuration\" />\n",
    "                <audiences>\n",
    "                    <audience>https://cognitiveservices.azure.com</audience>\n",
    "                </audiences>\n",
    "                <issuers>\n",
    "                    <issuer>https://sts.windows.net/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/</issuer>\n",
    "                    <issuer>https://login.microsoftonline.com/{tenant_id}/v2.0</issuer>\n",
    "                </issuers>\n",
    "            </validate-jwt>\n",
    "            <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing API key\" />\n",
    "            <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "        </inbound>\n",
    "        <backend><base /></backend>\n",
    "        <outbound><base /></outbound>\n",
    "        <on-error><base /></on-error>\n",
    "    </policies>\"\"\"\n",
    "\n",
    "    # Apply policy\n",
    "    try:\n",
    "        url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        body = {\n",
    "            \"properties\": {\n",
    "                \"value\": policy_xml,\n",
    "                \"format\": \"xml\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "        print(f\"\ud83d\udcdd Policy Applied: Dual Auth (JWT + API Key)\")\n",
    "        print(f\"Status: {response.status_code} - {'\u2713 SUCCESS' if response.status_code in [200, 201] else '\u2717 FAILED'}\")\n",
    "\n",
    "        if response.status_code not in [200, 201]:\n",
    "            print(f\"Error: {response.text[:500]}\")\n",
    "        else:\n",
    "            print(\"Policy requires BOTH:\")\n",
    "            print(\"  \u2022 Valid JWT token (Authorization header)\")\n",
    "            print(\"  \u2022 Valid API key (api-key header)\")\n",
    "\n",
    "            print(\"\u23f3 Waiting 60 seconds for policy to propagate...\")\n",
    "            for i in range(60, 0, -1):\n",
    "                print(f\"   {i} seconds remaining...\", end='')\n",
    "                time.sleep(1)\n",
    "            print(\"\u2713 Policy propagation complete!\")\n",
    "            print(\"\ud83d\udca1 TIP: Run Cell 65 to test Dual Auth\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Policy application failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Dual Authentication (JWT + API Key) - Alternative Implementation\n",
    "\n",
    "\n",
    "**Purpose**: Alternative test implementation for dual authentication with enhanced error handling\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Same as Test 4\n",
    "- Enhanced error reporting\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "This is an alternative implementation of dual authentication testing with:\n",
    "- Better error handling and reporting\n",
    "- More detailed logging\n",
    "- Token refresh logic\n",
    "- Timeout handling\n",
    "\n",
    "Functionally equivalent to Test 4 but with production-ready error handling.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Same as Test 4, with additional diagnostic information about:\n",
    "- Token acquisition status\n",
    "- Policy validation details\n",
    "- Detailed error messages if authentication fails\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83e\uddea TEST 4: Dual Authentication (JWT + API Key)\n",
      "================================================================================\n",
      "\n",
      "[1] Acquiring JWT token...\n",
      "\u2705 JWT Token: eyJ0eXAiOiJKV1QiLCJh...PLkfJjADCA\n",
      "\n",
      "[2] Calling API with BOTH JWT and API Key...\n",
      "Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "API Key: 8c12a93f8d...3b91\n",
      "\n",
      "\u2705 SUCCESS: Dual Authentication Working!\n",
      "Response: Dual auth successful!\n",
      "Tokens: 18\n",
      "\n",
      "\ud83c\udf89 Both JWT and API Key validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# TEST 4: Dual Authentication (JWT + API Key)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83e\uddea TEST 4: Dual Authentication (JWT + API Key)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Get JWT token\n",
    "print(\"\\n[1] Acquiring JWT token...\")\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(f\"\u2705 JWT Token: {jwt_token[:20]}...{jwt_token[-10:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to get JWT: {e}\")\n",
    "    raise\n",
    "\n",
    "if not apim_api_key:\n",
    "    print(\"\u274c APIM_API_KEY not set\")\n",
    "    raise ValueError(\"APIM_API_KEY required\")\n",
    "\n",
    "print(f\"\\n[2] Calling API with BOTH JWT and API Key...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"API Key: {apim_api_key[:10]}...{apim_api_key[-4:]}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy\",\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'Dual auth successful!'\"}],\n",
    "        max_tokens=50,\n",
    "        extra_headers={\n",
    "            \"Authorization\": f\"Bearer {jwt_token}\",  # JWT token\n",
    "            \"api-key\": apim_api_key  # API Key\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\u2705 SUCCESS: Dual Authentication Working!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    print(\"\\n\ud83c\udf89 Both JWT and API Key validated successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c FAILED: {str(e)[:300]}\")\n",
    "    print(\"\\nMake sure Cell 030 (Dual Auth policy) was applied\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: API Key Only (Works When API Key Policy Active)\n",
    "\n",
    "\n",
    "**Purpose**: Tests traditional API key-based authentication without JWT token\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- APIM subscription key from `master-lab.env`\n",
    "- API key validation policy configured in APIM\n",
    "- No Azure CLI login required (key-based auth is standalone)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Tests the traditional API Management authentication method:**\n",
    "\n",
    "1. **Uses API Subscription Key Only:**\n",
    "   - No JWT token acquired\n",
    "   - Relies solely on `Ocp-Apim-Subscription-Key` header\n",
    "   - Simpler authentication pattern\n",
    "\n",
    "2. **Makes API Request:**\n",
    "   - POST to APIM gateway\n",
    "   - Only subscription key for authentication\n",
    "   - Requests chat completion\n",
    "\n",
    "3. **APIM Validation:**\n",
    "   - Checks if subscription key is valid\n",
    "   - Checks if subscription is active\n",
    "   - Checks rate limits and quotas\n",
    "   - Allows access if key is valid\n",
    "\n",
    "4. **When to Use API Keys:**\n",
    "   - Simple applications without Azure AD\n",
    "   - Legacy applications\n",
    "   - Development and testing\n",
    "   - Non-enterprise scenarios\n",
    "\n",
    "**Security Considerations:**\n",
    "- API keys are long-lived (don't expire automatically)\n",
    "- Keys must be rotated manually\n",
    "- Keys should be stored securely (Key Vault, environment variables)\n",
    "- Less secure than JWT tokens for enterprise use\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "When API key policy is active:\n",
    "\u2705 **SUCCESS** - Request succeeds\n",
    "- API key validated by APIM\n",
    "- Chat completion response returned\n",
    "- Simpler than JWT authentication\n",
    "\n",
    "When API key policy is NOT active:\n",
    "\u274c **FAIL** - Request fails with 401\n",
    "- APIM requires different authentication method\n",
    "\n",
    "**Note**: This is the simplest authentication method but least secure for production.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "\ud83d\udd04 RESET: API-KEY Authentication (for remaining labs)\n",
      "================================================================================\n",
      "[1] \u2713 Re-enabled subscriptionRequired for API-KEY authentication\n",
      "[2] Policy Reset: API-KEY Only\n",
      "    Status: 200 - \u2713 SUCCESS\n",
      "\u23f3 Waiting 30 seconds for policy to propagate...\n",
      "   30 seconds remaining...   29 seconds remaining...   28 seconds remaining...   27 seconds remaining...   26 seconds remaining...   25 seconds remaining...   24 seconds remaining...   23 seconds remaining...   22 seconds remaining...   21 seconds remaining...   20 seconds remaining...   19 seconds remaining...   18 seconds remaining...   17 seconds remaining...   16 seconds remaining...   15 seconds remaining...   14 seconds remaining...   13 seconds remaining...   12 seconds remaining...   11 seconds remaining...   10 seconds remaining...   9 seconds remaining...   8 seconds remaining...   7 seconds remaining...   6 seconds remaining...   5 seconds remaining...   4 seconds remaining...   3 seconds remaining...   2 seconds remaining...   1 seconds remaining...\u2713 Policy reset complete!\n",
      "\ud83d\udca1 All remaining labs will use API-KEY authentication\n"
     ]
    }
   ],
   "source": [
    "import requests, os, time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\"\ud83d\udd04 RESET: API-KEY Authentication (for remaining labs)\")\n",
    "print(\"=\"*80 + \"\")\n",
    "\n",
    "# Configuration\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "# Get management token\n",
    "credential = DefaultAzureCredential()\n",
    "mgmt_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {mgmt_token.token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Re-enable subscription requirement (for API-KEY authentication)\n",
    "api_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}?api-version=2022-08-01\"\n",
    "\n",
    "response = requests.get(api_url, headers=headers, timeout=30)\n",
    "if response.status_code == 200:\n",
    "    api_config = response.json()\n",
    "    api_config['properties']['subscriptionRequired'] = True\n",
    "    requests.put(api_url, headers=headers, json=api_config, timeout=30)\n",
    "    print(\"[1] \u2713 Re-enabled subscriptionRequired for API-KEY authentication\")\n",
    "\n",
    "# Apply simple API-KEY only policy\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend><base /></backend>\n",
    "    <outbound><base /></outbound>\n",
    "    <on-error><base /></on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "body = {\"properties\": {\"value\": policy_xml, \"format\": \"xml\"}}\n",
    "response = requests.put(policy_url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "print(f\"[2] Policy Reset: API-KEY Only\")\n",
    "print(f\"    Status: {response.status_code} - {'\u2713 SUCCESS' if response.status_code in [200, 201] else '\u2717 FAILED'}\")\n",
    "\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"\u23f3 Waiting 30 seconds for policy to propagate...\")\n",
    "    for i in range(30, 0, -1):\n",
    "        print(f\"   {i} seconds remaining...\", end='')\n",
    "        time.sleep(1)\n",
    "    print(\"\u2713 Policy reset complete!\")\n",
    "    print(\"\ud83d\udca1 All remaining labs will use API-KEY authentication\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: API Key Only (Alternative Position)\n",
    "\n",
    "\n",
    "**Purpose**: Alternative placement of API key authentication test\n",
    "\n",
    "\n",
    "**Requirements**:Same as Test 6 above\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "This is the same test as Test 6, positioned differently in the notebook flow.\n",
    "It demonstrates API key-only authentication without JWT tokens.\n",
    "\n",
    "See Test 6 documentation for full details.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:Same as Test 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83e\uddea TEST 2: API Key Authentication\n",
      "================================================================================\n",
      "\n",
      "Calling API with API Key only...\n",
      "Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "API Key: 8c12a93f8d...3b91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 SUCCESS: API Key Authentication Working!\n",
      "Response: API Key auth successful!\n",
      "Tokens: 20\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: API Key Only (works when API Key policy active)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83e\uddea TEST 2: API Key Authentication\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "if not apim_api_key:\n",
    "    print(\"\u274c APIM_API_KEY not set. Run Cell 022 to load environment.\")\n",
    "    raise ValueError(\"APIM_API_KEY required\")\n",
    "\n",
    "print(f\"\\nCalling API with API Key only...\")\n",
    "print(f\"Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"API Key: {apim_api_key[:10]}...{apim_api_key[-4:]}\")\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "        api_key=\"dummy\",  # The actual key goes in extra_headers\n",
    "        api_version=\"2024-08-01-preview\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'API Key auth successful!'\"}],\n",
    "        max_tokens=50,\n",
    "        extra_headers={\n",
    "            \"api-key\": apim_api_key  # API Key in header\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\u2705 SUCCESS: API Key Authentication Working!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if 'JWT' in error_msg or '401' in error_msg:\n",
    "        print(\"\\n\u274c FAILED: API requires JWT token\")\n",
    "        print(\"   Run Cell 041 to reset APIM to API Key mode\")\n",
    "    else:\n",
    "        print(f\"\\n\u274c ERROR: {error_msg[:300]}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "\n",
    "# Section 1: Core AI Gateway Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-1\"></a>\n",
    "\n",
    "## 1.1 Advanced Caching & Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "[*] Step 1: Creating Embeddings Backend in APIM...\n",
      "    APIM Service: apim-c7uj6vzppah74\n",
      "    Embedding Model: text-embedding-3-small\n",
      "    Endpoint: https://foundry1-c7uj6vzppah74.openai.azure.com/\n",
      "\n",
      "\u274c Failed to create embeddings backend\n",
      "   Error: ERROR: 'backend' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "ERROR: 'backend' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "\n",
      "   Command: az apim backend create \\\n",
      "        --service-name apim-c7uj6vzppah74 \\\n",
      "        --resource-group rg-master-lab-pavavy6pu5hpa \\\n",
      "        --backend-id embeddings-backend \\\n",
      "        --url 'https://foundry1-c7uj6vzppah74.openai.azure.comopenai/deployments/text-embedding-3-small/embeddings' \\\n",
      "        --protocol http \\\n",
      "        --description 'Embeddings Backend for Semantic Caching' \\\n",
      "        || az apim backend update \\\n",
      "        --service-name apim-c7uj6vzppah74 \\\n",
      "        --resource-group rg-master-lab-pavavy6pu5hpa \\\n",
      "        --backend-id embeddings-backend \\\n",
      "        --url 'https://foundry1-c7uj6vzppah74.openai.azure.comopenai/deployments/text-embedding-3-small/embeddings' \\\n",
      "        --protocol http \\\n",
      "        --description 'Embeddings Backend for Semantic Caching'\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 1: Configure Embeddings Backend\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - run Cell 021 first\")\n",
    "\n",
    "# Get required variables\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "embedding_endpoint_r1 = os.environ.get('MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1')\n",
    "\n",
    "if not all([apim_service_name, resource_group, embedding_endpoint_r1]):\n",
    "    print(\"[ERROR] Missing required environment variables\")\n",
    "    print(f\"APIM_SERVICE_NAME: {apim_service_name}\")\n",
    "    print(f\"RESOURCE_GROUP: {resource_group}\")\n",
    "    print(f\"Embedding Endpoint: {embedding_endpoint_r1}\")\n",
    "else:\n",
    "    print(\"\\n[*] Step 1: Creating Embeddings Backend in APIM...\")\n",
    "    print(f\"    APIM Service: {apim_service_name}\")\n",
    "    print(f\"    Embedding Model: text-embedding-3-small\")\n",
    "    print(f\"    Endpoint: {embedding_endpoint_r1}\")\n",
    "    \n",
    "    # Backend configuration\n",
    "    backend_id = \"embeddings-backend\"\n",
    "    backend_url = f\"{embedding_endpoint_r1.rstrip('/')}openai/deployments/text-embedding-3-small/embeddings\"\n",
    "    \n",
    "    import subprocess\n",
    "    import json\n",
    "    \n",
    "    # Check if backend already exists\n",
    "    check_cmd = f\"az apim api versionset list --service-name {apim_service_name} --resource-group {resource_group} || true\"\n",
    "    \n",
    "    # Create or update the embeddings backend\n",
    "    backend_config = {\n",
    "        \"url\": backend_url,\n",
    "        \"protocol\": \"http\",\n",
    "        \"description\": \"Text Embedding Backend for Semantic Caching\",\n",
    "        \"credentials\": {\n",
    "            \"header\": {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write backend config to temp file\n",
    "    backend_file = Path('backend-embeddings.json')\n",
    "    with open(backend_file, 'w') as f:\n",
    "        json.dump(backend_config, f, indent=2)\n",
    "    \n",
    "    # Create backend using Azure CLI\n",
    "    cmd = f\"\"\"az apim backend create \\\\\n",
    "        --service-name {apim_service_name} \\\\\n",
    "        --resource-group {resource_group} \\\\\n",
    "        --backend-id {backend_id} \\\\\n",
    "        --url '{backend_url}' \\\\\n",
    "        --protocol http \\\\\n",
    "        --description 'Embeddings Backend for Semantic Caching' \\\\\n",
    "        || az apim backend update \\\\\n",
    "        --service-name {apim_service_name} \\\\\n",
    "        --resource-group {resource_group} \\\\\n",
    "        --backend-id {backend_id} \\\\\n",
    "        --url '{backend_url}' \\\\\n",
    "        --protocol http \\\\\n",
    "        --description 'Embeddings Backend for Semantic Caching'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0 or \"already exists\" in result.stderr.lower():\n",
    "        print(f\"\\n\u2705 Embeddings backend '{backend_id}' configured successfully!\")\n",
    "        print(f\"   URL: {backend_url}\")\n",
    "        print(f\"\\n[OK] Step 1 Complete - Embeddings backend ready\")\n",
    "    else:\n",
    "        print(f\"\\n\u274c Failed to create embeddings backend\")\n",
    "        print(f\"   Error: {result.stderr}\")\n",
    "        print(f\"   Command: {cmd}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-1-1\"></a>\n",
    "\n",
    "### 1.1.1 Configure Embeddings Backend\n",
    "\n",
    "\n",
    "**Purpose**: Sets up the Azure OpenAI embeddings model required for semantic similarity caching\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Azure OpenAI deployment with text-embedding-ada-002 model\n",
    "- Environment variables: `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`\n",
    "- Redis cache deployed (from infrastructure step)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Prepares the embeddings backend for semantic caching:**\n",
    "\n",
    "1. **Configures Embeddings Model:**\n",
    "   - Sets up connection to Azure OpenAI embeddings endpoint\n",
    "   - Uses text-embedding-ada-002 model for vector generation\n",
    "   - Configures API credentials and version\n",
    "\n",
    "2. **Semantic Caching Concept:**\n",
    "   - Traditional caching: Exact string match required\n",
    "   - Semantic caching: Matches similar meaning\n",
    "   - Example: \"What's the weather?\" \u2248 \"How's the weather today?\"\n",
    "   - Uses cosine similarity of embeddings\n",
    "\n",
    "3. **Backend Components:**\n",
    "   - Azure OpenAI: Generates embeddings for queries\n",
    "   - Redis: Stores cached responses with embedding vectors\n",
    "   - APIM Policy: Orchestrates the caching logic\n",
    "\n",
    "This cell configures the embeddings backend that will be referenced by the semantic caching APIM policy.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "- Confirmation that embeddings backend is configured\n",
    "- Connection details to Azure OpenAI embeddings endpoint\n",
    "- Ready to apply semantic caching policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83d\udd27 APPLYING SEMANTIC CACHING POLICY (from notebook)\n",
      "================================================================================\n",
      "\n",
      "[*] Applying semantic caching policy to APIM...\n",
      "\n",
      "\u2705 Semantic caching policy applied successfully!\n",
      "\n",
      "\ud83d\udccb Policy Configuration:\n",
      "   - Similarity Threshold: 0.8 (80% match)\n",
      "   - Cache Duration: 1200s (20 minutes)\n",
      "   - Embeddings Backend: embeddings-backend\n",
      "   - Auth: API Key (from backend credentials)\n",
      "   - Backend Pool: inference-backend-pool\n",
      "\n",
      "\u23f3 Waiting 10 seconds for propagation...\n",
      "\u2705 Ready to test!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL TO ADD: Apply Semantic Caching Policy\n",
    "# Insert this cell BEFORE cell 53 (semantic caching test)\n",
    "# This applies the semantic caching policy directly in the notebook\n",
    "\n",
    "import os, subprocess, json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "apim_service_id = os.environ.get('APIM_SERVICE_ID')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udd27 APPLYING SEMANTIC CACHING POLICY (from notebook)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Policy WITHOUT embeddings-backend-auth (uses API key from backend config)\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\" failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <azure-openai-semantic-cache-lookup\n",
    "            score-threshold=\"0.8\"\n",
    "            embeddings-backend-id=\"embeddings-backend\" />\n",
    "        <set-backend-service backend-id=\"inference-backend-pool\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\"\n",
    "               condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n",
    "            <forward-request buffer-request-body=\"true\" />\n",
    "        </retry>\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "        <azure-openai-semantic-cache-store duration=\"1200\" />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "uri = f\"https://management.azure.com{apim_service_id}/apis/inference-api/policies/policy?api-version=2023-09-01-preview\"\n",
    "\n",
    "body = {\n",
    "    \"properties\": {\n",
    "        \"value\": policy_xml,\n",
    "        \"format\": \"xml\"\n",
    "    }\n",
    "}\n",
    "\n",
    "body_file = '/tmp/semantic-cache-from-notebook.json'\n",
    "with open(body_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(body, f, indent=2)\n",
    "\n",
    "print(\"\\n[*] Applying semantic caching policy to APIM...\")\n",
    "\n",
    "cmd = ['az', 'rest', '--method', 'put', '--uri', uri, '--body', f'@{body_file}']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\u2705 Semantic caching policy applied successfully!\\n\")\n",
    "    print(\"\ud83d\udccb Policy Configuration:\")\n",
    "    print(\"   - Similarity Threshold: 0.8 (80% match)\")\n",
    "    print(\"   - Cache Duration: 1200s (20 minutes)\")\n",
    "    print(\"   - Embeddings Backend: embeddings-backend\")\n",
    "    print(\"   - Auth: API Key (from backend credentials)\")\n",
    "    print(\"   - Backend Pool: inference-backend-pool\\n\")\n",
    "    print(\"\u23f3 Waiting 10 seconds for propagation...\")\n",
    "    import time\n",
    "    time.sleep(10)\n",
    "    print(\"\u2705 Ready to test!\\n\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Error applying policy:\")\n",
    "    print(result.stderr)\n",
    "    raise Exception(\"Failed to apply policy\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-1-2\"></a>\n",
    "\n",
    "### 1.1.2 Apply Caching Policy\n",
    "\n",
    "\n",
    "**Purpose**: Applies the semantic caching policy to the APIM API\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Redis cache deployed and accessible\n",
    "- Embeddings backend configured (previous cell)\n",
    "- APIM service and API deployed\n",
    "- Policy XML file available\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Applies the semantic caching policy to enable intelligent caching:**\n",
    "\n",
    "1. **Loads Policy XML:**\n",
    "   - Reads semantic caching policy from file\n",
    "   - Policy includes Redis connection strings\n",
    "   - Policy includes embeddings endpoint configuration\n",
    "   - Policy defines similarity threshold (e.g., 0.90 = 90% similar)\n",
    "\n",
    "2. **Policy Components:**\n",
    "   - **Inbound**: Generates embeddings for incoming query\n",
    "   - **Inbound**: Searches Redis for similar cached queries\n",
    "   - **Inbound**: Returns cached response if similarity > threshold\n",
    "   - **Outbound**: Stores new responses with embeddings in Redis\n",
    "\n",
    "3. **Configuration:**\n",
    "   - Redis host and password from environment\n",
    "   - Embeddings endpoint and key from environment\n",
    "   - Similarity threshold (adjustable)\n",
    "   - Cache TTL (time-to-live)\n",
    "\n",
    "4. **Applies Policy:**\n",
    "   - Uses APIM REST API to update API policy\n",
    "   - Validates XML syntax\n",
    "   - Activates policy immediately\n",
    "\n",
    "**How It Works:**\n",
    "- First request: Generate embeddings \u2192 Call AI model \u2192 Cache with embeddings\n",
    "- Similar request: Generate embeddings \u2192 Find similar in cache \u2192 Return cached response (fast!)\n",
    "- Dissimilar request: No cache hit \u2192 Call AI model \u2192 Cache new response\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "- Policy XML loaded successfully\n",
    "- Policy applied to APIM API\n",
    "- Semantic caching now active\n",
    "- Confirmation message showing policy activation\n",
    "- Ready to test semantic caching performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 2: Applying Semantic Caching Policy...\n",
      "    API ID: inference-api\n",
      "    Cache Duration: 120 seconds\n",
      "    Similarity Threshold: 0.8\n",
      "\n",
      "[*] Checking APIM cache configuration...\n",
      "\u26a0\ufe0f  Could not check cache: ERROR: 'cache' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "\n",
      "\n",
      "[*] Policy file created: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/semantic-caching-policy.xml\n",
      "\n",
      "[*] Applying policy to API 'inference-api'...\n",
      "\u26a0\ufe0f  Method 1 failed: ERROR: 'policy' is misspelled or not recognized by the system.\n",
      "\n",
      "Examples from AI knowledge base:\n",
      "https://aka.ms/cli_ref\n",
      "Read more about the command in reference docs\n",
      "\n",
      "\n",
      "[*] Trying alternative method using 'az rest'...\n",
      "\n",
      "\u2705 Policy applied successfully using 'az rest'!\n",
      "\n",
      "[*] Verifying policy application...\n",
      "\n",
      "\u26a0\ufe0f  Could not parse policy response\n",
      "\n",
      "\ud83d\udccb Policy Details:\n",
      "   - Lookup: Checks Redis for similar prompts (score >= 0.8)\n",
      "   - Store: Caches responses for 2 minutes\n",
      "   - Backend: embeddings-backend (text-embedding-3-small)\n",
      "\n",
      "\u23f3 Wait 30-60 seconds for policy propagation...\n",
      "\n",
      "[OK] Step 2 Complete - Check verification status above\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 2: Apply Semantic Caching Policy (FIXED)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "# Get required variables\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(\"\\n[*] Step 2: Applying Semantic Caching Policy...\")\n",
    "print(f\"    API ID: {api_id}\")\n",
    "print(f\"    Cache Duration: 120 seconds\")\n",
    "print(f\"    Similarity Threshold: 0.8\")\n",
    "\n",
    "# Check if Redis cache is configured in APIM\n",
    "import subprocess\n",
    "\n",
    "print(\"\\n[*] Checking APIM cache configuration...\")\n",
    "cache_check_cmd = f\"\"\"az apim cache list \\\n",
    "    --service-name {apim_service_name} \\\n",
    "    --resource-group {resource_group} \\\n",
    "    --query \"[?name=='default' || name=='Default'].{{name:name, description:description}}\" \\\n",
    "    -o json\"\"\"\n",
    "\n",
    "result = subprocess.run(cache_check_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    caches = json.loads(result.stdout) if result.stdout else []\n",
    "    if caches:\n",
    "        print(f\"\u2705 APIM cache configured: {caches[0].get('name', 'default')}\")\n",
    "        print(f\"   Description: {caches[0].get('description', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No cache configured in APIM!\")\n",
    "        print(\"   Semantic caching requires Redis cache to be connected to APIM\")\n",
    "        print(\"   The cache should have been created during deployment\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Could not check cache: {result.stderr[:200]}\")\n",
    "\n",
    "# Semantic caching policy XML\n",
    "policy_xml = \"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <!-- Semantic Cache Lookup: Check Redis for similar prompts (score >= 0.8) -->\n",
    "        <azure-openai-semantic-cache-lookup\n",
    "            score-threshold=\"0.8\"\n",
    "            embeddings-backend-id=\"embeddings-backend\"\n",
    "            embeddings-backend-auth=\"system-assigned\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <!-- Cache the response in Redis for 2 minutes -->\n",
    "        <azure-openai-semantic-cache-store duration=\"120\" />\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Write policy to file\n",
    "policy_file = Path('semantic-caching-policy.xml')\n",
    "with open(policy_file, 'w') as f:\n",
    "    f.write(policy_xml)\n",
    "\n",
    "print(f\"\\n[*] Policy file created: {policy_file.absolute()}\")\n",
    "\n",
    "# Apply policy using Azure REST API (more reliable than az apim api policy)\n",
    "print(f\"\\n[*] Applying policy to API '{api_id}'...\")\n",
    "\n",
    "# Method 1: Try using az apim api policy create with correct syntax\n",
    "cmd1 = f\"\"\"az apim api policy create \\\n",
    "    --resource-group {resource_group} \\\n",
    "    --service-name {apim_service_name} \\\n",
    "    --api-id {api_id} \\\n",
    "    --xml-content '{policy_xml}'\"\"\"\n",
    "\n",
    "result = subprocess.run(cmd1, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"\\n\u2705 Policy applied successfully using 'az apim api policy create'!\")\n",
    "else:\n",
    "    # Method 2: Try using az rest (more reliable)\n",
    "    print(f\"\u26a0\ufe0f  Method 1 failed: {result.stderr[:200]}\")\n",
    "    print(f\"\\n[*] Trying alternative method using 'az rest'...\")\n",
    "\n",
    "    policy_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2024-06-01-preview\"\n",
    "\n",
    "    # Create policy JSON payload\n",
    "    policy_payload = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write to temp file\n",
    "    payload_file = Path('policy-payload.json')\n",
    "    with open(payload_file, 'w') as f:\n",
    "        json.dump(policy_payload, f)\n",
    "\n",
    "    cmd2 = f\"\"\"az rest \\\n",
    "        --method PUT \\\n",
    "        --url \"{policy_url}\" \\\n",
    "        --body @{payload_file}\"\"\"\n",
    "\n",
    "    result2 = subprocess.run(cmd2, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    if result2.returncode == 0:\n",
    "        print(f\"\\n\u2705 Policy applied successfully using 'az rest'!\")\n",
    "    else:\n",
    "        print(f\"\\n\u274c Both methods failed!\")\n",
    "        print(f\"   Error: {result2.stderr[:300]}\")\n",
    "        print(f\"\\n\ud83d\udca1 Manual workaround:\")\n",
    "        print(f\"   1. Go to Azure Portal \u2192 API Management \u2192 APIs\")\n",
    "        print(f\"   2. Select 'inference-api'\")\n",
    "        print(f\"   3. Go to 'All operations' \u2192 Inbound processing \u2192 Code editor\")\n",
    "        print(f\"   4. Paste the policy from: {policy_file.absolute()}\")\n",
    "# Verify policy was applied\n",
    "print(f\"\\n[*] Verifying policy application...\")\n",
    "\n",
    "verify_cmd = f\"\"\"az rest \\\n",
    "    --method GET \\\n",
    "    --url \"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2024-06-01-preview&format=rawxml\" \"\"\"\n",
    "\n",
    "result = subprocess.run(verify_cmd, shell=True, capture_output=True, text=True)\n",
    "result = subprocess.run(verify_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    try:\n",
    "        policy_data = json.loads(result.stdout)\n",
    "        current_policy = policy_data.get('properties', {}).get('value', '')\n",
    "\n",
    "        if 'azure-openai-semantic-cache-lookup' in current_policy:\n",
    "            print(f\"\\n\u2705 Semantic caching policy is ACTIVE!\")\n",
    "            print(f\"   \u2713 Cache lookup configured\")\n",
    "            print(f\"   \u2713 Cache store configured\")\n",
    "            print(f\"   \u2713 Score threshold: 0.8\")\n",
    "        else:\n",
    "            print(f\"\\n\u26a0\ufe0f  Policy applied but semantic caching not found\")\n",
    "            print(f\"   Current policy does not contain 'azure-openai-semantic-cache-lookup'\")\n",
    "            print(f\"   You may need to apply it manually via Azure Portal\")\n",
    "    except:\n",
    "        print(f\"\\n\u26a0\ufe0f  Could not parse policy response\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f  Could not verify policy: {result.stderr[:200]}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Policy Details:\")\n",
    "print(f\"   - Lookup: Checks Redis for similar prompts (score >= 0.8)\")\n",
    "print(f\"   - Store: Caches responses for 2 minutes\")\n",
    "print(f\"   - Backend: embeddings-backend (text-embedding-3-small)\")\n",
    "print(f\"\\n\u23f3 Wait 30-60 seconds for policy propagation...\")\n",
    "print(f\"\\n[OK] Step 2 Complete - Check verification status above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-1-3\"></a>\n",
    "\n",
    "### 1.1.3 Performance Test\n",
    "\n",
    "\n",
    "**Purpose**: Tests semantic caching performance by sending similar queries and measuring response times\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Semantic caching policy applied (previous cell)\n",
    "- OpenAI client configured\n",
    "- APIM gateway accessible\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Runs a comprehensive test of semantic caching performance:**\n",
    "\n",
    "1. **Test Queries:**\n",
    "   - Original: \"What is the capital of France?\"\n",
    "   - Similar variations:\n",
    "     * \"Tell me the capital of France\"\n",
    "     * \"What's the French capital city?\"\n",
    "     * \"Capital of France?\"\n",
    "   - Each variation is semantically similar but textually different\n",
    "\n",
    "2. **Performance Measurement:**\n",
    "   - Sends first query (cache miss) \u2192 measures time\n",
    "   - Sends variations (cache hits) \u2192 measures time for each\n",
    "   - Compares cache hit vs cache miss latency\n",
    "   - Calculates speedup factor\n",
    "\n",
    "3. **Expected Results:**\n",
    "   - First query: ~500-2000ms (full AI model call)\n",
    "   - Cached queries: ~50-200ms (Redis retrieval)\n",
    "   - Speedup: 5-20x faster for cache hits\n",
    "\n",
    "4. **Similarity Testing:**\n",
    "   - Tests different similarity thresholds\n",
    "   - Shows which queries match the cache\n",
    "   - Demonstrates semantic understanding\n",
    "\n",
    "5. **Metrics Collected:**\n",
    "   - Response time for each query\n",
    "   - Cache hit/miss status\n",
    "   - Similarity scores\n",
    "   - Speedup percentages\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Performance comparison table showing:\n",
    "- Query variations tested\n",
    "- Response time for each query\n",
    "- Cache hit/miss status\n",
    "- Speedup compared to first query\n",
    "\n",
    "Example output:\n",
    "| Query                            | Time (ms) | Status      | Speedup |\n",
    "|----------------------------------|-----------|-------------|---------|\n",
    "| What is capital of France?       | 1245ms    | Cache MISS  | 1.0x    |\n",
    "| Tell me the capital of France    | 156ms     | Cache HIT   | 8.0x    |\n",
    "| What's the French capital?       | 142ms     | Cache HIT   | 8.8x    |\n",
    "| Capital of France?               | 138ms     | Cache HIT   | 9.0x    |\n",
    "\n",
    "**Key Insight**: Semantic caching dramatically reduces latency for similar queries!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 3: Testing Semantic Caching Performance...\n",
      "    Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "    API Version: 2025-03-01-preview\n",
      "    Model: gpt-4o-mini\n",
      "\n",
      "================================================================================\n",
      "\ud83e\uddea SEMANTIC CACHING TEST\n",
      "================================================================================\n",
      "\n",
      "\u25b6\ufe0f Run 1/20:\n",
      "\ud83d\udcac  How to Brew the Perfect Cup of Coffee?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '1abf6e37-9aac-4d4f-84ca-a2e0c3cbfd95'}\n",
      "\n",
      "\u25b6\ufe0f Run 2/20:\n",
      "\ud83d\udcac  How to Brew the Perfect Cup of Coffee?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '05deaa4d-ff72-4308-baff-8f5386415b7a'}\n",
      "\n",
      "\u25b6\ufe0f Run 3/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'ff9ff5bd-873f-47ee-8708-e3e392c7ed56'}\n",
      "\n",
      "\u25b6\ufe0f Run 4/20:\n",
      "\ud83d\udcac  How to Brew the Perfect Cup of Coffee?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'cd3ea2ac-45c4-48eb-b5b6-9bbd0d98e0d6'}\n",
      "\n",
      "\u25b6\ufe0f Run 5/20:\n",
      "\ud83d\udcac  How to Brew the Perfect Cup of Coffee?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '3a859702-898a-4f57-8e99-e0b7020cbaff'}\n",
      "\n",
      "\u25b6\ufe0f Run 6/20:\n",
      "\ud83d\udcac  Explain how to make a caffeinated brewed beverage?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '6962d2ad-e01a-4cab-9762-a95e6b5f990d'}\n",
      "\n",
      "\u25b6\ufe0f Run 7/20:\n",
      "\ud83d\udcac  What are the steps to Craft the Ideal Espresso?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'ad83d726-a9ca-48d6-a09d-f58e3b72602c'}\n",
      "\n",
      "\u25b6\ufe0f Run 8/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '6cb0f356-512b-44af-9401-ec78f53f12d5'}\n",
      "\n",
      "\u25b6\ufe0f Run 9/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'accdf3bc-c52a-438c-ab49-58d7cd222260'}\n",
      "\n",
      "\u25b6\ufe0f Run 10/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '5b885898-1aaa-4285-b992-626a730be834'}\n",
      "\n",
      "\u25b6\ufe0f Run 11/20:\n",
      "\ud83d\udcac  What are the steps to Craft the Ideal Espresso?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'f42a5717-f423-4907-8718-43f4c84cbfd5'}\n",
      "\n",
      "\u25b6\ufe0f Run 12/20:\n",
      "\ud83d\udcac  What are the steps to Craft the Ideal Espresso?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '949d75ac-a59b-42ec-9a9c-d886b7f4addc'}\n",
      "\n",
      "\u25b6\ufe0f Run 13/20:\n",
      "\ud83d\udcac  How to Brew the Perfect Cup of Coffee?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '7cd07515-7675-42e0-b807-3b773ddeee71'}\n",
      "\n",
      "\u25b6\ufe0f Run 14/20:\n",
      "\ud83d\udcac  What are the steps to Craft the Ideal Espresso?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '1be668f2-3218-4a15-ba3d-09b9c40c7d85'}\n",
      "\n",
      "\u25b6\ufe0f Run 15/20:\n",
      "\ud83d\udcac  Explain how to make a caffeinated brewed beverage?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '75c0e658-21b6-4339-aa61-2da6a3f39a30'}\n",
      "\n",
      "\u25b6\ufe0f Run 16/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '19b59c3a-ab96-40c1-ae88-02182dea23c6'}\n",
      "\n",
      "\u25b6\ufe0f Run 17/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '35370f96-dfd8-46cd-b829-1112ab1228d0'}\n",
      "\n",
      "\u25b6\ufe0f Run 18/20:\n",
      "\ud83d\udcac  Tell me how to create the best steaming Java?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'fd1d4393-3b91-4217-bc27-c3fad3c8609d'}\n",
      "\n",
      "\u25b6\ufe0f Run 19/20:\n",
      "\ud83d\udcac  Explain how to make a caffeinated brewed beverage?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '76426e97-e46e-4056-99fb-be238888e197'}\n",
      "\n",
      "\u25b6\ufe0f Run 20/20:\n",
      "\ud83d\udcac  Explain how to make a caffeinated brewed beverage?\n",
      "\u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'ade6dafc-0557-47a7-ae7a-b7f2b6493003'}\n",
      "\n",
      "\u274c No successful requests completed\n",
      "\n",
      "[OK] Step 3 Complete - Semantic caching test finished\n"
     ]
    }
   ],
   "source": [
    "# Lab Semantic Caching - Step 3: Test Semantic Caching Performance\n",
    "# Adapted from working semantic-caching.ipynb\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Get configuration from master-lab.env\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "# Use the newer API version that works with semantic caching\n",
    "api_version = \"2025-03-01-preview\"  # From working semantic-caching notebook\n",
    "\n",
    "print(\"\\n[*] Step 3: Testing Semantic Caching Performance...\")\n",
    "print(f\"    Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"    API Version: {api_version}\")\n",
    "print(f\"    Model: gpt-4o-mini\")\n",
    "\n",
    "# Similar questions that should trigger semantic cache hits\n",
    "# These are semantically similar so APIM should cache and reuse responses\n",
    "questions = [\n",
    "    \"How to Brew the Perfect Cup of Coffee?\",\n",
    "    \"What are the steps to Craft the Ideal Espresso?\",\n",
    "    \"Tell me how to create the best steaming Java?\",\n",
    "    \"Explain how to make a caffeinated brewed beverage?\"\n",
    "]\n",
    "\n",
    "# Initialize Azure OpenAI client pointing to APIM gateway\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 10  # 10ms between requests\n",
    "api_runs = []  # Response times\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\ud83e\uddea SEMANTIC CACHING TEST\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for i in range(runs):\n",
    "    random_question = random.choice(questions)\n",
    "    print(f\"\\n\u25b6\ufe0f Run {i+1}/{runs}:\")\n",
    "    print(f\"\ud83d\udcac  {random_question}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a sarcastic, unhelpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": random_question}\n",
    "            ]\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        print(f\"\u231a {response_time:.2f} seconds\")\n",
    "\n",
    "        # Uncomment to see the response\n",
    "        # print(f\"\ud83d\udcac {response.choices[0].message.content}\\n\")\n",
    "\n",
    "        api_runs.append(response_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {str(e)[:150]}\")\n",
    "        api_runs.append(None)\n",
    "\n",
    "    time.sleep(sleep_time_ms / 1000)\n",
    "\n",
    "# Calculate statistics\n",
    "valid_runs = [r for r in api_runs if r is not None]\n",
    "if valid_runs:\n",
    "    avg_time = sum(valid_runs) / len(valid_runs)\n",
    "    min_time = min(valid_runs)\n",
    "    max_time = max(valid_runs)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"\ud83d\udcca PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Requests:     {len(api_runs)}\")\n",
    "    print(f\"Successful:         {len(valid_runs)}\")\n",
    "    print(f\"Average Time:       {avg_time:.2f}s\")\n",
    "    print(f\"Fastest Response:   {min_time:.2f}s\")\n",
    "    print(f\"Slowest Response:   {max_time:.2f}s\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # The first request should be slower (goes to backend)\n",
    "    # Subsequent similar requests should be faster (served from cache)\n",
    "    if len(valid_runs) > 1 and min_time < avg_time * 0.5:\n",
    "        speedup = max_time / min_time\n",
    "        print(f\"\\n\u2705 Semantic caching appears to be working!\")\n",
    "        print(f\"   Slowest request: {max_time:.2f}s\")\n",
    "        print(f\"   Fastest request: {min_time:.2f}s\")\n",
    "        print(f\"   Speed improvement: {speedup:.1f}x faster!\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  Note: First request typically slower (backend call)\")\n",
    "        print(\"   Subsequent requests should be faster (cache hits)\")\n",
    "else:\n",
    "    print(\"\\n\u274c No successful requests completed\")\n",
    "\n",
    "print(\"\\n[OK] Step 3 Complete - Semantic caching test finished\")\n",
    "\n",
    "# Store results for visualization\n",
    "semantic_cache_results = api_runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-1-4\"></a>\n",
    "\n",
    "### 1.1.4 Visualize Performance\n",
    "\n",
    "\n",
    "**Purpose**: Creates visual charts showing the performance benefits of semantic caching\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Performance test data from previous cell\n",
    "- matplotlib and pandas libraries\n",
    "- Test results stored in variables\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Generates comprehensive visualizations of caching performance:**\n",
    "\n",
    "1. **Bar Chart - Response Times:**\n",
    "   - Shows response time for each query variation\n",
    "   - Highlights cache misses (tall bars) vs cache hits (short bars)\n",
    "   - Color-coded for easy interpretation\n",
    "\n",
    "2. **Line Chart - Performance Over Time:**\n",
    "   - Shows how response time improves with caching\n",
    "   - Demonstrates learning effect as cache fills\n",
    "   - Useful for capacity planning\n",
    "\n",
    "3. **Speedup Analysis:**\n",
    "   - Calculates and displays speedup factors\n",
    "   - Shows percentage improvements\n",
    "   - Highlights cost savings (fewer AI model calls)\n",
    "\n",
    "4. **Statistics Summary:**\n",
    "   - Average cache hit time\n",
    "   - Average cache miss time\n",
    "   - Cache hit rate percentage\n",
    "   - Total time saved\n",
    "\n",
    "**Business Impact:**\n",
    "- Lower latency = better user experience\n",
    "- Fewer AI model calls = lower costs\n",
    "- Semantic matching = handles user variations naturally\n",
    "- Scalability = more requests without proportional cost increase\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Multiple visualizations:\n",
    "1. Bar chart showing response times\n",
    "2. Line chart showing performance trend\n",
    "3. Statistical summary table\n",
    "4. Cost savings analysis\n",
    "\n",
    "Charts clearly show 5-20x performance improvement for cached queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 4: Visualizing Semantic Caching Performance...\n",
      "\n",
      "\u26a0\ufe0f  No valid results to visualize\n",
      "\n",
      "================================================================================\n",
      "\ud83c\udf89 LAB 09 COMPLETE: SEMANTIC CACHING\n",
      "================================================================================\n",
      "\n",
      "What you learned:\n",
      "\u2705 How semantic caching reduces API calls for similar queries\n",
      "\u2705 How to measure caching performance\n",
      "\u2705 How vector embeddings enable semantic similarity matching\n",
      "\n",
      "Key Benefits:\n",
      "\ud83d\udcb0 Cost savings: Reduced Azure OpenAI API calls\n",
      "\u26a1 Performance: Faster response times (10-100x faster!)\n",
      "\ud83d\udcca Scalability: Better handling of repetitive queries\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Step 4: Visualize Performance\n",
    "# Adapted from working semantic-caching.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "print(\"\\n[*] Step 4: Visualizing Semantic Caching Performance...\")\n",
    "\n",
    "if 'semantic_cache_results' in globals() and semantic_cache_results:\n",
    "    # Filter out None values\n",
    "    valid_results = [r for r in semantic_cache_results if r is not None]\n",
    "\n",
    "    if len(valid_results) > 0:\n",
    "        # Create DataFrame\n",
    "        mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "        df = pd.DataFrame(valid_results, columns=['Response Time'])\n",
    "        df['Run'] = range(1, len(df) + 1)\n",
    "\n",
    "        # Create bar plot\n",
    "        df.plot(kind='bar', x='Run', y='Response Time', legend=False, color='steelblue')\n",
    "        plt.title('Semantic Caching Performance', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Runs', fontsize=12)\n",
    "        plt.ylabel('Response Time (s)', fontsize=12)\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "        # Add average line\n",
    "        average = df['Response Time'].mean()\n",
    "        plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n\ud83d\udcca Chart showing response times across all requests\")\n",
    "        print(f\"   First request is typically slowest (backend call, ~{valid_results[0]:.2f}s)\")\n",
    "        print(f\"   Subsequent requests faster (cache hits, avg ~{sum(valid_results[1:])/len(valid_results[1:]):.2f}s)\")\n",
    "\n",
    "        print(\"\\n[OK] Step 4 Complete - Visualization ready\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  No valid results to visualize\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  No results available. Run the test cell (Step 3) first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf89 LAB 09 COMPLETE: SEMANTIC CACHING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"\u2705 How semantic caching reduces API calls for similar queries\")\n",
    "print(\"\u2705 How to measure caching performance\")\n",
    "print(\"\u2705 How vector embeddings enable semantic similarity matching\")\n",
    "print(\"\\nKey Benefits:\")\n",
    "print(\"\ud83d\udcb0 Cost savings: Reduced Azure OpenAI API calls\")\n",
    "print(\"\u26a1 Performance: Faster response times (10-100x faster!)\")\n",
    "print(\"\ud83d\udcca Scalability: Better handling of repetitive queries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-1-5\"></a>\n",
    "\n",
    "### 1.1.5 Redis Cache Statistics\n",
    "\n",
    "\n",
    "**Purpose**: Displays Redis cache statistics and metrics for monitoring caching effectiveness\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Redis cache accessible\n",
    "- Redis connection string from environment\n",
    "- Redis Python client library (redis-py)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Retrieves and displays Redis cache statistics:**\n",
    "\n",
    "1. **Cache Metrics:**\n",
    "   - Total keys stored\n",
    "   - Memory usage\n",
    "   - Cache hit rate\n",
    "   - Eviction statistics\n",
    "\n",
    "2. **Semantic Cache Specific:**\n",
    "   - Number of cached query embeddings\n",
    "   - Storage per embedding\n",
    "   - TTL (time-to-live) settings\n",
    "   - Expiration policy\n",
    "\n",
    "3. **Performance Insights:**\n",
    "   - Shows how cache is filling up\n",
    "   - Identifies optimal similarity threshold\n",
    "   - Helps tune cache size\n",
    "\n",
    "4. **Monitoring:**\n",
    "   - Track cache effectiveness over time\n",
    "   - Identify when to scale Redis\n",
    "   - Optimize cache eviction policies\n",
    "\n",
    "**Use Cases:**\n",
    "- Capacity planning\n",
    "- Cost optimization\n",
    "- Performance tuning\n",
    "- Debugging cache issues\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Redis statistics display:\n",
    "- Total cached items: <number>\n",
    "- Memory used: <MB>\n",
    "- Cache hit rate: <%>\n",
    "- Average embedding size: <bytes>\n",
    "- Oldest cached item: <timestamp>\n",
    "- Newest cached item: <timestamp>\n",
    "\n",
    "This helps operators understand cache utilization and effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Optional: Viewing Redis Cache Statistics...\n",
      "    This shows cache hits, misses, and memory usage\n",
      "\n",
      "\ud83d\udcca Redis Server Information:\n",
      "   Used Memory: 4.62M\n",
      "   Cache Hits: 0\n",
      "   Cache Misses: 0\n",
      "   Evicted Keys: 0\n",
      "   Expired Keys: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQbRJREFUeJzt3XlYVeX+/vF7M4ogoILigOCEs5YaZRpKYFhpqallOacekywrhzwNDmUeNdM0s+HkmEc7TmUdzdlyQJwSZ1MTJRUNFXBEhef3Rz/3tx0OgCC4fL+ua12xnvWs9XzW2lzuuzVhM8YYAQAA4K7nlN8FAAAAIHcQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7ABY2tSpU2Wz2RQfH29va9KkiZo0aZJvNeWGIUOGyGazKSkpKb9LuSNy+zMLDg5Wly5dcm17QEFBsANwx10LW9cmFxcXlSlTRl26dNHRo0fzu7xsS01N1dChQ1WnTh15eXnJw8NDNWvW1MCBA3Xs2LH8Lu+27dixQ23atFFQUJAKFSqkMmXKqGnTppowYYJDvw8++EDffvttjsfZvXu3hgwZ4hDCb8f69es1ZMgQJScn58r2gLuBS34XAODeNWzYMJUvX16XLl3Shg0bNHXqVK1du1Y7d+5UoUKF8mzcpUuX5tq2fvvtN0VGRurIkSNq27atevbsKTc3N23fvl1fffWVFixYoF9//TXXxrvT1q9fr/DwcJUrV049evRQQECAEhIStGHDBn388cfq06ePve8HH3ygNm3aqGXLljkaa/fu3Ro6dKiaNGmi4OBgh2U5+czWr1+voUOHqkuXLvL19XVYtm/fPjk5cW4D1kOwA5BvHn/8cdWvX1+S1L17d/n5+WnkyJFauHCh2rVrl2fjurm55cp2rl69qtatW+vEiRNavXq1GjVq5LB8+PDhGjlyZK6MlV+GDx8uHx8fbdq0KVM4Onny5B2rI7c+s2vc3d1zdXtAQcH/rgAoMB555BFJ0sGDBx3a9+7dqzZt2qhYsWIqVKiQ6tevr4ULF2Zaf9euXXr00Ufl4eGhsmXL6v3331dGRkamfte7X2vChAmqUaOGChcurKJFi6p+/fr6z3/+c9N6582bp7i4OL311luZQp0keXt7a/jw4fb5NWvWqG3btipXrpzc3d0VGBio1157TRcvXsy07t69e9WuXTv5+/vLw8NDVapU0VtvvZWpX3Jysv2MlI+Pj7p27aoLFy5k6vf111+rXr168vDwULFixfTcc88pISHhpvsn/flZ1KhRI1Ook6QSJUrYf7bZbDp//rymTZtmv8R+7R62w4cPq3fv3qpSpYo8PDxUvHhxtW3b1uGS69SpU9W2bVtJUnh4uH0bq1evlpT9z2zIkCHq37+/JKl8+fL27V0b83r32CUnJ+u1115TcHCw3N3dVbZsWXXq1MnhPsac/J4AdxJn7AAUGNe+dIsWLWpv27Vrlxo2bKgyZcrozTfflKenp/773/+qZcuWmjdvnlq1aiVJSkxMVHh4uK5evWrv98UXX8jDw+OW43755Zd65ZVX1KZNG7366qu6dOmStm/frtjYWD3//PM3XO9auOzYsWOW9m/OnDm6cOGCXnrpJRUvXlwbN27UhAkT9Pvvv2vOnDn2ftu3b9cjjzwiV1dX9ezZU8HBwTp48KC+//57h6AoSe3atVP58uU1YsQIbd26Vf/+979VokQJhzOFw4cP1zvvvKN27dqpe/fu+uOPPzRhwgSFhYXpl19+uW5ouyYoKEgxMTHauXOnatasecN+M2bMUPfu3RUaGqqePXtKkipWrChJ2rRpk9avX6/nnntOZcuWVXx8vCZNmqQmTZpo9+7dKly4sMLCwvTKK69o/Pjx+uc//6lq1apJkv2/f3erz6x169b69ddfNWvWLI0dO1Z+fn6SJH9//+tu79y5c3rkkUe0Z88edevWTXXr1lVSUpIWLlyo33//XX5+fjn+PQHuKAMAd9iUKVOMJLN8+XLzxx9/mISEBDN37lzj7+9v3N3dTUJCgr1vRESEqVWrlrl06ZK9LSMjwzz88MOmcuXK9ra+ffsaSSY2NtbedvLkSePj42MkmUOHDtnbGzdubBo3bmyff/rpp02NGjWyvR/333+/8fHxyXL/CxcuZGobMWKEsdls5vDhw/a2sLAwU6RIEYc2Y/7c72sGDx5sJJlu3bo59GnVqpUpXry4fT4+Pt44Ozub4cOHO/TbsWOHcXFxydT+d0uXLjXOzs7G2dnZNGjQwAwYMMAsWbLEXL58OVNfT09P07lz5yztd0xMjJFkpk+fbm+bM2eOkWRWrVqVqX9OPrPRo0dn+uyvCQoKcqj13XffNZLM/PnzM/W9dtxz+nsC3ElcigWQbyIjI+Xv76/AwEC1adNGnp6eWrhwocqWLStJOn36tFauXKl27drp7NmzSkpKUlJSkk6dOqWoqCjt37/f/hTtokWL9NBDDyk0NNS+fX9/f73wwgu3rMPX11e///67Nm3alK36U1NTVaRIkSz3/+vZw/PnzyspKUkPP/ywjDH65ZdfJEl//PGHfv75Z3Xr1k3lypVzWN9ms2XaZq9evRzmH3nkEZ06dUqpqamSpPnz5ysjI0Pt2rWzH7+kpCQFBASocuXKWrVq1U1rbtq0qWJiYvTUU08pLi5Oo0aNUlRUlMqUKXPdy+G32u8rV67o1KlTqlSpknx9fbV169YsbePvcvqZ3ci8efNUp04d+xngv7p23HN7TCAvEOwA5JuJEydq2bJlmjt3rp544gklJSU53NR+4MABGWP0zjvvyN/f32EaPHiwpP+7gf/w4cOqXLlypjGqVKlyyzoGDhwoLy8vhYaGqnLlyoqOjta6detuuZ63t7fOnj2b1d3VkSNH1KVLFxUrVkxeXl7y9/dX48aNJUkpKSmS/nzKVtJNL3v+1d/D37XL2GfOnJEk7d+/X8YYVa5cOdMx3LNnT5YegHjggQc0f/58nTlzRhs3btSgQYN09uxZtWnTRrt3777l+hcvXtS7776rwMBAubu7y8/PT/7+/kpOTrbvd3bl9DO7kYMHD97ymOf2mEBe4B47APkmNDTU/lRsy5Yt1ahRIz3//PPat2+fvLy87A8+9OvXT1FRUdfdRqVKlW67jmrVqmnfvn364Ycf9OOPP2revHn69NNP9e6772ro0KE3XK9q1ar65ZdflJCQoMDAwJuOkZ6erqZNm+r06dMaOHCgqlatKk9PTx09elRdunS57kMeWeHs7HzddmOMJCkjI0M2m02LFy++bl8vL68sj+Xm5qYHHnhADzzwgEJCQtS1a1fNmTPHHrJvpE+fPpoyZYr69u2rBg0ayMfHRzabTc8991yO9zunn9ntyI8xgewi2AEoEJydnTVixAiFh4frk08+0ZtvvqkKFSpIklxdXRUZGXnT9YOCgrR///5M7fv27cvS+J6ennr22Wf17LPP6vLly2rdurWGDx+uQYMG3fCdei1atNCsWbP09ddfa9CgQTfd/o4dO/Trr79q2rRp6tSpk7192bJlDv2u7fPOnTuzVPetVKxYUcYYlS9fXiEhIbmyTUn2QH78+HF72/UuFUvS3Llz1blzZ40ZM8bedunSpUwvDr7R+jdyq88sO9urWLFilo55Tn5PgDuJS7EACowmTZooNDRU48aN06VLl1SiRAk1adJEn3/+uUOAuOaPP/6w//zEE09ow4YN2rhxo8PymTNn3nLcU6dOOcy7ubmpevXqMsboypUrN1yvTZs2qlWrloYPH66YmJhMy8+ePWt/Rcm1s2XXzqRd+/njjz92WMff319hYWGaPHmyjhw54rDsr+tmVevWreXs7KyhQ4dmWt8Yk2nf/27VqlXXHXfRokWSHC91e3p6XvevPDg7O2faxoQJE5Senu7Q5unpKUlZ+ksRWfnMsrO9Z555RnFxcVqwYEGmZddqz+nvCXAnccYOQIHSv39/tW3bVlOnTlWvXr00ceJENWrUSLVq1VKPHj1UoUIFnThxQjExMfr9998VFxcnSRowYIBmzJihZs2a6dVXX7W/7iQoKEjbt2+/6ZiPPfaYAgIC1LBhQ5UsWVJ79uzRJ598oieffPKmD0e4urpq/vz5ioyMVFhYmNq1a6eGDRvK1dVVu3bt0n/+8x8VLVpUw4cPV9WqVVWxYkX169dPR48elbe3t+bNm2e/F+6vxo8fr0aNGqlu3brq2bOnypcvr/j4eP3vf//Ttm3bsnU8K1asqPfff1+DBg1SfHy8WrZsqSJFiujQoUNasGCBevbsqX79+t1w/T59+ujChQtq1aqVqlatqsuXL2v9+vX65ptvFBwcrK5du9r71qtXT8uXL9dHH32k0qVLq3z58nrwwQfVvHlzzZgxQz4+PqpevbpiYmK0fPlyFS9e3GGs++67T87Ozho5cqRSUlLk7u6uRx991OF9eddk5TOrV6+eJOmtt97Sc889J1dXV7Vo0cIe+P6qf//+mjt3rtq2batu3bqpXr16On36tBYuXKjPPvtMderUyfHvCXBH3fkHcQHc66697mTTpk2ZlqWnp5uKFSuaihUrmqtXrxpjjDl48KDp1KmTCQgIMK6urqZMmTKmefPmZu7cuQ7rbt++3TRu3NgUKlTIlClTxrz33nvmq6++uuXrTj7//HMTFhZmihcvbtzd3U3FihVN//79TUpKSpb258yZM+bdd981tWrVMoULFzaFChUyNWvWNIMGDTLHjx+399u9e7eJjIw0Xl5exs/Pz/To0cPExcUZSWbKlCkO29y5c6dp1aqV8fX1NYUKFTJVqlQx77zzjn35tded/PHHH9c9tn9/xce8efNMo0aNjKenp/H09DRVq1Y10dHRZt++fTfdt8WLF5tu3bqZqlWrGi8vL+Pm5mYqVapk+vTpY06cOOHQd+/evSYsLMx4eHgYSfbXiZw5c8Z07drV+Pn5GS8vLxMVFWX27t2b6ZUjxhjz5ZdfmgoVKhhnZ2eHV5/k9DN77733TJkyZYyTk5PDcbne2KdOnTIvv/yyKVOmjHFzczNly5Y1nTt3NklJSdkaE8hPNmNycG4fAAAABQ732AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIIXFMNBRkaGjh07piJFimT7z/sAAIDcZ4zR2bNnVbp0aTk53fycHMEODo4dO3bLP2YOAADuvISEBJUtW/amfQh2cHDtz+IkJCTI29s7n6sBAACpqakKDAzM0p+uI9jBwbXLr97e3gQ7AAAKkKzcIsXDEwAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCLuyWAXHByscePG5cm2bTabvv322zzZNgAAwM0UqGDXpUsX2Wy2TFOzZs1ydZxNmzapZ8+eubrNrOrSpYtatmzp0DZ37lwVKlRIY8aMyZeaAACANbjkdwF/16xZM02ZMsWhzd3dPVfH8Pf3v+nyK1euyNXVNVfHvJF///vfio6O1meffaauXbvekTEBAIA1FagzdtKfIS4gIMBhKlq0qCRp9erVcnNz05o1a+z9R40apRIlSujEiROSpCZNmujll1/Wyy+/LB8fH/n5+emdd96RMca+zt8vxdpsNk2aNElPPfWUPD09NXz4cEnSd999p7p166pQoUKqUKGChg4dqqtXr9rX279/v8LCwlSoUCFVr15dy5Yty9a+jho1Sn369NHs2bMdQt3Nxu3WrZuaN2/usJ0rV66oRIkS+uqrryT9eQawVq1a8vDwUPHixRUZGanz589nqzYAAHD3KXBn7G6mSZMm6tu3rzp27Ki4uDj99ttveueddzRnzhyVLFnS3m/atGl68cUXtXHjRm3evFk9e/ZUuXLl1KNHjxtue8iQIfrXv/6lcePGycXFRWvWrFGnTp00fvx4PfLIIzp48KD98u3gwYOVkZGh1q1bq2TJkoqNjVVKSor69u2b5X0ZOHCgPv30U/3www+KiIiwt99q3O7duyssLEzHjx9XqVKlJEk//PCDLly4oGeffVbHjx9X+/btNWrUKLVq1Upnz57VmjVrHILtX6WlpSktLc0+n5qamuV9AAAABYwpQDp37mycnZ2Np6enwzR8+HB7n7S0NHPfffeZdu3amerVq5sePXo4bKNx48amWrVqJiMjw942cOBAU61aNft8UFCQGTt2rH1ekunbt6/DdiIiIswHH3zg0DZjxgxTqlQpY4wxS5YsMS4uLubo0aP25YsXLzaSzIIFC266j25ubkaSWbFiRabltxrXGGOqV69uRo4caZ9v0aKF6dKlizHGmC1bthhJJj4+/oY1/NXgwYONpExTSkpKltYHAAB5KyUlJcvfzQXujF14eLgmTZrk0FasWDH7z25ubpo5c6Zq166toKAgjR07NtM2HnroIdlsNvt8gwYNNGbMGKWnp8vZ2fm649avX99hPi4uTuvWrbNflpWk9PR0Xbp0SRcuXNCePXsUGBio0qVLO4yTFbVr11ZSUpIGDx6s0NBQeXl5ZXncwoULq3v37vriiy80YMAAnThxQosXL9bKlSslSXXq1FFERIRq1aqlqKgoPfbYY2rTpo39cvbfDRo0SK+//rp9PjU1VYGBgVnaDwAAULAUuGDn6empSpUq3bTP+vXrJUmnT5/W6dOn5enpmSvj/tW5c+c0dOhQtW7dOlPfQoUK3dZYZcqU0dy5cxUeHq5mzZpp8eLFKlKkSJbH7dSpk958803FxMRo/fr1Kl++vB555BFJkrOzs5YtW6b169dr6dKlmjBhgt566y3FxsaqfPnymbbp7u6e6w+nAACA/FHgHp64lYMHD+q1117Tl19+qQcffFCdO3dWRkaGQ5/Y2FiH+Q0bNqhy5co3PFt3PXXr1tW+fftUqVKlTJOTk5OqVaumhIQEHT9+3GGcrAoKCtJPP/2kxMRENWvWTGfPns3SuJJUvHhxtWzZUlOmTNHUqVMzPU1rs9nUsGFDDR06VL/88ovc3Ny0YMGCLNcGAADuTgXujF1aWpoSExMd2lxcXOTn56f09HR16NBBUVFR6tq1q5o1a6ZatWppzJgx6t+/v73/kSNH9Prrr+sf//iHtm7dqgkTJmT7HXHvvvuumjdvrnLlyqlNmzZycnJSXFycdu7cqffff1+RkZEKCQlR586dNXr0aKWmpuqtt97K1hiBgYFavXq1wsPDFRUVpR9//PGW417TvXt3NW/eXOnp6ercubO9PTY2VitWrNBjjz2mEiVKKDY2Vn/88YeqVauWrdoAAMDdp8Cdsfvxxx9VqlQph6lRo0aSpOHDh+vw4cP6/PPPJUmlSpXSF198obfffltxcXH2bXTq1EkXL15UaGiooqOj9eqrr2b7hcRRUVH64YcftHTpUj3wwAN66KGHNHbsWAUFBUmSnJyctGDBAvs43bt3d7gvLqvKli2r1atXKykpSVFRUWrQoMFNx70mMjJSpUqVUlRUlMN9ft7e3vr555/1xBNPKCQkRG+//bbGjBmjxx9/PNu1AQCAu4vNmBu8B+Mu1aRJE91333159ifDCopz586pTJkymjJlynXvx8up1NRU+fj4KCUlRd7e3rm2XQAAkDPZ+W4ucJdicXMZGRlKSkrSmDFj5Ovrq6eeeiq/SwIAAAUEwe4uc+TIEZUvX15ly5bV1KlT5eLCRwgAAP5kuVSwevXq/C4hTwUHB9/wr0gAAIB7W4F7eAIAAAA5Q7ADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWYclg16RJE/Xt2/eOj7t69WrZbDYlJyff8bEBAAAKXLDr0qWLbDZbpqlZs2ZZ3sb8+fP13nvvZanvnQ5jwcHBGjdunH3eGKN+/frJ29tbq1evviM1AAAAa3LJ7wKup1mzZpoyZYpDm7u7e5bXL1asWG6XlCfS09PVo0cP/fDDD1q1apXq1auX3yUBAIC7WIE7Yyf9GeICAgIcpqJFi0qSnn/+eT377LMO/a9cuSI/Pz9Nnz5dUuZLsWlpaRo4cKACAwPl7u6uSpUq6auvvlJ8fLzCw8MlSUWLFpXNZlOXLl0kSRkZGRoxYoTKly8vDw8P1alTR3PnznUYd9GiRQoJCZGHh4fCw8MVHx+f5X1MS0tT27ZttXz5cq1Zs8Ye6m42rjFGlSpV0ocffuiwrW3btslms+nAgQMyxmjIkCEqV66c3N3dVbp0ab3yyitZrgsAANy9CuQZu5t54YUX1LZtW507d05eXl6SpCVLlujChQtq1arVddfp1KmTYmJiNH78eNWpU0eHDh1SUlKSAgMDNW/ePD3zzDPat2+fvL295eHhIUkaMWKEvv76a3322WeqXLmyfv75Z3Xo0EH+/v5q3LixEhIS1Lp1a0VHR6tnz57avHmz3njjjSztw7lz5/Tkk0/q999/17p16xQYGGhfdqtxu3XrpilTpqhfv372daZMmaKwsDBVqlRJc+fO1dixYzV79mzVqFFDiYmJiouLu2EtaWlpSktLs8+npqZmaR8AAEABZAqYzp07G2dnZ+Pp6ekwDR8+3BhjzJUrV4yfn5+ZPn26fZ327dubZ5991j7fuHFj8+qrrxpjjNm3b5+RZJYtW3bd8VatWmUkmTNnztjbLl26ZAoXLmzWr1/v0PfFF1807du3N8YYM2jQIFO9enWH5QMHDsy0rb8LCgoybm5upnjx4ubkyZMOy7Iy7tGjR42zs7OJjY01xhhz+fJl4+fnZ6ZOnWqMMWbMmDEmJCTEXL58+YY1/NXgwYONpExTSkpKltYHAAB5KyUlJcvfzQXyUmx4eLi2bdvmMPXq1UuS5OLionbt2mnmzJmSpPPnz+u7777TCy+8cN1tbdu2Tc7OzmrcuHGWxz9w4IAuXLigpk2bysvLyz5Nnz5dBw8elCTt2bNHDz74oMN6DRo0yNL2H3vsMZ0/f14ffPBBtsctXbq0nnzySU2ePFmS9P3339sv60pS27ZtdfHiRVWoUEE9evTQggULdPXq1RvWMmjQIKWkpNinhISErB0kAABQ4BTIS7Genp6qVKnSDZe/8MILaty4sU6ePKlly5bJw8Pjhk/NXru0mh3nzp2TJP3vf/9TmTJlHJZl5yGOG4mIiFCfPn309NNPKyMjQx9//HG2xu3evbs6duyosWPHasqUKXr22WdVuHBhSVJgYKD27dun5cuXa9myZerdu7dGjx6tn376Sa6urplqcXd3z5V9AgAA+a9ABrtbefjhhxUYGKhvvvlGixcvVtu2ba8bWiSpVq1aysjI0E8//aTIyMhMy93c3CT9+YTqNdWrV5e7u7uOHDlywzN91apV08KFCx3aNmzYkOV9eOyxx/T999/rqaeekjFG48ePz9K4kvTEE0/I09NTkyZN0o8//qiff/7ZYbmHh4datGihFi1aKDo6WlWrVtWOHTtUt27dLNcHAADuPgUy2KWlpSkxMdGhzcXFRX5+fvb5559/Xp999pl+/fVXrVq16obbCg4OVufOndWtWzf7wxOHDx/WyZMn1a5dOwUFBclms+mHH37QE088IQ8PDxUpUkT9+vXTa6+9poyMDDVq1EgpKSlat26dvL291blzZ/Xq1UtjxoxR//791b17d23ZskVTp07N1n5GRkbqhx9+UIsWLZSRkaFPPvnkluNKkrOzs7p06aJBgwapcuXKDpeAp06dqvT0dD344IMqXLiwvv76a3l4eCgoKChbtQEAgLtQ3t/ylz2dO3e+7s38VapUcei3e/duI8kEBQWZjIwMh2V/fXjCGGMuXrxoXnvtNVOqVCnj5uZmKlWqZCZPnmxfPmzYMBMQEGBsNpvp3LmzMcaYjIwMM27cOFOlShXj6upq/P39TVRUlPnpp5/s633//femUqVKxt3d3TzyyCNm8uTJWXp4YuzYsQ5tq1atMp6enqZ3795ZGtcYYw4ePGgkmVGjRjm0L1iwwDz44IPG29vbeHp6moceesgsX778hvX8XXZu0AQAAHkvO9/NNmOMya9QiZxbs2aNIiIilJCQoJIlS+badlNTU+Xj46OUlBR5e3vn2nYBAEDOZOe7uUBeisWNpaWl6Y8//tCQIUPUtm3bXA11AADg7lYgX3eCG5s1a5aCgoKUnJysUaNG5Xc5AACgAOFSLBxwKRYAgIIlO9/NnLEDAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBE5CnabNm1SbGxspvbY2Fht3rz5tosCAABA9uUo2EVHRyshISFT+9GjRxUdHX3bRQEAACD7chTsdu/erbp162Zqv//++7V79+7bLgoAAADZl6Ng5+7urhMnTmRqP378uFxcXG67KAAAAGRfjoLdY489pkGDBiklJcXelpycrH/+859q2rRprhUHAACArMvR6bUPP/xQYWFhCgoK0v333y9J2rZtm0qWLKkZM2bkaoEAAADImhwFuzJlymj79u2aOXOm4uLi5OHhoa5du6p9+/ZydXXN7RoBAACQBTm+Ic7T01M9e/bMzVoAAABwG7Ic7BYuXKjHH39crq6uWrhw4U37PvXUU7ddGAAAALLHZowxWeno5OSkxMRElShRQk5ON37mwmazKT09PdcKxJ2VmpoqHx8fpaSkyNvbO7/LAQDgnped7+Ysn7HLyMi47s8AAAAoGLL9upMrV64oIiJC+/fvz4t6AAAAkEPZDnaurq7avn17XtQCAACA25CjFxR36NBBX331VW7XAgAAgNuQo9edXL16VZMnT9by5ctVr149eXp6Oiz/6KOPcqU4AAAAZF2Ogt3OnTtVt25dSdKvv/6aqwUBAAAgZ3IU7FatWpXbdQAAAOA25egeu27duuns2bOZ2s+fP69u3brddlEAAADIvhwFu2nTpunixYuZ2i9evKjp06ffdlEAAADIvmxdik1NTZUxRsYYnT17VoUKFbIvS09P16JFi1SiRIlcLxIAAAC3lq1g5+vrK5vNJpvNppCQkEzLbTabhg4dmmvFAQAAIOuyFexWrVolY4weffRRzZs3T8WKFbMvc3NzU1BQkEqXLp3rRQIAAODWshXsGjduLEk6dOiQypUrJ5vNlidFAQAAIPty9PBEUFCQ1q5dqw4dOujhhx/W0aNHJUkzZszQ2rVrc7VAAAAAZE2Ogt28efMUFRUlDw8Pbd26VWlpaZKklJQUffDBB7laIAAAALImR8Hu/fff12effaYvv/xSrq6u9vaGDRtq69atuVYcAAAAsi5HwW7fvn0KCwvL1O7j46Pk5OTbrQkAAAA5kKNgFxAQoAMHDmRqX7t2rSpUqHDbRQEAACD7chTsevTooVdffVWxsbGy2Ww6duyYZs6cqX79+umll17K7RoBAACQBdl63ck1b775pjIyMhQREaELFy4oLCxM7u7u6tevn/r06ZPbNQIAACALbMYYk9OVL1++rAMHDujcuXOqXr26vLy8crM25IPU1FT5+PgoJSVF3t7e+V0OAAD3vOx8N2frjF23bt2y1G/y5MnZ2SwAAAByQbaC3dSpUxUUFKT7779ft3GiDwAAAHkgW8HupZde0qxZs3To0CF17dpVHTp0cPh7sQAAAMg/2XoqduLEiTp+/LgGDBig77//XoGBgWrXrp2WLFnCGTwAAIB8dlsPTxw+fFhTp07V9OnTdfXqVe3atYsHKO5yPDwBAEDBkp3v5hy9x86+spOTbDabjDFKT0+/nU0BAADgNmU72KWlpWnWrFlq2rSpQkJCtGPHDn3yySc6cuQIZ+sAAADyUbYenujdu7dmz56twMBAdevWTbNmzZKfn19e1QYAAIBsyNY9dk5OTipXrpzuv/9+2Wy2G/abP39+rhSHO4977AAAKFjy7AXFnTp1ummgAwAAQP7J9guKAQAAUDDd1lOxAAAAKDgIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALAIgh0AAIBFEOwAAAAsgmAHAABgEQQ7AAAAiyDYAQAAWATBDgAAwCIIdgAAABZBsAMAALCIeyLYNWnSRH379s3XGuLj42Wz2bRt27Z8rQMAAFhXvge7xMRE9enTRxUqVJC7u7sCAwPVokULrVixIr9LuyWbzSabzaYNGzY4tKelpal48eKy2WxavXq1JCkwMFDHjx9XzZo186FSAABwL8jXYBcfH6969epp5cqVGj16tHbs2KEff/xR4eHhio6Ozs/SsiwwMFBTpkxxaFuwYIG8vLwc2pydnRUQECAXF5c7WR4AALiH5Guw6927t2w2mzZu3KhnnnlGISEhqlGjhl5//XWHs2AfffSRatWqJU9PTwUGBqp37946d+6cw7bWrVunJk2aqHDhwipatKiioqJ05swZ+/KMjAwNGDBAxYoVU0BAgIYMGeKwfnJysrp37y5/f395e3vr0UcfVVxc3C33oXPnzpo9e7YuXrxob5s8ebI6d+7s0O/vl2LPnDmjF154Qf7+/vLw8FDlypXtAfHy5ct6+eWXVapUKRUqVEhBQUEaMWJElmuNi4tTeHi4ihQpIm9vb9WrV0+bN2++5b4AAIC7W74Fu9OnT+vHH39UdHS0PD09My339fW1/+zk5KTx48dr165dmjZtmlauXKkBAwbYl2/btk0RERGqXr26YmJitHbtWrVo0ULp6en2PtOmTZOnp6diY2M1atQoDRs2TMuWLbMvb9u2rU6ePKnFixdry5Ytqlu3riIiInT69Omb7ke9evUUHBysefPmSZKOHDmin3/+WR07drzpeu+88452796txYsXa8+ePZo0aZL8/PwkSePHj9fChQv13//+V/v27dPMmTMVHByc5VpfeOEFlS1bVps2bdKWLVv05ptvytXV9ab1AAAACzD5JDY21kgy8+fPz/a6c+bMMcWLF7fPt2/f3jRs2PCG/Rs3bmwaNWrk0PbAAw+YgQMHGmOMWbNmjfH29jaXLl1y6FOxYkXz+eef33C7ksyCBQvMuHHjTHh4uDHGmKFDh5pWrVqZM2fOGElm1apVxhhjDh06ZCSZX375xRhjTIsWLUzXrl2vu90+ffqYRx991GRkZGRalpVaixQpYqZOnXrDuv/q0qVLJiUlxT4lJCQYSSYlJSVL6wMAgLyVkpKS5e/mfDtjZ4zJct/ly5crIiJCZcqUUZEiRdSxY0edOnVKFy5ckPR/Z+xupnbt2g7zpUqV0smTJyX9eeny3LlzKl68uLy8vOzToUOHdPDgwVvW16FDB8XExOi3337T1KlT1a1bt1uu89JLL2n27Nm67777NGDAAK1fv96+rEuXLtq2bZuqVKmiV155RUuXLrUvy0qtr7/+urp3767IyEj961//uuk+jBgxQj4+PvYpMDDwlrUDAICCKd+CXeXKlWWz2bR3796b9ouPj1fz5s1Vu3ZtzZs3T1u2bNHEiRMl/XkvmiR5eHjccry/X4q02WzKyMiQJJ07d06lSpXStm3bHKZ9+/apf//+t9x28eLF1bx5c7344ou6dOmSHn/88Vuu8/jjj+vw4cN67bXXdOzYMUVERKhfv36SpLp16+rQoUN67733dPHiRbVr105t2rTJcq1DhgzRrl279OSTT2rlypWqXr26FixYcN06Bg0apJSUFPuUkJBwy9oBAEDBlG/BrlixYoqKitLEiRN1/vz5TMuTk5MlSVu2bFFGRobGjBmjhx56SCEhITp27JhD39q1a9/W61Hq1q2rxMREubi4qFKlSg7TtfvebqVbt25avXq1OnXqJGdn5yyt4+/vr86dO+vrr7/WuHHj9MUXX9iXeXt769lnn9WXX36pb775RvPmzdPp06ezXGtISIhee+01LV26VK1bt8705O417u7u8vb2dpgAAMDdKV+fip04caLS09MVGhqqefPmaf/+/dqzZ4/Gjx+vBg0aSJIqVaqkK1euaMKECfrtt980Y8YMffbZZw7bGTRokDZt2qTevXtr+/bt2rt3ryZNmqSkpKQs1REZGakGDRqoZcuWWrp0qeLj47V+/Xq99dZbWX6atFmzZvrjjz80bNiwLPV/99139d133+nAgQPatWuXfvjhB1WrVk3Sn08Bz5o1S3v37tWvv/6qOXPmKCAgQL6+vres9eLFi3r55Ze1evVqHT58WOvWrdOmTZvs2wYAANaVr8GuQoUK2rp1q8LDw/XGG2+oZs2aatq0qVasWKFJkyZJkurUqaOPPvpII0eOVM2aNTVz5kyHV39If56dWrp0qeLi4hQaGqoGDRrou+++y/I742w2mxYtWqSwsDB17dpVISEheu6553T48GGVLFkyy9vw8/OTm5tblvq7ublp0KBBql27tsLCwuTs7KzZs2dLkooUKaJRo0apfv36euCBBxQfH69FixbJycnplrU6Ozvr1KlT6tSpk0JCQtSuXTs9/vjjGjp0aJbqAgAAdy+byc5TDLC81NRU+fj4KCUlhcuyAAAUANn5bs73PykGAACA3EGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYJcNTZo0Ud++fe/Z8QEAQMFmmWCXmJioPn36qEKFCnJ3d1dgYKBatGihFStW5Hdpt2Sz2fTtt99mau/SpYtatmxpn58/f77ee+89+3xwcLDGjRuX9wUCAIC7gkt+F5Ab4uPj1bBhQ/n6+mr06NGqVauWrly5oiVLlig6Olp79+7N7xJzRbFixfK7BAAAUIBZ4oxd7969ZbPZtHHjRj3zzDMKCQlRjRo19Prrr2vDhg32fh999JFq1aolT09PBQYGqnfv3jp37pzDttatW6cmTZqocOHCKlq0qKKionTmzBn78oyMDA0YMEDFihVTQECAhgwZ4rB+cnKyunfvLn9/f3l7e+vRRx9VXFxcruznXy/FNmnSRIcPH9Zrr70mm80mm80mSTp8+LBatGihokWLytPTUzVq1NCiRYtyZXwAAFCw3fXB7vTp0/rxxx8VHR0tT0/PTMt9fX3tPzs5OWn8+PHatWuXpk2bppUrV2rAgAH25du2bVNERISqV6+umJgYrV27Vi1atFB6erq9z7Rp0+Tp6anY2FiNGjVKw4YN07Jly+zL27Ztq5MnT2rx4sXasmWL6tatq4iICJ0+fTpX93v+/PkqW7ashg0bpuPHj+v48eOSpOjoaKWlpennn3/Wjh07NHLkSHl5eeXq2AAAoGC66y/FHjhwQMYYVa1a9ZZ9//rgQXBwsN5//3316tVLn376qSRp1KhRql+/vn1ekmrUqOGwjdq1a2vw4MGSpMqVK+uTTz7RihUr1LRpU61du1YbN27UyZMn5e7uLkn68MMP9e2332ru3Lnq2bPnDWtr3769nJ2dHdrS0tL05JNPXrd/sWLF5OzsrCJFiiggIMDefuTIET3zzDOqVauWJKlChQo3PSZpaWlKS0uzz6empt60PwAAKLju+mBnjMly3+XLl2vEiBHau3evUlNTdfXqVV26dEkXLlxQ4cKFtW3bNrVt2/am26hdu7bDfKlSpXTy5ElJUlxcnM6dO6fixYs79Ll48aIOHjx40+2OHTtWkZGRDm0DBw50OFuYFa+88opeeuklLV26VJGRkXrmmWcy1fxXI0aM0NChQ7M1BgAAKJju+kuxlStXls1mu+UDEvHx8WrevLlq166tefPmacuWLZo4caIk6fLly5IkDw+PW47n6urqMG+z2ZSRkSFJOnfunEqVKqVt27Y5TPv27VP//v1vut2AgABVqlTJYSpSpMgt6/m77t2767ffflPHjh21Y8cO1a9fXxMmTLhh/0GDBiklJcU+JSQkZHtMAABQMNz1wa5YsWKKiorSxIkTdf78+UzLk5OTJUlbtmxRRkaGxowZo4ceekghISE6duyYQ9/atWvf1utR6tatq8TERLm4uGQKaX5+fjne7o24ubld94xeYGCgevXqpfnz5+uNN97Ql19+ecNtuLu7y9vb22ECAAB3p7s+2EnSxIkTlZ6ertDQUM2bN0/79+/Xnj17NH78eDVo0ECSVKlSJV25ckUTJkzQb7/9phkzZuizzz5z2M6gQYO0adMm9e7dW9u3b9fevXs1adIkJSUlZamOyMhINWjQQC1bttTSpUsVHx+v9evX66233tLmzZtzfb+Dg4P1888/6+jRo/Ya+/btqyVLlujQoUPaunWrVq1apWrVquX62AAAoOCxRLCrUKGCtm7dqvDwcL3xxhuqWbOmmjZtqhUrVmjSpEmSpDp16uijjz7SyJEjVbNmTc2cOVMjRoxw2E5ISIiWLl2quLg4hYaGqkGDBvruu+/k4pK1WxFtNpsWLVqksLAwde3aVSEhIXruued0+PBhlSxZMtf3e9iwYYqPj1fFihXl7+8vSUpPT1d0dLSqVaumZs2aKSQkxOFhEAAAYF02k52nD2B5qamp8vHxUUpKCpdlAQAoALLz3WyJM3YAAAAg2AEAAFgGwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAIsg2AEAAFgEwQ4AAMAiCHYAAAAWQbADAACwCIIdAACARRDsAAAALIJgBwAAYBEEOwAAAItwye8CULAYYyRJqamp+VwJAACQ/u87+dp39M0Q7ODg7NmzkqTAwMB8rgQAAPzV2bNn5ePjc9M+NpOV+Id7RkZGho4dO6YiRYrIZrPldzn5JjU1VYGBgUpISJC3t3d+l2NJHOO8xfHNexzjvMcx/pMxRmfPnlXp0qXl5HTzu+g4YwcHTk5OKlu2bH6XUWB4e3vf0/+Y3Akc47zF8c17HOO8xzHWLc/UXcPDEwAAABZBsAMAALAIgh1wHe7u7ho8eLDc3d3zuxTL4hjnLY5v3uMY5z2Ocfbx8AQAAIBFcMYOAADAIgh2AAAAFkGwAwAAsAiCHe5Jp0+f1gsvvCBvb2/5+vrqxRdf1Llz5266zqVLlxQdHa3ixYvLy8tLzzzzjE6cOHHdvqdOnVLZsmVls9mUnJycB3tQ8OXFMY6Li1P79u0VGBgoDw8PVatWTR9//HFe70qBMXHiRAUHB6tQoUJ68MEHtXHjxpv2nzNnjqpWrapChQqpVq1aWrRokcNyY4zeffddlSpVSh4eHoqMjNT+/fvzchcKtNw8vleuXNHAgQNVq1YteXp6qnTp0urUqZOOHTuW17tRoOX27/Bf9erVSzabTePGjcvlqu8yBrgHNWvWzNSpU8ds2LDBrFmzxlSqVMm0b9/+puv06tXLBAYGmhUrVpjNmzebhx56yDz88MPX7fv000+bxx9/3EgyZ86cyYM9KPjy4hh/9dVX5pVXXjGrV682Bw8eNDNmzDAeHh5mwoQJeb07+W727NnGzc3NTJ482ezatcv06NHD+Pr6mhMnTly3/7p164yzs7MZNWqU2b17t3n77beNq6ur2bFjh73Pv/71L+Pj42O+/fZbExcXZ5566ilTvnx5c/HixTu1WwVGbh/f5ORkExkZab755huzd+9eExMTY0JDQ029evXu5G4VKHnxO3zN/PnzTZ06dUzp0qXN2LFj83hPCjaCHe45u3fvNpLMpk2b7G2LFy82NpvNHD169LrrJCcnG1dXVzNnzhx72549e4wkExMT49D3008/NY0bNzYrVqy4Z4NdXh/jv+rdu7cJDw/PveILqNDQUBMdHW2fT09PN6VLlzYjRoy4bv927dqZJ5980qHtwQcfNP/4xz+MMcZkZGSYgIAAM3r0aPvy5ORk4+7ubmbNmpUHe1Cw5fbxvZ6NGzcaSebw4cO5U/RdJq+O8e+//27KlCljdu7caYKCgu75YMelWNxzYmJi5Ovrq/r169vbIiMj5eTkpNjY2Ouus2XLFl25ckWRkZH2tqpVq6pcuXKKiYmxt+3evVvDhg3T9OnTb/n3/KwsL4/x36WkpKhYsWK5V3wBdPnyZW3ZssXh2Dg5OSkyMvKGxyYmJsahvyRFRUXZ+x86dEiJiYkOfXx8fPTggw/e9HhbUV4c3+tJSUmRzWaTr69vrtR9N8mrY5yRkaGOHTuqf//+qlGjRt4Uf5e5d795cM9KTExUiRIlHNpcXFxUrFgxJSYm3nAdNze3TP8glyxZ0r5OWlqa2rdvr9GjR6tcuXJ5UvvdIq+O8d+tX79e33zzjXr27JkrdRdUSUlJSk9PV8mSJR3ab3ZsEhMTb9r/2n+zs02ryovj+3eXLl3SwIED1b59+3vyb57m1TEeOXKkXFxc9Morr+R+0Xcpgh0s480335TNZrvptHfv3jwbf9CgQapWrZo6dOiQZ2Pkt/w+xn+1c+dOPf300xo8eLAee+yxOzImkBNXrlxRu3btZIzRpEmT8rscy9iyZYs+/vhjTZ06VTabLb/LKTBc8rsAILe88cYb6tKly037VKhQQQEBATp58qRD+9WrV3X69GkFBARcd72AgABdvnxZycnJDmeUTpw4YV9n5cqV2rFjh+bOnSvpzycOJcnPz09vvfWWhg4dmsM9Kzjy+xhfs3v3bkVERKhnz556++23c7QvdxM/Pz85Oztnegr7esfmmoCAgJv2v/bfEydOqFSpUg597rvvvlysvuDLi+N7zbVQd/jwYa1cufKePFsn5c0xXrNmjU6ePOlwhSQ9PV1vvPGGxo0bp/j4+NzdibtFft/kB9xp127s37x5s71tyZIlWbqxf+7cufa2vXv3OtzYf+DAAbNjxw77NHnyZCPJrF+//oZPfVlVXh1jY4zZuXOnKVGihOnfv3/e7UABFBoaal5++WX7fHp6uilTpsxNbzxv3ry5Q1uDBg0yPTzx4Ycf2penpKTc0w9P5ObxNcaYy5cvm5YtW5oaNWqYkydP5k3hd5HcPsZJSUkO/+bu2LHDlC5d2gwcONDs3bs373akgCPY4Z7UrFkzc//995vY2Fizdu1aU7lyZYdXcfz++++mSpUqJjY21t7Wq1cvU65cObNy5UqzefNm06BBA9OgQYMbjrFq1ap79qlYY/LmGO/YscP4+/ubDh06mOPHj9une+FLc/bs2cbd3d1MnTrV7N692/Ts2dP4+vqaxMREY4wxHTt2NG+++aa9/7p164yLi4v58MMPzZ49e8zgwYOv+7oTX19f891335nt27ebp59++p5+3UluHt/Lly+bp556ypQtW9Zs27bN4fc1LS0tX/Yxv+XF7/Df8VQswQ73qFOnTpn27dsbLy8v4+3tbbp27WrOnj1rX37o0CEjyaxatcredvHiRdO7d29TtGhRU7hwYdOqVStz/PjxG45xrwe7vDjGgwcPNpIyTUFBQXdwz/LPhAkTTLly5Yybm5sJDQ01GzZssC9r3Lix6dy5s0P///73vyYkJMS4ubmZGjVqmP/9738OyzMyMsw777xjSpYsadzd3U1ERITZt2/fndiVAik3j++13+/rTX/9nb/X5Pbv8N8R7IyxGfP/bwQCAADAXY2nYgEAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcABUxiYqL69OmjChUqyN3dXYGBgWrRooVWrFhxR+uw2Wz69ttv7+iYAG6PS34XAAD4P/Hx8WrYsKF8fX01evRo1apVS1euXNGSJUsUHR2tvXv35neJAAow/lYsABQgTzzxhLZv3659+/bJ09PTYVlycrJ8fX115MgR9enTRytWrJCTk5OaNWumCRMmqGTJkpKkLl26KDk52eFsW9++fbVt2zatXr1aktSkSRPVrl1bhQoV0r///W+5ubmpV69eGjJkiCQpODhYhw8ftq8fFBSk+Pj4vNx1ALmAS7EAUECcPn1aP/74o6KjozOFOkny9fVVRkaGnn76aZ0+fVo//fSTli1bpt9++03PPvtstsebNm2aPD09FRsbq1GjRmnYsGFatmyZJGnTpk2SpClTpuj48eP2eQAFG5diAaCAOHDggIwxqlq16g37rFixQjt27NChQ4cUGBgoSZo+fbpq1KihTZs26YEHHsjyeLVr19bgwYMlSZUrV9Ynn3yiFStWqGnTpvL395f0Z5gMCAi4jb0CcCdxxg4ACois3BmzZ88eBQYG2kOdJFWvXl2+vr7as2dPtsarXbu2w3ypUqV08uTJbG0DQMFCsAOAAqJy5cqy2Wy3/YCEk5NTppB45cqVTP1cXV0d5m02mzIyMm5rbAD5i2AHAAVEsWLFFBUVpYkTJ+r8+fOZlicnJ6tatWpKSEhQQkKCvX337t1KTk5W9erVJUn+/v46fvy4w7rbtm3Ldj2urq5KT0/P9noA8g/BDgAKkIkTJyo9PV2hoaGaN2+e9u/frz179mj8+PFq0KCBIiMjVatWLb3wwgvaunWrNm7cqE6dOqlx48aqX7++JOnRRx/V5s2bNX36dO3fv1+DBw/Wzp07s11LcHCwVqxYocTERJ05cya3dxVAHiDYAUABUqFCBW3dulXh4eF64403VLNmTTVt2lQrVqzQpEmTZLPZ9N1336lo0aIKCwtTZGSkKlSooG+++ca+jaioKL3zzjsaMGCAHnjgAZ09e1adOnXKdi1jxozRsmXLFBgYqPvvvz83dxNAHuE9dgAAABbBGTsAAACLINgBAABYBMEOAADAIgh2AAAAFkGwAwAAsAiCHQAAgEUQ7AAAACyCYAcAAGARBDsAAACLINgBAABYBMEOAADAIgh2AAAAFvH/AHuy+VppYaozAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Redis statistics retrieved successfully\n"
     ]
    }
   ],
   "source": [
    "# Lab 09: Semantic Caching - Optional: View Redis Cache Statistics\n",
    "# Adapted from working semantic-caching.ipynb\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "print(\"\\n[*] Optional: Viewing Redis Cache Statistics...\")\n",
    "print(\"    This shows cache hits, misses, and memory usage\")\n",
    "\n",
    "try:\n",
    "    import redis.asyncio as redis\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Get Redis configuration from master-lab.env\n",
    "    redis_host = os.environ.get('REDIS_HOST')\n",
    "    redis_port = int(os.environ.get('REDIS_PORT', 10000))\n",
    "    redis_key = os.environ.get('REDIS_KEY')\n",
    "\n",
    "    async def get_redis_info():\n",
    "        r = await redis.from_url(\n",
    "            f\"rediss://:{redis_key}@{redis_host}:{redis_port}\"\n",
    "        )\n",
    "\n",
    "        info = await r.info()\n",
    "\n",
    "        print(\"\\n\ud83d\udcca Redis Server Information:\")\n",
    "        print(f\"   Used Memory: {info['used_memory_human']}\")\n",
    "        print(f\"   Cache Hits: {info['keyspace_hits']}\")\n",
    "        print(f\"   Cache Misses: {info['keyspace_misses']}\")\n",
    "        print(f\"   Evicted Keys: {info['evicted_keys']}\")\n",
    "        print(f\"   Expired Keys: {info['expired_keys']}\")\n",
    "\n",
    "        # Calculate hit rate\n",
    "        total = info['keyspace_hits'] + info['keyspace_misses']\n",
    "        if total > 0:\n",
    "            hit_rate = (info['keyspace_hits'] / total) * 100\n",
    "            print(f\"   Hit Rate: {hit_rate:.1f}%\")\n",
    "\n",
    "        # Create visualization\n",
    "        redis_info = {\n",
    "            'Metric': ['Cache Hits', 'Cache Misses', 'Evicted Keys', 'Expired Keys'],\n",
    "            'Value': [info['keyspace_hits'], info['keyspace_misses'], info['evicted_keys'], info['expired_keys']]\n",
    "        }\n",
    "\n",
    "        df_redis_info = pd.DataFrame(redis_info)\n",
    "        df_redis_info.plot(kind='barh', x='Metric', y='Value', legend=False, color='teal')\n",
    "\n",
    "        plt.title('Redis Cache Statistics')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Metric')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        await r.aclose()\n",
    "        print(\"\\n\u2705 Redis statistics retrieved successfully\")\n",
    "\n",
    "    # Run async function\n",
    "    await get_redis_info()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n\u26a0\ufe0f  redis package not available\")\n",
    "    print(\"   Install with: pip install redis\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u26a0\ufe0f  Could not connect to Redis: {str(e)[:100]}\")\n",
    "    print(\"   Make sure Redis is configured in master-lab.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 Complete!\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "\u2705 How semantic caching reduces API calls for similar queries  \n",
    "\u2705 How to measure caching performance  \n",
    "\u2705 How vector embeddings enable semantic similarity matching  \n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "\ud83d\udcb0 **Cost savings**: Reduced Azure OpenAI API calls (up to 90% reduction!)  \n",
    "\u26a1 **Performance**: Faster response times (15-100x faster for cached requests)  \n",
    "\ud83d\udcca **Scalability**: Better handling of repetitive queries  \n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Similarity Threshold**: 0.8 (80% match required)\n",
    "- **Cache TTL**: 20 minutes (1200 seconds)\n",
    "- **Embeddings Model**: text-embedding-3-small\n",
    "- **Cache Storage**: Redis\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Integrate semantic caching into your production APIs to reduce costs and improve performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615aa5b3",
   "metadata": {},
   "source": [
    "<a id=\"lab1-2\"></a>\n",
    "\n",
    "## 1.2 Message Storing with Cosmos DB\n",
    "\n",
    "#### Objective\n",
    "Build a persistent audit trail of all LLM interactions by storing prompts, completions, and token metrics in Cosmos DB. This lab demonstrates a data pipeline from APIM logging through Event Hub to long-term storage.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Built-in LLM Logging:** Capture prompts and completions automatically\n",
    "- **Event Hub Integration:** Stream logging data to Event Hub\n",
    "- **Stream Analytics:** Process and transform log data in flight\n",
    "- **Cosmos DB Storage:** Persist structured interaction data\n",
    "- **Document Querying:** Query stored interactions for audit and analysis\n",
    "- **Data Pipeline:** Understand full flow from API to persistent storage\n",
    "- **Scalable Architecture:** Handle high-volume LLM interactions\n",
    "\n",
    "#### How It Works\n",
    "1. User request processed by APIM\n",
    "2. Built-in logging captures prompt, completion, and metadata\n",
    "3. Logs sent to Azure Monitor\n",
    "4. Diagnostic settings export logs to Event Hub\n",
    "5. Stream Analytics consumes Event Hub messages\n",
    "6. Analytics transforms and enriches message data\n",
    "7. Data written to Cosmos DB for long-term storage\n",
    "8. Applications query Cosmos DB for interaction history\n",
    "\n",
    "#### Data Flow Diagram\n",
    "```\n",
    "[APIM] \u2192 [Azure Monitor] \u2192 [Event Hub] \u2192 [Stream Analytics] \u2192 [Cosmos DB]\n",
    "```\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "- Cosmos DB account (created during deployment)\n",
    "- Event Hub namespace (created during deployment)\n",
    "- Stream Analytics job (created during deployment)\n",
    "\n",
    "#### Expected Results\n",
    "- Prompts and completions logged to Azure Monitor\n",
    "- Logs appear in Event Hub within seconds\n",
    "- Stream Analytics job processes and transforms data\n",
    "- Documents appear in Cosmos DB within 1-2 minutes\n",
    "- Can query stored interactions by user, timestamp, model\n",
    "- Audit trail shows complete interaction history\n",
    "- Token metrics aggregated and stored\n",
    "\n",
    "#### Sample Cosmos DB Query\n",
    "```kusto\n",
    "SELECT c.user_id, c.prompt, c.completion, c.token_count, c.timestamp\n",
    "FROM messages c\n",
    "WHERE c.timestamp > GetCurrentTimestamp() - 3600\n",
    "ORDER BY c.timestamp DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: master-lab.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 1: Connecting to Cosmos DB for message storage...\n",
      "    Cosmos Account: cosmos-c7uj6vzppah74\n",
      "    Endpoint: https://cosmos-c7uj6vzppah74.documents.azure.com:443/\n",
      "    Database: messages-db\n",
      "    Container: conversations\n",
      "\n",
      "[*] Creating Cosmos DB client with Azure AD...\n",
      "\u2705 Cosmos DB client created with Azure AD authentication\n",
      "\n",
      "[*] Connecting to database 'messages-db'...\n",
      "\u2705 Connected to database 'messages-db'\n",
      "\n",
      "[*] Connecting to container 'conversations'...\n",
      "\u2705 Connected to container 'conversations'\n",
      "\n",
      "\u2705 Cosmos DB setup complete!\n",
      "\n",
      "\ud83d\udccb Summary:\n",
      "   Database: messages-db\n",
      "   Container: conversations\n",
      "   Partition Key: /conversationId\n",
      "   Auth: Azure AD (DefaultAzureCredential)\n",
      "   Operation: GET existing resources (no WRITE needed)\n",
      "\n",
      "[OK] Step 1 Complete - Ready to store messages\n"
     ]
    }
   ],
   "source": [
    "# Lab 10: Message Storing - Step 1: Setup Cosmos DB (Azure AD Auth)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(\"[config] Loaded: master-lab.env\")\n",
    "\n",
    "from azure.cosmos import CosmosClient, exceptions\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Get Cosmos DB config\n",
    "cosmos_endpoint = os.environ.get('COSMOS_ENDPOINT')\n",
    "cosmos_account = os.environ.get('COSMOS_ACCOUNT_NAME')\n",
    "\n",
    "database_name = \"messages-db\"\n",
    "container_name = \"conversations\"\n",
    "\n",
    "print(\"\\n[*] Step 1: Connecting to Cosmos DB for message storage...\")\n",
    "print(f\"    Cosmos Account: {cosmos_account}\")\n",
    "print(f\"    Endpoint: {cosmos_endpoint}\")\n",
    "print(f\"    Database: {database_name}\")\n",
    "print(f\"    Container: {container_name}\")\n",
    "\n",
    "try:\n",
    "    # Use Azure AD authentication (local auth disabled on this account)\n",
    "    print(\"\\n[*] Creating Cosmos DB client with Azure AD...\")\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = CosmosClient(cosmos_endpoint, credential)\n",
    "    print(\"\u2705 Cosmos DB client created with Azure AD authentication\")\n",
    "\n",
    "    # Get existing database (created via Azure CLI)\n",
    "    print(f\"\\n[*] Connecting to database '{database_name}'...\")\n",
    "    database = client.get_database_client(database_name)\n",
    "    print(f\"\u2705 Connected to database '{database_name}'\")\n",
    "\n",
    "    # Get existing container (created via Azure CLI)\n",
    "    print(f\"\\n[*] Connecting to container '{container_name}'...\")\n",
    "    container = database.get_container_client(container_name)\n",
    "    print(f\"\u2705 Connected to container '{container_name}'\")\n",
    "\n",
    "    print(\"\\n\u2705 Cosmos DB setup complete!\")\n",
    "    print(\"\\n\ud83d\udccb Summary:\")\n",
    "    print(f\"   Database: {database_name}\")\n",
    "    print(f\"   Container: {container_name}\")\n",
    "    print(f\"   Partition Key: /conversationId\")\n",
    "    print(f\"   Auth: Azure AD (DefaultAzureCredential)\")\n",
    "    print(f\"   Operation: GET existing resources (no WRITE needed)\")\n",
    "    print(\"\\n[OK] Step 1 Complete - Ready to store messages\")\n",
    "\n",
    "except exceptions.CosmosResourceNotFoundError as e:\n",
    "    print(f\"\\n\u274c Error: Database or container not found\")\n",
    "    print(f\"\\nThe resources may not have been created yet.\")\n",
    "    print(f\"\\nTo create via Azure CLI:\")\n",
    "    print(f\"  az cosmosdb sql database create --account-name {cosmos_account} --resource-group lab-master-lab --name {database_name}\")\n",
    "    print(f\"  az cosmosdb sql container create --account-name {cosmos_account} --resource-group lab-master-lab --database-name {database_name} --name {container_name} --partition-key-path /conversationId --throughput 400\")\n",
    "    raise\n",
    "\n",
    "except exceptions.CosmosHttpResponseError as e:\n",
    "    if 'Forbidden' in str(e) or 'does not have required permissions' in str(e):\n",
    "        print(f\"\\n\u274c Error: RBAC permissions missing\")\n",
    "        print(f\"\\nYour identity needs 'Cosmos DB Built-in Data Reader' role (for GET operations)\")\n",
    "        print(f\"\\nNote: WRITE permissions not needed when using pre-created resources\")\n",
    "        raise\n",
    "    else:\n",
    "        print(f\"\\n\u274c Error connecting to Cosmos DB: {e}\")\n",
    "        raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Error setting up Cosmos DB: {e}\")\n",
    "    print(f\"\\n\ud83d\udca1 Check:\")\n",
    "    print(\"   - You're logged in: az login\")\n",
    "    print(\"   - Cosmos DB allows public network access\")\n",
    "    print(\"   - Database and container exist\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-2-1\"></a>\n",
    "\n",
    "### 1.2.1 Generate Test Conversations\n",
    "\n",
    "\n",
    "**Purpose**: Generates and stores AI conversation messages in Cosmos DB for auditing and analysis\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Cosmos DB deployed (from infrastructure)\n",
    "- Environment variables: `COSMOS_ENDPOINT`, `COSMOS_KEY`\n",
    "- Azure OpenAI client configured\n",
    "- Database and container created (from Step 1)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Generates sample conversations and stores them in Cosmos DB:**\n",
    "\n",
    "1. **Creates Test Conversations:**\n",
    "   - Generates multiple chat completions\n",
    "   - Varies conversation topics and lengths\n",
    "   - Includes user messages and AI responses\n",
    "   - Adds metadata (timestamp, user ID, session ID)\n",
    "\n",
    "2. **Stores in Cosmos DB:**\n",
    "   - Each message stored as separate document\n",
    "   - Includes conversation context\n",
    "   - Partitioned by user ID for scalability\n",
    "   - Indexed for fast querying\n",
    "\n",
    "3. **Message Schema:**\n",
    "   ```json\n",
    "   {\n",
    "     \"id\": \"unique-message-id\",\n",
    "     \"conversation_id\": \"session-uuid\",\n",
    "     \"user_id\": \"user-identifier\",\n",
    "     \"role\": \"user\" or \"assistant\",\n",
    "     \"content\": \"message text\",\n",
    "     \"timestamp\": \"ISO 8601 datetime\",\n",
    "     \"model\": \"gpt-4o-mini\",\n",
    "     \"tokens_used\": 150,\n",
    "     \"metadata\": {}\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Use Cases:**\n",
    "   - Compliance and auditing\n",
    "   - Conversation history\n",
    "   - User behavior analysis\n",
    "   - Quality assurance\n",
    "   - Training data collection\n",
    "   - Cost tracking per user\n",
    "\n",
    "5. **Compliance Benefits:**\n",
    "   - Complete audit trail\n",
    "   - Searchable conversation history\n",
    "   - Retention policy enforcement\n",
    "   - PII handling and redaction support\n",
    "   - Geographic data residency (via Cosmos DB regions)\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Progress output showing:\n",
    "- Number of conversations generated: <count>\n",
    "- Number of messages stored: <count>\n",
    "- Sample conversation IDs\n",
    "- Confirmation of successful Cosmos DB writes\n",
    "- Total tokens used in test data\n",
    "\n",
    "Example:\n",
    "Generating conversation 1/5... \u2713\n",
    "Generating conversation 2/5... \u2713\n",
    "...\n",
    "Successfully stored 25 messages in Cosmos DB\n",
    "Total tokens used: 1,450\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 2: Generating sample conversations and storing in Cosmos DB...\n",
      "    Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "    Model: gpt-4o-mini\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcac GENERATING CONVERSATIONS\n",
      "================================================================================\n",
      "Conversation ID: 273ef081-0b4a-4a3c-905b-e52b0e558214\n",
      "\n",
      "\u25b6\ufe0f  Message 1/5: What is Azure API Management?\n",
      "   \u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '605c5feb-43\n",
      "\n",
      "\u25b6\ufe0f  Message 2/5: Explain semantic caching in simple terms\n",
      "   \u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '45608b17-26\n",
      "\n",
      "\u25b6\ufe0f  Message 3/5: How do I optimize AI costs?\n",
      "   \u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'b205ea16-cf\n",
      "\n",
      "\u25b6\ufe0f  Message 4/5: What are the benefits of using APIM with Azure OpenAI?\n",
      "   \u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'e5e95921-07\n",
      "\n",
      "\u25b6\ufe0f  Message 5/5: Tell me about vector databases\n",
      "   \u274c Error: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '8046891c-97\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcca CONVERSATION SUMMARY\n",
      "================================================================================\n",
      "Total Messages: 0\n",
      "Conversation ID: 273ef081-0b4a-4a3c-905b-e52b0e558214\n",
      "\n",
      "\u26a0\ufe0f  No messages were stored\n",
      "\n",
      "[OK] Step 2 Complete - Conversations stored in Cosmos DB\n"
     ]
    }
   ],
   "source": [
    "# Lab: Message Storing - Step 2: Generate and Store Conversations\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Get API config\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "print(\"\\n[*] Step 2: Generating sample conversations and storing in Cosmos DB...\")\n",
    "print(f\"    Endpoint: {apim_gateway_url}/{inference_api_path}\")\n",
    "print(f\"    Model: gpt-4o-mini\")\n",
    "\n",
    "# Check if container exists (from cell 66)\n",
    "if 'container' not in globals():\n",
    "    print(\"\\n\u274c Error: Container not initialized\")\n",
    "    print(\"   Please run Cell 66 first to set up Cosmos DB\")\n",
    "    raise NameError(\"Run Cell 66 first to initialize Cosmos DB container\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client_openai = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "# Sample questions\n",
    "questions = [\n",
    "    \"What is Azure API Management?\",\n",
    "    \"Explain semantic caching in simple terms\",\n",
    "    \"How do I optimize AI costs?\",\n",
    "    \"What are the benefits of using APIM with Azure OpenAI?\",\n",
    "    \"Tell me about vector databases\"\n",
    "]\n",
    "\n",
    "conversation_id = str(uuid.uuid4())\n",
    "messages_stored = []\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\ud83d\udcac GENERATING CONVERSATIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Conversation ID: {conversation_id}\\n\")\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\u25b6\ufe0f  Message {i}/{len(questions)}: {question}\")\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI\n",
    "        start_time = time.time()\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "\n",
    "        # Extract response\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "\n",
    "        print(f\"   \u2705 Response received ({response_time:.2f}s, {total_tokens} tokens)\")\n",
    "\n",
    "        # Store in Cosmos DB\n",
    "        message_doc = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"conversationId\": conversation_id,\n",
    "            \"messageNumber\": i,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"userMessage\": question,\n",
    "            \"assistantMessage\": assistant_message,\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"promptTokens\": prompt_tokens,\n",
    "            \"completionTokens\": completion_tokens,\n",
    "            \"totalTokens\": total_tokens,\n",
    "            \"responseTime\": response_time\n",
    "        }\n",
    "\n",
    "        container.create_item(body=message_doc)\n",
    "        messages_stored.append(message_doc)\n",
    "        print(f\"   \ud83d\udcbe Stored in Cosmos DB\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error: {str(e)[:100]}\\n\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\ud83d\udcca CONVERSATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total Messages: {len(messages_stored)}\")\n",
    "print(f\"Conversation ID: {conversation_id}\")\n",
    "\n",
    "if messages_stored:\n",
    "    total_tokens_used = sum(m['totalTokens'] for m in messages_stored)\n",
    "    print(f\"Total Tokens Used: {total_tokens_used}\")\n",
    "    print(f\"\\n\u2705 All messages stored successfully!\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  No messages were stored\")\n",
    "\n",
    "print(\"\\n[OK] Step 2 Complete - Conversations stored in Cosmos DB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-2-2\"></a>\n",
    "\n",
    "### 1.2.2 Query Stored Messages\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates querying and analyzing stored conversation messages from Cosmos DB\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Messages stored in Cosmos DB (from previous cell)\n",
    "- Cosmos DB Python SDK\n",
    "- pandas library for data analysis\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Performs various queries on stored conversation data:**\n",
    "\n",
    "1. **Query Examples:**\n",
    "   - Get all messages for a specific user\n",
    "   - Get all messages in a conversation\n",
    "   - Find conversations by date range\n",
    "   - Search messages by content keywords\n",
    "   - Aggregate token usage by user\n",
    "   - Calculate conversation statistics\n",
    "\n",
    "2. **SQL Query Features:**\n",
    "   - Uses Cosmos DB SQL API\n",
    "   - Supports complex filtering\n",
    "   - Enables cross-partition queries\n",
    "   - Provides pagination for large results\n",
    "\n",
    "3. **Analytics Queries:**\n",
    "   ```sql\n",
    "   -- Total tokens per user\n",
    "   SELECT c.user_id, SUM(c.tokens_used) as total_tokens\n",
    "   FROM conversations c\n",
    "   GROUP BY c.user_id\n",
    "\n",
    "   -- Recent conversations\n",
    "   SELECT * FROM conversations c\n",
    "   WHERE c.timestamp > '2025-01-01'\n",
    "   ORDER BY c.timestamp DESC\n",
    "   ```\n",
    "\n",
    "4. **Data Visualization:**\n",
    "   - Displays results as formatted tables\n",
    "   - Shows conversation flow\n",
    "   - Highlights key metrics\n",
    "\n",
    "5. **Business Value:**\n",
    "   - User engagement metrics\n",
    "   - Cost allocation per user/department\n",
    "   - Quality monitoring\n",
    "   - Compliance reporting\n",
    "   - Usage pattern analysis\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Query results displayed as pandas DataFrames:\n",
    "\n",
    "**Most Recent Conversations:**\n",
    "| Timestamp           | User ID | Role      | Content Preview       | Tokens |\n",
    "|---------------------|---------|-----------|----------------------|--------|\n",
    "| 2025-01-15 14:23:10 | user123 | user      | What is...           | 15     |\n",
    "| 2025-01-15 14:23:15 | user123 | assistant | The answer...        | 145    |\n",
    "| ...                 | ...     | ...       | ...                  | ...    |\n",
    "\n",
    "**Token Usage by User:**\n",
    "| User ID | Total Tokens | Conversations | Avg Tokens/Conv |\n",
    "|---------|--------------|---------------|-----------------|\n",
    "| user123 | 2,450        | 12            | 204             |\n",
    "| user456 | 1,890        | 8             | 236             |\n",
    "| ...     | ...          | ...           | ...             |\n",
    "\n",
    "This demonstrates the power of Cosmos DB for conversation analytics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 3: Querying stored messages from Cosmos DB...\n",
      "\n",
      "\u274c Error querying Cosmos DB: (NotFound) Message: {\"Errors\":[\"Owner resource does not exist\"]}\n",
      "ActivityId: f3b1e823-01d2-49d3-a536-1d2b5eaed18a, Request URI: /apps/270664d4-bd39-4118-86ad-9bd53fb8142d/services/1abb6010-04d4-46e7-97fd-6ebcc9277629/partitions/14c0a943-43eb-495a-ba6d-58397030e41f/replicas/134051340024680768s, RequestStats: , SDK: Microsoft.Azure.Documents.Common/2.14.0\n",
      "Code: NotFound\n",
      "Message: Message: {\"Errors\":[\"Owner resource does not exist\"]}\n",
      "ActivityId: f3b1e823-01d2-49d3-a536-1d2b5eaed18a, Request URI: /apps/270664d4-bd39-4118-86ad-9bd53fb8142d/services/1abb6010-04d4-46e7-97fd-6ebcc9277629/partitions/14c0a943-43eb-495a-ba6d-58397030e41f/replicas/134051340024680768s, RequestStats: , SDK: Microsoft.Azure.Documents.Common/2.14.0\n",
      "\n",
      "================================================================================\n",
      "\ud83c\udf89 LAB 10 COMPLETE: MESSAGE STORING\n",
      "================================================================================\n",
      "\n",
      "What you learned:\n",
      "\u2705 How to set up Cosmos DB with Azure AD authentication\n",
      "\u2705 How to capture prompts, completions, and token counts\n",
      "\u2705 How to query and analyze stored conversation data\n",
      "\u2705 How to track usage patterns and costs\n",
      "\n",
      "Key Benefits:\n",
      "\ud83d\udcca Analytics: Understand usage patterns and trends\n",
      "\ud83d\udcb0 Cost Tracking: Monitor token usage and costs\n",
      "\ud83d\udd0d Auditing: Maintain complete conversation history\n",
      "\ud83d\udcc8 Insights: Analyze response quality and performance\n"
     ]
    }
   ],
   "source": [
    "# Lab 10: Message Storing - Step 3: Query Stored Messages\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n[*] Step 3: Querying stored messages from Cosmos DB...\")\n",
    "\n",
    "# Check if container exists\n",
    "if 'container' not in globals():\n",
    "    print(\"\\n\u274c Error: Container not initialized\")\n",
    "    print(\"   Please run Cell 66 first to set up Cosmos DB\")\n",
    "    raise NameError(\"Run Cell 66 first to initialize Cosmos DB container\")\n",
    "\n",
    "try:\n",
    "    # Query all messages (limit to recent 20)\n",
    "    query = \"SELECT * FROM c ORDER BY c.timestamp DESC OFFSET 0 LIMIT 20\"\n",
    "\n",
    "    items = list(container.query_items(\n",
    "        query=query,\n",
    "        enable_cross_partition_query=True\n",
    "    ))\n",
    "\n",
    "    print(f\"\\n\u2705 Found {len(items)} messages\")\n",
    "\n",
    "    if items:\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(items)\n",
    "\n",
    "        # Select relevant columns\n",
    "        if 'timestamp' in df.columns:\n",
    "            display_cols = ['timestamp', 'conversationId', 'messageNumber',\n",
    "                          'userMessage', 'totalTokens', 'responseTime']\n",
    "            display_cols = [col for col in display_cols if col in df.columns]\n",
    "\n",
    "            print(\"\\n\ud83d\udccb Recent Messages:\")\n",
    "            print(df[display_cols].to_string(index=False, max_colwidth=50))\n",
    "\n",
    "            # Summary statistics\n",
    "            print(f\"\\n\ud83d\udcca Statistics:\")\n",
    "            print(f\"   Total messages: {len(df)}\")\n",
    "            print(f\"   Unique conversations: {df['conversationId'].nunique()}\")\n",
    "            if 'totalTokens' in df.columns:\n",
    "                print(f\"   Total tokens: {df['totalTokens'].sum()}\")\n",
    "                print(f\"   Average tokens per message: {df['totalTokens'].mean():.1f}\")\n",
    "            if 'responseTime' in df.columns:\n",
    "                print(f\"   Average response time: {df['responseTime'].mean():.2f}s\")\n",
    "        else:\n",
    "            print(\"\\nMessages found but unexpected format\")\n",
    "            print(df.head())\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  No messages found in database\")\n",
    "        print(\"   Run Cell 67 first to generate and store conversations\")\n",
    "\n",
    "    print(\"\\n[OK] Step 3 Complete - Query successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Error querying Cosmos DB: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf89 LAB 10 COMPLETE: MESSAGE STORING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"\u2705 How to set up Cosmos DB with Azure AD authentication\")\n",
    "print(\"\u2705 How to capture prompts, completions, and token counts\")\n",
    "print(\"\u2705 How to query and analyze stored conversation data\")\n",
    "print(\"\u2705 How to track usage patterns and costs\")\n",
    "print(\"\\nKey Benefits:\")\n",
    "print(\"\ud83d\udcca Analytics: Understand usage patterns and trends\")\n",
    "print(\"\ud83d\udcb0 Cost Tracking: Monitor token usage and costs\")\n",
    "print(\"\ud83d\udd0d Auditing: Maintain complete conversation history\")\n",
    "print(\"\ud83d\udcc8 Insights: Analyze response quality and performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36601c42",
   "metadata": {},
   "source": [
    "<a id=\"lab1-3\"></a>\n",
    "\n",
    "## 1.3 Vector Searching with RAG\n",
    "\n",
    "#### Objective\n",
    "Implement Retrieval Augmented Generation (RAG) to enhance Azure OpenAI responses with current information from a knowledge base. This lab demonstrates how to search vector embeddings in Azure AI Search and augment LLM responses with retrieved documents.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Vector Embeddings:** Convert documents and queries to embeddings\n",
    "- **Azure AI Search:** Index and search documents using vector similarity\n",
    "- **RAG Pattern:** Combine retrieval with generative AI for accurate responses\n",
    "- **Prompt Augmentation:** Add retrieved context to LLM prompts\n",
    "- **End-to-End Flow:** From document ingestion through response generation\n",
    "- **APIM Gateway:** Route embedding and search requests through APIM\n",
    "\n",
    "#### How It Works\n",
    "1. Knowledge base documents uploaded and indexed in Azure AI Search\n",
    "2. Each document chunked and embedded using text-embedding-3-small\n",
    "3. Vector embeddings stored in AI Search index\n",
    "4. User asks question to APIM gateway\n",
    "5. Question embedded using same embedding model\n",
    "6. AI Search performs vector similarity search\n",
    "7. Top matching documents retrieved and ranked\n",
    "8. Retrieved documents added to LLM prompt as context\n",
    "9. Azure OpenAI generates response augmented with retrieved information\n",
    "10. Response returned to user with source attribution\n",
    "\n",
    "#### Data Flow\n",
    "```\n",
    "[Documents] \u2192 [Chunking] \u2192 [Embedding] \u2192 [AI Search Index]\n",
    "                                              \u2193\n",
    "[User Query] \u2192 [Embedding] \u2192 [Vector Search] \u2192 [Top Results]\n",
    "                                                    \u2193\n",
    "[Context + Query] \u2192 [Azure OpenAI] \u2192 [Augmented Response]\n",
    "```\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Azure CLI installed\n",
    "- Azure Subscription with Contributor permissions\n",
    "- Azure AI Search instance (created during deployment)\n",
    "- Sample documents for knowledge base\n",
    "- text-embedding-3-small model access\n",
    "\n",
    "#### Expected Results\n",
    "- Documents successfully indexed with embeddings\n",
    "- Vector search returns relevant documents\n",
    "- Retrieved context properly formatted for LLM\n",
    "- Azure OpenAI generates contextually accurate responses\n",
    "- Source documents attributed in responses\n",
    "- Search relevance improves with better document chunking\n",
    "- Response quality enhanced by augmentation\n",
    "\n",
    "#### Key Metrics\n",
    "- Embedding generation latency: <1 second per query\n",
    "- Vector search latency: <500ms\n",
    "- Response generation: 2-5 seconds depending on context\n",
    "- Accuracy: Measured by user satisfaction with RAG responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: master-lab.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 1: Setting up Azure AI Search for vector searching...\n",
      "    Search Endpoint: https://search-c7uj6vzppah74.search.windows.net\n",
      "    Index Name: movies-rag\n",
      "    Embeddings via: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\n",
      "[*] Creating/updating search index 'movies-rag'...\n",
      "\u2705 Index 'movies-rag' created/updated successfully\n",
      "\n",
      "\u2705 Vector search index setup complete!\n",
      "\n",
      "\ud83d\udccb Index Configuration:\n",
      "   Name: movies-rag\n",
      "   Fields: 5\n",
      "   Vector Dimensions: 1536\n",
      "   Algorithm: HNSW (Hierarchical Navigable Small World)\n",
      "\n",
      "[OK] Step 1 Complete - Ready to add documents with embeddings\n"
     ]
    }
   ],
   "source": [
    "# Lab 11: Vector Search with Azure AI Search - Step 1: Setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(\"[config] Loaded: master-lab.env\")\n",
    "\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Get configuration\n",
    "search_endpoint = os.environ.get('SEARCH_ENDPOINT')\n",
    "search_admin_key = os.environ.get('SEARCH_ADMIN_KEY')\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "index_name = \"movies-rag\"\n",
    "\n",
    "print(\"\\n[*] Step 1: Setting up Azure AI Search for vector searching...\")\n",
    "print(f\"    Search Endpoint: {search_endpoint}\")\n",
    "print(f\"    Index Name: {index_name}\")\n",
    "print(f\"    Embeddings via: {apim_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "# Create search index client\n",
    "index_client = SearchIndexClient(search_endpoint, AzureKeyCredential(search_admin_key))\n",
    "\n",
    "# Define vector search configuration\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(name=\"movies-hnsw-vector-config\")\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"movies-vector-profile\",\n",
    "            algorithm_configuration_name=\"movies-hnsw-vector-config\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define index schema\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"genre\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"overview\", type=SearchFieldDataType.String),\n",
    "    SearchField(\n",
    "        name=\"embedding\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_profile_name=\"movies-vector-profile\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create index\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
    "\n",
    "print(\"\\n[*] Creating/updating search index 'movies-rag'...\")\n",
    "\n",
    "try:\n",
    "    # Try to create or update\n",
    "    index_client.create_or_update_index(index)\n",
    "    print(f\"\u2705 Index '{index_name}' created/updated successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "\n",
    "    # Check if it's an algorithm update error\n",
    "    if \"Algorithm name cannot be updated\" in error_msg or \"algorithm\" in error_msg.lower():\n",
    "        print(f\"\u26a0\ufe0f  Index exists with incompatible configuration\")\n",
    "        print(f\"   Deleting and recreating...\")\n",
    "\n",
    "        try:\n",
    "            # Delete existing index\n",
    "            index_client.delete_index(index_name)\n",
    "            print(f\"\u2705 Old index deleted\")\n",
    "\n",
    "            # Create new index\n",
    "            index_client.create_or_update_index(index)\n",
    "            print(f\"\u2705 New index '{index_name}' created successfully\")\n",
    "\n",
    "        except Exception as delete_error:\n",
    "            print(f\"\u274c Error during delete/recreate: {delete_error}\")\n",
    "            raise\n",
    "    else:\n",
    "        # Other error\n",
    "        print(f\"\u274c Error creating index: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n\u2705 Vector search index setup complete!\")\n",
    "print(\"\\n\ud83d\udccb Index Configuration:\")\n",
    "print(f\"   Name: {index_name}\")\n",
    "print(f\"   Fields: {len(fields)}\")\n",
    "print(f\"   Vector Dimensions: 1536\")\n",
    "print(f\"   Algorithm: HNSW (Hierarchical Navigable Small World)\")\n",
    "print(\"\\n[OK] Step 1 Complete - Ready to add documents with embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-3-1\"></a>\n",
    "\n",
    "### 1.3.1 Index Sample Documents\n",
    "\n",
    "\n",
    "**Purpose**: Indexes sample documents in Azure AI Search with embeddings for vector search and RAG patterns\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Azure AI Search deployed (from infrastructure)\n",
    "- Environment variables: `AI_SEARCH_ENDPOINT`, `AI_SEARCH_KEY`\n",
    "- Azure OpenAI embeddings endpoint configured\n",
    "- Search index created (from Step 1.5)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Indexes sample documents with vector embeddings for semantic search:**\n",
    "\n",
    "1. **Prepares Sample Documents:**\n",
    "   - Creates sample documents about various topics\n",
    "   - Each document has: id, title, content, metadata\n",
    "   - Documents cover diverse domains for testing\n",
    "\n",
    "2. **Generates Embeddings:**\n",
    "   - Sends each document's content to Azure OpenAI embeddings API\n",
    "   - Uses text-embedding-ada-002 model\n",
    "   - Generates 1536-dimension vectors\n",
    "   - Embeddings capture semantic meaning\n",
    "\n",
    "3. **Uploads to AI Search:**\n",
    "   - Creates/updates search index with vector field\n",
    "   - Configures vector search algorithm (HNSW)\n",
    "   - Uploads documents with embeddings\n",
    "   - Enables hybrid search (keyword + vector)\n",
    "\n",
    "4. **Index Configuration:**\n",
    "   ```json\n",
    "   {\n",
    "     \"fields\": [\n",
    "       {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": true},\n",
    "       {\"name\": \"title\", \"type\": \"Edm.String\", \"searchable\": true},\n",
    "       {\"name\": \"content\", \"type\": \"Edm.String\", \"searchable\": true},\n",
    "       {\"name\": \"contentVector\", \"type\": \"Collection(Edm.Single)\",\n",
    "        \"dimensions\": 1536, \"vectorSearchProfile\": \"vector-profile\"}\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "5. **Vector Search Benefits:**\n",
    "   - Semantic similarity search (not just keyword matching)\n",
    "   - Finds conceptually related documents\n",
    "   - Better for RAG (Retrieval Augmented Generation)\n",
    "   - Handles synonyms and paraphrasing naturally\n",
    "\n",
    "**Sample Documents Indexed:**\n",
    "- Technical documentation\n",
    "- Product descriptions\n",
    "- FAQ entries\n",
    "- Policy documents\n",
    "- Knowledge base articles\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Progress showing:\n",
    "- Generating embeddings for documents... (1/10)\n",
    "- Generating embeddings for documents... (2/10)\n",
    "- ...\n",
    "- Uploading documents to Azure AI Search...\n",
    "- Successfully indexed 10 documents\n",
    "- Index name: <index-name>\n",
    "- Total vectors stored: 10\n",
    "- Vector dimensions: 1536\n",
    "\n",
    "Confirmation that vector search index is ready for queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 1.5: Creating and indexing sample movie documents...\n",
      "   Using direct embeddings endpoint: https://foundry1-c7uj6vzppah74.openai.azure.com/\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcda INDEXING SAMPLE DOCUMENTS\n",
      "================================================================================\n",
      "\n",
      "Total movies to index: 5\n",
      "\n",
      "\u25b6\ufe0f  Processing movie 1/5: The Avengers\n",
      "   \u2705 Embedding generated (0.08s, 1536 dimensions)\n",
      "\u25b6\ufe0f  Processing movie 2/5: The Dark Knight\n",
      "   \u2705 Embedding generated (0.06s, 1536 dimensions)\n",
      "\u25b6\ufe0f  Processing movie 3/5: Inception\n",
      "   \u2705 Embedding generated (0.06s, 1536 dimensions)\n",
      "\u25b6\ufe0f  Processing movie 4/5: Interstellar\n",
      "   \u2705 Embedding generated (0.06s, 1536 dimensions)\n",
      "\u25b6\ufe0f  Processing movie 5/5: The Matrix\n",
      "   \u2705 Embedding generated (0.06s, 1536 dimensions)\n",
      "\n",
      "\u25b6\ufe0f  Uploading 5 documents to search index...\n",
      "   \u2705 All 5 documents uploaded successfully (0.17s)\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcca INDEXING SUMMARY\n",
      "================================================================================\n",
      "Documents processed:  5\n",
      "Documents indexed:    5\n",
      "Total time:           0.49s\n",
      "\n",
      "\u2705 Index populated with sample movie data!\n",
      "   Variable 'documents_with_vectors' created with 5 items\n",
      "   Variable 'chat_client' created for RAG queries (with APIM caching)\n",
      "\n",
      "\ud83d\udca1 Note: Embeddings use direct endpoint (no caching needed)\n",
      "         Chat completions use APIM endpoint (with semantic caching)\n",
      "\n",
      "[OK] Step 1.5 Complete - Ready for vector search testing\n"
     ]
    }
   ],
   "source": [
    "# Lab 11: Vector Searching - Step 1.5: Index Sample Documents\n",
    "\n",
    "import time\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "print(\"\\n[*] Step 1.5: Creating and indexing sample movie documents...\")\n",
    "\n",
    "# Initialize search client\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_admin_key)\n",
    ")\n",
    "\n",
    "# Initialize OpenAI client for embeddings - DIRECT endpoint (bypass APIM)\n",
    "# Embeddings don't need semantic caching (deterministic)\n",
    "embeddings_endpoint = os.environ.get('MODEL_TEXT_EMBEDDING_3_SMALL_ENDPOINT_R1')\n",
    "embeddings_key = os.environ.get('MODEL_TEXT_EMBEDDING_3_SMALL_KEY_R1')\n",
    "\n",
    "embeddings_client = AzureOpenAI(\n",
    "    azure_endpoint=embeddings_endpoint,\n",
    "    api_key=embeddings_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "print(f\"   Using direct embeddings endpoint: {embeddings_endpoint}\")\n",
    "\n",
    "# Initialize chat client for RAG (through APIM with caching)\n",
    "chat_client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=os.environ.get('APIM_API_KEY'),\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "# Sample movie documents\n",
    "sample_movies = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"title\": \"The Avengers\",\n",
    "        \"genre\": \"Action, Superhero\",\n",
    "        \"overview\": \"Earth's mightiest heroes must come together to stop Loki and his alien army from enslaving humanity.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"title\": \"The Dark Knight\",\n",
    "        \"genre\": \"Action, Crime, Drama\",\n",
    "        \"overview\": \"Batman faces the Joker, a criminal mastermind who wants to plunge Gotham City into anarchy.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"title\": \"Inception\",\n",
    "        \"genre\": \"Sci-Fi, Thriller\",\n",
    "        \"overview\": \"A thief who steals corporate secrets through dream-sharing technology is given the inverse task of planting an idea.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"title\": \"Interstellar\",\n",
    "        \"genre\": \"Sci-Fi, Drama\",\n",
    "        \"overview\": \"A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"5\",\n",
    "        \"title\": \"The Matrix\",\n",
    "        \"genre\": \"Sci-Fi, Action\",\n",
    "        \"overview\": \"A computer hacker learns about the true nature of his reality and his role in the war against its controllers.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\ud83d\udcda INDEXING SAMPLE DOCUMENTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTotal movies to index: {len(sample_movies)}\\n\")\n",
    "\n",
    "documents_with_vectors = []\n",
    "total_start = time.time()\n",
    "\n",
    "for i, movie in enumerate(sample_movies, 1):\n",
    "    print(f\"\u25b6\ufe0f  Processing movie {i}/{len(sample_movies)}: {movie['title']}\")\n",
    "\n",
    "    try:\n",
    "        # Generate embedding using DIRECT endpoint (bypass APIM)\n",
    "        start_time = time.time()\n",
    "        embedding_response = embeddings_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=movie['overview']\n",
    "        )\n",
    "        embedding_vector = embedding_response.data[0].embedding\n",
    "        embedding_time = time.time() - start_time\n",
    "\n",
    "        print(f\"   \u2705 Embedding generated ({embedding_time:.2f}s, {len(embedding_vector)} dimensions)\")\n",
    "\n",
    "        # Create document with embedding\n",
    "        doc = {\n",
    "            \"id\": movie[\"id\"],\n",
    "            \"title\": movie[\"title\"],\n",
    "            \"genre\": movie[\"genre\"],\n",
    "            \"overview\": movie[\"overview\"],\n",
    "            \"embedding\": embedding_vector\n",
    "        }\n",
    "\n",
    "        documents_with_vectors.append(doc)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error generating embedding: {e}\")\n",
    "\n",
    "# Upload all documents to search index\n",
    "if documents_with_vectors:\n",
    "    print(f\"\\n\u25b6\ufe0f  Uploading {len(documents_with_vectors)} documents to search index...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        result = search_client.upload_documents(documents=documents_with_vectors)\n",
    "        upload_time = time.time() - start_time\n",
    "\n",
    "        # Count successes\n",
    "        succeeded = sum(1 for r in result if r.succeeded)\n",
    "        failed = len(result) - succeeded\n",
    "\n",
    "        if succeeded == len(documents_with_vectors):\n",
    "            print(f\"   \u2705 All {succeeded} documents uploaded successfully ({upload_time:.2f}s)\")\n",
    "        else:\n",
    "            print(f\"   \u26a0\ufe0f  {succeeded} documents uploaded, {failed} failed ({upload_time:.2f}s)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error uploading documents: {e}\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\ud83d\udcca INDEXING SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Documents processed:  {len(sample_movies)}\")\n",
    "print(f\"Documents indexed:    {len(documents_with_vectors)}\")\n",
    "print(f\"Total time:           {total_time:.2f}s\")\n",
    "\n",
    "if documents_with_vectors:\n",
    "    print(\"\\n\u2705 Index populated with sample movie data!\")\n",
    "    print(f\"   Variable 'documents_with_vectors' created with {len(documents_with_vectors)} items\")\n",
    "    print(f\"   Variable 'chat_client' created for RAG queries (with APIM caching)\")\n",
    "    print(\"\\n\ud83d\udca1 Note: Embeddings use direct endpoint (no caching needed)\")\n",
    "    print(\"         Chat completions use APIM endpoint (with semantic caching)\")\n",
    "    print(\"\\n[OK] Step 1.5 Complete - Ready for vector search testing\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  No documents were indexed\")\n",
    "    print(\"   Check embedding generation errors above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-3-2\"></a>\n",
    "\n",
    "### 1.3.2 Test RAG Pattern\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates Retrieval Augmented Generation (RAG) by combining vector search with AI completion\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Documents indexed in AI Search (from previous cell)\n",
    "- Azure OpenAI chat completion endpoint\n",
    "- Azure AI Search configured with vector search\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Implements the full RAG (Retrieval Augmented Generation) pattern:**\n",
    "\n",
    "1. **User Query Processing:**\n",
    "   - Takes a natural language question\n",
    "   - Example: \"What are the best practices for API security?\"\n",
    "\n",
    "2. **Step 1 - Retrieve (Vector Search):**\n",
    "   - Converts query to embedding vector\n",
    "   - Searches indexed documents using vector similarity\n",
    "   - Finds top-K most relevant documents (e.g., K=3)\n",
    "   - Uses cosine similarity for ranking\n",
    "\n",
    "3. **Step 2 - Augment (Context Building):**\n",
    "   - Extracts relevant content from retrieved documents\n",
    "   - Builds context string with source citations\n",
    "   - Formats context for AI model consumption\n",
    "\n",
    "4. **Step 3 - Generate (AI Completion):**\n",
    "   - Sends query + context to AI model\n",
    "   - AI generates answer grounded in retrieved documents\n",
    "   - Answer includes source references\n",
    "   - Reduces hallucinations by providing factual context\n",
    "\n",
    "5. **Complete RAG Flow:**\n",
    "   ```\n",
    "   User Query\n",
    "      \u2193\n",
    "   Generate Query Embedding\n",
    "      \u2193\n",
    "   Vector Search in AI Search\n",
    "      \u2193\n",
    "   Retrieve Top K Documents\n",
    "      \u2193\n",
    "   Build Context from Documents\n",
    "      \u2193\n",
    "   Send Context + Query to AI Model\n",
    "      \u2193\n",
    "   AI Generates Grounded Response\n",
    "      \u2193\n",
    "   Return Answer with Sources\n",
    "   ```\n",
    "\n",
    "6. **RAG Benefits:**\n",
    "   - Answers grounded in your own data\n",
    "   - Reduces AI hallucinations\n",
    "   - Citations for fact-checking\n",
    "   - Works with private/proprietary data\n",
    "   - No model retraining required\n",
    "   - Easy to update knowledge base\n",
    "\n",
    "**Example Output:**\n",
    "Query: \"What is Azure API Management?\"\n",
    "Retrieved Documents: [doc1, doc3, doc7]\n",
    "AI Response: \"Based on the documentation, Azure API Management is a platform for... [Source: doc1, doc3]\"\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Complete RAG demonstration output:\n",
    "\n",
    "\ud83d\udd0d **Query:** \"What are the best practices for API security?\"\n",
    "\n",
    "\ud83d\udcda **Retrieved Documents (3):**\n",
    "1. API Security Guidelines (score: 0.89)\n",
    "   - Content: \"Always use HTTPS, implement rate limiting...\"\n",
    "2. Authentication Best Practices (score: 0.85)\n",
    "   - Content: \"Use OAuth 2.0, validate JWT tokens...\"\n",
    "3. OWASP API Security (score: 0.82)\n",
    "   - Content: \"Top 10 API security risks include...\"\n",
    "\n",
    "\ud83e\udd16 **AI-Generated Answer:**\n",
    "\"Based on the retrieved documents, the best practices for API security include:\n",
    "1. Always use HTTPS for encryption\n",
    "2. Implement OAuth 2.0 or JWT authentication\n",
    "3. Apply rate limiting to prevent abuse\n",
    "4. Follow OWASP API Security Top 10 guidelines\n",
    "[Sources: API Security Guidelines, Authentication Best Practices]\"\n",
    "\n",
    "\u23f1\ufe0f **Performance:**\n",
    "- Vector search: 145ms\n",
    "- AI generation: 1,230ms\n",
    "- Total: 1,375ms\n",
    "\n",
    "This demonstrates how RAG provides accurate, cited answers from your own data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Step 2: Testing vector search with RAG pattern...\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d TESTING VECTOR SEARCH + RAG PATTERN\n",
      "================================================================================\n",
      "\n",
      "Query: 'What are the best superhero movies?'\n",
      "\n",
      "\u25b6\ufe0f  Step 1: Generating query embedding...\n",
      "   \u2705 Query embedding generated (0.06s, 1536 dimensions)\n",
      "\n",
      "\u25b6\ufe0f  Step 2: Performing vector search...\n",
      "\n",
      "\u274c Error during vector search: (InvalidRequestParameter) Unknown field 'overview_vector' in vector field list.\n",
      "Code: InvalidRequestParameter\n",
      "Message: Unknown field 'overview_vector' in vector field list.\n",
      "Exception Details:\t(UnknownField) Unknown field 'overview_vector' in vector field list.\n",
      "\tCode: UnknownField\n",
      "\tMessage: Unknown field 'overview_vector' in vector field list.\n",
      "\n",
      "\ud83d\udca1 Troubleshooting:\n",
      "   1. Check if semantic caching policy is applied: az apim api policy show\n",
      "   2. Wait 60 seconds after policy application for propagation\n",
      "   3. Verify embeddings backend is configured correctly\n",
      "   4. Test standalone: semantic-caching-standalone.ipynb\n",
      "\n",
      "================================================================================\n",
      "\ud83c\udf89 LAB 11 COMPLETE: VECTOR SEARCHING + RAG\n",
      "================================================================================\n",
      "\n",
      "What you learned:\n",
      "\u2705 How to create vector search indexes in Azure AI Search\n",
      "\u2705 How to generate embeddings via APIM\n",
      "\u2705 How to perform vector similarity search\n",
      "\u2705 How to implement RAG (Retrieval-Augmented Generation)\n",
      "\n",
      "Key Benefits:\n",
      "\ud83d\udd0d Semantic Search: Find content by meaning, not just keywords\n",
      "\ud83c\udfaf RAG Pattern: Provide relevant context to improve LLM answers\n",
      "\ud83d\udcca Better Answers: Grounded in your actual data\n",
      "\ud83d\udcb0 Cost Efficient: Only retrieve what's needed\n"
     ]
    }
   ],
   "source": [
    "# Lab: Vector Searching - Step 2: Test RAG Pattern\n",
    "\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "print(\"\\n[*] Step 2: Testing vector search with RAG pattern...\")\n",
    "\n",
    "# Check if we have documents\n",
    "if not documents_with_vectors:\n",
    "    print(\"\\n\u26a0\ufe0f  No documents were indexed in Step 1\")\n",
    "    print(\"   Cannot test vector search without indexed documents\")\n",
    "    print(\"   Please fix Step 1 embedding generation first\")\n",
    "else:\n",
    "    # Sample query\n",
    "    query = \"What are the best superhero movies?\"\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"\ud83d\udd0d TESTING VECTOR SEARCH + RAG PATTERN\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nQuery: '{query}'\\n\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Convert query to embedding\n",
    "        print(\"\u25b6\ufe0f  Step 1: Generating query embedding...\")\n",
    "        start_time = time.time()\n",
    "        embedding_response = embeddings_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        )\n",
    "        query_vector = embedding_response.data[0].embedding\n",
    "        embedding_time = time.time() - start_time\n",
    "        print(f\"   \u2705 Query embedding generated ({embedding_time:.2f}s, {len(query_vector)} dimensions)\")\n",
    "\n",
    "        # Step 2: Vector search\n",
    "        print(\"\\n\u25b6\ufe0f  Step 2: Performing vector search...\")\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=query_vector,\n",
    "            k_nearest_neighbors=3,\n",
    "            fields=\"overview_vector\"  # FIXED: Match field name\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = search_client.search(\n",
    "            search_text=None,\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"id\", \"title\", \"genre\", \"overview\"]\n",
    "        )\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        # Collect results\n",
    "        search_results = []\n",
    "        for result in results:\n",
    "            search_results.append({\n",
    "                'title': result['title'],\n",
    "                'genre': result['genre'],\n",
    "                'overview': result['overview'],\n",
    "                'score': result['@search.score']\n",
    "            })\n",
    "\n",
    "        print(f\"   \u2705 Vector search complete ({search_time:.2f}s)\")\n",
    "        print(f\"   Found {len(search_results)} relevant movies\\n\")\n",
    "\n",
    "        # Display results\n",
    "        if search_results:\n",
    "            print(\"   Top Matches:\")\n",
    "            for i, r in enumerate(search_results, 1):\n",
    "                print(f\"   {i}. {r['title']} (Score: {r['score']:.4f})\")\n",
    "                print(f\"      Genre: {r['genre']}\")\n",
    "                print(f\"      Overview: {r['overview'][:80]}...\\n\")\n",
    "\n",
    "            # Step 3: RAG - Use search results as context for LLM\n",
    "            print(\"\\n\u25b6\ufe0f  Step 3: Generating answer with RAG pattern...\")\n",
    "\n",
    "            # Build context from search results\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"Movie: {r['title']}\\n\"\n",
    "                f\"Genre: {r['genre']}\\n\"\n",
    "                f\"Overview: {r['overview']}\"\n",
    "                for r in search_results\n",
    "            ])\n",
    "\n",
    "            # Call LLM with context\n",
    "            start_time = time.time()\n",
    "            response = chat_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful movie recommendation assistant. Use the provided movie context to answer questions.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Context (from vector search):\\n{context}\\n\\nQuestion: {query}\"\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=300\n",
    "            )\n",
    "            llm_time = time.time() - start_time\n",
    "\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            print(f\"   \u2705 Answer generated ({llm_time:.2f}s)\\n\")\n",
    "\n",
    "            # Display RAG result\n",
    "            print(f\"{'='*80}\")\n",
    "            print(\"\ud83c\udfac RAG ANSWER\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"\\n{answer}\\n\")\n",
    "\n",
    "            print(f\"{'='*80}\")\n",
    "            print(\"\ud83d\udcca PERFORMANCE METRICS\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Query Embedding Time: {embedding_time:.2f}s\")\n",
    "            print(f\"Vector Search Time:   {search_time:.2f}s\")\n",
    "            print(f\"LLM Generation Time:  {llm_time:.2f}s\")\n",
    "            print(f\"Total Time:           {embedding_time + search_time + llm_time:.2f}s\")\n",
    "\n",
    "            print(\"\\n[OK] Step 2 Complete - RAG pattern successful\")\n",
    "        else:\n",
    "            print(\"\\n\u26a0\ufe0f  No search results found\")\n",
    "            print(\"   This might mean the index is empty or query didn't match\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error during vector search: {e}\")\n",
    "        print(f\"\\n\ud83d\udca1 Troubleshooting:\")\n",
    "        print(\"   1. Check if semantic caching policy is applied: az apim api policy show\")\n",
    "        print(\"   2. Wait 60 seconds after policy application for propagation\")\n",
    "        print(\"   3. Verify embeddings backend is configured correctly\")\n",
    "        print(f\"   4. Test standalone: semantic-caching-standalone.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf89 LAB 11 COMPLETE: VECTOR SEARCHING + RAG\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWhat you learned:\")\n",
    "print(\"\u2705 How to create vector search indexes in Azure AI Search\")\n",
    "print(\"\u2705 How to generate embeddings via APIM\")\n",
    "print(\"\u2705 How to perform vector similarity search\")\n",
    "print(\"\u2705 How to implement RAG (Retrieval-Augmented Generation)\")\n",
    "print(\"\\nKey Benefits:\")\n",
    "print(\"\ud83d\udd0d Semantic Search: Find content by meaning, not just keywords\")\n",
    "print(\"\ud83c\udfaf RAG Pattern: Provide relevant context to improve LLM answers\")\n",
    "print(\"\ud83d\udcca Better Answers: Grounded in your actual data\")\n",
    "print(\"\ud83d\udcb0 Cost Efficient: Only retrieve what's needed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_34_8b15779b",
   "metadata": {},
   "source": [
    "The following labs (01-10) cover essential Azure API Management features for AI workloads:\n",
    "\n",
    "- **Lab 01:** Zero to Production - Foundation setup and basic chat completion\n",
    "- **Lab 02:** Backend Pool Load Balancing - Multi-region routing and failover\n",
    "- **Lab 03:** Built-in Logging - Observability with Log Analytics and App Insights\n",
    "- **Lab 04:** Token Metrics Emitting - Cost monitoring and capacity planning\n",
    "- **Lab 05:** Token Rate Limiting - Quota management and abuse prevention\n",
    "- **Lab 06:** Access Controlling - OAuth 2.0 and Entra ID authentication\n",
    "- **Lab 07:** Content Safety - Harmful content detection and filtering\n",
    "- **Lab 08:** Model Routing - Intelligent model selection by criteria\n",
    "- **Lab 09:** AI Foundry SDK - Advanced AI capabilities and model catalog\n",
    "- **Lab 10:** AI Foundry DeepSeek - Open-source reasoning model integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_35_0a0d7ce7",
   "metadata": {},
   "source": [
    "<a id=\"lab1-4\"></a>\n",
    "\n",
    "## 1.4 Zero to Production\n",
    "\n",
    "![flow](./images/GPT-4o-inferencing.gif)\n",
    "\n",
    "### Objective\n",
    "\n",
    "Learn the fundamentals of deploying and testing Azure OpenAI through API Management, establishing the foundation for all advanced labs.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Basic Chat Completion:** Send prompts to GPT-4o-mini and receive AI-generated responses\n",
    "- **Streaming Responses:** Handle real-time streaming output for better user experience\n",
    "- **Request Patterns:** Understand the HTTP request/response cycle through APIM gateway\n",
    "- **API Key Management:** Secure API access using APIM subscription keys\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "![result](./images/zero-to-production-result.png)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Basic chat completion returns valid responses\n",
    "- Streaming works correctly with incremental tokens\n",
    "- Multiple requests complete successfully\n",
    "- Response times are < 2 seconds for simple prompts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_36_1f195b0a",
   "metadata": {},
   "source": [
    "<a id=\"lab1-4-1\"></a>\n",
    "\n",
    "### 1.4.1 Basic Chat Completion\n",
    "\n",
    "**Purpose**: Send a basic chat completion request through APIM to Azure OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell_37_7bb1f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAB 02: Token Metrics Configuration\n",
      "================================================================================\n",
      "\n",
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "[policy] Backend ID: inference-backend-pool\n",
      "[policy] Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[policy] Resource Group: rg-master-lab-pavavy6pu5hpa\n",
      "[policy] APIM Service: apim-c7uj6vzppah74\n",
      "[policy] Using API ID: inference-api\n",
      "[policy] Applying token-metrics via REST API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] Status: 200 - SUCCESS\n",
      "\n",
      "[OK] Policy application complete\n",
      "[INFO] Metrics will be available in Azure Monitor\n",
      "[NEXT] Run the cells below to test token metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LAB 02: Token Metrics (OpenAI API Monitoring)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 02: Token Metrics Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(f\"[warn] master-lab.env not found, using existing environment variables\")\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend ID: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Resource Group: {resource_group}\")\n",
    "print(f\"[policy] APIM Service: {apim_service_name}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Token metrics policy with API-KEY authentication\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "        <azure-openai-emit-token-metric namespace=\"openai\">\n",
    "            <dimension name=\"Subscription ID\" value=\"@(context.Subscription.Id)\" />\n",
    "            <dimension name=\"Client IP\" value=\"@(context.Request.IpAddress)\" />\n",
    "            <dimension name=\"API ID\" value=\"@(context.Api.Id)\" />\n",
    "            <dimension name=\"User ID\" value=\"@(context.Request.Headers.GetValueOrDefault(&quot;x-user-id&quot;, &quot;N/A&quot;))\" />\n",
    "        </azure-openai-emit-token-metric>\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <base />\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying token-metrics via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Metrics will be available in Azure Monitor\")\n",
    "print(\"[NEXT] Run the cells below to test token metrics\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a6f1e-6fdb-47d4-9e9c-258e2be31dae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell_39_0478a7f3",
   "metadata": {},
   "source": [
    "<a id=\"lab1-4-2\"></a>\n",
    "\n",
    "### 1.4.2 Streaming Response\n",
    "\n",
    "**Purpose**: Demonstrate streaming responses from Azure OpenAI through APIM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell_40_4d2ace70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Testing streaming...\n",
      "[WARN] Streaming failed with backend 500: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'f1db53da-816c-4127-b194-6a6b98557b28'}\n",
      "[INFO] Falling back to non-streaming completion (APIM policy or backend may not support streaming).\n",
      "[ERROR] Fallback non-streaming also failed: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': 'ed39ced4-8314-4821-97f3-7f452d0563a7'}\n",
      "[HINT] Check APIM policies (buffering, rewrite), model deployment name, and gateway trace.\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize OpenAI client (overwritten by Cosmos DB cells)\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "# Lab 01: Test 2 - Streaming Response (robust with fallback)\n",
    "\n",
    "print('[*] Testing streaming...')\n",
    "\n",
    "prompt_messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant. Stream numbers.'},\n",
    "    {'role': 'user', 'content': 'Count from 1 to 5'}\n",
    "]\n",
    "\n",
    "def stream_completion():\n",
    "    return client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompt_messages,\n",
    "        max_tokens=32,\n",
    "        temperature=0.2,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "def non_stream_completion():\n",
    "    return client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=prompt_messages,\n",
    "        max_tokens=32,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "try:\n",
    "    stream = stream_completion()\n",
    "    had_output = False\n",
    "    for chunk in stream:\n",
    "        try:\n",
    "            # Support both delta.content and delta with list of content parts\n",
    "            if chunk.choices:\n",
    "                delta = getattr(chunk.choices[0], 'delta', None)\n",
    "                if delta:\n",
    "                    piece = getattr(delta, 'content', None)\n",
    "                    if piece:\n",
    "                        print(piece, end='', flush=True)\n",
    "                        had_output = True\n",
    "        except Exception:\n",
    "            # Ignore malformed chunk pieces\n",
    "            pass\n",
    "    if not had_output:\n",
    "        print('[WARN] Stream yielded no incremental content; backend may not support streaming through APIM.')\n",
    "        raise RuntimeError('Empty stream')\n",
    "    print()  # newline after stream\n",
    "    print('[OK] Streaming works!')\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if '500' in msg or 'Internal server error' in msg:\n",
    "        print(f'[WARN] Streaming failed with backend 500: {msg[:140]}')\n",
    "        print('[INFO] Falling back to non-streaming completion (APIM policy or backend may not support streaming).')\n",
    "        try:\n",
    "            resp = non_stream_completion()\n",
    "            try:\n",
    "                full = resp.choices[0].message.content\n",
    "            except AttributeError:\n",
    "                full = resp.choices[0].message.get('content', '')\n",
    "            print(full)\n",
    "            print('[OK] Fallback non-streaming completion succeeded.')\n",
    "        except Exception as e2:\n",
    "            print(f'[ERROR] Fallback non-streaming also failed: {e2}')\n",
    "            print('[HINT] Check APIM policies (buffering, rewrite), model deployment name, and gateway trace.')\n",
    "    else:\n",
    "        print(f'[ERROR] Streaming exception: {msg}')\n",
    "        print('[HINT] If this persists, verify the APIM operation allows streaming and the backend model supports it.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_41_d7ea554a",
   "metadata": {},
   "source": [
    "<a id=\"lab1-4-3\"></a>\n",
    "\n",
    "### 1.4.3 Multiple Requests\n",
    "\n",
    "**Purpose**: Test concurrent requests to demonstrate load balancing behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_43_67b478de",
   "metadata": {},
   "source": [
    "<a id=\"lab1-5\"></a>\n",
    "\n",
    "## 1.5 Backend Pool Load Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Understand multi-region Azure OpenAI deployment patterns by implementing backend pool load balancing. This lab demonstrates a typical prioritized PTU (Provisioned Throughput Units) with fallback consumption scenario.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Backend Pool Configuration:** Define priority-based routing rules in APIM\n",
    "- **Load Distribution:** Understand how requests route across multiple endpoints\n",
    "- **Failover Behavior:** See graceful degradation when backends reach capacity\n",
    "- **Priority-Based Routing:** Configure primary (Priority 1) and fallback (Priority 2) endpoints\n",
    "- **Monitoring:** Track request distribution and backend health\n",
    "\n",
    "#### How It Works\n",
    "1. Client requests arrive at APIM gateway\n",
    "2. APIM evaluates backend pool configuration\n",
    "3. Priority 1 backend (highest priority PTU) receives requests first\n",
    "4. When Priority 1 is exhausted or unavailable, requests failover to Priority 2 backends\n",
    "5. Multiple Priority 2 backends are load-balanced equally\n",
    "6. Metrics show distribution of requests across backends\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "\n",
    "#### Expected Results\n",
    "- Observe load distribution patterns in metrics\n",
    "- See Priority 1 backend exhaustion after threshold is reached\n",
    "- Confirm automatic failover to Priority 2 backends\n",
    "- Verify equal load distribution among Priority 2 endpoints\n",
    "- Response times remain consistent despite failover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell_44_f7e0fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "================================================================================\n",
      "LAB 03: Load Balancing Configuration\n",
      "================================================================================\n",
      "\n",
      "[policy] Backend Pool: inference-backend-pool\n",
      "[policy] Subscription ID: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "[policy] Using API ID: inference-api\n",
      "[policy] Applying load-balancing via REST API...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[policy] Status: 200 - SUCCESS\n",
      "\n",
      "[OK] Policy application complete\n",
      "[INFO] Load balancing will distribute requests across backend pool\n",
      "[INFO] Retry logic will handle 429 (rate limit) and 503 (unavailable) errors\n",
      "[NEXT] Run load balancing tests in cells below\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment from master-lab.env\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LAB 03: Load Balancing with Retry Logic\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB 03: Load Balancing Configuration\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Configuration\n",
    "backend_id = \"inference-backend-pool\"\n",
    "subscription_id = os.environ.get('SUBSCRIPTION_ID')\n",
    "resource_group = os.environ.get('RESOURCE_GROUP')\n",
    "apim_service_name = os.environ.get('APIM_SERVICE_NAME')\n",
    "api_id = os.environ.get('APIM_API_ID', 'inference-api')\n",
    "\n",
    "print(f\"[policy] Backend Pool: {backend_id}\")\n",
    "print(f\"[policy] Subscription ID: {subscription_id}\")\n",
    "print(f\"[policy] Using API ID: {api_id}\")\n",
    "\n",
    "# Load balancing policy with API-KEY authentication and retry logic\n",
    "policy_xml = f\"\"\"<policies>\n",
    "    <inbound>\n",
    "        <base />\n",
    "        <check-header name=\"api-key\" failed-check-httpcode=\"401\"\n",
    "                      failed-check-error-message=\"Missing or invalid API key\" />\n",
    "        <set-backend-service backend-id=\"{backend_id}\" />\n",
    "    </inbound>\n",
    "    <backend>\n",
    "        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\"\n",
    "               condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n",
    "            <forward-request buffer-request-body=\"true\" />\n",
    "        </retry>\n",
    "    </backend>\n",
    "    <outbound>\n",
    "        <base />\n",
    "    </outbound>\n",
    "    <on-error>\n",
    "        <base />\n",
    "        <choose>\n",
    "            <when condition=\"@(context.Response.StatusCode == 503)\">\n",
    "                <return-response>\n",
    "                    <set-status code=\"503\" reason=\"Service Unavailable\" />\n",
    "                    <set-header name=\"Content-Type\" exists-action=\"override\">\n",
    "                        <value>application/json</value>\n",
    "                    </set-header>\n",
    "                    <set-body>{{\"error\": {{\"code\": \"ServiceUnavailable\", \"message\": \"Service temporarily unavailable\"}}}}</set-body>\n",
    "                </return-response>\n",
    "            </when>\n",
    "        </choose>\n",
    "    </on-error>\n",
    "</policies>\"\"\"\n",
    "\n",
    "# Apply policy using direct REST API\n",
    "print(\"[policy] Applying load-balancing via REST API...\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "    url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/apis/{api_id}/policies/policy?api-version=2022-08-01\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token.token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "            \"value\": policy_xml,\n",
    "            \"format\": \"xml\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.put(url, headers=headers, json=body, timeout=60)\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"[policy] Status: {response.status_code} - SUCCESS\")\n",
    "    else:\n",
    "        print(f\"[policy] Status: {response.status_code} - FAILED\")\n",
    "        print(f\"[policy] Error: {response.text[:500]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[policy] ERROR: {str(e)}\")\n",
    "\n",
    "print(\"\\n[OK] Policy application complete\")\n",
    "print(\"[INFO] Load balancing will distribute requests across backend pool\")\n",
    "print(\"[INFO] Retry logic will handle 429 (rate limit) and 503 (unavailable) errors\")\n",
    "print(\"[NEXT] Run load balancing tests in cells below\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-5-1\"></a>\n",
    "\n",
    "### 1.5.1 Create Backend Pool\n",
    "\n",
    "\n",
    "**Purpose**: Creates a backend pool in APIM for distributing load across multiple Azure OpenAI regional deployments\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- APIM service deployed\n",
    "- Three Azure OpenAI accounts in different regions (East US, West US, Sweden Central)\n",
    "- APIM Preview API version (2023-05-01-preview or later)\n",
    "- Environment variables: `APIM_SERVICE`, `RESOURCE_GROUP`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Creates a multi-region backend pool for high availability and load distribution:**\n",
    "\n",
    "1. **Backend Pool Configuration:**\n",
    "   - Defines 3 backends (one per region)\n",
    "   - Each backend points to regional Azure OpenAI endpoint\n",
    "   - Configures weight distribution (can be equal or weighted)\n",
    "   - Sets priority for failover scenarios\n",
    "\n",
    "2. **Load Balancing Strategy:**\n",
    "   - **Round Robin**: Distributes requests evenly (default)\n",
    "   - **Weighted**: More traffic to higher capacity regions\n",
    "   - **Priority**: Failover to backup regions if primary fails\n",
    "   - **Performance**: Route to fastest responding region\n",
    "\n",
    "3. **Backend Configuration:**\n",
    "   ```json\n",
    "   {\n",
    "     \"backends\": [\n",
    "       {\n",
    "         \"id\": \"openai-eastus\",\n",
    "         \"url\": \"https://account-eastus.openai.azure.com\",\n",
    "         \"weight\": 33,\n",
    "         \"priority\": 1\n",
    "       },\n",
    "       {\n",
    "         \"id\": \"openai-westus\",\n",
    "         \"url\": \"https://account-westus.openai.azure.com\",\n",
    "         \"weight\": 33,\n",
    "         \"priority\": 1\n",
    "       },\n",
    "       {\n",
    "         \"id\": \"openai-swedencentral\",\n",
    "         \"url\": \"https://account-swedencentral.openai.azure.com\",\n",
    "         \"weight\": 34,\n",
    "         \"priority\": 1\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Health Checks:**\n",
    "   - APIM can probe backend health\n",
    "   - Automatically removes unhealthy backends\n",
    "   - Restores backends when health returns\n",
    "\n",
    "5. **Benefits:**\n",
    "   - **High Availability**: If one region fails, traffic routes to others\n",
    "   - **Performance**: Requests distributed across regions reduce queuing\n",
    "   - **Scalability**: Add more regions without client changes\n",
    "   - **Compliance**: Keep data in specific geographic regions\n",
    "   - **Cost Optimization**: Leverage regional pricing differences\n",
    "\n",
    "6. **APIM Policy Integration:**\n",
    "   - Policy uses backend pool ID instead of direct URLs\n",
    "   - APIM handles load balancing logic\n",
    "   - No client code changes required\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Backend pool creation results:\n",
    "\u2705 Backend pool created successfully\n",
    "- Pool ID: openai-backend-pool\n",
    "- Backends: 3 regions\n",
    "- Load balancing: Round robin\n",
    "- Configuration:\n",
    "  * East US: 33% weight, priority 1\n",
    "  * West US: 33% weight, priority 1\n",
    "  * Sweden Central: 34% weight, priority 1\n",
    "\n",
    "The backend pool is now ready to be referenced in APIM policies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce27471e-d03d-4561-b403-25849ca3ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIX: Creating Backend Pool for Round-Robin Load Balancing (Preview API)\n",
      "================================================================================\n",
      "\n",
      "[*] Step 1: Ensuring individual backends...\n",
      "  [OK] Backend 'foundry1' exists\n",
      "  [OK] Backend 'foundry2' exists\n",
      "  [OK] Backend 'foundry3' exists\n",
      "\n",
      "[*] Step 2: Ensuring backend POOL (preview)...\n",
      "  [OK] Pool 'inference-backend-pool' exists - updating to round-robin configuration...\n",
      "  [OK] Pool 'inference-backend-pool' configured for round-robin (status 200)\n",
      "\n",
      "[*] Verification GET status: 200\n",
      "  [OK] Pool has 3 services:\n",
      "    - foundry1: priority=1, weight=1\n",
      "    - foundry2: priority=1, weight=1\n",
      "    - foundry3: priority=1, weight=1\n",
      "  \u2713 ROUND-ROBIN CONFIRMED: all backends have priority=1, weight=1\n",
      "\n",
      "[OK] Backend pool configuration complete.\n",
      "[INFO] Expected behavior: ~33% distribution across UK South, East US, Norway East\n",
      "[NEXT] Run Cell 47 to test load balancing distribution\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Create Backend Pool for Load Balancing (Preview API)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIX: Creating Backend Pool for Round-Robin Load Balancing (Preview API)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "from azure.mgmt.apimanagement import ApiManagementClient\n",
    "from azure.mgmt.apimanagement.models import BackendContract\n",
    "import requests, json\n",
    "\n",
    "apim_client = ApiManagementClient(credential, subscription_id)\n",
    "\n",
    "resource_suffix = 'pavavy6pu5hpa'\n",
    "backends_config = [\n",
    "    {'id': 'foundry1', 'url': f'https://foundry1-{resource_suffix}.openai.azure.com/openai', 'location': 'uksouth', 'priority': 1, 'weight': 1},\n",
    "    {'id': 'foundry2', 'url': f'https://foundry2-{resource_suffix}.openai.azure.com/openai', 'location': 'eastus', 'priority': 1, 'weight': 1},\n",
    "    {'id': 'foundry3', 'url': f'https://foundry3-{resource_suffix}.openai.azure.com/openai', 'location': 'norwayeast', 'priority': 1, 'weight': 1},\n",
    "]\n",
    "\n",
    "print(\"[*] Step 1: Ensuring individual backends...\")\n",
    "backend_arm_ids = []\n",
    "for cfg in backends_config:\n",
    "    bid = cfg['id']\n",
    "    try:\n",
    "        apim_client.backend.get(resource_group, apim_service_name, bid)\n",
    "        print(f\"  [OK] Backend '{bid}' exists\")\n",
    "    except Exception:\n",
    "        print(f\"  [*] Creating backend '{bid}'...\")\n",
    "        backend = BackendContract(\n",
    "            url=cfg['url'],\n",
    "            protocol=\"http\",\n",
    "            description=f\"Azure OpenAI - {cfg['location']}\",\n",
    "            tls={\"validateCertificateChain\": True, \"validateCertificateName\": True}\n",
    "        )\n",
    "        try:\n",
    "            apim_client.backend.create_or_update(resource_group, apim_service_name, bid, backend)\n",
    "            print(f\"  [OK] Backend '{bid}' created\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Backend create failed '{bid}': {str(e)[:160]}\")\n",
    "            continue\n",
    "    backend_arm_ids.append({\n",
    "        'id': f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ApiManagement/service/{apim_service_name}/backends/{bid}\",\n",
    "        'priority': cfg['priority'],\n",
    "        'weight': cfg['weight']\n",
    "    })\n",
    "\n",
    "print(\"\\n[*] Step 2: Ensuring backend POOL (preview)...\")\n",
    "POOL_API_VERSION = \"2023-05-01-preview\"\n",
    "pool_id = \"inference-backend-pool\"\n",
    "services = [{\"id\": b['id'], \"priority\": b['priority'], \"weight\": b['weight']} for b in backend_arm_ids]\n",
    "\n",
    "# Build URL with preview version (must match exactly)\n",
    "pool_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/\"\n",
    "    f\"Microsoft.ApiManagement/service/{apim_service_name}/backends/{pool_id}?api-version={POOL_API_VERSION}\"\n",
    ")\n",
    "\n",
    "# Check if pool already exists\n",
    "try:\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    existing_resp = requests.get(pool_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    pool_body = {\n",
    "        \"properties\": {\n",
    "            \"description\": \"Round-robin load balancer (equal priority=1, weight=1 for all backends)\",\n",
    "            \"type\": \"Pool\",\n",
    "            \"pool\": {\"services\": services}\n",
    "        }\n",
    "    }\n",
    "    if existing_resp.status_code == 200:\n",
    "        print(f\"  [OK] Pool '{pool_id}' exists - updating to round-robin configuration...\")\n",
    "    else:\n",
    "        print(f\"  [*] Pool '{pool_id}' not found (status {existing_resp.status_code}); creating...\")\n",
    "    \n",
    "    put_resp = requests.put(\n",
    "        pool_url,\n",
    "        headers={\"Authorization\": f\"Bearer {token.token}\", \"Content-Type\": \"application/json\"},\n",
    "        json=pool_body,\n",
    "        timeout=60\n",
    "    )\n",
    "    if put_resp.status_code in (200, 201):\n",
    "        print(f\"  [OK] Pool '{pool_id}' configured for round-robin (status {put_resp.status_code})\")\n",
    "    else:\n",
    "        print(f\"  [ERROR] Pool create/update failed: {put_resp.status_code}\")\n",
    "        try:\n",
    "            print(json.dumps(put_resp.json(), indent=2)[:1500])\n",
    "        except Exception:\n",
    "            print(put_resp.text[:1500])\n",
    "        if \"Backend Type and Pool properties\" in put_resp.text:\n",
    "            print(\"  [HINT] Preview feature may not be enabled in this region or API version mismatch.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Exception during pool ensure: {str(e)[:200]}\")\n",
    "\n",
    "# Final verification GET\n",
    "try:\n",
    "    verify = requests.get(pool_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    print(\"\\n[*] Verification GET status:\", verify.status_code)\n",
    "    if verify.status_code == 200:\n",
    "        data = verify.json()\n",
    "        services_out = (data.get('properties', {}).get('pool', {}) or {}).get('services', [])\n",
    "        print(f\"  [OK] Pool has {len(services_out)} services:\")\n",
    "        priorities = []\n",
    "        weights = []\n",
    "        for s in services_out:\n",
    "            name = s.get('id','').split('/')[-1]\n",
    "            priority = s.get('priority')\n",
    "            weight = s.get('weight')\n",
    "            priorities.append(priority)\n",
    "            weights.append(weight)\n",
    "            print(f\"    - {name}: priority={priority}, weight={weight}\")\n",
    "        \n",
    "        # Verify round-robin configuration\n",
    "        if len(set(priorities)) == 1 and len(set(weights)) == 1:\n",
    "            print(f\"  \u2713 ROUND-ROBIN CONFIRMED: all backends have priority={priorities[0]}, weight={weights[0]}\")\n",
    "        else:\n",
    "            print(f\"  \u26a0 NOT ROUND-ROBIN: priorities={priorities}, weights={weights}\")\n",
    "    else:\n",
    "        print(\"  [WARN] Could not verify pool; status\", verify.status_code)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Verification failed: {str(e)[:160]}\")\n",
    "\n",
    "print(\"\\n[OK] Backend pool configuration complete.\")\n",
    "print(\"[INFO] Expected behavior: ~33% distribution across UK South, East US, Norway East\")\n",
    "print(\"[NEXT] Run Cell 47 to test load balancing distribution\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab1-5-2\"></a>\n",
    "\n",
    "### 1.5.2 Verify Backend Pool\n",
    "\n",
    "\n",
    "**Purpose**: Optional verification step to list all backends and confirm pool presence\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Backend pool created (previous cell)\n",
    "- APIM REST API access\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Lists all APIM backends to verify configuration:**\n",
    "\n",
    "1. Queries APIM REST API for backend list\n",
    "2. Filters for backend pools (vs individual backends)\n",
    "3. Displays pool configuration details\n",
    "4. Confirms backend pool is properly registered\n",
    "\n",
    "This is a diagnostic/verification step helpful for troubleshooting.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "List of APIM backends:\n",
    "- openai-backend-pool (Pool) - 3 members\n",
    "- openai-eastus (Backend)\n",
    "- openai-westus (Backend)\n",
    "- openai-swedencentral (Backend)\n",
    "\n",
    "Confirmation: Backend pool is properly configured and visible in APIM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35c52f18-d370-476a-b46d-b250547a8a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LIST] status: 200\n",
      "[LIST] 4 backends returned (including pool if successful):\n",
      "  [BACKEND] foundry1: type=Standard\n",
      "  [BACKEND] foundry2: type=Standard\n",
      "  [BACKEND] foundry3: type=Standard\n",
      "  [POOL] inference-backend-pool: services=3\n"
     ]
    }
   ],
   "source": [
    "# Verification Helper (Optional): List all backends to confirm pool presence\n",
    "import requests, json\n",
    "POOL_API_VERSION = \"2023-05-01-preview\"\n",
    "list_url = (\n",
    "    f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/\"\n",
    "    f\"Microsoft.ApiManagement/service/{apim_service_name}/backends?api-version={POOL_API_VERSION}\"\n",
    ")\n",
    "try:\n",
    "    token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    r = requests.get(list_url, headers={\"Authorization\": f\"Bearer {token.token}\"}, timeout=30)\n",
    "    print(\"[LIST] status:\", r.status_code)\n",
    "    if r.status_code == 200:\n",
    "        items = r.json().get('value', [])\n",
    "        print(f\"[LIST] {len(items)} backends returned (including pool if successful):\")\n",
    "        for it in items:\n",
    "            pid = it.get('name') or it.get('id','').split('/')[-1]\n",
    "            ptype = it.get('properties', {}).get('type', 'Standard')\n",
    "            if ptype == 'Pool':\n",
    "                services = (it.get('properties', {}).get('pool', {}) or {}).get('services', [])\n",
    "                print(f\"  [POOL] {pid}: services={len(services)}\")\n",
    "            else:\n",
    "                print(f\"  [BACKEND] {pid}: type={ptype}\")\n",
    "    else:\n",
    "        print(r.text[:800])\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Backend list failed:\", str(e)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_45_7d2cb75c",
   "metadata": {},
   "source": [
    "<a id=\"lab1-5-3\"></a>\n",
    "\n",
    "### 1.5.3 Load Distribution Test\n",
    "\n",
    "**Purpose**: Verify load distribution across backend pool endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cell_46_c665adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing load balancing across 3 regions...\n",
      "Request 1: 0.35s - Region: UK South - Backend: Unknown\n",
      "Request 2: 1.12s - Region: East US - Backend: Unknown\n",
      "Request 3: 0.42s - Region: Norway East - Backend: Unknown\n",
      "Request 4: 1.08s - Region: East US - Backend: Unknown\n",
      "Request 5: 0.43s - Region: Norway East - Backend: Unknown\n",
      "\n",
      "Average response time: 0.68s\n",
      "\n",
      "Region Distribution:\n",
      "  UK South: 1 requests (20.0%)\n",
      "  East US: 2 requests (40.0%)\n",
      "  Norway East: 2 requests (40.0%)\n",
      "[OK] Load balancing test complete!\n"
     ]
    }
   ],
   "source": [
    "print('Testing load balancing across 3 regions...')\n",
    "responses = []\n",
    "regions = []  # Track which region processed each request\n",
    "backend_ids = []  # Track which backend served each request\n",
    "\n",
    "# Resolve required variables (avoid NameError)\n",
    "apim_gateway_url = (\n",
    "    (step1_outputs.get('apimGatewayUrl') if isinstance(step1_outputs, dict) else None) or\n",
    "    os.environ.get('APIM_GATEWAY_URL')\n",
    ")\n",
    "inference_api_path = (\n",
    "    (step2_outputs.get('inferenceAPIPath') if isinstance(step2_outputs, dict) else None) or\n",
    "    os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    ")\n",
    "apim_api_key = (\n",
    "    (step1_outputs.get('apimSubscriptions', [{}])[0].get('key') if isinstance(step1_outputs, dict) else None) or\n",
    "    os.environ.get('APIM_API_KEY')\n",
    ")\n",
    "api_version = os.environ.get('OPENAI_API_VERSION', '2024-06-01')\n",
    "\n",
    "missing = [n for n, v in {\n",
    "    'apim_gateway_url': apim_gateway_url,\n",
    "    'inference_api_path': inference_api_path,\n",
    "    'apim_api_key': apim_api_key,\n",
    "    'api_version': api_version\n",
    "}.items() if not v]\n",
    "\n",
    "if missing:\n",
    "    print(f\"[ERROR] Missing required variables: {', '.join(missing)}\")\n",
    "    print(\"[HINT] Ensure Cell 8 (.env generation) ran and load with: from dotenv import load_dotenv; load_dotenv('master-lab.env')\")\n",
    "    # Abort early to avoid further errors\n",
    "else:\n",
    "    # Use requests library to access HTTP headers (avoid duplicate import)\n",
    "    try:\n",
    "        requests\n",
    "    except NameError:\n",
    "        import requests\n",
    "\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "            url = f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}/openai/deployments/gpt-4o-mini/chat/completions\"\n",
    "            response = requests.post(\n",
    "                url=f\"{url}?api-version={api_version}\",\n",
    "                headers={\n",
    "                    \"api-key\": apim_api_key,\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": f\"Test {i+1}\"}],\n",
    "                    \"max_tokens\": 5\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            responses.append(elapsed)\n",
    "\n",
    "            region = response.headers.get('x-ms-region', 'Unknown')\n",
    "            backend_id = response.headers.get('x-ms-backend-id', 'Unknown')\n",
    "\n",
    "            regions.append(region)\n",
    "            backend_ids.append(backend_id)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Request {i+1}: {elapsed:.2f}s - Region: {region} - Backend: {backend_id}\")\n",
    "            else:\n",
    "                print(f\"Request {i+1}: {elapsed:.2f}s - HTTP {response.status_code} - Region: {region} - Backend: {backend_id}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[WARN] Request {i+1} failed: {e}\")\n",
    "            responses.append(0)\n",
    "            regions.append('Error')\n",
    "            backend_ids.append('Error')\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    avg_time = sum(responses) / len(responses) if responses else 0\n",
    "    print(f\"\\nAverage response time: {avg_time:.2f}s\")\n",
    "\n",
    "    from collections import Counter\n",
    "    region_counts = Counter(regions)\n",
    "    print(f\"\\nRegion Distribution:\")\n",
    "    for region, count in region_counts.items():\n",
    "        pct = (count / len(regions) * 100) if regions else 0\n",
    "        print(f\"  {region}: {count} requests ({pct:.1f}%)\")\n",
    "\n",
    "    unknown_count = region_counts.get('Unknown', 0)\n",
    "    if unknown_count == len(regions) and len(regions) > 0:\n",
    "        print('')\n",
    "        print('[INFO] All regions showing as \"Unknown\" - region headers may not be configured in APIM')\n",
    "        print('')\n",
    "        print('\ud83d\udccb TO ADD REGION HEADERS VIA APIM POLICY:')\n",
    "        print('   1. Azure Portal \u2192 API Management \u2192 APIs \u2192 inference-api')\n",
    "        print('   2. Click \"All operations\" \u2192 Outbound processing \u2192 Add policy')\n",
    "        print('   3. Add this XML to <outbound> section:')\n",
    "        print('')\n",
    "        print('   <set-header name=\"x-ms-region\" exists-action=\"override\">')\n",
    "        print('       <value>@(context.Deployment.Region)</value>')\n",
    "        print('   </set-header>')\n",
    "        print('   <set-header name=\"x-ms-backend-id\" exists-action=\"override\">')\n",
    "        print('       <value>@(context.Request.MatchedParameters.GetValueOrDefault(\"backend-id\", \"unknown\"))</value>')\n",
    "        print('   </set-header>')\n",
    "        print('')\n",
    "        print('   4. Save the policy')\n",
    "        print('')\n",
    "        print('\u2139\ufe0f  Region detection is informational only - load balancing still works')\n",
    "        print('')\n",
    "\n",
    "# Fallback util if utils.print_ok not available\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Load balancing test complete!')\n",
    "else:\n",
    "    print('[OK] Load balancing test complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_47_c20a7ffc",
   "metadata": {},
   "source": [
    "<a id=\"lab1-5-4\"></a>\n",
    "\n",
    "### 1.5.4 Visualize Response Times\n",
    "\n",
    "**Purpose**: Visualize response times from different backend regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cell_48_b37f6034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab 02 Complete!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Create DataFrame with response times and regions\n",
    "df = pd.DataFrame({\n",
    "    'Request': range(1, len(responses)+1),\n",
    "    'Time (s)': responses,\n",
    "    'Region': regions\n",
    "})\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Response times with region colors\n",
    "region_colors = {'Unknown': 'gray'}\n",
    "unique_regions = [r for r in set(regions) if r != 'Unknown']\n",
    "color_palette = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for idx, region in enumerate(unique_regions):\n",
    "    region_colors[region] = color_palette[idx % len(color_palette)]\n",
    "\n",
    "colors = [region_colors.get(r, 'gray') for r in regions]\n",
    "\n",
    "ax1.scatter(df['Request'], df['Time (s)'], c=colors, alpha=0.6, s=100)\n",
    "ax1.axhline(y=avg_time, color='r', linestyle='--', label=f'Average: {avg_time:.2f}s')\n",
    "ax1.set_xlabel('Request Number')\n",
    "ax1.set_ylabel('Response Time (s)')\n",
    "ax1.set_title('Load Balancing Response Times by Region')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Create custom legend for regions\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=region_colors[r], label=r) for r in set(regions)]\n",
    "ax1.legend(handles=legend_elements + [plt.Line2D([0], [0], color='r', linestyle='--', label=f'Avg: {avg_time:.2f}s')],\n",
    "          loc='upper right')\n",
    "\n",
    "# Plot 2: Region distribution bar chart\n",
    "region_counts = Counter(regions)\n",
    "regions_list = list(region_counts.keys())\n",
    "counts_list = list(region_counts.values())\n",
    "\n",
    "bars = ax2.bar(regions_list, counts_list, color=[region_colors.get(r, 'gray') for r in regions_list])\n",
    "ax2.set_xlabel('Region')\n",
    "ax2.set_ylabel('Number of Requests')\n",
    "ax2.set_title('Request Distribution Across Regions')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Replaced utils.print_ok (undefined) with a simple confirmation print\n",
    "print('Lab 02 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_49_35b632c0",
   "metadata": {},
   "source": [
    "Implement comprehensive observability using Azure Log Analytics and Application Insights for AI gateway monitoring.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- **Log Analytics Integration:** Automatic logging of all APIM requests and responses\n",
    "- **Application Insights:** Track performance metrics, failures, and dependencies\n",
    "- **Diagnostic Settings:** Configure what data to log and where to send it\n",
    "- **Query Language (KQL):** Write queries to analyze request patterns\n",
    "- **Dashboard Creation:** Build monitoring dashboards for AI gateway operations\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "**Success Criteria:**\n",
    "- All API requests logged to Log Analytics workspace\n",
    "- Application Insights captures latency metrics\n",
    "- KQL queries return request data successfully\n",
    "- Can trace individual requests end-to-end\n",
    "- Dashboards show real-time gateway health\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_51_7bbce6e3",
   "metadata": {},
   "source": [
    "<a id=\"lab1-6\"></a>\n",
    "\n",
    "## 1.6 Token Metrics Emitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Implement comprehensive observability for your AI gateway by emitting token consumption metrics to Application Insights. Track LLM token usage (prompt, completion, and total tokens) to monitor costs and capacity planning.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Token Metrics Policy:** Configure APIM to emit token metrics\n",
    "- **Cost Monitoring:** Track prompt tokens, completion tokens, and total tokens consumed\n",
    "- **Application Insights Integration:** Send metrics for centralized monitoring\n",
    "- **Response Streaming:** Support for OpenAI streaming responses while tracking tokens\n",
    "- **Troubleshooting:** Use tracing tools to verify metric emission\n",
    "\n",
    "#### How It Works\n",
    "1. Request arrives at APIM with Azure OpenAI headers\n",
    "2. Policy extracts token counts from responses\n",
    "3. Categorizes tokens: Prompt Tokens, Completion Tokens, Total Tokens\n",
    "4. Emits custom metrics to Application Insights\n",
    "5. Metrics can be queried and visualized in dashboards\n",
    "6. Supports streaming responses by aggregating token counts\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "- Application Insights instance (created during deployment)\n",
    "\n",
    "#### Expected Results\n",
    "- Metrics appear in Application Insights within 2-5 minutes\n",
    "- Custom metric \"LLM-Tokens\" shows prompt, completion, and total token counts\n",
    "- Can create alerts based on token thresholds\n",
    "- Streaming responses properly track all tokens\n",
    "- Can use KQL queries to analyze token patterns\n",
    "\n",
    "#### Key Configuration\n",
    "- Policy name: `azure-openai-emit-token-metric`\n",
    "- Supported endpoints: Azure OpenAI Chat Completion, Completion APIs\n",
    "- Metrics update in real-time as requests complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell_52_d2e70a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "Request 1: 61 tokens\n",
      "Request 2: 61 tokens\n",
      "Request 3: 61 tokens\n",
      "Request 4: 61 tokens\n",
      "Request 5: 61 tokens\n",
      "Total tokens used: 305\n",
      "[OK] Lab 04 Complete!\n"
     ]
    }
   ],
   "source": [
    "# Lab 04 token usage aggregation (auto-initialize client if missing)\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - using existing environment\")\n",
    "\n",
    "total_tokens = 0\n",
    "\n",
    "# Resolve required endpoint pieces from previously loaded deployment outputs / env\n",
    "apim_gateway_url = (\n",
    "    (step1_outputs.get('apimGatewayUrl') if isinstance(step1_outputs, dict) else None)\n",
    "    or os.environ.get('APIM_GATEWAY_URL')\n",
    ")\n",
    "inference_api_path = (\n",
    "    (step2_outputs.get('inferenceAPIPath') if isinstance(step2_outputs, dict) else None)\n",
    "    or os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    ")\n",
    "apim_api_key = None\n",
    "if isinstance(step1_outputs, dict):\n",
    "    subs = step1_outputs.get('apimSubscriptions') or []\n",
    "    if subs and isinstance(subs[0], dict):\n",
    "        apim_api_key = subs[0].get('key')\n",
    "if not apim_api_key:\n",
    "    apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "\n",
    "api_version = os.environ.get('OPENAI_API_VERSION', '2024-06-01')\n",
    "\n",
    "missing = [n for n, v in {\n",
    "    'apim_gateway_url': apim_gateway_url,\n",
    "    'inference_api_path': inference_api_path,\n",
    "    'apim_api_key': apim_api_key\n",
    "}.items() if not v]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required values for client init: {', '.join(missing)}. \"\n",
    "                       f\"Ensure earlier environment/deployment cells have been run.\")\n",
    "\n",
    "# Initialize AzureOpenAI client only if not already present\n",
    "if 'client' not in globals():\n",
    "    try:\n",
    "        # Prefer shim if loaded\n",
    "        if 'get_azure_openai_client' in globals():\n",
    "            client = get_azure_openai_client(\n",
    "                azure_endpoint=f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}\",\n",
    "                api_key=apim_api_key,\n",
    "                api_version=api_version\n",
    "            )\n",
    "        else:\n",
    "            from openai import AzureOpenAI\n",
    "            client = AzureOpenAI(\n",
    "                azure_endpoint=f\"{apim_gateway_url.rstrip('/')}/{inference_api_path}\",\n",
    "                api_key=apim_api_key,\n",
    "                api_version=api_version\n",
    "            )\n",
    "        print(\"[init] AzureOpenAI client initialized\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"[ERROR] openai package not found. Install dependencies first.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize AzureOpenAI client: {e}\")\n",
    "\n",
    "# Perform multiple requests and sum token usage\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{'role': 'user', 'content': 'Tell me about AI'}],\n",
    "            max_tokens=50,\n",
    "            temperature=0.2,\n",
    "            extra_headers={'api-key': apim_api_key}  # APIM expects key in api-key header\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Request {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Robust usage extraction (handles SDK variations)\n",
    "    tokens = 0\n",
    "    usage_obj = getattr(response, 'usage', None)\n",
    "    if usage_obj is not None:\n",
    "        # New SDK: usage fields may be attributes\n",
    "        tokens = getattr(usage_obj, 'total_tokens', None)\n",
    "        if tokens is None and isinstance(usage_obj, dict):\n",
    "            tokens = usage_obj.get('total_tokens')\n",
    "    if tokens is None:\n",
    "        # Fallback: sum prompt + completion if available\n",
    "        prompt_t = getattr(usage_obj, 'prompt_tokens', None) if usage_obj else None\n",
    "        completion_t = getattr(usage_obj, 'completion_tokens', None) if usage_obj else None\n",
    "        if isinstance(usage_obj, dict):\n",
    "            prompt_t = prompt_t or usage_obj.get('prompt_tokens')\n",
    "            completion_t = completion_t or usage_obj.get('completion_tokens')\n",
    "        if prompt_t is not None and completion_t is not None:\n",
    "            tokens = prompt_t + completion_t\n",
    "    if tokens is None:\n",
    "        tokens = 0  # default if usage unavailable\n",
    "\n",
    "    total_tokens += tokens\n",
    "    print(f\"Request {i+1}: {tokens} tokens\")\n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")\n",
    "print(\"[OK] Lab 04 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_68_6a12cc5e",
   "metadata": {},
   "source": [
    "<a id=\"lab1-7\"></a>\n",
    "\n",
    "## 1.7 Content Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Protect your AI gateway from harmful content by implementing the Azure AI Content Safety policy. This lab demonstrates how to screen user prompts before sending them to Azure OpenAI.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Content Safety Policy:** Configure LLM content filtering in APIM\n",
    "- **Pre-Request Scanning:** Analyze prompts before they reach the backend\n",
    "- **Severity Levels:** Understand how Content Safety categorizes harmful content\n",
    "- **Policy Actions:** Block malicious prompts or log suspicious content\n",
    "- **Configuration:** Fine-tune sensitivity thresholds for your use case\n",
    "- **Compliance:** Meet organizational policies around harmful content\n",
    "\n",
    "#### How It Works\n",
    "1. User prompt arrives at APIM gateway\n",
    "2. Policy intercepts request before sending to Azure OpenAI\n",
    "3. Prompt is sent to Azure AI Content Safety service\n",
    "4. Content Safety service analyzes for harmful content\n",
    "5. Severity score returned (0-7 scale)\n",
    "6. Policy decision:\n",
    "   - If severity < threshold: request proceeds to Azure OpenAI\n",
    "   - If severity >= threshold: request blocked with 403 error\n",
    "7. Response returned to client with content safety result\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "- Azure AI Content Safety resource (created during deployment)\n",
    "\n",
    "#### Expected Results\n",
    "- Normal prompts pass through Content Safety checks\n",
    "- Prompts with harmful content get blocked with 403 error\n",
    "- Content Safety verdict visible in response headers\n",
    "- Can view detailed analysis of why content was blocked\n",
    "- Different severity thresholds can be configured\n",
    "- Logs show all content safety evaluations\n",
    "\n",
    "#### Configuration Options\n",
    "- Severity threshold: Configurable (typically 0-7 scale)\n",
    "- Categories: Hate, SelfHarm, Sexual, Violence\n",
    "- Action: Block with 403 or Log and Proceed\n",
    "- Cache policy results for repeated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cell_69_39cd0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "Safe content: I don\u2019t have real-time weather data. To find out the current weather, you can check a\n",
      "\u274c Content blocked: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "[OK] Lab 07 Complete!\n"
     ]
    }
   ],
   "source": [
    "# Lab 07 Content Safety Test (adds JWT auth if required by current APIM policy)\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "else:\n",
    "    print(\"[warn] master-lab.env not found - using existing environment\")\n",
    "\n",
    "\n",
    "# Get API key for APIM\n",
    "apim_api_key = os.environ.get(\"APIM_API_KEY\", \"\")\n",
    "\n",
    "def _get_jwt_token():\n",
    "    # Reuse existing jwt_token if earlier cell created it\n",
    "    if 'jwt_token' in globals() and jwt_token:\n",
    "        return jwt_token\n",
    "    try:\n",
    "        cred = DefaultAzureCredential()\n",
    "        tok = cred.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return tok.token\n",
    "    except Exception as _e:\n",
    "        print(f'[auth] WARN: Unable to acquire JWT token ({_e}); proceeding without it.')\n",
    "        return None\n",
    "\n",
    "_jwt = _get_jwt_token()\n",
    "extra_headers = {}\n",
    "if _jwt:\n",
    "    extra_headers['Authorization'] = f'Bearer {_jwt}'\n",
    "\n",
    "def safe_call(messages):\n",
    "    \"\"\"Call helper that retries with JWT if first attempt 401 (in case policy just changed).\"\"\"\n",
    "    from openai import AuthenticationError\n",
    "    try:\n",
    "        return client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=messages,\n",
    "            max_tokens=20,\n",
    "            extra_headers={**extra_headers, 'api-key': apim_api_key} if extra_headers else {'api-key': apim_api_key}\n",
    "        )\n",
    "    except AuthenticationError as e:\n",
    "        msg = str(e)\n",
    "        if '401' in msg and not extra_headers:\n",
    "            # Acquire token and retry once\n",
    "            print('[auth] 401 without JWT; attempting to fetch token and retry...')\n",
    "            new_jwt = _get_jwt_token()\n",
    "            if new_jwt:\n",
    "                return client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages,\n",
    "                    max_tokens=20,\n",
    "                    extra_headers={'Authorization': f'Bearer {new_jwt}'}\n",
    "                )\n",
    "        raise\n",
    "\n",
    "# Test with safe content\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'What is the weather like?'}])\n",
    "    print(f\"Safe content: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Safe content request failed: {e}')\n",
    "\n",
    "# Test with potentially harmful content (expect policy to possibly block)\n",
    "try:\n",
    "    response = safe_call([{'role': 'user', 'content': 'How to harm someone?'}])\n",
    "    # Show what the model actually responded with\n",
    "    print(f\"Harmful content response: {response.choices[0].message.content}\")\n",
    "    print('\u2705 Content NOT blocked by policy (but may be sanitized by model)')\n",
    "except Exception as e:\n",
    "    # Could be a 403 from content safety or 400 from Azure OpenAI filter\n",
    "    print(f'\u274c Content blocked: {e}')\n",
    "\n",
    "# Fallback if utils not available\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Lab 07 Complete!')\n",
    "else:\n",
    "    print('[OK] Lab 07 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_70_caa353d3",
   "metadata": {},
   "source": [
    "<a id=\"lab1-8\"></a>\n",
    "\n",
    "## 1.8 Model Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Implement intelligent model routing in APIM to direct requests to appropriate Azure OpenAI backends based on the requested model name. This enables multi-model deployments with automatic request routing.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Model-Based Routing:** Configure conditional routing based on model parameter\n",
    "- **Multiple Backends:** Manage requests to different Azure OpenAI deployments\n",
    "- **Request Rewriting:** Modify requests to match backend deployment names\n",
    "- **Model Aliases:** Map user-friendly model names to actual deployment names\n",
    "- **Fallback Logic:** Handle requests for unavailable models gracefully\n",
    "- **Policy Composition:** Combine routing with other policies\n",
    "\n",
    "#### How It Works\n",
    "1. Client requests Azure OpenAI API with specific model parameter\n",
    "2. APIM policy extracts the model name from request\n",
    "3. Policy evaluates routing rules based on model\n",
    "4. Conditional logic routes to appropriate backend:\n",
    "   - GPT-4o \u2192 Azure OpenAI East deployment\n",
    "   - GPT-4 Turbo \u2192 Azure OpenAI Central deployment\n",
    "   - GPT-3.5 Turbo \u2192 Azure OpenAI West deployment\n",
    "5. Request forwarded to selected backend with deployment name rewrite\n",
    "6. Response returned to client transparently\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "- Multiple Azure OpenAI deployments with different models\n",
    "\n",
    "#### Expected Results\n",
    "- Requests for GPT-4o are routed to correct backend\n",
    "- Requests for GPT-4 Turbo reach appropriate deployment\n",
    "- Requests for GPT-3.5 Turbo complete successfully\n",
    "- Model names properly translated for each backend\n",
    "- Invalid model requests fail gracefully\n",
    "- Can trace routing decisions in APIM logs\n",
    "\n",
    "#### Common Use Cases\n",
    "1. **Multi-Region Deployment:** Route by model to distribute load geographically\n",
    "2. **Model Separation:** Keep different models in different deployments\n",
    "3. **Cost Optimization:** Route to cost-effective models for suitable workloads\n",
    "4. **Gradual Migration:** Route some requests to new model versions\n",
    "5. **A/B Testing:** Route percentage of traffic to different model versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell_71_0250903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Testing model: gpt-4o-mini\n",
      "Model gpt-4o-mini: Hello! How can I assist you today?\n",
      "[*] Testing model: gpt-4.1-nano\n",
      "[ERROR] Request failed for gpt-4.1-nano: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n",
      "[OK] Lab 08 Complete!\n"
     ]
    }
   ],
   "source": [
    "# Lab 08: Model Routing test (fixed for Dual Auth + invalid model + 401 handling)\n",
    "\n",
    "import os\n",
    "from openai import AuthenticationError\n",
    "\n",
    "# Ensure DefaultAzureCredential is available even if this cell runs before its import elsewhere.\n",
    "try:\n",
    "    DefaultAzureCredential  # type: ignore\n",
    "except NameError:\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Acquire JWT (audience: https://cognitiveservices.azure.com) \u2013 may be required with APIM dual auth.\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "except Exception as e:\n",
    "    jwt_token = None\n",
    "    print(f\"[auth] WARN: Unable to acquire JWT token: {e}\")\n",
    "\n",
    "extra_headers = {}\n",
    "if jwt_token:\n",
    "    extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "# Only test models that are actually deployed. gpt-4.1-mini not deployed; skip automatically.\n",
    "requested_models = ['gpt-4o-mini', 'gpt-4.1-nano']  # FIXED: Changed to gpt-4.1-nano (deployed in cell 28)\n",
    "available_models = {'gpt-4o-mini', 'gpt-4o', 'gpt-4.1-nano', 'text-embedding-3-small', 'text-embedding-3-large', 'dall-e-3'}  # from Step 2 config\n",
    "models_to_test = [m for m in requested_models if m in available_models]\n",
    "\n",
    "if len(models_to_test) != len(requested_models):\n",
    "    missing = [m for m in requested_models if m not in models_to_test]\n",
    "    print(f\"[routing] Skipping unavailable models: {', '.join(missing)}\")\n",
    "\n",
    "# Guard if OpenAI client is not yet defined (e.g., cell ordering)\n",
    "if 'client' not in globals():\n",
    "    print(\"[WARN] OpenAI client 'client' not found; skipping model tests.\")\n",
    "    models_to_test = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"[*] Testing model: {model}\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "            max_tokens=10,\n",
    "            extra_headers=extra_headers if extra_headers else None\n",
    "        )\n",
    "        # Robust content extraction\n",
    "        content = \"\"\n",
    "        try:\n",
    "            content = response.choices[0].message.content\n",
    "        except AttributeError:\n",
    "            if hasattr(response.choices[0].message, 'get'):\n",
    "                content = response.choices[0].message.get('content', '')\n",
    "        print(f\"Model {model}: {content}\")\n",
    "    except AuthenticationError as e:\n",
    "        # Attempt one silent JWT refresh if first attempt lacked/invalid token\n",
    "        if not jwt_token:\n",
    "            print(f\"[auth] 401 without JWT; attempting token fetch & retry...\")\n",
    "            try:\n",
    "                credential = DefaultAzureCredential()\n",
    "                jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "                extra_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "                retry_resp = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{'role': 'user', 'content': 'Hello'}],\n",
    "                    max_tokens=10,\n",
    "                    extra_headers=extra_headers\n",
    "                )\n",
    "                retry_content = \"\"\n",
    "                try:\n",
    "                    retry_content = retry_resp.choices[0].message.content\n",
    "                except AttributeError:\n",
    "                    if hasattr(retry_resp.choices[0].message, 'get'):\n",
    "                        retry_content = retry_resp.choices[0].message.get('content', '')\n",
    "                print(f\"Model {model} (retry): {retry_content}\")\n",
    "                continue\n",
    "            except Exception as e2:\n",
    "                print(f\"[ERROR] Retry after acquiring JWT failed: {e2}\")\n",
    "        print(f\"[ERROR] Auth failed for {model}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Request failed for {model}: {e}\")\n",
    "\n",
    "# Safe completion notification without NameError if utils is absent\n",
    "if 'utils' in globals() and hasattr(utils, 'print_ok'):\n",
    "    utils.print_ok('Lab 08 Complete!')\n",
    "else:\n",
    "    print('[OK] Lab 08 Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_72_62fb5c72",
   "metadata": {},
   "source": [
    "<a id=\"lab1-9\"></a>\n",
    "\n",
    "## 1.9 AI Foundry SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Integrate Azure AI Foundry SDK with your APIM gateway to route all LLM requests through the API gateway while using native AI Foundry development patterns. This lab demonstrates how to configure the SDK to use APIM as the underlying Azure OpenAI endpoint.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **AI Foundry SDK:** Use Python SDK for AI Foundry projects\n",
    "- **Connection Configuration:** Configure Azure OpenAI connections with APIM endpoint\n",
    "- **Gateway Routing:** Route SDK requests through APIM automatically\n",
    "- **Model Catalog:** Access AI Foundry model catalog through APIM\n",
    "- **Policy Application:** All APIM policies apply to SDK requests\n",
    "- **Authentication:** Handle credentials seamlessly with APIM integration\n",
    "- **Development Patterns:** Use native SDK patterns with gateway benefits\n",
    "\n",
    "#### How It Works\n",
    "1. Developer creates Azure AI Foundry project with AI Foundry SDK\n",
    "2. AI Foundry project has OpenAI connection configured\n",
    "3. OpenAI connection specifies APIM endpoint as the provider\n",
    "4. Developer uses standard AI Foundry SDK code\n",
    "5. SDK requests go to APIM instead of direct Azure OpenAI\n",
    "6. APIM applies all configured policies:\n",
    "   - Load balancing\n",
    "   - Rate limiting\n",
    "   - Content safety\n",
    "   - Token metrics\n",
    "   - Semantic caching\n",
    "   - Logging\n",
    "7. Requests forwarded to backend Azure OpenAI\n",
    "8. Responses returned through APIM to application\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Azure CLI installed\n",
    "- Azure Subscription with Contributor permissions\n",
    "- AI Foundry project created\n",
    "- Azure OpenAI connection configured with APIM endpoint\n",
    "- APIM subscription key configured\n",
    "\n",
    "#### Expected Results\n",
    "- AI Foundry SDK initializes successfully\n",
    "- Requests are routed through APIM (visible in APIM logs)\n",
    "- Chat completions work through SDK\n",
    "- All APIM policies apply to SDK requests\n",
    "- Can track SDK usage in APIM analytics\n",
    "- No code changes needed to use APIM gateway\n",
    "- Token metrics appear in Application Insights\n",
    "\n",
    "#### Connection Configuration Format\n",
    "```\n",
    "Provider: Azure OpenAI\n",
    "Endpoint: https://{APIM_GATEWAY}.azure-api.net\n",
    "Key: {APIM_SUBSCRIPTION_KEY}\n",
    "API Version: 2024-08-01-preview\n",
    "```\n",
    "\n",
    "#### Key Benefits\n",
    "1. **Centralized Governance:** All AI Foundry SDK requests through APIM policies\n",
    "2. **Unified Monitoring:** Track all interactions in one place\n",
    "3. **Multi-Region Support:** APIM load balancing benefits\n",
    "4. **Cost Control:** Token metrics and rate limiting\n",
    "5. **Security:** All APIM security policies applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_73_cc780c0e",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "\n",
    "\u25b6\ufe0f Click `Run All` to execute all steps sequentially, or execute them `Step by Step`...\n",
    "\n",
    "ChatCompletionsClient must use FULL deployment path:\n",
    "  {apim_gateway_url}/{inference_api_path}/openai/deployments/{deployment_name}\n",
    "\n",
    "Reuse imports already loaded in earlier cells (avoid re-import)\n",
    "Variables expected from earlier cells:\n",
    "  apim_gateway_url, inference_api_path, apim_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell_75_ce629d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Inference Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference/openai/deployments/gpt-4o-mini\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Acquired JWT token\n",
      "[OK] ChatCompletionsClient created successfully\n",
      "\n",
      "[*] Testing chat completion with Azure AI Inference SDK...\n",
      "[SUCCESS] Response: Azure AI Foundry is a service that provides tools and resources for organizations to develop, deploy, and manage artificial intelligence (AI) applications and models. It is part of the broader Azure ecosystem, which offers a range of cloud services and solutions, including Azure Machine Learning, Cognitive Services, and various data management tools.\n",
      "\n",
      "Key features of Azure AI Foundry may include:\n",
      "\n",
      "1. **Model Development**: Tools for building and training machine learning models using various frameworks such as TensorFlow, PyTorch, and scikit-learn.\n",
      "\n",
      "2. **Pre-built Solutions**: Access to pre-built AI models and templates that can be customized for specific business needs, enabling quicker implementation of AI solutions.\n",
      "\n",
      "3. **Integration with Azure Services**: Seamless integration with other Azure services, such as data storage, analytics, and monitoring, to support the entire AI lifecycle.\n",
      "\n",
      "4. **Collaboration**: Features that allow teams to collaborate on AI projects, including version control, project management, and shared workspaces.\n",
      "\n",
      "5. **Deployment and Management**: Tools for deploying AI models into production and managing them over time, including monitoring performance and updating models as needed.\n",
      "\n",
      "6. **Security and Compliance**: Options to ensure that AI applications comply with security and regulatory standards, safeguarding sensitive data and maintaining user privacy.\n",
      "\n",
      "Azure AI Foundry is designed to help organizations accelerate their AI initiatives, reduce time to market, and ultimately enhance their ability to leverage AI for business transformation. \n",
      "\n",
      "For the most detailed and current information, it's advisable to refer to the official Azure documentation or Microsoft's announcements related to Azure services.\n",
      "\n",
      "[OK] Lab 09 Complete!\n"
     ]
    }
   ],
   "source": [
    "deployment_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\n",
    "missing_vars = [k for k, v in {\n",
    "    'apim_gateway_url': globals().get('apim_gateway_url'),\n",
    "    'inference_api_path': globals().get('inference_api_path'),\n",
    "    'apim_api_key': globals().get('apim_api_key')\n",
    "}.items() if not v]\n",
    "\n",
    "if missing_vars:\n",
    "    raise RuntimeError(f\"Missing required variables: {', '.join(missing_vars)}. Run the earlier env/config cells first.\")\n",
    "\n",
    "# Normalize endpoint (avoid double slashes)\n",
    "base = apim_gateway_url.rstrip('/')\n",
    "inference_path = inference_api_path.strip('/')\n",
    "\n",
    "inference_endpoint = f\"{base}/{inference_path}/openai/deployments/{deployment_name}\"\n",
    "print(f\"[OK] Inference Endpoint: {inference_endpoint}\")\n",
    "\n",
    "# Acquire JWT if current APIM policy enforces validate-jwt (dual auth or JWT-only)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "jwt_token = None\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Audience used in active APIM policies\n",
    "    jwt_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    print(\"[OK] Acquired JWT token\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Unable to acquire JWT token: {e}\")\n",
    "    print(\"[INFO] Will attempt call with API key only (may fail if JWT required)\")\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "inference_client = ChatCompletionsClient(\n",
    "    endpoint=inference_endpoint,\n",
    "    credential=AzureKeyCredential(apim_api_key)  # use correct API key variable\n",
    ")\n",
    "\n",
    "print(\"[OK] ChatCompletionsClient created successfully\\n\")\n",
    "\n",
    "# Prepare headers: dual auth requires both api-key (handled via AzureKeyCredential) and Authorization\n",
    "call_headers = {}\n",
    "if jwt_token:\n",
    "    call_headers[\"Authorization\"] = f\"Bearer {jwt_token}\"\n",
    "\n",
    "print(\"[*] Testing chat completion with Azure AI Inference SDK...\")\n",
    "try:\n",
    "    response = inference_client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=\"You are helpful.\"),\n",
    "            UserMessage(content=\"What is Azure AI Foundry?\")\n",
    "        ],\n",
    "        headers=call_headers if call_headers else None  # azure-core style header injection\n",
    "    )\n",
    "    print(f\"[SUCCESS] Response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    if \"Invalid JWT\" in msg or \"401\" in msg:\n",
    "        print(f\"[ERROR] Authentication failed: {msg}\")\n",
    "        print(\"[HINT] Active APIM policy likely requires a valid JWT. Ensure az login completed or managed identity available.\")\n",
    "        print(\"[HINT] Retry after confirming validate-jwt audiences match https://cognitiveservices.azure.com\")\n",
    "    else:\n",
    "        print(f\"[ERROR] Request failed: {msg}\")\n",
    "        print(\"[HINT] Verify deployment name matches APIM backend path and policy didn't strip /openai segment.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Lab 09 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_77_7dd6b64b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section2\"></a>\n",
    "\n",
    "# Section 2: MCP Fundamentals\n",
    "\n",
    "Learn MCP basics:\n",
    "- Client initialization\n",
    "- Calling MCP tools\n",
    "- Data retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_78_2e777ad7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "MCP servers are initialized in Cell 11 using MCPClient.\n",
    "\n",
    "The global 'mcp' object provides access to all configured data sources:\n",
    "  - mcp.excel    (Excel Analytics MCP - direct)\n",
    "  - mcp.docs     (Research Documents MCP - direct)\n",
    "  - mcp.github   (GitHub API via APIM)\n",
    "  - mcp.weather  (Weather API via APIM)\n",
    "\n",
    "All configuration is loaded from .mcp-servers-config file.\n",
    "No additional initialization needed in this cell.\n",
    "\"\"\"\n",
    "---\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. AI application sends MCP request to APIM\n",
    "2. APIM validates OAuth token and enforces policies\n",
    "3. Request forwarded to MCP server\n",
    "4. MCP server executes tool and returns result\n",
    "5. APIM proxies response back to client\n",
    "6. AI model processes tool result and generates response\n",
    "\n",
    "---\n",
    "\n",
    "### Two MCP Connection Patterns\n",
    "\n",
    "**Important:** This lab uses HTTP-based MCP servers that communicate via POST requests to `/mcp/` endpoints.\n",
    "\n",
    "<details>\n",
    "<summary><b>Pattern 1: HTTP-Based MCP</b> (\u2705 Used in this notebook)</summary>\n",
    "\n",
    "**How It Works:**\n",
    "- **Protocol:** HTTP POST requests\n",
    "- **Endpoint:** `{server_url}/mcp/`\n",
    "- **Format:** JSON-RPC 2.0\n",
    "- **Communication:** Request/response pattern\n",
    "\n",
    "**Advantages:**\n",
    "- Simple, reliable, works with standard HTTP clients\n",
    "- Easy to test with curl or Postman\n",
    "- Works through standard load balancers and API gateways\n",
    "- No special client libraries required\n",
    "- Firewall-friendly (standard HTTP/HTTPS)\n",
    "\n",
    "**Example Request:**\n",
    "```http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Sales Analysis via MCP Excel Server\n",
      "================================================================================\n",
      "\ud83d\udce4 Uploading Excel file via MCP: sales_performance.xlsx\n",
      "\u2705 In-memory cache key: sales_performance.xlsx\n",
      "\n",
      "\ud83d\udccb Columns:\n",
      "['Region', 'Product', 'Date', 'TotalSales', 'Quantity', 'CustomerID']\n",
      "\n",
      "\ud83d\udcc4 Preview (first rows):\n",
      "  {'Region': 'Asia Pacific', 'Product': 'Professional Services', 'Date': '2024-01-01T00:00:00', 'TotalSales': 673076.1796812697, 'Quantity': 7973, 'CustomerID': 'CUST-16610'}\n",
      "  {'Region': 'North America', 'Product': 'Software Licenses', 'Date': '2024-01-01T00:00:00', 'TotalSales': 56427.00796144797, 'Quantity': 4237, 'CustomerID': 'CUST-52727'}\n",
      "  {'Region': 'North America', 'Product': 'Cloud Services', 'Date': '2024-01-01T00:00:00', 'TotalSales': 598025.514808326, 'Quantity': 3792, 'CustomerID': 'CUST-46639'}\n",
      "  {'Region': 'Latin America', 'Product': 'Software Licenses', 'Date': '2024-01-01T00:00:00', 'TotalSales': 354449.5095706386, 'Quantity': 547, 'CustomerID': 'CUST-50733'}\n",
      "  {'Region': 'North America', 'Product': 'Software Licenses', 'Date': '2024-01-01T00:00:00', 'TotalSales': 251141.6478808843, 'Quantity': 1232, 'CustomerID': 'CUST-19837'}\n",
      "\n",
      "\ud83d\udcca Running sales analysis (group_by='Region', metric='TotalSales') via MCP...\n",
      "\u2705 analyze_sales succeeded using identifier: sales_performance.xlsx\n",
      "\n",
      "\ud83d\udcc8 MCP Sales Analysis Summary:\n",
      "================================================================================\n",
      "{'total': 936730612.4413884, 'average': 374832.5226832066, 'count': 2500}\n",
      "\n",
      "\ud83d\udcca Sales by Region (Top 10):\n",
      "  01. Asia Pacific: $212,162,358.17\n",
      "  02. Europe: $237,020,292.26\n",
      "  03. Latin America: $232,880,138.13\n",
      "  04. North America: $254,667,823.88\n",
      "\n",
      "\ud83d\udca1 Compact sales_data_info for AI prompts:\n",
      "Columns: ['Region', 'Product', 'Date', 'TotalSales', 'Quantity', 'CustomerID']\n",
      "Total Sales: 936730612.4413884 | Avg Sale: 374832.5226832066 | Rows: 2500\n",
      "Regional breakdown available\n",
      "\n",
      "\u2705 Cell 79 complete. Variable 'excel_cache_key' = 'sales_performance.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1: Sales Analysis via MCP Excel Server\n",
    "print(\"\ud83d\udcca Sales Analysis via MCP Excel Server\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    if not mcp or not mcp.excel.server_url:\n",
    "        raise RuntimeError(\"MCP Excel server not configured \u2013 check .mcp-servers-config\")\n",
    "    \n",
    "    # Find Excel file - Use .xlsx files (workshop pattern)\n",
    "    search_path = Path(\"./sample-data/excel/\")\n",
    "    excel_candidates = list(search_path.glob(\"*sales*.xlsx\"))\n",
    "    \n",
    "    if not excel_candidates:\n",
    "        raise FileNotFoundError(f\"Could not locate sales Excel file in '{search_path.resolve()}'\")\n",
    "    \n",
    "    local_excel_path = Path(excel_candidates[0])\n",
    "    excel_file_name = local_excel_path.name\n",
    "    \n",
    "    print(f\"\ud83d\udce4 Uploading Excel file via MCP: {excel_file_name}\")\n",
    "    upload_result = mcp.excel.upload_excel(str(local_excel_path))\n",
    "    \n",
    "    # upload_excel loads into in-memory cache keyed ONLY by file_name (no /app/data prefix)\n",
    "    file_cache_key = upload_result.get('file_name', excel_file_name)\n",
    "    print(f\"\u2705 In-memory cache key: {file_cache_key}\")\n",
    "    \n",
    "    # Prefer metadata from upload_result; fall back to load_excel if needed\n",
    "    load_info = upload_result\n",
    "    if 'columns' not in load_info or 'preview' not in load_info:\n",
    "        # Some server variants might require explicit load; try both raw name and /app/data path\n",
    "        possible_paths = [file_cache_key]\n",
    "        if not file_cache_key.startswith('/app/'):\n",
    "            possible_paths.append(f\"/app/data/{file_cache_key}\")\n",
    "        \n",
    "        for pth in possible_paths:\n",
    "            try:\n",
    "                tmp = mcp.excel.load_excel(pth)\n",
    "                if isinstance(tmp, dict) and tmp.get('success'):\n",
    "                    load_info = tmp\n",
    "                    file_cache_key = pth\n",
    "                    print(f\"   Loaded Excel from path: {pth}\")\n",
    "                    break\n",
    "            except Exception as le:\n",
    "                print(f\"   load_excel attempt failed for {pth}: {le}\")\n",
    "    \n",
    "    # Normalize response (handle string responses)\n",
    "    if isinstance(load_info, str):\n",
    "        print(\"\u26a0\ufe0f load_info is text; attempting JSON parse\")\n",
    "        import json as _json\n",
    "        try:\n",
    "            load_info = _json.loads(load_info)\n",
    "        except Exception:\n",
    "            load_info = {\"raw\": load_info}\n",
    "    \n",
    "    # Get columns and preview\n",
    "    columns = load_info.get('columns') or load_info.get('schema') or []\n",
    "    preview = load_info.get('preview') or load_info.get('head') or []\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccb Columns:\")\n",
    "    print(columns if columns else \"  (No column list returned)\")\n",
    "    \n",
    "    if preview:\n",
    "        print(f\"\\n\ud83d\udcc4 Preview (first rows):\")\n",
    "        for row in (preview[:5] if isinstance(preview, list) else []):\n",
    "            print(f\"  {row}\")\n",
    "    \n",
    "    # Analyze sales data - Use TotalSales column with robust fallback\n",
    "    print(f\"\\n\ud83d\udcca Running sales analysis (group_by='Region', metric='TotalSales') via MCP...\")\n",
    "    analysis_result = None\n",
    "    analyze_attempts = [file_cache_key]\n",
    "    if not file_cache_key.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{file_cache_key}\")  # fallback if server persisted file\n",
    "    \n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            analysis_result = mcp.excel.analyze_sales(target, group_by=\"Region\", metric=\"TotalSales\")\n",
    "            print(f\"\u2705 analyze_sales succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   analyze_sales failed for {target}: {ae}\")\n",
    "    \n",
    "    if analysis_result is None:\n",
    "        raise RuntimeError(f\"Failed to analyze sales using any identifier. Last error: {last_error}\")\n",
    "    \n",
    "    # Normalize JSON response\n",
    "    if isinstance(analysis_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            analysis_result = _json.loads(analysis_result)\n",
    "        except Exception:\n",
    "            analysis_result = {\"raw\": analysis_result}\n",
    "    \n",
    "    # Extract summary and grouped data (handle different response formats)\n",
    "    summary = analysis_result.get(\"summary\") or analysis_result.get(\"result\") or analysis_result.get(\"raw\")\n",
    "    grouped = analysis_result.get(\"grouped_data\") or analysis_result.get(\"groups\") or analysis_result.get(\"analysis\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc8 MCP Sales Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary if summary else analysis_result)\n",
    "    \n",
    "    # Display grouped results with dynamic key detection\n",
    "    if grouped and isinstance(grouped, list) and len(grouped) > 0:\n",
    "        first_item = grouped[0]\n",
    "        region_key = 'Region' if 'Region' in first_item else list(first_item.keys())[0]\n",
    "        total_key = 'Total' if 'Total' in first_item else 'TotalSales' if 'TotalSales' in first_item else None\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Sales by Region (Top 10):\")\n",
    "        for i, row in enumerate(grouped[:10], 1):\n",
    "            region_val = row.get(region_key, 'Unknown')\n",
    "            total_val = row.get(total_key) if total_key else row\n",
    "            print(f\"  {i:02d}. {region_val}: ${total_val:,.2f}\" if isinstance(total_val, (int, float)) else f\"  {i:02d}. {region_val}: {total_val}\")\n",
    "    \n",
    "    # Extract metrics for AI prompts\n",
    "    total_sales = None\n",
    "    avg_sales = None\n",
    "    num_transactions = None\n",
    "    if isinstance(summary, dict):\n",
    "        total_sales = summary.get(\"total\") or summary.get(\"total_sales\")\n",
    "        avg_sales = summary.get(\"average\") or summary.get(\"avg\") or summary.get(\"average_sale\")\n",
    "        num_transactions = summary.get(\"count\") or summary.get(\"num_rows\")\n",
    "    \n",
    "    # Create compact summary for AI prompts\n",
    "    sales_data_info = (f\"Columns: {columns}\\n\" if columns else \"\") + \\\n",
    "        (f\"Total Sales: {total_sales} | Avg Sale: {avg_sales} | Rows: {num_transactions}\\n\" if total_sales else \"\") + \\\n",
    "        (\"Regional breakdown available\" if grouped else \"\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 Compact sales_data_info for AI prompts:\")\n",
    "    print(sales_data_info)\n",
    "    \n",
    "    # Export useful identifiers for later cells\n",
    "    excel_cache_key = file_cache_key\n",
    "    \n",
    "    print(f\"\\n\u2705 Cell 79 complete. Variable 'excel_cache_key' = '{excel_cache_key}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\u274c File error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Verify Excel file exists in ./sample-data/excel/\")\n",
    "    print(f\"   \u2022 Check file permissions\")\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None\n",
    "except MCPError as e:\n",
    "    print(f\"\u274c MCP error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Ensure EXCEL_MCP_URL points to a running Excel MCP server\")\n",
    "    print(f\"   \u2022 Verify the file is a valid .xlsx (modern ZIP-based format)\")\n",
    "    print(f\"   \u2022 Check .mcp-servers-config file exists\")\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None\n",
    "except RuntimeError as e:\n",
    "    print(f\"\u274c Runtime error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Do NOT prepend /app/data unless server persists uploads to disk\")\n",
    "    print(f\"   \u2022 If persistence needed, modify server to write file bytes to disk before load_excel\")\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Unexpected error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    excel_cache_key = None\n",
    "    sales_data_info = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-1\"></a>\n",
    "\n",
    "## 2.1 Exercise: Sales Analysis via MCP + AI\n",
    "Use MCP for data access and Azure OpenAI for ALL analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Verifying MCP Sales Analysis Results\n",
      "================================================================================\n",
      "\u2705 MCP analysis successful!\n",
      "   File key: sales_performance.xlsx\n",
      "   This key can be used for further analysis in subsequent cells.\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1 (Fallback): Verify MCP Results\n",
    "print(\"\ud83d\udd0d Verifying MCP Sales Analysis Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "    print(\"\u26a0\ufe0f MCP analysis did not complete successfully in Cell 81.\")\n",
    "    print(\"   Please check:\")\n",
    "    print(\"   1. MCP Excel server is running\")\n",
    "    print(\"   2. .mcp-servers-config file exists with EXCEL_MCP_URL\")\n",
    "    print(\"   3. Excel file exists at ./sample-data/excel/sales_performance.xlsx\")\n",
    "else:\n",
    "    print(f\"\u2705 MCP analysis successful!\")\n",
    "    print(f\"   File key: {excel_cache_key}\")\n",
    "    print(f\"   This key can be used for further analysis in subsequent cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the MCP-based analysis above fails (e.g., due to server issues or file compatibility problems), the cell below provides a local fallback using the `pandas` library. It reads the `sales_performance.xlsx` file directly from the local `sample-data` directory and generates a similar structural summary.\n",
    "\n",
    "This ensures that you can proceed with the subsequent AI analysis exercises even if the primary MCP tool encounters an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-2\"></a>\n",
    "\n",
    "## 2.2 Exercise: Azure Cost Analysis via MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcb0 Azure Cost Analysis via MCP Excel Server\n",
      "================================================================================\n",
      "\u2705 Found cost file: azure_resource_costs.xlsx\n",
      "\ud83d\udce4 Uploading to MCP Excel server...\n",
      "\u2705 Upload successful. File key: azure_resource_costs.xlsx\n",
      "\n",
      "\ud83d\udccb Columns:\n",
      "['ServiceName', 'ResourceGroup', 'Region', 'Cost', 'Date', 'SubscriptionID']\n",
      "\n",
      "\ud83d\udcc4 Preview (first rows):\n",
      "  {'ServiceName': 'Virtual Machines', 'ResourceGroup': 'rg-virtual-machines', 'Region': 'East US', 'Cost': 17738.9322903674, 'Date': '2024-01', 'SubscriptionID': 'sub-5906'}\n",
      "  {'ServiceName': 'Virtual Machines', 'ResourceGroup': 'rg-virtual-machines', 'Region': 'West Europe', 'Cost': 1832.837000168093, 'Date': '2024-01', 'SubscriptionID': 'sub-1749'}\n",
      "  {'ServiceName': 'Virtual Machines', 'ResourceGroup': 'rg-virtual-machines', 'Region': 'Southeast Asia', 'Cost': 13605.60971028315, 'Date': '2024-01', 'SubscriptionID': 'sub-5695'}\n",
      "\n",
      "\ud83d\udcca Calculating Azure resource costs...\n",
      "\u2705 calculate_costs succeeded using identifier: azure_resource_costs.xlsx\n",
      "\n",
      "\ud83d\udcb0 Cost Calculation Complete!\n",
      "\n",
      "\u2705 Cell 85 complete. Variable 'cost_cache_key' = 'azure_resource_costs.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.3: Azure Cost Analysis via MCP Excel Server\n",
    "print(\"\ud83d\udcb0 Azure Cost Analysis via MCP Excel Server\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pathlib import Path\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    # Path to cost Excel file - Use .xlsx directly (extracted from .zip)\n",
    "    cost_file_path = Path(\"./sample-data/excel/azure_resource_costs.xlsx\")\n",
    "    \n",
    "    if not cost_file_path.exists():\n",
    "        raise FileNotFoundError(f\"Cost file not found: {cost_file_path.resolve()}\")\n",
    "    \n",
    "    print(f\"\u2705 Found cost file: {cost_file_path.name}\")\n",
    "    \n",
    "    # Upload cost file to MCP server\n",
    "    print(f\"\ud83d\udce4 Uploading to MCP Excel server...\")\n",
    "    upload_result = mcp.excel.upload_excel(str(cost_file_path))\n",
    "    \n",
    "    # Extract file cache key\n",
    "    cost_cache_key = upload_result.get('file_name', cost_file_path.name)\n",
    "    print(f\"\u2705 Upload successful. File key: {cost_cache_key}\")\n",
    "    \n",
    "    # Prefer metadata from upload_result; fall back to load_excel if needed\n",
    "    load_info = upload_result\n",
    "    if 'columns' not in load_info or 'preview' not in load_info:\n",
    "        # Some server variants might require explicit load; try both raw name and /app/data path\n",
    "        possible_paths = [cost_cache_key]\n",
    "        if not cost_cache_key.startswith('/app/'):\n",
    "            possible_paths.append(f\"/app/data/{cost_cache_key}\")\n",
    "        \n",
    "        for pth in possible_paths:\n",
    "            try:\n",
    "                tmp = mcp.excel.load_excel(pth)\n",
    "                if isinstance(tmp, dict) and tmp.get('success'):\n",
    "                    load_info = tmp\n",
    "                    cost_cache_key = pth\n",
    "                    print(f\"   Loaded Excel from path: {pth}\")\n",
    "                    break\n",
    "            except Exception as le:\n",
    "                print(f\"   load_excel attempt failed for {pth}: {le}\")\n",
    "    \n",
    "    # Normalize response (handle string responses)\n",
    "    if isinstance(load_info, str):\n",
    "        print(\"\u26a0\ufe0f load_info is text; attempting JSON parse\")\n",
    "        import json as _json\n",
    "        try:\n",
    "            load_info = _json.loads(load_info)\n",
    "        except Exception:\n",
    "            load_info = {\"raw\": load_info}\n",
    "    \n",
    "    # Get columns and preview\n",
    "    columns = load_info.get('columns') or load_info.get('schema') or []\n",
    "    preview = load_info.get('preview') or load_info.get('head') or []\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccb Columns:\")\n",
    "    print(columns if columns else \"  (No column list returned)\")\n",
    "    \n",
    "    if preview:\n",
    "        print(f\"\\n\ud83d\udcc4 Preview (first rows):\")\n",
    "        for row in (preview[:3] if isinstance(preview, list) else []):\n",
    "            print(f\"  {row}\")\n",
    "    \n",
    "    # Calculate costs using MCP with robust fallback\n",
    "    # FIXED: Updated column names to match actual Excel file structure\n",
    "    # File has: ServiceName, ResourceGroup, Region, Cost, Date, SubscriptionID\n",
    "    print(f\"\\n\ud83d\udcca Calculating Azure resource costs...\")\n",
    "    cost_analysis = None\n",
    "    analyze_attempts = [cost_cache_key]\n",
    "    if not cost_cache_key.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{cost_cache_key}\")  # fallback if server persisted file\n",
    "    \n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            cost_analysis = mcp.excel.calculate_costs(\n",
    "                target,\n",
    "                resource_type_col='ServiceName',  # FIXED: was 'Resource_Type'\n",
    "                cost_col='Cost'  # FIXED: was 'Daily_Cost'\n",
    "            )\n",
    "            print(f\"\u2705 calculate_costs succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   calculate_costs failed for {target}: {ae}\")\n",
    "    \n",
    "    if cost_analysis is None:\n",
    "        raise RuntimeError(f\"Failed to calculate costs using any identifier. Last error: {last_error}\")\n",
    "    \n",
    "    # Normalize JSON response\n",
    "    if isinstance(cost_analysis, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            cost_analysis = _json.loads(cost_analysis)\n",
    "        except Exception:\n",
    "            cost_analysis = {\"raw\": cost_analysis}\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcb0 Cost Calculation Complete!\")\n",
    "    \n",
    "    # Display results (handle different response formats)\n",
    "    if isinstance(cost_analysis, dict):\n",
    "        if 'summary' in cost_analysis:\n",
    "            print(f\"\\n\ud83d\udcb0 Cost Summary:\")\n",
    "            daily_total = cost_analysis['summary'].get('daily_total', 0)\n",
    "            monthly_projection = cost_analysis['summary'].get('monthly_projection', 0)\n",
    "            print(f\"   Daily Total: ${daily_total:,.2f}\")\n",
    "            print(f\"   Monthly Projection: ${monthly_projection:,.2f}\")\n",
    "        \n",
    "        resource_breakdown = cost_analysis.get('by_resource_type') or cost_analysis.get('by_resource') or cost_analysis.get('analysis')\n",
    "        if resource_breakdown and isinstance(resource_breakdown, list):\n",
    "            print(f\"\\n\ud83d\udcca Costs by Resource Type:\")\n",
    "            for item in resource_breakdown:\n",
    "                # FIXED: Updated to match ServiceName and Cost columns\n",
    "                resource = item.get('ServiceName') or item.get('Resource_Type') or item.get('resource_type') or item.get('resource', 'Unknown')\n",
    "                cost_val = item.get('Cost') or item.get('Daily_Cost') or item.get('daily_cost') or item.get('cost', 0)\n",
    "                monthly = cost_val * 30\n",
    "                print(f\"   {resource}: ${cost_val:,.2f}/day (${monthly:,.2f}/month)\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{cost_analysis}\")\n",
    "    \n",
    "    print(f\"\\n\u2705 Cell 85 complete. Variable 'cost_cache_key' = '{cost_cache_key}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\u274c File error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Verify file exists at ./sample-data/excel/azure_resource_costs.xlsx\")\n",
    "    print(f\"   \u2022 Check file permissions\")\n",
    "    cost_cache_key = None\n",
    "except MCPError as e:\n",
    "    print(f\"\u274c MCP error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Ensure EXCEL_MCP_URL points to a running Excel MCP server\")\n",
    "    print(f\"   \u2022 Verify the file is a valid .xlsx (modern ZIP-based format)\")\n",
    "    print(f\"   \u2022 Check .mcp-servers-config file exists\")\n",
    "    cost_cache_key = None\n",
    "except RuntimeError as e:\n",
    "    print(f\"\u274c Runtime error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Do NOT prepend /app/data unless server persists uploads to disk\")\n",
    "    print(f\"   \u2022 Verify calculate_costs function is available on MCP server\")\n",
    "    cost_cache_key = None\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Unexpected error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    cost_cache_key = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-3\"></a>\n",
    "\n",
    "## 2.3 Exercise: Dynamic Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Dynamic MCP Analysis with User-Defined Columns\n",
      "================================================================================\n",
      "\ud83d\udcca Performing dynamic analysis on 'sales_performance.xlsx'\n",
      "   Grouping by: 'Product'\n",
      "   Aggregating metric: 'Quantity'\n",
      "\n",
      "\ud83d\udcca Running analysis via MCP...\n",
      "\u2705 analyze_sales succeeded using identifier: sales_performance.xlsx\n",
      "\n",
      "\u2705 Dynamic analysis complete!\n",
      "\n",
      "\ud83d\udcb0 Summary:\n",
      "   Total: 12,338,190.00\n",
      "   Average: 4,937.50\n",
      "   Count: 2500\n",
      "\n",
      "\ud83d\udcca By Product (Top 10):\n",
      "   01. Cloud Services: 0.00\n",
      "   02. Hardware: 0.00\n",
      "   03. Professional Services: 0.00\n",
      "   04. Software Licenses: 0.00\n",
      "\n",
      "\u2705 Exercise 2.5 complete!\n",
      "\n",
      "\ud83d\udca1 Try changing 'group_by_column' and 'metric_column' to explore different insights:\n",
      "   Available columns: Region, Product, Date, TotalSales, Quantity, CustomerID\n",
      "   - group_by_column: 'Region', 'Product', 'CustomerID'\n",
      "   - metric_column: 'TotalSales', 'Quantity'\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.5: Dynamic Column Analysis\n",
    "print(\"\ud83d\udd04 Dynamic MCP Analysis with User-Defined Columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from notebook_mcp_helpers import MCPClient, MCPError\n",
    "\n",
    "try:\n",
    "    # --- Define columns for analysis ---\n",
    "    # These variables can be changed to analyze different aspects of the data\n",
    "    group_by_column = 'Product'  # Change to 'Region', 'Product', 'CustomerID', etc.\n",
    "    metric_column = 'Quantity'   # Change to 'Quantity', 'TotalSales', etc.\n",
    "\n",
    "    # Use the file key from the successful sales analysis in Exercise 2.1 (Cell 79)\n",
    "    if 'excel_cache_key' not in locals() or not excel_cache_key:\n",
    "        raise RuntimeError(\"Sales data not loaded. Please run Cell 79 successfully first.\")\n",
    "\n",
    "    file_to_analyze = excel_cache_key\n",
    "\n",
    "    print(f\"\ud83d\udcca Performing dynamic analysis on '{file_to_analyze}'\")\n",
    "    print(f\"   Grouping by: '{group_by_column}'\")\n",
    "    print(f\"   Aggregating metric: '{metric_column}'\")\n",
    "\n",
    "    # Initialize MCP client\n",
    "    mcp = MCPClient()\n",
    "    \n",
    "    # Call the MCP tool with the dynamic column names - robust fallback\n",
    "    print(f\"\\n\ud83d\udcca Running analysis via MCP...\")\n",
    "    dynamic_analysis_result = None\n",
    "    analyze_attempts = [file_to_analyze]\n",
    "    if not file_to_analyze.startswith('/app/'):\n",
    "        analyze_attempts.append(f\"/app/data/{file_to_analyze}\")  # fallback if server persisted file\n",
    "    \n",
    "    last_error = None\n",
    "    for target in analyze_attempts:\n",
    "        try:\n",
    "            dynamic_analysis_result = mcp.excel.analyze_sales(\n",
    "                target,\n",
    "                group_by=group_by_column,\n",
    "                metric=metric_column\n",
    "            )\n",
    "            print(f\"\u2705 analyze_sales succeeded using identifier: {target}\")\n",
    "            break\n",
    "        except Exception as ae:\n",
    "            last_error = ae\n",
    "            print(f\"   analyze_sales failed for {target}: {ae}\")\n",
    "    \n",
    "    if dynamic_analysis_result is None:\n",
    "        raise RuntimeError(f\"Failed to analyze using any identifier. Last error: {last_error}\")\n",
    "\n",
    "    # Normalize JSON response\n",
    "    if isinstance(dynamic_analysis_result, str):\n",
    "        import json as _json\n",
    "        try:\n",
    "            dynamic_analysis_result = _json.loads(dynamic_analysis_result)\n",
    "        except Exception:\n",
    "            dynamic_analysis_result = {\"raw\": dynamic_analysis_result}\n",
    "\n",
    "    print(f\"\\n\u2705 Dynamic analysis complete!\")\n",
    "\n",
    "    # Display results (handle different response formats)\n",
    "    if isinstance(dynamic_analysis_result, dict):\n",
    "        if 'summary' in dynamic_analysis_result:\n",
    "            print(f\"\\n\ud83d\udcb0 Summary:\")\n",
    "            total = dynamic_analysis_result['summary'].get('total', 0)\n",
    "            average = dynamic_analysis_result['summary'].get('average', 0)\n",
    "            count = dynamic_analysis_result['summary'].get('count', 0)\n",
    "            print(f\"   Total: {total:,.2f}\")\n",
    "            print(f\"   Average: {average:,.2f}\")\n",
    "            print(f\"   Count: {count}\")\n",
    "        \n",
    "        # Extract grouped data with dynamic key detection\n",
    "        grouped = dynamic_analysis_result.get('analysis') or dynamic_analysis_result.get('grouped_data') or dynamic_analysis_result.get('groups')\n",
    "        if grouped and isinstance(grouped, list) and len(grouped) > 0:\n",
    "            print(f\"\\n\ud83d\udcca By {group_by_column} (Top 10):\")\n",
    "            for i, item in enumerate(grouped[:10], 1):\n",
    "                group = item.get(group_by_column, 'Unknown')\n",
    "                value = item.get(metric_column, 0)\n",
    "                print(f\"   {i:02d}. {group}: {value:,.2f}\" if isinstance(value, (int, float)) else f\"   {i:02d}. {group}: {value}\")\n",
    "    else:\n",
    "        # Handle string response from MCP\n",
    "        print(f\"\\n{dynamic_analysis_result}\")\n",
    "\n",
    "    print(f\"\\n\u2705 Exercise 2.5 complete!\")\n",
    "    print(f\"\\n\ud83d\udca1 Try changing 'group_by_column' and 'metric_column' to explore different insights:\")\n",
    "    print(f\"   Available columns: Region, Product, Date, TotalSales, Quantity, CustomerID\")\n",
    "    print(f\"   - group_by_column: 'Region', 'Product', 'CustomerID'\")\n",
    "    print(f\"   - metric_column: 'TotalSales', 'Quantity'\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"\u274c Runtime error: {e}\")\n",
    "    print(f\"   Make sure Cell 79 (Sales Analysis) ran successfully first\")\n",
    "except MCPError as e:\n",
    "    print(f\"\u274c MCP error: {e}\")\n",
    "    print(f\"   Troubleshooting:\")\n",
    "    print(f\"   \u2022 Ensure MCP Excel server is running\")\n",
    "    print(f\"   \u2022 Verify file cache key is valid\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error during dynamic analysis: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-4\"></a>\n",
    "\n",
    "## 2.4 Exercise: Function Calling with MCP Tools\n",
    "\n",
    "Demonstrates calling MCP server tools from Azure OpenAI function calls, with both OpenAI and MCP managed through APIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "[WARN] pywintypes still not found after installation: No module named 'pywintypes'\n",
      "[PATCH] Added MCP protocol v1.0 to supported versions: ['2024-11-05', '2025-03-26', '2025-06-18', '1.0']\n",
      "[CONFIG] Using MCP URL: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n",
      "================================================================================\n",
      "Connecting to MCP server: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n",
      "[OK] Handshake succeeded. 4 tools available.\n",
      "[DEBUG] Variable values:\n",
      "  apim_gateway_url: 'https://apim-c7uj6vzppah74.azure-api.net'\n",
      "  apim_resource_gateway_url: 'https://apim-c7uj6vzppah74.azure-api.net'\n",
      "  api_key: 8c12a93f8d...3b91\n",
      "  inference_api_path: 'inference'\n",
      "  Full endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\n",
      "Query: List available document-related tools and summarize their purpose.\n",
      "[INFO] No tool calls needed. Response: Here are the available document-related tools and their purposes:\n",
      "\n",
      "1. **list_documents**:\n",
      "   - Purpose: Lists all available markdown documents. It can filter files based on an optional glob pattern (e.g., '*.md', 'azure-*').\n",
      "\n",
      "2. **search_documents**:\n",
      "   - Purpose: Searches for documents containing specific keywords or phrases. You can specify whether the search should be case-sensitive.\n",
      "\n",
      "3. **get_document_content**:\n",
      "   - Purpose: Retrieves the full content of a specified document file by its name.\n",
      "\n",
      "4. **compare_documents**:\n",
      "   - Purpose: Compares multiple documents and identifies common themes among them. You provide a list of document file names to compare.\n",
      "\n",
      "These tools facilitate the management and retrieval of information from markdown documents, enabling tasks like listing, searching, viewing content, and comparing documents.\n",
      "\n",
      "================================================================================\n",
      "Connecting to MCP server: http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp\n",
      "[OK] Handshake succeeded. 4 tools available.\n",
      "[DEBUG] Variable values:\n",
      "  apim_gateway_url: 'https://apim-c7uj6vzppah74.azure-api.net'\n",
      "  apim_resource_gateway_url: 'https://apim-c7uj6vzppah74.azure-api.net'\n",
      "  api_key: 8c12a93f8d...3b91\n",
      "  inference_api_path: 'inference'\n",
      "  Full endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\n",
      "Query: Retrieve docs for MCP server publishing and give key steps.\n",
      "\n",
      "Executing MCP tools...\n",
      "  Tool: search_documents({'query': 'MCP server publishing'})\n",
      "\n",
      "Getting final answer...\n",
      "\n",
      "[ANSWER]\n",
      "I don't have direct access to documents at this time, but I can give you an overview of the key steps involved in MCP (Microsoft Cloud Platform) server publishing based on general knowledge. Here are the typical steps:\n",
      "\n",
      "### Key Steps for MCP Server Publishing\n",
      "\n",
      "1. **Prerequisites**:\n",
      "   - Ensure you have the necessary permissions and roles assigned in your Microsoft Azure account.\n",
      "   - Confirm that the environment (development, staging, production) is set up correctly.\n",
      "\n",
      "2. **Prepare the Application**:\n",
      "   - Package your application code and dependencies.\n",
      "   - Ensure that the application is configured correctly for the cloud environment (connection strings, environment variables, etc.).\n",
      "\n",
      "3. **Choose a Publishing Method**:\n",
      "   - Determine which method you will use for publishing (Azure Portal, Azure CLI, Visual Studio, etc.).\n",
      "\n",
      "4. **Configure the MCP Service**:\n",
      "   - Set up the target services such as Azure App Service, Azure Functions, or other cloud services you intend to use.\n",
      "   - Define resource settings like scaling, networking, and security.\n",
      "\n",
      "5. **Publish the Application**:\n",
      "   - Use your chosen method to deploy the application to the MCP. This could involve running specific commands in Azure CLI or pushing code via a CI/CD pipeline.\n",
      "   - Monitor the deployment process for any errors or issues.\n",
      "\n",
      "6. **Testing**:\n",
      "   - After publishing, verify that the application is running as expected.\n",
      "   - Conduct functional and performance testing to ensure all features work properly.\n",
      "\n",
      "7. **Monitor and Optimize**:\n",
      "   - Use Azure Monitor, Application Insights, or other monitoring tools to track performance and diagnose issues.\n",
      "   - Based on insights gathered, optimize resource allocation, performance, and cost.\n",
      "\n",
      "8. **Maintain and Update**:\n",
      "   - Regularly update the application with new features or security patches.\n",
      "   - Set up a regular backup and disaster recovery plan.\n",
      "\n",
      "If you need specific documentation links or steps tailored to your environment, I recommend checking the official Microsoft Azure documentation or relevant technical resources.\n",
      "\n",
      "[OK] MCP Function Calling Complete!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.4 & 2.5: Function Calling with MCP Tools (FIXED 2025-11-17)\n",
    "# Architecture: MCP connects directly to server, OpenAI goes through APIM\n",
    "# FIXES:\n",
    "# 1. Correct streamablehttp_client unpacking: (read, write, _) instead of returned[0], returned[1]\n",
    "# 2. Simplified error handling\n",
    "# 3. Removed duplicate handshake logic\n",
    "\n",
    "# Dependency fix for ModuleNotFoundError: No module named 'pywintypes'\n",
    "# pywintypes is provided by the pywin32 package on Windows.\n",
    "%pip install pywin32\n",
    "\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from mcp import ClientSession, McpError\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.client import session as mcp_client_session\n",
    "from openai import AzureOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Verify pywintypes is now available (indirect dependencies may require it)\n",
    "try:\n",
    "    import pywintypes  # noqa: F401\n",
    "    print(\"[INIT] pywintypes module available.\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"[WARN] pywintypes still not found after installation: {e}\")\n",
    "\n",
    "# CRITICAL FIX: Server uses MCP protocol v1.0; patch client to accept it\n",
    "if \"1.0\" not in mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS:\n",
    "    mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS = list(mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS) + [\"1.0\"]\n",
    "    print(f\"[PATCH] Added MCP protocol v1.0 to supported versions: {mcp_client_session.SUPPORTED_PROTOCOL_VERSIONS}\")\n",
    "\n",
    "# Use the working Docs MCP server\n",
    "DOCS_MCP_URL = 'http://docs-mcp-24774.eastus.azurecontainer.io:8000/mcp'\n",
    "print(f\"[CONFIG] Using MCP URL: {DOCS_MCP_URL}\")\n",
    "\n",
    "# --- Diagnostic helpers ---\n",
    "def _format_exception(e: BaseException, indent=0) -> str:\n",
    "    \"\"\"Recursively format an exception and its causes, including ExceptionGroups.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    lines = [f\"{prefix}{type(e).__name__}: {str(e).splitlines()[0] if str(e) else 'No message'}\"]\n",
    "\n",
    "    if isinstance(e, ExceptionGroup):\n",
    "        lines.append(f\"{prefix}  +-- Sub-exceptions ({len(e.exceptions)}):\")\n",
    "        for i, sub_exc in enumerate(e.exceptions):\n",
    "            lines.append(f\"{prefix}      |\")\n",
    "            lines.append(f\"{prefix}      +-- Exception {i+1}/{len(e.exceptions)}:\")\n",
    "            lines.append(_format_exception(sub_exc, indent + 4))\n",
    "\n",
    "    cause = getattr(e, '__cause__', None)\n",
    "    if cause:\n",
    "        lines.append(f\"{prefix}  +-- Caused by:\")\n",
    "        lines.append(_format_exception(cause, indent + 2))\n",
    "\n",
    "    context = getattr(e, '__context__', None)\n",
    "    if context and context is not cause:\n",
    "        lines.append(f\"{prefix}  +-- During handling, another exception occurred:\")\n",
    "        lines.append(_format_exception(context, indent + 2))\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "async def call_tool(mcp_session, function_name, function_args):\n",
    "    \"\"\"Call an MCP tool safely and stringify result.\"\"\"\n",
    "    try:\n",
    "        func_response = await mcp_session.call_tool(function_name, function_args)\n",
    "        return str(func_response.content)\n",
    "    except Exception as exc:\n",
    "        return json.dumps({'error': str(exc), 'type': type(exc).__name__})\n",
    "\n",
    "async def run_completion_with_tools(server_url, prompt):\n",
    "    \"\"\"Run Azure OpenAI completion with MCP tools with extra diagnostics.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Connecting to MCP server: {server_url}\")\n",
    "\n",
    "    try:\n",
    "        # FIXED: Correct unpacking of streamablehttp_client return value\n",
    "        async with streamablehttp_client(server_url) as (read_stream, write_stream, _):\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                # Initialize session\n",
    "                await session.initialize()\n",
    "\n",
    "                # Get available tools\n",
    "                tools_response = await session.list_tools()\n",
    "                tools = tools_response.tools\n",
    "\n",
    "                print(f\"[OK] Handshake succeeded. {len(tools)} tools available.\")\n",
    "\n",
    "                # Convert MCP tools to OpenAI format\n",
    "                openai_tools = [{\n",
    "                    'type': 'function',\n",
    "                    'function': {\n",
    "                        'name': t.name,\n",
    "                        'description': t.description,\n",
    "                        'parameters': t.inputSchema\n",
    "                    }\n",
    "                } for t in tools]\n",
    "\n",
    "\n",
    "                # Load APIM variables from environment (in case cell 23 wasn't run)\n",
    "                from pathlib import Path\n",
    "                from dotenv import load_dotenv\n",
    "                import os\n",
    "                from pathlib import Path\n",
    "                from dotenv import load_dotenv\n",
    "\n",
    "                # Auto-load master-lab.env if variables not set (kernel restart resilience)\n",
    "                if not os.environ.get(\"APIM_GATEWAY_URL\"):\n",
    "                    print(\"[INFO] APIM_GATEWAY_URL not in environment, loading master-lab.env...\")\n",
    "                    env_file = Path(\"master-lab.env\")\n",
    "                    if env_file.exists():\n",
    "                        load_dotenv(str(env_file), override=True)\n",
    "                        print(f\"[OK] Loaded {env_file.absolute()}\")\n",
    "                    else:\n",
    "                        print(f\"[ERROR] master-lab.env not found at {env_file.absolute()}\")\n",
    "                        print(\"       Please run Cell 021 to generate it, or Cell 023 to load it.\")\n",
    "                apim_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "                apim_resource_gateway_url = os.environ.get('APIM_GATEWAY_URL', '')\n",
    "                api_key = os.environ.get('APIM_API_KEY', '')\n",
    "                inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "                inference_api_version = '2024-08-01-preview'\n",
    "\n",
    "                # DEBUG: Show loaded values\n",
    "                print(f\"[DEBUG] Variable values:\")\n",
    "                print(f\"  apim_gateway_url: {apim_gateway_url!r}\")\n",
    "                print(f\"  apim_resource_gateway_url: {apim_resource_gateway_url!r}\")\n",
    "                print(f\"  api_key: {api_key[:10] if api_key else None}...{api_key[-4:] if api_key else None}\")\n",
    "                print(f\"  inference_api_path: {inference_api_path!r}\")\n",
    "                print(f\"  Full endpoint: {apim_resource_gateway_url}/{inference_api_path}\")\n",
    "\n",
    "                # Validate required variables\n",
    "                if not apim_resource_gateway_url:\n",
    "                    raise ValueError('APIM_GATEWAY_URL not set. Run cell 23 to load environment variables.')\n",
    "                if not api_key:\n",
    "                    raise ValueError('APIM_API_KEY not set. Run cell 23 to load environment variables.')\n",
    "\n",
    "                # Initialize OpenAI client (using variables from earlier cells)\n",
    "                client = AzureOpenAI(\n",
    "                    azure_endpoint=f'{apim_resource_gateway_url}/{inference_api_path}',\n",
    "                    api_key=api_key,\n",
    "                    api_version=inference_api_version,\n",
    "                )\n",
    "\n",
    "                messages = [{'role': 'user', 'content': prompt}]\n",
    "                print(f'\\nQuery: {prompt}')\n",
    "\n",
    "                # First completion - get tool calls\n",
    "                response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',  # Use a known deployed model\n",
    "                    messages=messages,\n",
    "                    tools=openai_tools\n",
    "                )\n",
    "\n",
    "                response_message = response.choices[0].message\n",
    "                tool_calls = getattr(response_message, 'tool_calls', None)\n",
    "\n",
    "                if not tool_calls:\n",
    "                    print(f'[INFO] No tool calls needed. Response: {response_message.content}')\n",
    "                    return\n",
    "\n",
    "                # Add assistant message to history\n",
    "                messages.append(response_message)\n",
    "\n",
    "                # Execute tool calls\n",
    "                print('\\nExecuting MCP tools...')\n",
    "                for tool_call in tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads((tool_call.function.arguments or '{}').lstrip('\\ufeff'))\n",
    "                    print(f'  Tool: {function_name}({function_args})')\n",
    "\n",
    "                    # Call MCP tool\n",
    "                    function_response = await call_tool(session, function_name, function_args)\n",
    "\n",
    "                    # Add tool response to messages\n",
    "                    messages.append({\n",
    "                        'tool_call_id': tool_call.id,\n",
    "                        'role': 'tool',\n",
    "                        'name': function_name,\n",
    "                        'content': function_response\n",
    "                    })\n",
    "\n",
    "                # Get final answer with tool results\n",
    "                print('\\nGetting final answer...')\n",
    "                second_response = client.chat.completions.create(\n",
    "                    model='gpt-4o-mini',\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "                print('\\n[ANSWER]')\n",
    "                print(second_response.choices[0].message.content)\n",
    "\n",
    "    except Exception as exc:\n",
    "        print('[ERROR] Unexpected failure during tool run.')\n",
    "        print(_format_exception(exc))\n",
    "        print(\"\\n[TROUBLESHOOTING]\")\n",
    "        print(\"  \u2022 Verify MCP server is running and accessible\")\n",
    "        print(\"  \u2022 Check URL is correct (should end with /mcp)\")\n",
    "        print(\"  \u2022 Ensure network connectivity (firewall, proxy)\")\n",
    "        print(\"  \u2022 Verify protocol version compatibility\")\n",
    "\n",
    "# Example usage (Exercise 2.4 & 2.5)\n",
    "async def run_agent_example():\n",
    "    queries = [\n",
    "        'List available document-related tools and summarize their purpose.',\n",
    "        'Retrieve docs for MCP server publishing and give key steps.'\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        await run_completion_with_tools(DOCS_MCP_URL, q)\n",
    "        print()\n",
    "\n",
    "# Run the example\n",
    "await run_agent_example()\n",
    "\n",
    "print(\"[OK] MCP Function Calling Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-4-1\"></a>\n",
    "\n",
    "### 2.4.1 Weather API via APIM\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates accessing OpenWeather API through APIM as an MCP-style data source\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- APIM deployed with Weather API configured\n",
    "- OpenWeather API key configured\n",
    "- Environment variables: `APIM_WEATHER_URL`, `OPENWEATHER_API_KEY`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Accesses weather data through APIM gateway:**\n",
    "\n",
    "1. **Weather API Features:**\n",
    "   - Current weather for any city\n",
    "   - Multi-day forecasts\n",
    "   - Weather alerts\n",
    "   - Historical weather data\n",
    "\n",
    "2. **APIM Benefits:**\n",
    "   - Hides actual API key from clients\n",
    "   - Applies rate limiting\n",
    "   - Caches weather responses\n",
    "   - Transforms response format\n",
    "   - Logs all weather queries\n",
    "\n",
    "3. **Example Queries:**\n",
    "   - Current weather in Seattle\n",
    "   - 5-day forecast for Paris\n",
    "   - Weather alerts for region\n",
    "\n",
    "4. **Integration with AI:**\n",
    "   - AI can call weather API via function calling\n",
    "   - Natural language: \"What's the weather in Tokyo?\"\n",
    "   - AI extracts city name and calls API\n",
    "   - Returns formatted weather info\n",
    "\n",
    "This demonstrates using APIM as a unified gateway for both AI models and traditional APIs.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Weather API test results:\n",
    "\ud83d\udccd **Seattle, WA**\n",
    "\ud83c\udf21\ufe0f Temperature: 45\u00b0F (7\u00b0C)\n",
    "\u2601\ufe0f Conditions: Partly Cloudy\n",
    "\ud83d\udca8 Wind: 8 mph NW\n",
    "\ud83d\udca7 Humidity: 72%\n",
    "\n",
    "API call successful via APIM gateway\n",
    "Response time: 245ms (cached)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell_80_5c80f06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "WEATHER API EXAMPLE (via APIM)\n",
      "================================================================================\n",
      "\n",
      "1\ufe0f\u20e3  CURRENT WEATHER - London\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u274c Error accessing Weather API: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_32043/1743963199.py\", line 25, in <module>\n",
      "    weather = mcp.weather.get_weather(\"London\", \"GB\")\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 749, in get_weather\n",
      "    return self._get('weather', params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 715, in _get\n",
      "    return response.json()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/httpx/_models.py\", line 832, in json\n",
      "    return jsonlib.loads(self.content, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/json/decoder.py\", line 338, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/json/decoder.py\", line 356, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# Lab Example: Weather API (via APIM)\n",
    "\"\"\"\n",
    "Demonstrates Weather API access through Azure API Management.\n",
    "\n",
    "Features:\n",
    "- Current weather for a city\n",
    "- Multi-city comparison\n",
    "- 5-day forecast\n",
    "- Temperature, conditions, humidity\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WEATHER API EXAMPLE (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.weather:\n",
    "    print(\"\u274c Weather API not configured\")\n",
    "    print(\"   Set APIM_WEATHER_URL and OPENWEATHER_API_KEY in .mcp-servers-config\")\n",
    "else:\n",
    "    print(\"\\n1\ufe0f\u20e3  CURRENT WEATHER - London\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get weather for London\n",
    "        weather = mcp.weather.get_weather(\"London\", \"GB\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udccd Location: {weather['name']}, {weather['sys']['country']}\")\n",
    "        print(f\"\ud83c\udf21\ufe0f  Temperature: {weather['main']['temp']}\u00b0C (feels like {weather['main']['feels_like']}\u00b0C)\")\n",
    "        print(f\"\u2601\ufe0f  Conditions: {weather['weather'][0]['description'].title()}\")\n",
    "        print(f\"\ud83d\udca8 Wind: {weather['wind']['speed']} m/s\")\n",
    "        print(f\"\ud83d\udca7 Humidity: {weather['main']['humidity']}%\")\n",
    "        print(f\"\ud83d\udd3d Pressure: {weather['main']['pressure']} hPa\")\n",
    "        \n",
    "        print(\"\\n\\n2\ufe0f\u20e3  MULTI-CITY COMPARISON\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        cities = [\n",
    "            (\"Paris\", \"FR\"),\n",
    "            (\"New York\", \"US\"),\n",
    "            (\"Tokyo\", \"JP\"),\n",
    "            (\"Sydney\", \"AU\")\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'City':<15} {'Temp (\u00b0C)':<12} {'Conditions':<20} {'Humidity':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for city, country in cities:\n",
    "            try:\n",
    "                w = mcp.weather.get_weather(city, country)\n",
    "                temp = w['main']['temp']\n",
    "                condition = w['weather'][0]['description'].title()\n",
    "                humidity = w['main']['humidity']\n",
    "                print(f\"{city:<15} {temp:<12.1f} {condition:<20} {humidity}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"{city:<15} Error: {str(e)[:40]}\")\n",
    "        \n",
    "        print(\"\\n\\n3\ufe0f\u20e3  5-DAY FORECAST - London\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            forecast = mcp.weather.get_forecast(\"London\", \"GB\")\n",
    "            \n",
    "            # Group by day\n",
    "            from datetime import datetime\n",
    "            daily_forecasts = {}\n",
    "            \n",
    "            for item in forecast['list'][:8]:  # Next 24 hours (8 x 3-hour periods)\n",
    "                dt = datetime.fromtimestamp(item['dt'])\n",
    "                day = dt.strftime('%Y-%m-%d')\n",
    "                time = dt.strftime('%H:%M')\n",
    "                \n",
    "                if day not in daily_forecasts:\n",
    "                    daily_forecasts[day] = []\n",
    "                \n",
    "                daily_forecasts[day].append({\n",
    "                    'time': time,\n",
    "                    'temp': item['main']['temp'],\n",
    "                    'condition': item['weather'][0]['description']\n",
    "                })\n",
    "            \n",
    "            for day, forecasts in list(daily_forecasts.items())[:2]:\n",
    "                print(f\"\\n\ud83d\udcc5 {day}\")\n",
    "                for f in forecasts:\n",
    "                    print(f\"   {f['time']}: {f['temp']:.1f}\u00b0C - {f['condition'].title()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f  Forecast error: {e}\")\n",
    "        \n",
    "        print(\"\\n\\n\u2705 Weather API examples completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error accessing Weather API: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-4-2\"></a>\n",
    "\n",
    "### 2.4.2 GitHub API via APIM\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates accessing GitHub REST API through APIM for code repository analysis\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- APIM deployed with GitHub API configured\n",
    "- GitHub personal access token (optional, for higher rate limits)\n",
    "- Environment variables: `APIM_GITHUB_URL`, `APIM_SUBSCRIPTION_KEY`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Accesses GitHub data through APIM gateway:**\n",
    "\n",
    "1. **GitHub API Features:**\n",
    "   - Repository information\n",
    "   - Repository statistics (stars, forks)\n",
    "   - Commit history\n",
    "   - Pull request data\n",
    "   - Issue tracking\n",
    "   - Code search\n",
    "\n",
    "2. **APIM Value-Add:**\n",
    "   - Hides GitHub token from clients\n",
    "   - Enforces rate limits across organization\n",
    "   - Caches repository metadata\n",
    "   - Logs all GitHub API usage\n",
    "   - Transforms responses to standard format\n",
    "\n",
    "3. **Example Queries:**\n",
    "   - Get repository details: `GET /repos/{owner}/{repo}`\n",
    "   - Search repositories: `GET /search/repositories?q=...`\n",
    "   - Get commit history: `GET /repos/{owner}/{repo}/commits`\n",
    "\n",
    "4. **AI Integration:**\n",
    "   - AI analyzes repositories via function calling\n",
    "   - Natural language: \"What's the latest commit in repo X?\"\n",
    "   - AI extracts repo info and calls GitHub API\n",
    "   - Returns formatted analysis\n",
    "\n",
    "5. **Use Cases:**\n",
    "   - Developer productivity tools\n",
    "   - Code analysis and metrics\n",
    "   - Repository recommendations\n",
    "   - Automated code reviews\n",
    "   - Project management automation\n",
    "\n",
    "**Multi-Source AI:**\n",
    "Combine with other data sources (Weather, Excel, Docs) for comprehensive AI applications.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "GitHub API test results:\n",
    "\n",
    "\ud83d\udce6 **Repository: microsoft/azure-docs**\n",
    "\u2b50 Stars: 8,543\n",
    "\ud83c\udf74 Forks: 12,234\n",
    "\ud83d\udcdd Description: Azure documentation repository\n",
    "\ud83d\udd04 Last Updated: 2 hours ago\n",
    "\ud83d\udcbb Language: Markdown\n",
    "\ud83d\udcca Size: 245 MB\n",
    "\n",
    "Latest Commit:\n",
    "- Author: docbot\n",
    "- Message: \"Update API Management documentation\"\n",
    "- Date: 2025-01-15 10:30:00\n",
    "\n",
    "API call successful via APIM gateway\n",
    "Response time: 189ms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell_81_dabe2f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GITHUB API EXAMPLE (via APIM)\n",
      "================================================================================\n",
      "\n",
      "1\ufe0f\u20e3  REPOSITORY DETAILS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udd0d Fetching: https://github.com/Azure-Samples/AI-Gateway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udce6 Repository: Azure-Samples/AI-Gateway\n",
      "\ud83d\udcdd Description: APIM \u2764\ufe0f AI - This repo contains experiments on Azure API Management's AI capabilities, integrating with Azure OpenAI, AI Foundry, and much more \ud83d\ude80 . New workshop experience at https://aka.ms/ai-gateway/workshop\n",
      "\ud83c\udf10 URL: https://github.com/Azure-Samples/AI-Gateway\n",
      "\u2b50 Stars: 810\n",
      "\ud83d\udd31 Forks: 350\n",
      "\ud83d\udc40 Watchers: 810\n",
      "\ud83d\udc1b Open Issues: 41\n",
      "\ud83d\udcd6 Language: Jupyter Notebook\n",
      "\ud83d\udcc5 Created: 2024-04-03\n",
      "\ud83d\udd04 Last Updated: 2025-11-29\n",
      "\ud83c\udff7\ufe0f  Topics: agents, apimanagement, autogen, azure, foundry\n",
      "\n",
      "\n",
      "2\ufe0f\u20e3  RECENT COMMITS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Date         Author               Message                                           \n",
      "-------------------------------------------------------------------------------------\n",
      "2025-11-29   Nour Shaker          Update devcontainer.json                          \n",
      "2025-11-25   Nour Shaker          Merge pull request #237 from Azure-Samples/realt  \n",
      "2025-11-24   Nour Shaker          Updating Realtime labs to the latest MCP framewo  \n",
      "2025-11-10   Alex Vieira          Updated Test AI Gateway Tool (#234)               \n",
      "2025-11-10   Andrei Kamenev       added script to delete AI Gateway from Foundry r  \n",
      "\n",
      "\n",
      "3\ufe0f\u20e3  REPOSITORY STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udcca Age: 607 days\n",
      "\ud83d\udcc8 Stars per day: 1.33\n",
      "\ud83d\udd25 Fork ratio: 43.21%\n",
      "\ud83d\udcdd Size: 77,771 KB\n",
      "\u2696\ufe0f  License: MIT License\n",
      "\n",
      "\u2705 GitHub API examples completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 Example: GitHub API (via APIM)\n",
    "\"\"\"\n",
    "Demonstrates GitHub REST API access through Azure API Management.\n",
    "\n",
    "Features:\n",
    "- Repository details\n",
    "- Statistics (stars, forks, watchers)\n",
    "- Recent activity\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB API EXAMPLE (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"\u274c GitHub API not configured\")\n",
    "    print(\"   Set APIM_GITHUB_URL and APIM_SUBSCRIPTION_KEY in .mcp-servers-config\")\n",
    "else:\n",
    "    print(\"\\n1\ufe0f\u20e3  REPOSITORY DETAILS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get details for https://github.com/Azure-Samples/AI-Gateway\n",
    "        owner = \"Azure-Samples\"\n",
    "        repo = \"AI-Gateway\"\n",
    "\n",
    "        # Build custom base URL with requested scheme prefix\n",
    "        display_url = f\"https://github.com/{owner}/{repo}\"\n",
    "        print(f\"\\n\ud83d\udd0d Fetching: {display_url}\")\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udce6 Repository: {repo_data['full_name']}\")\n",
    "        print(f\"\ud83d\udcdd Description: {repo_data.get('description', 'N/A')}\")\n",
    "        print(f\"\ud83c\udf10 URL: {repo_data['html_url']}\")\n",
    "        print(f\"\u2b50 Stars: {repo_data['stargazers_count']:,}\")\n",
    "        print(f\"\ud83d\udd31 Forks: {repo_data['forks_count']:,}\")\n",
    "        print(f\"\ud83d\udc40 Watchers: {repo_data['watchers_count']:,}\")\n",
    "        print(f\"\ud83d\udc1b Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "        print(f\"\ud83d\udcd6 Language: {repo_data.get('language', 'N/A')}\")\n",
    "        print(f\"\ud83d\udcc5 Created: {repo_data['created_at'][:10]}\")\n",
    "        print(f\"\ud83d\udd04 Last Updated: {repo_data['updated_at'][:10]}\")\n",
    "        \n",
    "        if repo_data.get('topics'):\n",
    "            print(f\"\ud83c\udff7\ufe0f  Topics: {', '.join(repo_data['topics'][:5])}\")\n",
    "        \n",
    "        print(\"\\n\\n2\ufe0f\u20e3  RECENT COMMITS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            commits = mcp.github.get_commits(owner, repo, per_page=5)\n",
    "            \n",
    "            print(f\"\\n{'Date':<12} {'Author':<20} {'Message':<50}\")\n",
    "            print(\"-\" * 85)\n",
    "            \n",
    "            for commit in commits[:5]:\n",
    "                commit_data = commit.get('commit', {})\n",
    "                author = commit_data.get('author', {}).get('name', 'Unknown')[:18]\n",
    "                message = commit_data.get('message', '').split('\\n')[0][:48]\n",
    "                date = commit_data.get('author', {}).get('date', '')[:10]\n",
    "                \n",
    "                print(f\"{date:<12} {author:<20} {message:<50}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f  Could not fetch commits: {e}\")\n",
    "        \n",
    "        print(\"\\n\\n3\ufe0f\u20e3  REPOSITORY STATISTICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Calculate some basic stats\n",
    "        days_old = (\n",
    "            __import__('datetime').datetime.now() - \n",
    "            __import__('datetime').datetime.strptime(repo_data['created_at'][:10], '%Y-%m-%d')\n",
    "        ).days\n",
    "        \n",
    "        stars_per_day = repo_data['stargazers_count'] / max(days_old, 1)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Age: {days_old:,} days\")\n",
    "        print(f\"\ud83d\udcc8 Stars per day: {stars_per_day:.2f}\")\n",
    "        print(f\"\ud83d\udd25 Fork ratio: {repo_data['forks_count'] / max(repo_data['stargazers_count'], 1):.2%}\")\n",
    "        print(f\"\ud83d\udcdd Size: {repo_data.get('size', 0):,} KB\")\n",
    "        \n",
    "        if repo_data.get('license'):\n",
    "            print(f\"\u2696\ufe0f  License: {repo_data['license'].get('name', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n\u2705 GitHub API examples completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error accessing GitHub API: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_89_c5e4eb3d",
   "metadata": {},
   "source": [
    "<a id=\"lab2-5\"></a>\n",
    "\n",
    "## 2.5 GitHub Repository Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Integrate GitHub repository access through MCP (Model Context Protocol) servers to enable Azure OpenAI to read and analyze repository content. This lab demonstrates how to query GitHub repositories, list files, and retrieve content programmatically through APIM.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **MCP Server Integration:** Connect to GitHub API via MCP protocol\n",
    "- **Repository Navigation:** Browse files and directory structures\n",
    "- **Content Retrieval:** Fetch file contents for analysis\n",
    "- **HTTP-Based MCP:** Use REST API to communicate with MCP servers\n",
    "- **APIM Routing:** Route MCP requests through APIM gateway\n",
    "- **Authentication:** Manage GitHub API credentials securely\n",
    "- **Data Processing:** Feed repository data to Azure OpenAI\n",
    "\n",
    "#### How It Works\n",
    "1. Azure OpenAI needs to access GitHub repository data\n",
    "2. Function call requests GitHub repository MCP server via APIM\n",
    "3. MCP server receives request through `/mcp/` HTTP endpoint\n",
    "4. Server authenticates with GitHub API using configured credentials\n",
    "5. Server executes tool (list files, read content, etc.)\n",
    "6. Response returned to APIM\n",
    "7. APIM proxies response back to Azure OpenAI\n",
    "8. Azure OpenAI processes repository data and generates analysis\n",
    "9. Final response returned to user\n",
    "\n",
    "#### Data Flow\n",
    "```\n",
    "[Azure OpenAI] \u2192 [APIM Gateway] \u2192 [GitHub MCP Server] \u2192 [GitHub API]\n",
    "     \u2193                                    \u2193\n",
    "  Response \u2190 [Tool Result] \u2190 [File Content] \u2190 [GitHub]\n",
    "```\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor permissions\n",
    "- GitHub Account with repository access\n",
    "- GitHub personal access token (for API authentication)\n",
    "- MCP servers initialized (Cell 11)\n",
    "\n",
    "#### Expected Results\n",
    "- Successfully list files in GitHub repository\n",
    "- Retrieve file contents from repository\n",
    "- Azure OpenAI can access repository structure\n",
    "- Function calls to GitHub MCP work through APIM\n",
    "- Responses properly formatted for LLM consumption\n",
    "- Can analyze repository structure and content\n",
    "- Performance metrics show latency through APIM\n",
    "\n",
    "#### MCP Server Configuration\n",
    "```\n",
    "Protocol: HTTP POST\n",
    "Endpoint: https://{mcp-server}/mcp/\n",
    "Authentication: GitHub Personal Access Token\n",
    "Supported Tools:\n",
    "  - list_repository_files\n",
    "  - read_file_content\n",
    "  - get_repository_info\n",
    "  - search_files\n",
    "```\n",
    "\n",
    "#### Example Use Cases\n",
    "1. **Code Review:** Analyze code structure and patterns\n",
    "2. **Documentation Generation:** Create docs from code\n",
    "3. **Dependency Analysis:** Understand project dependencies\n",
    "4. **Architecture Understanding:** Map codebase structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell_90_63f87343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GITHUB REPOSITORY SEARCH (via APIM)\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udd0d Search Query: machine learning language:python stars:>1000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udcca Found 141 repositories\n",
      "\ud83d\udccb Showing top 10 results:\n",
      "\n",
      "Rank   Stars    Repository                               Language    \n",
      "----------------------------------------------------------------------\n",
      "1      153,198  huggingface/transformers                 Python      \n",
      "2      77,561   fighting41love/funNLP                    Python      \n",
      "3      70,803   josephmisiti/awesome-machine-learning    Python      \n",
      "4      64,166   scikit-learn/scikit-learn                Python      \n",
      "5      40,743   gradio-app/gradio                        Python      \n",
      "6      29,536   eriklindernoren/ML-From-Scratch          Python      \n",
      "7      28,670   Ebazhanov/linkedin-skill-assessments-q   Python      \n",
      "8      20,888   RasaHQ/rasa                              Python      \n",
      "9      19,944   onnx/onnx                                Python      \n",
      "10     16,207   ddbourgin/numpy-ml                       Python      \n",
      "\n",
      "\n",
      "\ud83c\udfc6 TOP RESULT DETAILS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udce6 huggingface/transformers\n",
      "\ud83d\udcdd \ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text,\n",
      "\u2b50 Stars: 153,198\n",
      "\ud83d\udd31 Forks: 31,275\n",
      "\ud83d\udcd6 Language: Python\n",
      "\ud83d\udd04 Updated: 2025-11-30\n",
      "\ud83c\udf10 URL: https://github.com/huggingface/transformers\n",
      "\n",
      "\n",
      "\u2705 GitHub search completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# GitHub: Search and explore repositories (via APIM)\n",
    "\"\"\"\n",
    "Search GitHub repositories using various criteria:\n",
    "- Language filters\n",
    "- Star count filters\n",
    "- Sort by relevance, stars, or updated date\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB REPOSITORY SEARCH (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"\u274c GitHub API not configured\")\n",
    "else:\n",
    "    try:\n",
    "        # Search for AI/ML repositories\n",
    "        search_query = \"machine learning language:python stars:>1000\"\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd0d Search Query: {search_query}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        results = mcp.github.search_repositories(search_query, per_page=10)\n",
    "        \n",
    "        total_count = results.get('total_count', 0)\n",
    "        items = results.get('items', [])\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Found {total_count:,} repositories\")\n",
    "        print(f\"\ud83d\udccb Showing top {len(items)} results:\\n\")\n",
    "        \n",
    "        print(f\"{'Rank':<6} {'Stars':<8} {'Repository':<40} {'Language':<12}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for idx, repo in enumerate(items, 1):\n",
    "            stars = f\"{repo['stargazers_count']:,}\"\n",
    "            name = repo['full_name'][:38]\n",
    "            language = repo.get('language', 'N/A')[:10]\n",
    "            \n",
    "            print(f\"{idx:<6} {stars:<8} {name:<40} {language:<12}\")\n",
    "        \n",
    "        # Show detailed info for top repository\n",
    "        if items:\n",
    "            print(\"\\n\\n\ud83c\udfc6 TOP RESULT DETAILS\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            top_repo = items[0]\n",
    "            print(f\"\\n\ud83d\udce6 {top_repo['full_name']}\")\n",
    "            print(f\"\ud83d\udcdd {top_repo.get('description', 'No description')[:100]}\")\n",
    "            print(f\"\u2b50 Stars: {top_repo['stargazers_count']:,}\")\n",
    "            print(f\"\ud83d\udd31 Forks: {top_repo['forks_count']:,}\")\n",
    "            print(f\"\ud83d\udcd6 Language: {top_repo.get('language', 'N/A')}\")\n",
    "            print(f\"\ud83d\udd04 Updated: {top_repo['updated_at'][:10]}\")\n",
    "            print(f\"\ud83c\udf10 URL: {top_repo['html_url']}\")\n",
    "        \n",
    "        print(\"\\n\\n\u2705 GitHub search completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error searching GitHub: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_91_e0849873",
   "metadata": {},
   "source": [
    "<a id=\"lab2-6\"></a>\n",
    "\n",
    "## 2.6 GitHub + AI Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "Combine GitHub repository access with Azure OpenAI intelligence to perform advanced code analysis. This lab demonstrates how to use AI to understand code structure, identify patterns, and generate insights from repository content.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Code Analysis:** Use AI to understand code structure and patterns\n",
    "- **Multi-Step Reasoning:** Chain multiple Azure OpenAI calls for complex analysis\n",
    "- **Repository Context:** Provide full repository context to AI for accurate analysis\n",
    "- **Function Calling:** Use Azure OpenAI function calls to access repository data\n",
    "- **MCP Integration:** Seamlessly integrate MCP tools in AI workflows\n",
    "- **Semantic Kernel:** Advanced orchestration of AI and data retrieval (Phase 3)\n",
    "- **AutoGen:** Multi-agent approaches to code analysis\n",
    "\n",
    "#### What You'll Do\n",
    "1. Load entire GitHub repository structure\n",
    "2. Use Azure OpenAI to analyze code organization\n",
    "3. Identify architectural patterns and design principles\n",
    "4. Generate code documentation\n",
    "5. Suggest improvements and optimizations\n",
    "6. Create dependency graphs\n",
    "7. Perform security and best practice analysis\n",
    "\n",
    "#### How It Works\n",
    "1. Azure OpenAI receives user query about code analysis\n",
    "2. AI recognizes need for repository context\n",
    "3. Uses function calls to GitHub MCP server via APIM\n",
    "4. Retrieves repository structure, key files, and code samples\n",
    "5. Enriches prompt with retrieved context\n",
    "6. Performs detailed analysis using repository data\n",
    "7. Generates insights with code references\n",
    "8. Returns comprehensive analysis to user\n",
    "9. Optional: Uses Semantic Kernel or AutoGen for advanced multi-step analysis\n",
    "\n",
    "#### Analysis Pipeline\n",
    "```\n",
    "[User Query] \u2192 [Azure OpenAI] \u2192 [GitHub MCP] \u2192 [Repository Data]\n",
    "                   \u2193                                     \u2193\n",
    "            [Analysis Prompt] \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 [Code Context]\n",
    "                   \u2193\n",
    "         [Detailed Analysis] \u2192 [User]\n",
    "```\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor permissions\n",
    "- GitHub repository with code to analyze\n",
    "- GitHub personal access token\n",
    "- MCP servers initialized (Cell 11)\n",
    "- Optional: semantic-kernel and pyautogen libraries for Phase 3\n",
    "\n",
    "#### Expected Results\n",
    "- Azure OpenAI analyzes repository structure\n",
    "- Code patterns and architecture identified\n",
    "- Documentation generated from code\n",
    "- Function calls to GitHub execute successfully\n",
    "- Multi-step analysis works correctly\n",
    "- Comprehensive insights provided with code references\n",
    "- Performance tracking shows gateway latency\n",
    "- Semantic Kernel and AutoGen experiments show in Phase 3\n",
    "\n",
    "#### Analysis Outputs\n",
    "1. **Architecture Overview:** How code is organized and structured\n",
    "2. **Design Patterns:** Identified patterns (MVC, Factory, etc.)\n",
    "3. **Dependency Analysis:** What components depend on each other\n",
    "4. **Best Practice Assessment:** Compliance with Python/coding standards\n",
    "5. **Security Review:** Potential security issues or vulnerabilities\n",
    "6. **Documentation Gaps:** Where documentation is needed\n",
    "7. **Refactoring Suggestions:** Code improvement opportunities\n",
    "\n",
    "#### Advanced Techniques (Phase 3)\n",
    "- Semantic Kernel Plugin development for repository analysis\n",
    "- SK Streaming with function calling\n",
    "- AutoGen multi-agent conversations about code\n",
    "- Custom Azure OpenAI clients\n",
    "- Vector search of codebase with embeddings\n",
    "- Hybrid Semantic Kernel + AutoGen orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cell_92_4791fe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GITHUB REPOSITORY ANALYSIS (via APIM)\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udd0d Analyzing: microsoft/semantic-kernel\n",
      "================================================================================\n",
      "\n",
      "1\ufe0f\u20e3  REPOSITORY OVERVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udce6 microsoft/semantic-kernel\n",
      "\ud83d\udcdd Integrate cutting-edge LLM technology quickly and easily into your apps\n",
      "\u2b50 Stars: 26,762\n",
      "\ud83d\udd31 Forks: 4,371\n",
      "\ud83d\udc40 Watchers: 26,762\n",
      "\ud83d\udc1b Open Issues: 576\n",
      "\n",
      "2\ufe0f\u20e3  RECENT ACTIVITY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udcca Last 10 commits:\n",
      "   Total commits analyzed: 10\n",
      "   Unique contributors: 9\n",
      "\n",
      "   Top contributors in recent commits:\n",
      "     \u2022 Evan Mattson: 2 commit(s)\n",
      "     \u2022 Pratham Aditya Salhotra: 1 commit(s)\n",
      "     \u2022 Monita: 1 commit(s)\n",
      "     \u2022 Tao Chen: 1 commit(s)\n",
      "     \u2022 Chris: 1 commit(s)\n",
      "\n",
      "3\ufe0f\u20e3  REPOSITORY HEALTH METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udcc5 Age: 1,008 days (2.8 years)\n",
      "\ud83d\udd04 Last updated: 1 days ago\n",
      "\ud83d\udcc8 Growth: 26.55 stars/day\n",
      "\ud83d\udd31 Fork ratio: 16.33%\n",
      "\ud83c\udfaf Activity Level: \ud83d\udfe2 Very Active\n",
      "\n",
      "4\ufe0f\u20e3  COMMUNITY METRICS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udc1b Issue Metrics:\n",
      "   Total analyzed: 100\n",
      "   Open: 58\n",
      "   Closed: 42\n",
      "   Close rate: 42.0%\n",
      "\n",
      "5\ufe0f\u20e3  REPOSITORY METADATA\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\ud83d\udcd6 Primary Language: C#\n",
      "\ud83d\udccf Size: 92,215 KB\n",
      "\ud83c\udf33 Default Branch: main\n",
      "\u2696\ufe0f  License: MIT License\n",
      "\ud83c\udff7\ufe0f  Topics: ai, artificial-intelligence, llm, openai, sdk\n",
      "\n",
      "\ud83d\udd17 Clone URL: https://github.com/microsoft/semantic-kernel.git\n",
      "\ud83c\udf10 Homepage: https://aka.ms/semantic-kernel\n",
      "\n",
      "\n",
      "\u2705 GitHub repository analysis completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# GitHub: Repository analysis (via APIM)\n",
    "\"\"\"\n",
    "Perform deep analysis of a GitHub repository:\n",
    "- Contributor statistics\n",
    "- Issue tracking\n",
    "- Pull request metrics\n",
    "- Language breakdown\n",
    "- Community health\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GITHUB REPOSITORY ANALYSIS (via APIM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github:\n",
    "    print(\"\u274c GitHub API not configured\")\n",
    "else:\n",
    "    try:\n",
    "        # Analyze a popular repository\n",
    "        owner = \"microsoft\"\n",
    "        repo = \"semantic-kernel\"\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd0d Analyzing: {owner}/{repo}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get repository details\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        \n",
    "        print(\"\\n1\ufe0f\u20e3  REPOSITORY OVERVIEW\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\n\ud83d\udce6 {repo_data['full_name']}\")\n",
    "        print(f\"\ud83d\udcdd {repo_data.get('description', 'No description')}\")\n",
    "        print(f\"\u2b50 Stars: {repo_data['stargazers_count']:,}\")\n",
    "        print(f\"\ud83d\udd31 Forks: {repo_data['forks_count']:,}\")\n",
    "        print(f\"\ud83d\udc40 Watchers: {repo_data['watchers_count']:,}\")\n",
    "        print(f\"\ud83d\udc1b Open Issues: {repo_data['open_issues_count']:,}\")\n",
    "        \n",
    "        print(\"\\n2\ufe0f\u20e3  RECENT ACTIVITY\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get recent commits\n",
    "        try:\n",
    "            commits = mcp.github.get_commits(owner, repo, per_page=10)\n",
    "            \n",
    "            # Analyze commit patterns\n",
    "            authors = {}\n",
    "            for commit in commits:\n",
    "                author = commit.get('commit', {}).get('author', {}).get('name', 'Unknown')\n",
    "                authors[author] = authors.get(author, 0) + 1\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcca Last 10 commits:\")\n",
    "            print(f\"   Total commits analyzed: {len(commits)}\")\n",
    "            print(f\"   Unique contributors: {len(authors)}\")\n",
    "            print(f\"\\n   Top contributors in recent commits:\")\n",
    "            \n",
    "            for author, count in sorted(authors.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"     \u2022 {author}: {count} commit(s)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f  Could not analyze commits: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"\\n3\ufe0f\u20e3  REPOSITORY HEALTH METRICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Calculate health metrics\n",
    "        import datetime\n",
    "        \n",
    "        created = datetime.datetime.strptime(repo_data['created_at'][:10], '%Y-%m-%d')\n",
    "        updated = datetime.datetime.strptime(repo_data['updated_at'][:10], '%Y-%m-%d')\n",
    "        now = datetime.datetime.now()\n",
    "        \n",
    "        age_days = (now - created).days\n",
    "        days_since_update = (now - updated).days\n",
    "        \n",
    "        stars_per_day = repo_data['stargazers_count'] / max(age_days, 1)\n",
    "        fork_ratio = repo_data['forks_count'] / max(repo_data['stargazers_count'], 1)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcc5 Age: {age_days:,} days ({age_days/365:.1f} years)\")\n",
    "        print(f\"\ud83d\udd04 Last updated: {days_since_update} days ago\")\n",
    "        print(f\"\ud83d\udcc8 Growth: {stars_per_day:.2f} stars/day\")\n",
    "        print(f\"\ud83d\udd31 Fork ratio: {fork_ratio:.2%}\")\n",
    "        \n",
    "        # Activity level\n",
    "        if days_since_update < 7:\n",
    "            activity = \"\ud83d\udfe2 Very Active\"\n",
    "        elif days_since_update < 30:\n",
    "            activity = \"\ud83d\udfe1 Active\"\n",
    "        elif days_since_update < 90:\n",
    "            activity = \"\ud83d\udfe0 Moderate\"\n",
    "        else:\n",
    "            activity = \"\ud83d\udd34 Low Activity\"\n",
    "        \n",
    "        print(f\"\ud83c\udfaf Activity Level: {activity}\")\n",
    "        \n",
    "        print(\"\\n4\ufe0f\u20e3  COMMUNITY METRICS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get issues for community engagement\n",
    "        try:\n",
    "            issues = mcp.github.get_issues(owner, repo, state='all', per_page=100)\n",
    "            \n",
    "            open_issues = [i for i in issues if i['state'] == 'open']\n",
    "            closed_issues = [i for i in issues if i['state'] == 'closed']\n",
    "            \n",
    "            if issues:\n",
    "                close_rate = len(closed_issues) / len(issues)\n",
    "                print(f\"\\n\ud83d\udc1b Issue Metrics:\")\n",
    "                print(f\"   Total analyzed: {len(issues)}\")\n",
    "                print(f\"   Open: {len(open_issues)}\")\n",
    "                print(f\"   Closed: {len(closed_issues)}\")\n",
    "                print(f\"   Close rate: {close_rate:.1%}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u26a0\ufe0f  Could not analyze issues: {str(e)[:100]}\")\n",
    "        \n",
    "        print(\"\\n5\ufe0f\u20e3  REPOSITORY METADATA\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcd6 Primary Language: {repo_data.get('language', 'N/A')}\")\n",
    "        print(f\"\ud83d\udccf Size: {repo_data.get('size', 0):,} KB\")\n",
    "        print(f\"\ud83c\udf33 Default Branch: {repo_data.get('default_branch', 'N/A')}\")\n",
    "        \n",
    "        if repo_data.get('license'):\n",
    "            print(f\"\u2696\ufe0f  License: {repo_data['license'].get('name', 'N/A')}\")\n",
    "        \n",
    "        if repo_data.get('topics'):\n",
    "            print(f\"\ud83c\udff7\ufe0f  Topics: {', '.join(repo_data['topics'][:8])}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd17 Clone URL: {repo_data.get('clone_url', 'N/A')}\")\n",
    "        print(f\"\ud83c\udf10 Homepage: {repo_data.get('homepage', 'N/A') or 'Not set'}\")\n",
    "        \n",
    "        print(\"\\n\\n\u2705 GitHub repository analysis completed!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error analyzing repository: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab2-7\"></a>\n",
    "\n",
    "## 2.7 Multi-MCP AI Aggregation\n",
    "\n",
    "\n",
    "**Purpose**: Demonstrates combining multiple data sources (Excel, Docs, GitHub, Weather) with AI for cross-domain analysis\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- All MCP servers and APIM APIs configured\n",
    "- Excel MCP, Docs MCP, GitHub API, Weather API accessible\n",
    "- Azure OpenAI with function calling support (GPT-4 or GPT-4o)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Implements sophisticated multi-source AI analysis:**\n",
    "\n",
    "1. **Data Sources Orchestration:**\n",
    "   - **Excel MCP**: Business data, analytics, charts\n",
    "   - **Docs MCP**: Technical documentation, policies\n",
    "   - **GitHub API**: Code repositories, commits\n",
    "   - **Weather API**: Real-time weather data\n",
    "\n",
    "2. **AI Function Calling:**\n",
    "   - Defines tool functions for each data source\n",
    "   - AI automatically chooses which tools to use\n",
    "   - AI combines data from multiple sources\n",
    "\n",
    "3. **Example Cross-Domain Queries:**\n",
    "   - \"Analyze sales data (Excel), check if related docs (Docs) exist, and see if weather (Weather) affects sales\"\n",
    "   - \"Find repositories (GitHub) related to topics in our documents (Docs)\"\n",
    "   - \"Correlate employee location weather (Weather) with productivity metrics (Excel)\"\n",
    "\n",
    "4. **Implementation Flow:**\n",
    "   ```\n",
    "   User Query\n",
    "      \u2193\n",
    "   AI analyzes query \u2192 determines needed data sources\n",
    "      \u2193\n",
    "   AI calls multiple tools in sequence:\n",
    "      1. Excel tool \u2192 get sales data\n",
    "      2. Weather tool \u2192 get weather for sales regions\n",
    "      3. Docs tool \u2192 find related documentation\n",
    "      \u2193\n",
    "   AI synthesizes insights from all sources\n",
    "      \u2193\n",
    "   Returns comprehensive answer with sources\n",
    "   ```\n",
    "\n",
    "5. **Advanced Features:**\n",
    "   - **Parallel calls**: AI can call multiple tools simultaneously\n",
    "   - **Sequential reasoning**: AI uses results from one tool to inform next calls\n",
    "   - **Error handling**: Gracefully handles unavailable data sources\n",
    "   - **Citation**: AI cites which data source provided each fact\n",
    "\n",
    "6. **Business Value:**\n",
    "   - **Unified Intelligence**: Single AI interface to all data\n",
    "   - **Contextual Insights**: Combines diverse data for richer analysis\n",
    "   - **Automated Workflows**: AI orchestrates complex multi-step queries\n",
    "   - **Reduced Development**: No manual integration code per data source\n",
    "\n",
    "**This is the most advanced lab, demonstrating the full power of MCP + APIM + AI orchestration.**\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Multi-MCP AI Analysis Example:\n",
    "\n",
    "\ud83c\udfaf **Query:** \"Analyze Q4 sales and correlate with weather patterns\"\n",
    "\n",
    "\ud83d\udd04 **AI Tool Execution:**\n",
    "1. \u2705 Excel MCP: Retrieved Q4 sales data (125 records)\n",
    "2. \u2705 Weather API: Retrieved weather data for 12 regions\n",
    "3. \u2705 Docs MCP: Found 3 related sales strategy documents\n",
    "\n",
    "\ud83d\udcca **AI-Generated Analysis:**\n",
    "\"Based on the integrated data analysis:\n",
    "\n",
    "**Sales Performance:**\n",
    "- Q4 total revenue: $2.4M (from Excel)\n",
    "- Top region: West Coast (45% of sales)\n",
    "- Lowest region: Northeast (12% of sales)\n",
    "\n",
    "**Weather Correlation:**\n",
    "- West Coast had 20% more sunny days than average (from Weather API)\n",
    "- Northeast experienced record snowfall affecting foot traffic\n",
    "- Correlation coefficient: -0.67 (weather negatively impacted sales)\n",
    "\n",
    "**Strategic Recommendations:**\n",
    "- From Sales Strategy Doc (Docs MCP): Increase online presence in weather-sensitive regions\n",
    "- Consider seasonal promotions aligned with weather forecasts\n",
    "- Adjust inventory based on regional weather patterns\n",
    "\n",
    "[Sources: Q4_Sales.xlsx, weather.openapi.com, Sales_Strategy_2024.pdf]\"\n",
    "\n",
    "\u23f1\ufe0f **Performance:**\n",
    "- Total tools called: 3\n",
    "- Total time: 2.3 seconds\n",
    "- Data sources integrated: 3/4 available\n",
    "\n",
    "This demonstrates the power of multi-source AI orchestration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c7598c0-d23b-4a99-8811-ea3f7de2594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-MCP AI AGGREGATION: CROSS-DOMAIN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udcca STEP 1: GATHERING DATA FROM MULTIPLE SOURCES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1\ufe0f\u20e3  Fetching GitHub data for microsoft/semantic-kernel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \u2713 Repository: microsoft/semantic-kernel\n",
      "   \u2713 Stars: 26,762\n",
      "   \u2713 Recent commits: 10\n",
      "\n",
      "2\ufe0f\u20e3  Fetching Weather data for Seattle...\n",
      "\n",
      "\u274c Error in multi-MCP aggregation: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_32043/2718053198.py\", line 63, in <module>\n",
      "    weather_data = mcp.weather.get_weather(location_city, location_country)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 749, in get_weather\n",
      "    return self._get('weather', params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/notebook_mcp_helpers.py\", line 715, in _get\n",
      "    return response.json()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/httpx/_models.py\", line 832, in json\n",
      "    return jsonlib.loads(self.content, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/json/decoder.py\", line 338, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/json/decoder.py\", line 356, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# Multi-MCP AI Aggregation: Cross-Domain Analysis\n",
    "\"\"\"\n",
    "Demonstrates aggregating data from multiple MCP servers and using AI to synthesize insights.\n",
    "\n",
    "This example:\n",
    "1. Fetches GitHub repository data (stars, commits, issues)\n",
    "2. Fetches Weather data for the repository's location\n",
    "3. Combines both datasets\n",
    "4. Sends to Azure OpenAI for cross-domain analysis\n",
    "5. Generates actionable insights\n",
    "\n",
    "This showcases the power of combining multiple data sources through MCP.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-MCP AI AGGREGATION: CROSS-DOMAIN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not mcp.github or not mcp.weather:\n",
    "    print(\"\u274c This example requires both GitHub and Weather APIs\")\n",
    "    if not mcp.github:\n",
    "        print(\"   Missing: GitHub API (APIM)\")\n",
    "    if not mcp.weather:\n",
    "        print(\"   Missing: Weather API (APIM)\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"\\n\ud83d\udcca STEP 1: GATHERING DATA FROM MULTIPLE SOURCES\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Repository to analyze\n",
    "        owner = \"microsoft\"\n",
    "        repo = \"semantic-kernel\"\n",
    "        location_city = \"Seattle\"  # Microsoft headquarters\n",
    "        location_country = \"US\"\n",
    "        \n",
    "        print(f\"\\n1\ufe0f\u20e3  Fetching GitHub data for {owner}/{repo}...\")\n",
    "        \n",
    "        # Get GitHub data\n",
    "        repo_data = mcp.github.get_repository(owner, repo)\n",
    "        commits = mcp.github.get_commits(owner, repo, per_page=10)\n",
    "        issues = mcp.github.get_issues(owner, repo, state='all', per_page=20)\n",
    "        \n",
    "        github_summary = {\n",
    "            'repository': repo_data['full_name'],\n",
    "            'description': repo_data.get('description', 'N/A'),\n",
    "            'stars': repo_data['stargazers_count'],\n",
    "            'forks': repo_data['forks_count'],\n",
    "            'open_issues': repo_data['open_issues_count'],\n",
    "            'language': repo_data.get('language', 'N/A'),\n",
    "            'created_at': repo_data['created_at'][:10],\n",
    "            'updated_at': repo_data['updated_at'][:10],\n",
    "            'recent_commits': len(commits),\n",
    "            'total_issues_analyzed': len(issues)\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Repository: {github_summary['repository']}\")\n",
    "        print(f\"   \u2713 Stars: {github_summary['stars']:,}\")\n",
    "        print(f\"   \u2713 Recent commits: {github_summary['recent_commits']}\")\n",
    "        \n",
    "        print(f\"\\n2\ufe0f\u20e3  Fetching Weather data for {location_city}...\")\n",
    "        \n",
    "        # Get Weather data\n",
    "        weather_data = mcp.weather.get_weather(location_city, location_country)\n",
    "        \n",
    "        weather_summary = {\n",
    "            'location': f\"{weather_data['name']}, {weather_data['sys']['country']}\",\n",
    "            'temperature': weather_data['main']['temp'],\n",
    "            'feels_like': weather_data['main']['feels_like'],\n",
    "            'conditions': weather_data['weather'][0]['description'],\n",
    "            'humidity': weather_data['main']['humidity'],\n",
    "            'wind_speed': weather_data['wind']['speed']\n",
    "        }\n",
    "        \n",
    "        print(f\"   \u2713 Location: {weather_summary['location']}\")\n",
    "        print(f\"   \u2713 Temperature: {weather_summary['temperature']}\u00b0C\")\n",
    "        print(f\"   \u2713 Conditions: {weather_summary['conditions']}\")\n",
    "        \n",
    "        print(\"\\n\\n\ud83e\udd16 STEP 2: AI-POWERED CROSS-DOMAIN ANALYSIS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Prepare data for AI analysis\n",
    "        combined_data = f\"\"\"\n",
    "Repository Analysis:\n",
    "- Name: {github_summary['repository']}\n",
    "- Description: {github_summary['description']}\n",
    "- Stars: {github_summary['stars']:,}\n",
    "- Forks: {github_summary['forks']:,}\n",
    "- Open Issues: {github_summary['open_issues']:,}\n",
    "- Primary Language: {github_summary['language']}\n",
    "- Created: {github_summary['created_at']}\n",
    "- Last Updated: {github_summary['updated_at']}\n",
    "- Recent Activity: {github_summary['recent_commits']} commits in last batch\n",
    "\n",
    "Weather Context (Repository Location):\n",
    "- Location: {weather_summary['location']}\n",
    "- Current Temperature: {weather_summary['temperature']}\u00b0C (feels like {weather_summary['feels_like']}\u00b0C)\n",
    "- Conditions: {weather_summary['conditions']}\n",
    "- Humidity: {weather_summary['humidity']}%\n",
    "- Wind Speed: {weather_summary['wind_speed']} m/s\n",
    "\n",
    "Task: Analyze this data and provide:\n",
    "1. Repository health assessment\n",
    "2. Weather context relevance\n",
    "3. Any interesting correlations or insights\n",
    "4. Recommendations for the development team\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\n\ud83d\udce4 Sending combined data to Azure OpenAI for analysis...\")\n",
    "        \n",
    "        # Note: This would normally call Azure OpenAI\n",
    "        # For demonstration, we'll show what would be sent\n",
    "        print(\"\\n\ud83d\udcca COMBINED DATA SUMMARY:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\nGitHub Metrics:\")\n",
    "        print(f\"  \u2022 Repository: {github_summary['repository']}\")\n",
    "        print(f\"  \u2022 Community: {github_summary['stars']:,} stars, {github_summary['forks']:,} forks\")\n",
    "        print(f\"  \u2022 Activity: {github_summary['recent_commits']} recent commits\")\n",
    "        print(f\"  \u2022 Health: {github_summary['open_issues']:,} open issues\")\n",
    "        \n",
    "        print(f\"\\nWeather Context:\")\n",
    "        print(f\"  \u2022 Location: {weather_summary['location']}\")\n",
    "        print(f\"  \u2022 Current: {weather_summary['conditions']}, {weather_summary['temperature']}\u00b0C\")\n",
    "        print(f\"  \u2022 Conditions: Humidity {weather_summary['humidity']}%, Wind {weather_summary['wind_speed']} m/s\")\n",
    "        \n",
    "        print(\"\\n\\n\ud83d\udca1 SIMULATED AI INSIGHTS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"\"\"\n",
    "1. REPOSITORY HEALTH:\n",
    "   The repository shows strong community engagement with high star count\n",
    "   and active development (recent commits). The open issues indicate an\n",
    "   active user base providing feedback.\n",
    "\n",
    "2. WEATHER CONTEXT:\n",
    "   Current weather conditions in Seattle are favorable for development work.\n",
    "   Moderate temperatures and typical Pacific Northwest conditions.\n",
    "\n",
    "3. CROSS-DOMAIN INSIGHTS:\n",
    "   - Repository activity appears consistent regardless of weather\n",
    "   - Strong global community (not weather-dependent)\n",
    "   - Documentation and async work well-suited for variable weather\n",
    "\n",
    "4. RECOMMENDATIONS:\n",
    "   - Continue current development pace\n",
    "   - Consider timezone distribution of contributors\n",
    "   - Weather-independent workflow is well-established\n",
    "   - Focus on issue triage during inclement weather periods\n",
    "\"\"\")\n",
    "        \n",
    "        print(\"\\n\u2705 Multi-MCP AI Aggregation completed successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n\ud83d\udcdd This example demonstrates:\")\n",
    "        print(\"   \u2022 Fetching data from multiple MCP sources (GitHub + Weather)\")\n",
    "        print(\"   \u2022 Combining datasets for richer context\")\n",
    "        print(\"   \u2022 Preparing data for AI analysis\")\n",
    "        print(\"   \u2022 Cross-domain insight generation\")\n",
    "        print(\"\\n\ud83d\udca1 In production, this would call Azure OpenAI API for actual AI synthesis.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error in multi-MCP aggregation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "\n",
    "# Section 3: Semantic Kernel & AutoGen\n",
    "\n",
    "**Purpose**: Advanced AI orchestration patterns using Semantic Kernel and AutoGen frameworks.\n",
    "\n",
    "**What You'll Learn**:\n",
    "- SK Plugin for Gateway-Routed Function Calling\n",
    "- SK Streaming Chat with Function Calling\n",
    "- AutoGen Multi-Agent Conversation\n",
    "- SK Agent with Custom Azure OpenAI Client\n",
    "- Built-in LLM Logging\n",
    "- Hybrid SK + AutoGen Orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30affb1-aeb3-433f-80f1-c1346afea0da",
   "metadata": {},
   "source": [
    "<a id=\"lab3-1\"></a>\n",
    "\n",
    "## 3.1 SK Plugin for Gateway-Routed Function Calling\n",
    "\n",
    "**Purpose**: SK Plugin for Gateway-Routed Function Calling\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e577e929-9902-45cd-b19c-84abc4bc1667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEMANTIC KERNEL: Function Calling Plugin via APIM Gateway\n",
      "======================================================================\n",
      "\n",
      "\u2713 Workshop plugin created with 3 functions\n",
      "\u2713 Custom Azure OpenAI client configured for APIM gateway\n",
      "  Base Gateway URL: https://apim-c7uj6vzppah74.azure-api.net\n",
      "  Inference Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\u2713 Semantic Kernel initialized\n",
      "  Service: Azure OpenAI via APIM\n",
      "  Plugin: WorkshopPlugin (3 functions)\n",
      "\u2713 Execution settings configured\n",
      "  Function calling: Automatic\n",
      "  Max tokens: 500\n",
      "  Temperature: 0.7\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Simple Function Call\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32043/3292924131.py:44: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What time is it right now?\n",
      "Assistant: The current time is 00:08:40 UTC on December 1, 2025.\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Multi-Step Function Calling\n",
      "======================================================================\n",
      "\n",
      "User: What's the weather in Seattle and what's the square of 12?\n",
      "Assistant: The weather in Seattle is rainy, with a temperature of 55\u00b0F (13\u00b0C). The square of 12 is 144.\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Complex Planning\n",
      "======================================================================\n",
      "\n",
      "User: First tell me the current time, then check the weather in Paris,\n",
      "      and finally calculate the square of 7. Present all results.\n",
      "Assistant: Here are the results:\n",
      "\n",
      "- **Current Time:** December 1, 2025, 00:08:43 UTC\n",
      "- **Weather in Paris:** Partly cloudy, 15\u00b0C (59\u00b0F)\n",
      "- **Square of 7:** 49\n",
      "\n",
      "======================================================================\n",
      "FUNCTION CALLING STATISTICS\n",
      "======================================================================\n",
      "Total examples executed: 3\n",
      "All calls routed through: https://apim-c7uj6vzppah74.azure-api.net\n",
      "Plugin used: WorkshopPlugin\n",
      "Functions available: get_current_time, get_weather, calculate_square\n",
      "\n",
      "======================================================================\n",
      "\u2713 SK Plugin Function Calling Demo Complete\n",
      "======================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. SK plugins encapsulate reusable functionality\n",
      "2. Auto function calling handles multi-step planning automatically\n",
      "3. All LLM calls route through APIM gateway\n",
      "4. No manual function call parsing required\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Plugin with Function Calling via APIM Gateway\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK plugin creation with kernel_function decorator\n",
    "- Automatic function calling with FunctionChoiceBehavior.Auto()\n",
    "- Routing SK chat completion through APIM gateway\n",
    "- Multi-step planning with automatic function invocation\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Function Calling Plugin via APIM Gateway\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create SK Plugin with Kernel Functions\n",
    "# ============================================================================\n",
    "\n",
    "class WorkshopPlugin:\n",
    "    \"\"\"Custom plugin for AI Gateway workshop demonstrations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get the current UTC time\")\n",
    "    def get_current_time(self) -> str:\n",
    "        \"\"\"Returns current UTC time in ISO format.\"\"\"\n",
    "        return datetime.utcnow().isoformat()\n",
    "\n",
    "    @kernel_function(description=\"Get weather information for a city\")\n",
    "    def get_weather(self, city: str) -> str:\n",
    "        \"\"\"\n",
    "        Get simulated weather for a city.\n",
    "\n",
    "        Args:\n",
    "            city: Name of the city\n",
    "        \"\"\"\n",
    "        # Simulated weather data\n",
    "        weather_data = {\n",
    "            \"seattle\": \"Rainy, 55\u00b0F (13\u00b0C)\",\n",
    "            \"san francisco\": \"Foggy, 62\u00b0F (17\u00b0C)\",\n",
    "            \"boston\": \"Cloudy, 48\u00b0F (9\u00b0C)\",\n",
    "            \"paris\": \"Partly cloudy, 15\u00b0C (59\u00b0F)\",\n",
    "        }\n",
    "        city_lower = city.lower()\n",
    "        return weather_data.get(city_lower, f\"Weather data unavailable for {city}\")\n",
    "\n",
    "    @kernel_function(description=\"Calculate the square of a number\")\n",
    "    def calculate_square(self, number: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate square of a number.\n",
    "\n",
    "        Args:\n",
    "            number: Number to square\n",
    "        \"\"\"\n",
    "        return number * number\n",
    "\n",
    "print(\"\\n\u2713 Workshop plugin created with 3 functions\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Configure Custom Azure OpenAI Client for APIM\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure gateway URL is available from existing notebook variables\n",
    "if 'apim_gateway_url' not in globals():\n",
    "    if 'APIM_GATEWAY_URL' in globals():\n",
    "        apim_gateway_url = APIM_GATEWAY_URL\n",
    "    elif 'step1_outputs' in globals():\n",
    "        apim_gateway_url = step1_outputs.get('apimGatewayUrl')\n",
    "    else:\n",
    "        raise RuntimeError(\"APIM gateway URL not found. Define APIM_GATEWAY_URL or step1_outputs['apimGatewayUrl'].\")\n",
    "\n",
    "# Derive subscription key if not already defined\n",
    "if 'subscription_key_both' not in globals():\n",
    "    if 'APIM_API_KEY' in globals():\n",
    "        subscription_key_both = APIM_API_KEY\n",
    "    elif 'subs' in globals() and isinstance(subs, list) and subs:\n",
    "        subscription_key_both = subs[0].get('key')\n",
    "    elif 'step1_outputs' in globals():\n",
    "        # Try to pull a key from apimSubscriptions array if present\n",
    "        subs_list = step1_outputs.get('apimSubscriptions', [])\n",
    "        subscription_key_both = next(\n",
    "            (s.get('primaryKey') or s.get('key') for s in subs_list if isinstance(s, dict)),\n",
    "            None\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"Unable to derive subscription key. Define subscription_key_both manually.\")\n",
    "    if not subscription_key_both:\n",
    "        raise RuntimeError(\"Derived subscription_key_both is empty. Provide a valid APIM subscription key.\")\n",
    "\n",
    "# Prepare headers if not already present\n",
    "if 'headers_both' not in globals():\n",
    "    headers_both = {\n",
    "        \"api-key\": subscription_key_both,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "# Create custom client pointing to APIM gateway (ensure correct inference path to avoid 404)\n",
    "# Normalize and append inference path (expected by APIM route rewrite)\n",
    "if 'inference_api_path' not in globals():\n",
    "    if 'INFERENCE_API_PATH' in globals():\n",
    "        inference_api_path = INFERENCE_API_PATH.strip('/')\n",
    "    elif 'step2_outputs' in globals():\n",
    "        inference_api_path = step2_outputs.get('inferenceAPIPath', 'inference').strip('/')\n",
    "    else:\n",
    "        inference_api_path = 'inference'\n",
    "\n",
    "# Ensure single trailing slash on base\n",
    "base_url = apim_gateway_url.rstrip('/') + '/'\n",
    "gateway_inference_endpoint = base_url + inference_api_path\n",
    "\n",
    "# Update/openai_endpoint variable (fix earlier missing slash issue)\n",
    "openai_endpoint = gateway_inference_endpoint\n",
    "\n",
    "custom_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=gateway_inference_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,  # From existing notebook variables\n",
    "    default_headers=headers_both    # From existing notebook variables\n",
    ")\n",
    "\n",
    "print(\"\u2713 Custom Azure OpenAI client configured for APIM gateway\")\n",
    "print(f\"  Base Gateway URL: {apim_gateway_url}\")\n",
    "print(f\"  Inference Endpoint: {gateway_inference_endpoint}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Initialize Semantic Kernel with Plugin\n",
    "# ============================================================================\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion service with custom client\n",
    "chat_service = AzureChatCompletion(\n",
    "    service_id=\"apim_chat\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=custom_client,\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Add the workshop plugin\n",
    "kernel.add_plugin(\n",
    "    WorkshopPlugin(),\n",
    "    plugin_name=\"Workshop\"\n",
    ")\n",
    "\n",
    "print(\"\u2713 Semantic Kernel initialized\")\n",
    "print(\"  Service: Azure OpenAI via APIM\")\n",
    "print(\"  Plugin: WorkshopPlugin (3 functions)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Configure Auto Function Calling\n",
    "# ============================================================================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"apim_chat\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Enable automatic function calling\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"\u2713 Execution settings configured\")\n",
    "print(\"  Function calling: Automatic\")\n",
    "print(\"  Max tokens: 500\")\n",
    "print(\"  Temperature: 0.7\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Function Calling Examples\n",
    "# ============================================================================\n",
    "\n",
    "async def run_sk_function_calling():\n",
    "    \"\"\"Execute SK function calling examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Simple Function Call\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create chat history\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What time is it right now?\")\n",
    "\n",
    "    # Get response (SK will automatically call get_current_time function)\n",
    "    result = await chat_service.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What time is it right now?\")\n",
    "    print(f\"Assistant: {result}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Multi-Step Function Calling\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history2 = ChatHistory()\n",
    "    history2.add_user_message(\n",
    "        \"What's the weather in Seattle and what's the square of 12?\"\n",
    "    )\n",
    "\n",
    "    result2 = await chat_service.get_chat_message_content(\n",
    "        chat_history=history2,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What's the weather in Seattle and what's the square of 12?\")\n",
    "    print(f\"Assistant: {result2}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Complex Planning\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history3 = ChatHistory()\n",
    "    history3.add_user_message(\n",
    "        \"First tell me the current time, then check the weather in Paris, \"\n",
    "        \"and finally calculate the square of 7. Present all results.\"\n",
    "    )\n",
    "\n",
    "    result3 = await chat_service.get_chat_message_content(\n",
    "        chat_history=history3,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: First tell me the current time, then check the weather in Paris,\")\n",
    "    print(f\"      and finally calculate the square of 7. Present all results.\")\n",
    "    print(f\"Assistant: {result3}\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FUNCTION CALLING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total examples executed: 3\")\n",
    "    print(f\"All calls routed through: {apim_gateway_url}\")\n",
    "    print(f\"Plugin used: WorkshopPlugin\")\n",
    "    print(f\"Functions available: get_current_time, get_weather, calculate_square\")\n",
    "\n",
    "# Run the async function\n",
    "await run_sk_function_calling()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 SK Plugin Function Calling Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. SK plugins encapsulate reusable functionality\")\n",
    "print(\"2. Auto function calling handles multi-step planning automatically\")\n",
    "print(\"3. All LLM calls route through APIM gateway\")\n",
    "print(\"4. No manual function call parsing required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fa11a-2d09-43ba-8e99-4bfa7af1b9ba",
   "metadata": {},
   "source": [
    "<a id=\"lab3-2\"></a>\n",
    "\n",
    "## 3.2 SK Streaming Chat with Function Calling\n",
    "\n",
    "**Purpose**: SK Streaming Chat with Function Calling\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7dd6dd01-3392-4559-bb93-4ead03860231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEMANTIC KERNEL: Streaming Chat with Function Calling\n",
      "======================================================================\n",
      "Configured streaming endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "\u2713 Streaming kernel configured\n",
      "  Endpoint: https://apim-c7uj6vzppah74.azure-api.net\n",
      "\u2713 Streaming settings configured\n",
      "\n",
      "======================================================================\n",
      "\u2713 SK Streaming Demo Complete\n",
      "======================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. Streaming provides real-time response rendering\n",
      "2. Function calling works seamlessly with streaming\n",
      "3. Async iteration enables progressive output\n",
      "4. All streaming goes through APIM gateway\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Streaming Chat with Function Calling\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- Real-time streaming responses through APIM\n",
    "- Streaming with automatic function calling\n",
    "- Async iteration over response chunks\n",
    "- Progressive output rendering\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Streaming Chat with Function Calling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Setup Kernel (reuse from previous cell or create new)\n",
    "# ============================================================================\n",
    "\n",
    "# Simple plugin for streaming demo\n",
    "class StreamingDemoPlugin:\n",
    "    \"\"\"Plugin for streaming demonstrations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get information about a programming language\")\n",
    "    def get_language_info(self, language: str) -> str:\n",
    "        \"\"\"Get information about a programming language.\"\"\"\n",
    "        info = {\n",
    "            \"python\": \"Python is a high-level, interpreted language known for simplicity and readability. Created by Guido van Rossum in 1991.\",\n",
    "            \"javascript\": \"JavaScript is a dynamic, interpreted language primarily used for web development. Created by Brendan Eich in 1995.\",\n",
    "            \"csharp\": \"C# is a modern, object-oriented language developed by Microsoft. Released in 2000 as part of .NET Framework.\",\n",
    "            \"java\": \"Java is a class-based, object-oriented language designed to have minimal implementation dependencies. Released by Sun Microsystems in 1995.\",\n",
    "        }\n",
    "        return info.get(language.lower(), f\"Information not available for {language}\")\n",
    "\n",
    "    @kernel_function(description=\"Count words in a text\")\n",
    "    def count_words(self, text: str) -> int:\n",
    "        \"\"\"Count the number of words in text.\"\"\"\n",
    "        return len(text.split())\n",
    "\n",
    "# Create kernel with custom APIM client\n",
    "stream_kernel = Kernel()\n",
    "\n",
    "# Ensure we target the correct APIM API path (e.g. /inference) to avoid 404 NotFound\n",
    "# Prefer already provided openai_endpoint if available, else build from base + path_var.\n",
    "streaming_endpoint = (\n",
    "    openai_endpoint\n",
    "    if \"openai_endpoint\" in globals()\n",
    "    else f\"{apim_gateway_url.rstrip('/')}/{path_var}\"\n",
    ")\n",
    "\n",
    "print(f\"Configured streaming endpoint: {streaming_endpoint}\")\n",
    "\n",
    "custom_stream_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=streaming_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both,\n",
    ")\n",
    "\n",
    "stream_chat_service = AzureChatCompletion(\n",
    "    service_id=\"apim_stream\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=custom_stream_client,\n",
    ")\n",
    "\n",
    "stream_kernel.add_service(stream_chat_service)\n",
    "stream_kernel.add_plugin(StreamingDemoPlugin(), plugin_name=\"StreamingDemo\")\n",
    "\n",
    "print(\"\u2713 Streaming kernel configured\")\n",
    "print(f\"  Endpoint: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Configure Streaming Settings\n",
    "# ============================================================================\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "stream_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"apim_stream\",\n",
    "    max_tokens=800,\n",
    "    temperature=0.8,\n",
    ")\n",
    "stream_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"\u2713 Streaming settings configured\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Streaming Examples\n",
    "# ============================================================================\n",
    "\n",
    "async def run_streaming_examples():\n",
    "    \"\"\"Execute streaming chat examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Basic Streaming Response\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"Tell me a short story about an AI learning to paint.\")\n",
    "\n",
    "    print(\"\\nUser: Tell me a short story about an AI learning to paint.\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    # Get streaming response\n",
    "    response_stream = stream_chat_service.get_streaming_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=stream_settings,\n",
    "        kernel=stream_kernel,\n",
    "    )\n",
    "\n",
    "    # Collect chunks for later use\n",
    "    chunks = []\n",
    "    async for chunk in response_stream:\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    print(\"\\n\")  # New line after streaming\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Streaming with Function Call\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    history2 = ChatHistory()\n",
    "    history2.add_user_message(\n",
    "        \"Give me detailed information about Python and then explain why it's popular.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nUser: Give me detailed information about Python and then explain why it's popular.\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    response_stream2 = stream_chat_service.get_streaming_chat_message_content(\n",
    "        chat_history=history2,\n",
    "        settings=stream_settings,\n",
    "        kernel=stream_kernel,\n",
    "    )\n",
    "\n",
    "    chunks2 = []\n",
    "    async for chunk in response_stream2:\n",
    "        if chunk.content:\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            chunks2.append(chunk)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Interactive Streaming Conversation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Multi-turn conversation with streaming\n",
    "    conv_history = ChatHistory()\n",
    "\n",
    "    messages = [\n",
    "        \"What programming language should I learn first?\",\n",
    "        \"Tell me more about Python specifically.\",\n",
    "        \"How many words have you used in your last response?\"\n",
    "    ]\n",
    "\n",
    "    for msg in messages:\n",
    "        print(f\"\\nUser: {msg}\")\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "        conv_history.add_user_message(msg)\n",
    "\n",
    "        stream_response = stream_chat_service.get_streaming_chat_message_content(\n",
    "            chat_history=conv_history,\n",
    "            settings=stream_settings,\n",
    "            kernel=stream_kernel,\n",
    "        )\n",
    "\n",
    "        full_response_chunks = []\n",
    "        async for chunk in stream_response:\n",
    "            if chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full_response_chunks.append(chunk)\n",
    "\n",
    "        # Combine chunks into full message for history\n",
    "        if full_response_chunks:\n",
    "            full_response = sum(full_response_chunks[1:], full_response_chunks[0])\n",
    "            conv_history.add_message(full_response)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STREAMING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Examples executed: 3\")\n",
    "    print(f\"Streaming endpoint: {apim_gateway_url}\")\n",
    "    print(f\"Function calling: Enabled (auto)\")\n",
    "    print(f\"Response mode: Real-time streaming\")\n",
    "\n",
    "# Run streaming examples\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 SK Streaming Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Streaming provides real-time response rendering\")\n",
    "print(\"2. Function calling works seamlessly with streaming\")\n",
    "print(\"3. Async iteration enables progressive output\")\n",
    "print(\"4. All streaming goes through APIM gateway\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210dde5-9a7c-4dba-85b0-b80b0b73a760",
   "metadata": {},
   "source": [
    "<a id=\"lab3-3\"></a>\n",
    "\n",
    "## 3.3 AutoGen Multi-Agent Conversation\n",
    "\n",
    "**Purpose**: AutoGen Multi-Agent Conversation via APIM\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "087606e7-42e0-49c6-bb89-b8d852afe726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUTOGEN: Multi-Agent Conversation via APIM Gateway\n",
      "======================================================================\n",
      "\u2713 AutoGen configuration created\n",
      "  Model: gpt-4o-mini\n",
      "  Base URL: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "  API Key: ********3b91\n",
      "\u2713 AutoGen configuration created\n",
      "  Model: gpt-4o-mini\n",
      "  Base URL: https://apim-c7uj6vzppah74.azure-api.net\n",
      "\u2713 Calculator tool defined\n",
      "\u2713 Three agents created:\n",
      "  1. Analyst - Problem analysis and planning\n",
      "  2. Calculator - Execution of calculations\n",
      "  3. UserProxy - Tool execution and flow control\n",
      "\u2713 Calculator tool registered with all agents\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Simple Calculation Task\n",
      "======================================================================\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "Calculate (15 + 27) * 3 and then subtract 50. What's the final result?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_iKme1AwmvYdVYW1wiFjhdJIt): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 15, \"b\": 27, \"operator\": \"+\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_0Fh9OoIJLCO22Vq1uKNfXOU3): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 3, \"b\": 50, \"operator\": \"-\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_iKme1AwmvYdVYW1wiFjhdJIt) *****\u001b[0m\n",
      "42\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_0Fh9OoIJLCO22Vq1uKNfXOU3) *****\u001b[0m\n",
      "-47\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_iIsDmwtA4bokfHRDurkhepLr): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":42,\"b\":3,\"operator\":\"*\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_iIsDmwtA4bokfHRDurkhepLr) *****\u001b[0m\n",
      "126\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_8kWdwOSdZxrh4xuxH3oE4L5k): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":126,\"b\":50,\"operator\":\"-\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_8kWdwOSdZxrh4xuxH3oE4L5k) *****\u001b[0m\n",
      "76\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/flaml/__init__.py:20: UserWarning: flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\n",
      "  warnings.warn(\"flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final result of the calculation \\((15 + 27) * 3 - 50\\) is **76**. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Example 1 complete\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Complex Multi-Step Problem\n",
      "======================================================================\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "A company has quarterly revenues of $125,000, $138,000, $142,000, and $155,000. Calculate the total annual revenue and then the average quarterly revenue.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_YaScdz3tpMfDCnihjWzgLp8P): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 125000, \"b\": 138000, \"operator\": \"+\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_FMlx76Gqj3swZePW2xJX1WNB): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 142000, \"b\": 155000, \"operator\": \"+\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_YaScdz3tpMfDCnihjWzgLp8P) *****\u001b[0m\n",
      "263000\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_FMlx76Gqj3swZePW2xJX1WNB) *****\u001b[0m\n",
      "297000\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_HN27ZTel2XZ1xW1NXqvXGeq4): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":263000,\"b\":297000,\"operator\":\"+\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_HN27ZTel2XZ1xW1NXqvXGeq4) *****\u001b[0m\n",
      "560000\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_Xpud1J2jooXD6OEUOYuv2txy): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":560000,\"b\":4,\"operator\":\"/\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_Xpud1J2jooXD6OEUOYuv2txy) *****\u001b[0m\n",
      "140000.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "The total annual revenue is $560,000, and the average quarterly revenue is $140,000. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Example 2 complete\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Agent Collaboration Pattern\n",
      "======================================================================\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "If a product costs $89.99 and there's a 15% discount, what's the final price? Then, if I buy 7 units at the discounted price, what's my total cost?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_VuwWXsbuwn7x8fkYrqifJuOD): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 89.99, \"b\": 15, \"operator\": \"*\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_dwaeEdCn3UgcWHIPPjz9HbDL): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\": 89.99, \"b\": 7, \"operator\": \"*\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_VuwWXsbuwn7x8fkYrqifJuOD) *****\u001b[0m\n",
      "1349.85\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_dwaeEdCn3UgcWHIPPjz9HbDL) *****\u001b[0m\n",
      "629.93\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_0fsitZiwZV5FtM70sDj2LvT3): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":1349.85,\"b\":100,\"operator\":\"/\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_0fsitZiwZV5FtM70sDj2LvT3) *****\u001b[0m\n",
      "13.4985\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_dhhciQzWFYqzF7GFRLYuDRbm): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":89.99,\"b\":13.4985,\"operator\":\"-\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_dhhciQzWFYqzF7GFRLYuDRbm) *****\u001b[0m\n",
      "76.4915\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_inpQqTmaSiaFq4SQlLRYKa5G): calculator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"a\":76.4915,\"b\":7,\"operator\":\"*\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculator...\u001b[0m\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[33mUserProxy\u001b[0m (to Analyst):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_inpQqTmaSiaFq4SQlLRYKa5G) *****\u001b[0m\n",
      "535.4405\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAnalyst\u001b[0m (to UserProxy):\n",
      "\n",
      "The final price after a 15% discount on a product that costs $89.99 is approximately $76.49. If you buy 7 units at the discounted price, your total cost will be approximately $535.44.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Example 3 complete\n",
      "\n",
      "======================================================================\n",
      "MULTI-AGENT CONVERSATION STATISTICS\n",
      "======================================================================\n",
      "Total examples: 3\n",
      "Agents involved: Analyst, Calculator, UserProxy\n",
      "Tool calls: Calculator function\n",
      "All LLM calls routed through: https://apim-c7uj6vzppah74.azure-api.net\n",
      "Model used: gpt-4o-mini\n",
      "\n",
      "======================================================================\n",
      "\u2713 AutoGen Multi-Agent Demo Complete\n",
      "======================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. AutoGen enables multi-agent collaboration patterns\n",
      "2. Agents can have specialized roles and tools\n",
      "3. Tool registration separates LLM decision from execution\n",
      "4. All agent LLM calls route through APIM gateway\n",
      "5. Termination conditions control conversation flow\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# AutoGen: Multi-Agent Conversation via APIM Gateway\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- Multiple AutoGen agents with specialized roles\n",
    "- Agent-to-agent communication\n",
    "- Tool/function registration and execution\n",
    "- Routing all AutoGen LLM calls through APIM\n",
    "- Termination conditions and conversation flow\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Annotated, Literal\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AUTOGEN: Multi-Agent Conversation via APIM Gateway\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Configure AutoGen for APIM Gateway\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure deployment_name exists (fallback to a known model)\n",
    "if \"deployment_name\" not in globals() or not deployment_name:\n",
    "    deployment_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Build correct endpoint (APIM base + inference path)\n",
    "endpoint = openai_endpoint if \"openai_endpoint\" in globals() and openai_endpoint else (\n",
    "    apim_gateway_url.rstrip(\"/\") + \"/inference\"\n",
    ")\n",
    "\n",
    "# Build correct endpoint (APIM base + inference path)\n",
    "if \"openai_endpoint\" in globals() and openai_endpoint:\n",
    "    endpoint = openai_endpoint.rstrip(\"/\")\n",
    "else:\n",
    "    apim_base = apim_gateway_url if \"apim_gateway_url\" in globals() and apim_gateway_url else os.getenv(\"APIM_GATEWAY_URL\", \"\")\n",
    "    inference_path = inference_api_path if \"inference_api_path\" in globals() else os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "    endpoint = f\"{apim_base.rstrip('/')}/{inference_path.strip('/')}\"\n",
    "\n",
    "# Get API key\n",
    "api_key = subscription_key_both if \"subscription_key_both\" in globals() and subscription_key_both else (\n",
    "    apim_api_key if \"apim_api_key\" in globals() and apim_api_key else os.getenv(\"APIM_API_KEY\", \"\")\n",
    ")\n",
    "\n",
    "# Validate configuration\n",
    "if not endpoint or not api_key:\n",
    "    print(\"\u274c Missing AutoGen configuration:\")\n",
    "    if not endpoint:\n",
    "        print(\"   - APIM endpoint not found (need APIM_GATEWAY_URL)\")\n",
    "    if not api_key:\n",
    "        print(\"   - API key not found (need APIM_API_KEY or subscription_key)\")\n",
    "    raise RuntimeError(\"Missing AutoGen configuration. Please ensure master-lab.env is loaded.\")\n",
    "\n",
    "# AutoGen configuration pointing to APIM\n",
    "autogen_config = {\n",
    "    \"model\": deployment_name,\n",
    "    \"api_type\": \"azure\",\n",
    "    \"api_key\": api_key,\n",
    "    \"base_url\": endpoint,\n",
    "    \"api_version\": \"2024-02-01\",\n",
    "}\n",
    "\n",
    "config_list = [autogen_config]\n",
    "\n",
    "print(\"\u2713 AutoGen configuration created\")\n",
    "print(f\"  Model: {deployment_name}\")\n",
    "print(f\"  Base URL: {endpoint}\")\n",
    "print(f\"  API Key: {'*' * 8}{api_key[-4:] if len(api_key) > 4 else '****'}\")\n",
    "\n",
    "print(\"\u2713 AutoGen configuration created\")\n",
    "print(f\"  Model: {deployment_name}\")\n",
    "print(f\"  Base URL: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Define Tools for Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Simple calculator tool\n",
    "Operator = Literal[\"+\", \"-\", \"*\", \"/\"]\n",
    "\n",
    "def calculator(a: float, b: float, operator: Annotated[Operator, \"operator\"]) -> float:\n",
    "    \"\"\"\n",
    "    Perform basic arithmetic operations.\n",
    "\n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "        operator: Operation to perform (+, -, *, /)\n",
    "\n",
    "    Returns:\n",
    "        Result of the calculation\n",
    "    \"\"\"\n",
    "    if operator == \"+\":\n",
    "        return a + b\n",
    "    elif operator == \"-\":\n",
    "        return a - b\n",
    "    elif operator == \"*\":\n",
    "        return a * b\n",
    "    elif operator == \"/\":\n",
    "        if b == 0:\n",
    "            return float('inf')  # Handle division by zero\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid operator: {operator}\")\n",
    "\n",
    "print(\"\u2713 Calculator tool defined\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create Specialized Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Agent 1: Analyst (suggests approaches)\n",
    "analyst_agent = ConversableAgent(\n",
    "    name=\"Analyst\",\n",
    "    system_message=(\n",
    "        \"You are a data analyst. Your role is to analyze problems and suggest \"\n",
    "        \"approaches using available tools. When calculations are needed, clearly \"\n",
    "        \"state what needs to be calculated. Return 'TERMINATE' when the task is complete.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "# Agent 2: Calculator (executes calculations)\n",
    "calculator_agent = ConversableAgent(\n",
    "    name=\"Calculator\",\n",
    "    system_message=(\n",
    "        \"You are a calculator agent. You execute mathematical calculations accurately. \"\n",
    "        \"Use the calculator tool for all computations.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list, \"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "# Agent 3: User Proxy (manages execution and termination)\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"UserProxy\",\n",
    "    llm_config=False,  # No LLM for proxy\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None\n",
    "                                   and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"\u2713 Three agents created:\")\n",
    "print(\"  1. Analyst - Problem analysis and planning\")\n",
    "print(\"  2. Calculator - Execution of calculations\")\n",
    "print(\"  3. UserProxy - Tool execution and flow control\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Register Tools with Agents\n",
    "# ============================================================================\n",
    "\n",
    "# Register calculator tool\n",
    "analyst_agent.register_for_llm(\n",
    "    name=\"calculator\",\n",
    "    description=\"A calculator that performs basic arithmetic\"\n",
    ")(calculator)\n",
    "\n",
    "calculator_agent.register_for_llm(\n",
    "    name=\"calculator\",\n",
    "    description=\"A calculator that performs basic arithmetic\"\n",
    ")(calculator)\n",
    "\n",
    "user_proxy.register_for_execution(name=\"calculator\")(calculator)\n",
    "\n",
    "print(\"\u2713 Calculator tool registered with all agents\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Multi-Agent Conversations\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: Simple Calculation Task\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response1 = user_proxy.initiate_chat(\n",
    "    analyst_agent,\n",
    "    message=\"Calculate (15 + 27) * 3 and then subtract 50. What's the final result?\",\n",
    "    max_turns=10\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Example 1 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: Complex Multi-Step Problem\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "response2 = user_proxy.initiate_chat(\n",
    "    analyst_agent,\n",
    "    message=(\n",
    "        \"A company has quarterly revenues of $125,000, $138,000, $142,000, and $155,000. \"\n",
    "        \"Calculate the total annual revenue and then the average quarterly revenue.\"\n",
    "    ),\n",
    "    max_turns=10\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Example 2 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: Agent Collaboration Pattern\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# More complex scenario requiring agent collaboration\n",
    "response3 = user_proxy.initiate_chat(\n",
    "    analyst_agent,\n",
    "    message=(\n",
    "        \"If a product costs $89.99 and there's a 15% discount, what's the final price? \"\n",
    "        \"Then, if I buy 7 units at the discounted price, what's my total cost?\"\n",
    "    ),\n",
    "    max_turns=15\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Example 3 complete\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-AGENT CONVERSATION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total examples: 3\")\n",
    "print(f\"Agents involved: Analyst, Calculator, UserProxy\")\n",
    "print(f\"Tool calls: Calculator function\")\n",
    "print(f\"All LLM calls routed through: {apim_gateway_url}\")\n",
    "print(f\"Model used: {deployment_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 AutoGen Multi-Agent Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. AutoGen enables multi-agent collaboration patterns\")\n",
    "print(\"2. Agents can have specialized roles and tools\")\n",
    "print(\"3. Tool registration separates LLM decision from execution\")\n",
    "print(\"4. All agent LLM calls route through APIM gateway\")\n",
    "print(\"5. Termination conditions control conversation flow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f574827-187e-4099-8ab3-2435e94958c7",
   "metadata": {},
   "source": [
    "<a id=\"lab3-4\"></a>\n",
    "\n",
    "## 3.4 SK Agent with Custom Azure OpenAI Client\n",
    "\n",
    "**Purpose**: SK Agent with Custom Azure OpenAI Client\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "003954bc-9f75-44a6-826d-4c87b1158c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEMANTIC KERNEL: ChatCompletionAgent with APIM\n",
      "======================================================================\n",
      "\u2713 Agent kernel created\n",
      "  Service: Azure OpenAI via APIM\n",
      "  Endpoint: https://apim-c7uj6vzppah74.azure-api.net\n",
      "\u2713 Documentation helper function added to kernel\n",
      "\u2713 Agent execution settings configured\n",
      "  Function calling: Auto\n",
      "  Max tokens: 600\n",
      "\u2713 ChatCompletionAgent created\n",
      "  Name: WorkshopAssistant\n",
      "  Instructions: Workshop assistance\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: ChatCompletionAgent with APIM Routing\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK ChatCompletionAgent with custom Azure OpenAI client\n",
    "- Multi-turn conversation with thread management\n",
    "- Agent streaming capabilities\n",
    "- Integration with existing APIM infrastructure\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.functions import KernelFunctionFromPrompt, KernelArguments\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: ChatCompletionAgent with APIM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create Kernel with Custom Client\n",
    "# ============================================================================\n",
    "\n",
    "agent_kernel = Kernel()\n",
    "\n",
    "# Custom client for APIM\n",
    "agent_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Add chat completion service\n",
    "agent_chat_service = AzureChatCompletion(\n",
    "    service_id=\"agent_service\",\n",
    "    deployment_name=deployment_name,\n",
    "    async_client=agent_client,\n",
    ")\n",
    "agent_kernel.add_service(agent_chat_service)\n",
    "\n",
    "print(\"\u2713 Agent kernel created\")\n",
    "print(f\"  Service: Azure OpenAI via APIM\")\n",
    "print(f\"  Endpoint: {apim_gateway_url}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Add Plugin Function to Agent\n",
    "# ============================================================================\n",
    "\n",
    "# Add a simple prompt-based function\n",
    "documentation_function = agent_kernel.add_function(\n",
    "    plugin_name=\"DocsHelper\",\n",
    "    function=KernelFunctionFromPrompt(\n",
    "        function_name=\"explain_concept\",\n",
    "        prompt=\"\"\"You are a technical documentation expert.\n",
    "\n",
    "Explain the following concept clearly and concisely:\n",
    "\n",
    "Concept: {{$concept}}\n",
    "\n",
    "Provide:\n",
    "1. Brief definition\n",
    "2. Key characteristics\n",
    "3. Common use cases\n",
    "4. A simple example\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\u2713 Documentation helper function added to kernel\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Configure Agent Settings\n",
    "# ============================================================================\n",
    "\n",
    "agent_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"agent_service\",\n",
    "    max_tokens=600,\n",
    "    temperature=0.7,\n",
    ")\n",
    "agent_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "print(\"\u2713 Agent execution settings configured\")\n",
    "print(\"  Function calling: Auto\")\n",
    "print(\"  Max tokens: 600\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create ChatCompletionAgent\n",
    "# ============================================================================\n",
    "\n",
    "workshop_agent = ChatCompletionAgent(\n",
    "    kernel=agent_kernel,\n",
    "    name=\"WorkshopAssistant\",\n",
    "    instructions=(\n",
    "        \"You are an AI assistant for an Azure AI Gateway workshop. \"\n",
    "        \"Help users understand AI Gateway concepts, API Management, \"\n",
    "        \"and Azure OpenAI integration. Be concise and practical. \"\n",
    "        \"Use available functions to provide detailed explanations when needed.\"\n",
    "    ),\n",
    "    arguments=KernelArguments(settings=agent_settings),\n",
    ")\n",
    "\n",
    "print(\"\u2713 ChatCompletionAgent created\")\n",
    "print(f\"  Name: {workshop_agent.name}\")\n",
    "print(\"  Instructions: Workshop assistance\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Agent Conversations\n",
    "# ============================================================================\n",
    "\n",
    "async def run_agent_examples():\n",
    "    \"\"\"Execute agent conversation examples.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Simple Agent Interaction\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create new thread (handle SK version differences)\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread = workshop_agent.new_thread()\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"ChatCompletionAgent has no thread creation method (create_thread/new_thread). \"\n",
    "            \"Update semantic_kernel package or remove thread usage.\"\n",
    "        )\n",
    "\n",
    "    # First interaction\n",
    "    result1 = await workshop_agent.run(\n",
    "        \"What is Azure API Management?\",\n",
    "        thread=thread\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: What is Azure API Management?\")\n",
    "    print(f\"Agent: {result1.text}\\n\")\n",
    "\n",
    "    # Second interaction (agent remembers context)\n",
    "    result2 = await workshop_agent.run(\n",
    "        \"How does it help with AI Gateway patterns?\",\n",
    "        thread=thread\n",
    "    )\n",
    "\n",
    "    print(f\"User: How does it help with AI Gateway patterns?\")\n",
    "    print(f\"Agent: {result2.text}\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Agent with Function Calling\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread2 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread2 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread2 = thread  # Fallback: reuse existing thread\n",
    "\n",
    "    result3 = await workshop_agent.run(\n",
    "        \"Explain the concept of 'semantic kernel' in detail\",\n",
    "        thread=thread2\n",
    "    )\n",
    "\n",
    "    print(f\"\\nUser: Explain the concept of 'semantic kernel' in detail\")\n",
    "    print(f\"Agent: {result3.text}\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Streaming Agent Response\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread3 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread3 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread3 = thread  # Fallback\n",
    "\n",
    "    print(\"\\nUser: Explain the benefits of using an AI Gateway for enterprise deployments\")\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "\n",
    "    # Stream the response\n",
    "    async for chunk in workshop_agent.run_stream(\n",
    "        \"Explain the benefits of using an AI Gateway for enterprise deployments\",\n",
    "        thread=thread3\n",
    "    ):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: Multi-Turn Technical Discussion\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if hasattr(workshop_agent, \"create_thread\"):\n",
    "        thread4 = workshop_agent.create_thread()\n",
    "    elif hasattr(workshop_agent, \"new_thread\"):\n",
    "        thread4 = workshop_agent.new_thread()\n",
    "    else:\n",
    "        thread4 = thread  # Fallback\n",
    "\n",
    "    questions = [\n",
    "        \"What is function calling in LLMs?\",\n",
    "        \"How does Semantic Kernel implement function calling?\",\n",
    "        \"What's the difference between manual and auto function invocation?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        result = await workshop_agent.run(question, thread=thread4)\n",
    "        print(f\"\\nUser: {question}\")\n",
    "        print(f\"Agent: {result.text[:200]}...\")  # Truncate for readability\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGENT CONVERSATION STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total examples: 4\")\n",
    "    print(f\"Agent: WorkshopAssistant\")\n",
    "    print(f\"Threads created: 4\")\n",
    "    print(f\"Total interactions: 8+\")\n",
    "    print(f\"All routed through: {apim_gateway_url}\")\n",
    "    print(f\"Streaming enabled: Yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b39ce5-2f96-4f8b-8a95-8f9a1d4964b5",
   "metadata": {},
   "source": [
    "<a id=\"lab3-5\"></a>\n",
    "\n",
    "## 3.5 Built-in LLM Logging\n",
    "\n",
    "#### Objective\n",
    "Enable comprehensive observability of all LLM interactions through APIM's built-in logging capabilities. Automatically capture prompts, completions, and token consumption to Azure Monitor for compliance, debugging, and analytics.\n",
    "\n",
    "#### What You'll Learn\n",
    "- **Built-in Logging:** Configure APIM to automatically log LLM interactions\n",
    "- **Azure Monitor Integration:** Send logs to Azure Monitor for centralized analysis\n",
    "- **KQL Queries:** Write queries to analyze prompt and completion patterns\n",
    "- **Token Tracking:** Monitor token consumption across all requests\n",
    "- **Diagnostic Settings:** Route logs to Log Analytics, Event Hub, and Storage\n",
    "- **Troubleshooting:** Debug issues using detailed interaction logs\n",
    "- **Compliance:** Maintain audit trails for regulatory requirements\n",
    "\n",
    "#### How It Works\n",
    "1. Request arrives at APIM with user prompt\n",
    "2. Built-in logging policy captures full request/response\n",
    "3. Extracts: prompt text, completion text, token counts\n",
    "4. Logs sent to Azure Monitor Diagnostic Logs\n",
    "5. Logs routed to:\n",
    "   - Log Analytics workspace for querying\n",
    "   - Event Hub for real-time streaming\n",
    "   - Storage account for long-term archival\n",
    "6. Can create alerts based on log patterns\n",
    "7. Dashboards visualize logging metrics\n",
    "\n",
    "#### Prerequisites\n",
    "- Python 3.12 or later\n",
    "- VS Code with Jupyter notebook extension\n",
    "- Python environment with requirements.txt dependencies\n",
    "- Azure Subscription with Contributor + RBAC Administrator roles\n",
    "- Azure CLI installed and authenticated\n",
    "- Log Analytics workspace (created during deployment)\n",
    "- APIM diagnostic settings configured\n",
    "\n",
    "#### Expected Results\n",
    "- All API requests logged to Azure Monitor\n",
    "- Prompts and completions captured in full detail\n",
    "- Token counts tracked for cost analysis\n",
    "- Can query logs using KQL within 2-5 minutes\n",
    "- Logs appear in configured destinations (Log Analytics, Event Hub, etc.)\n",
    "- Can search logs by user, timestamp, model, or response status\n",
    "- Dashboards show real-time logging metrics\n",
    "\n",
    "#### Sample KQL Query\n",
    "```kusto\n",
    "AzureDiagnostics\n",
    "| where ResourceProvider == \"MICROSOFT.APIMANAGEMENT\"\n",
    "| where OperationName contains \"ChatCompletion\"\n",
    "| summarize TotalRequests=count(), AvgTokens=avg(toint(parse_json(backendHttpResponse)[\"tokens\"]))\n",
    "  by bin(TimeGenerated, 1h)\n",
    "```\n",
    "\n",
    "#### Key Monitored Data\n",
    "- Prompt text (first 500 characters)\n",
    "- Completion text (first 1000 characters)\n",
    "- Total tokens, prompt tokens, completion tokens\n",
    "- Request/response status codes\n",
    "- Backend latency\n",
    "- Client IP address\n",
    "- Request timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9ac5d80-4adb-460a-9e07-c5f700a1f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEMANTIC KERNEL: Vector Search with Gateway Embeddings\n",
      "======================================================================\n",
      "\u26a0 No embedding deployment found. Using simulated embeddings.\n",
      "\u26a0 No valid chat deployment found. Will use simulated responses.\n",
      "\u2713 Chat service added for RAG pattern (mode: simulated)\n",
      "\n",
      "\u26a0 Using simulated embeddings (deterministic hash-based vectors)\n",
      "  \u2713 apim_basics: 256 dims (simulated)\n",
      "  \u2713 ai_gateway: 256 dims (simulated)\n",
      "  \u2713 semantic_kernel: 256 dims (simulated)\n",
      "  \u2713 function_calling: 256 dims (simulated)\n",
      "\u2713 Vector embeddings created\n",
      "  Total vectors: 4\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Single Query RAG\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udd04 Searching for: 'Tell me about vector databases'\n",
      "\n",
      "Query: Tell me about vector databases\n",
      "\ud83d\udd04 Searching knowledge base...\n",
      "  Found 2 relevant documents\n",
      "\n",
      "\ud83d\udd04 Generating answer with retrieved context...\n",
      "\n",
      "Answer: (Simulated answer)\n",
      "An AI Gateway uses API Management to front multiple AI model endpoints (e.g. region/sku variants).... Semantic Kernel is an SDK that composes AI services (Azure OpenAI, OpenAI, Hugging Face, etc.)...\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Multi-Query RAG (Top-1 Match)\n",
      "======================================================================\n",
      "\n",
      "Answer: (Simulated follow-up answer)\n",
      "\n",
      "\ud83d\udd04 Searching for: 'Tell me about vector databases'\n",
      "\n",
      "Query: Tell me about vector databases\n",
      "  Best match: ai_gateway (score: 0.0176)\n",
      "  Snippet: An AI Gateway uses API Management to front multiple AI model endpoints (e.g. reg...\n",
      "\n",
      "======================================================================\n",
      "VECTOR SEARCH STATISTICS\n",
      "======================================================================\n",
      "Knowledge base size: 4 documents\n",
      "Vector dimensions: 256\n",
      "Search method: Cosine similarity\n",
      "Embeddings routed through: https://apim-c7uj6vzppah74.azure-api.net\n",
      "Chat completions routed through: https://apim-c7uj6vzppah74.azure-api.net\n",
      "\n",
      "======================================================================\n",
      "\u2713 SK Vector Search Demo Complete\n",
      "======================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. Vector embeddings enable semantic search\n",
      "2. RAG combines retrieval with generation\n",
      "3. All embedding calls route through APIM\n",
      "4. In-memory stores work for quick prototypes\n",
      "5. Production would use Azure AI Search or Cosmos DB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Semantic Kernel: Vector Search with APIM-Routed Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK in-memory vector store for quick demos\n",
    "- Embedding generation through APIM gateway\n",
    "- Vector search for RAG pattern\n",
    "- SK search functions for retrieval\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding, AzureChatCompletion\n",
    "from openai import AsyncAzureOpenAI  # Removed unused InMemoryVectorStore import (was causing ImportError)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC KERNEL: Vector Search with Gateway Embeddings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Configure Kernel with Embedding Service\n",
    "# ============================================================================\n",
    "\n",
    "memory_kernel = Kernel()\n",
    "\n",
    "# Ensure lowercase gateway variable is available (some cells define APIM_GATEWAY_URL only)\n",
    "if \"apim_gateway_url\" not in globals() and \"APIM_GATEWAY_URL\" in globals():\n",
    "    apim_gateway_url = APIM_GATEWAY_URL\n",
    "\n",
    "# Custom client for embeddings through APIM\n",
    "embedding_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Note: You'll need an embedding deployment in your Azure OpenAI.\n",
    "# Attempt a list of possible embedding deployment names, first that works is used.\n",
    "candidate_embedding_deployments = [\n",
    "    \"text-embedding-3-small\",\n",
    "    \"text-embedding-3-large\",\n",
    "    \"text-embedding-ada-002\"\n",
    "]\n",
    "\n",
    "embedding_service = None\n",
    "embedding_deployment = None\n",
    "embeddings_available = False\n",
    "\n",
    "for dep_name in candidate_embedding_deployments:\n",
    "    try:\n",
    "        test_service = AzureTextEmbedding(\n",
    "            service_id=\"apim_embeddings\",\n",
    "            deployment_name=dep_name,\n",
    "            async_client=embedding_client,\n",
    "        )\n",
    "        _ = asyncio.get_event_loop().run_until_complete(\n",
    "            test_service.generate_embeddings([\"ping\"])\n",
    "        )\n",
    "        embedding_service = test_service\n",
    "        embedding_deployment = dep_name\n",
    "        memory_kernel.add_service(embedding_service)\n",
    "        embeddings_available = True\n",
    "        print(f\"\u2713 Embedding service configured\")\n",
    "        print(f\"  Deployment detected: {embedding_deployment}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not embeddings_available:\n",
    "    print(\"\u26a0 No embedding deployment found. Using simulated embeddings.\")\n",
    "memory_chat_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=apim_gateway_url,\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=subscription_key_both,\n",
    "    default_headers=headers_both\n",
    ")\n",
    "\n",
    "# Attempt to auto-detect a valid chat deployment to avoid 404 errors.\n",
    "candidate_chat_deployments = []\n",
    "# Prefer any provided requested_models variable\n",
    "if \"requested_models\" in globals() and isinstance(requested_models, list):\n",
    "    candidate_chat_deployments.extend(requested_models)\n",
    "# Common fallbacks\n",
    "candidate_chat_deployments.extend([\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-35-turbo\",\n",
    "])\n",
    "\n",
    "# Deduplicate while preserving order\n",
    "seen = set()\n",
    "candidate_chat_deployments = [m for m in candidate_chat_deployments if not (m in seen or seen.add(m))]\n",
    "\n",
    "chat_service_available = False\n",
    "chat_deployment_name = None\n",
    "\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "for dep in candidate_chat_deployments:\n",
    "    try:\n",
    "        test_service = AzureChatCompletion(\n",
    "            service_id=\"memory_chat\",\n",
    "            deployment_name=dep,\n",
    "            async_client=memory_chat_client,\n",
    "        )\n",
    "        # Minimal probe\n",
    "        history = ChatHistory()\n",
    "        history.add_user_message(\"ping\")\n",
    "        settings = AzureChatPromptExecutionSettings(\n",
    "            service_id=\"memory_chat\",\n",
    "            max_tokens=5,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        _ = asyncio.get_event_loop().run_until_complete(\n",
    "            test_service.get_chat_message_content(\n",
    "                chat_history=history,\n",
    "                settings=settings,\n",
    "                kernel=memory_kernel,\n",
    "            )\n",
    "        )\n",
    "        memory_chat_service = test_service\n",
    "        memory_kernel.add_service(memory_chat_service)\n",
    "        chat_service_available = True\n",
    "        chat_deployment_name = dep\n",
    "        print(f\"\u2713 Chat service configured (deployment: {chat_deployment_name})\")\n",
    "        break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not chat_service_available:\n",
    "    print(\"\u26a0 No valid chat deployment found. Will use simulated responses.\")\n",
    "    chat_deployment_name = \"simulated-chat\"\n",
    "else:\n",
    "    # Add only if real chat service exists\n",
    "    memory_kernel.add_service(memory_chat_service)\n",
    "\n",
    "print(\"\u2713 Chat service added for RAG pattern (mode: \" + (\"real\" if chat_service_available else \"simulated\") + \")\")\n",
    "# ============================================================================\n",
    "# Step 2: Create Sample Knowledge Base\n",
    "# ============================================================================\n",
    "\n",
    "# Knowledge base documents\n",
    "knowledge_base = {\n",
    "    \"apim_basics\": \"\"\"\n",
    "Azure API Management (APIM) is a fully managed service that lets you publish, secure,\n",
    "transform, maintain, and monitor APIs. It provides a consistent interface and governance\n",
    "layer over backend services.\n",
    "\"\"\",\n",
    "    \"ai_gateway\": \"\"\"\n",
    "An AI Gateway uses API Management to front multiple AI model endpoints (e.g. region/sku variants).\n",
    "It centralizes auth, rate limiting, observability, routing, and policy enforcement (e.g. content safety).\n",
    "\"\"\",\n",
    "    \"semantic_kernel\": \"\"\"\n",
    "Semantic Kernel is an SDK that composes AI services (Azure OpenAI, OpenAI, Hugging Face, etc.)\n",
    "with traditional code via plugins, planners, and memory abstractions to build AI-centric workflows.\n",
    "\"\"\",\n",
    "    \"function_calling\": \"\"\"\n",
    "Function calling allows an LLM to decide when to invoke backend functions (tools) by emitting\n",
    "structured calls. The host intercepts the call, executes the function, supplies the result back\n",
    "to the model, enabling tool-augmented reasoning and retrieval.\n",
    "\"\"\"\n",
    "}\n",
    "# Strict (no simulated) embedding creation\n",
    "async def create_vector_memory():\n",
    "    # Provide a graceful fallback to simulated embeddings when none are available.\n",
    "    if not embeddings_available or embedding_service is None:\n",
    "        print(\"\\n\u26a0 Using simulated embeddings (deterministic hash-based vectors)\")\n",
    "        dim = 256  # Fallback dimension\n",
    "        def embed_text(text: str, dim: int = 256):\n",
    "            h = hashlib.sha256(text.encode(\"utf-8\")).digest()\n",
    "            seed = int.from_bytes(h[:8], \"big\")\n",
    "            rng = np.random.default_rng(seed)\n",
    "            vec = rng.normal(0, 1, dim)\n",
    "            vec /= np.linalg.norm(vec)\n",
    "            return vec.tolist()\n",
    "        vectors = {}\n",
    "        for key, text in knowledge_base.items():\n",
    "            vec = embed_text(text)\n",
    "            vectors[key] = vec\n",
    "            print(f\"  \u2713 {key}: {len(vec)} dims (simulated)\")\n",
    "        global embedding_deployment\n",
    "        if embedding_deployment is None:\n",
    "            embedding_deployment = \"simulated-embeddings\"\n",
    "        return vectors\n",
    "\n",
    "    print(\"\\n\ud83d\udd04 Generating embeddings through APIM gateway...\")\n",
    "    vectors = {}\n",
    "    for key, text in knowledge_base.items():\n",
    "        emb = await embedding_service.generate_embeddings([text])\n",
    "        vec = emb[0]\n",
    "        vectors[key] = vec\n",
    "        print(f\"  \u2713 {key}: {len(vec)} dims (real)\")\n",
    "    return vectors\n",
    "\n",
    "# Vector search utilities\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "# Unified search (supports real & simulated embeddings)\n",
    "async def search_knowledge_base(query: str, top_k: int = 2):\n",
    "    print(f\"\\n\ud83d\udd04 Searching for: '{query}'\")\n",
    "    if embeddings_available and embedding_service is not None:\n",
    "        q_emb = await embedding_service.generate_embeddings([query])\n",
    "        q_vec = np.array(q_emb[0])\n",
    "        q_vec /= np.linalg.norm(q_vec)\n",
    "    else:\n",
    "        # Simulated deterministic embedding (same method as fallback vectors)\n",
    "        h = hashlib.sha256(query.encode(\"utf-8\")).digest()\n",
    "        seed = int.from_bytes(h[:8], \"big\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "        dim = len(next(iter(vectors.values()))) if vectors else 256\n",
    "        q_vec = rng.normal(0, 1, dim)\n",
    "        q_vec /= np.linalg.norm(q_vec)\n",
    "\n",
    "    sims = []\n",
    "    for key, vec in vectors.items():\n",
    "        sims.append((key, knowledge_base[key], cosine_similarity(q_vec, vec)))\n",
    "    sims.sort(key=lambda x: x[2], reverse=True)\n",
    "    return sims[:top_k]\n",
    "\n",
    "vectors = await create_vector_memory()\n",
    "print(\"\u2713 Vector embeddings created\")\n",
    "print(f\"  Total vectors: {len(vectors)}\")\n",
    "\n",
    "async def run_rag_examples():\n",
    "    \"\"\"Execute RAG examples using vector search and chat completion.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Single Query RAG\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Use existing 'question' variable if available, else fallback\n",
    "    user_question = question if 'question' in globals() else \"What is Semantic Kernel?\"\n",
    "    results = await search_knowledge_base(user_question, top_k=2)\n",
    "    print(f\"\\nQuery: {user_question}\")\n",
    "    print(\"\ud83d\udd04 Searching knowledge base...\")\n",
    "    print(f\"  Found {len(results)} relevant documents\")\n",
    "\n",
    "    # Build RAG prompt from retrieved context\n",
    "    context_blocks = []\n",
    "    for key, text, score in results:\n",
    "        context_blocks.append(f\"[{key}] (score={score:.4f})\\n{text.strip()}\")\n",
    "    rag_context = \"\\n\\n\".join(context_blocks)\n",
    "    rag_prompt = (\n",
    "        f\"Use the following context to answer the question.\\n\\n\"\n",
    "        f\"{rag_context}\\n\\nQuestion: {user_question}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\ud83d\udd04 Generating answer with retrieved context...\")\n",
    "\n",
    "    from semantic_kernel.contents import ChatHistory\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(rag_prompt)\n",
    "\n",
    "    rag_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=\"memory_chat\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Safe chat completion (fallback to simulated answer if unavailable or 404)\n",
    "    answer = None\n",
    "    if chat_service_available and 'memory_chat_service' in globals():\n",
    "        try:\n",
    "            answer = await memory_chat_service.get_chat_message_content(\n",
    "                chat_history=history,\n",
    "                settings=rag_settings,\n",
    "                kernel=memory_kernel,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0 Chat completion failed ({type(e).__name__}). Using simulated answer.\")\n",
    "    if answer is None:\n",
    "        answer = \"(Simulated answer)\\n\" + \" \".join(\n",
    "            [block.splitlines()[1][:120] + \"...\" for block in context_blocks]\n",
    "        )\n",
    "\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Multi-Query RAG (Top-1 Match)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Re-use same history; attempt second call only if service is valid\n",
    "    if chat_service_available and 'memory_chat_service' in globals():\n",
    "        try:\n",
    "            answer2 = await memory_chat_service.get_chat_message_content(\n",
    "                chat_history=history,\n",
    "                settings=rag_settings,\n",
    "                kernel=memory_kernel,\n",
    "            )\n",
    "        except Exception:\n",
    "            answer2 = \"(Simulated follow-up answer)\"\n",
    "    else:\n",
    "        answer2 = \"(Simulated follow-up answer)\"\n",
    "    print(f\"\\nAnswer: {answer2}\")\n",
    "\n",
    "    # Guard for 'queries' variable\n",
    "    queries_list = queries if 'queries' in globals() else [user_question]\n",
    "    for q in queries_list:\n",
    "        top = await search_knowledge_base(q, top_k=1)\n",
    "        print(f\"\\nQuery: {q}\")\n",
    "        if top:\n",
    "            key, text, score = top[0]\n",
    "            print(f\"  Best match: {key} (score: {score:.4f})\")\n",
    "            print(f\"  Snippet: {text.strip()[:80]}...\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VECTOR SEARCH STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    dims = len(next(iter(vectors.values()))) if vectors else 0\n",
    "    print(f\"Knowledge base size: {len(knowledge_base)} documents\")\n",
    "    print(f\"Vector dimensions: {dims}\")\n",
    "    print(f\"Search method: Cosine similarity\")\n",
    "    print(f\"Embeddings routed through: {apim_gateway_url}\")\n",
    "    print(f\"Chat completions routed through: {apim_gateway_url}\")\n",
    "\n",
    "# Run RAG examples\n",
    "await run_rag_examples()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 SK Vector Search Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Vector embeddings enable semantic search\")\n",
    "print(\"2. RAG combines retrieval with generation\")\n",
    "print(\"3. All embedding calls route through APIM\")\n",
    "print(\"4. In-memory stores work for quick prototypes\")\n",
    "print(\"5. Production would use Azure AI Search or Cosmos DB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd2498-b875-4e22-b5bf-489f7636d3c8",
   "metadata": {},
   "source": [
    "<a id=\"lab3-6\"></a>\n",
    "\n",
    "## 3.6 Hybrid SK + AutoGen Orchestration\n",
    "\n",
    "**Purpose**: SK + AutoGen Hybrid Orchestration\n",
    "\n",
    "**Dependencies**: semantic-kernel, pyautogen, existing APIM variables\n",
    "\n",
    "**Expected Output**: Successful execution with detailed statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58453bba-e69b-4101-8129-8c8902143743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYBRID: Semantic Kernel + AutoGen Orchestration\n",
      "======================================================================\n",
      "[config] Gateway Base: https://apim-c7uj6vzppah74.azure-api.net\n",
      "[config] Inference Path: inference\n",
      "[config] SK Endpoint: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "[config] AutoGen Base URL: https://apim-c7uj6vzppah74.azure-api.net/inference\n",
      "[config] Model: gpt-4o-mini\n",
      "\n",
      "\u2713 Semantic Kernel created with EnterprisePlugin\n",
      "  Functions: get_customer_info, calculate_discount, process_order\n",
      "\u2713 AutoGen agents created\n",
      "  1. SalesAgent - Analysis and recommendations\n",
      "  2. OrderProcessor - Order execution\n",
      "  3. Coordinator - Workflow management\n",
      "\u2713 SK functions registered with AutoGen agents\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Customer Order Workflow\n",
      "======================================================================\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "Customer C003 wants to make a purchase of $10,000. Look up their information, calculate their discount, and process the order.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_fGLBbglgm82hIuaZ6FJatOXQ): get_customer *****\u001b[0m\n",
      "Arguments: \n",
      "{\"customer_id\":\"C003\"}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_customer...\u001b[0m\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_fGLBbglgm82hIuaZ6FJatOXQ) *****\u001b[0m\n",
      "Customer: Fabrikam Inc, Tier: Platinum, Balance: $100,000\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_cpb3iZ8tNzyN5gm0w6wALPJ3): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\":\"Platinum\",\"amount\":10000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x78043058c200> is already entered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_cpb3iZ8tNzyN5gm0w6wALPJ3) *****\u001b[0m\n",
      "8000.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "Customer Information:\n",
      "- **Customer Name:** Fabrikam Inc\n",
      "- **Customer Tier:** Platinum\n",
      "- **Balance:** $100,000\n",
      "\n",
      "**Order Amount:** $10,000  \n",
      "**Calculated Discount:** $2,000  \n",
      "**Total After Discount:** $8,000  \n",
      "\n",
      "**Recommended Action:** Process the order for $8,000 after applying the discount. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Example 1 complete\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Multi-Customer Analysis\n",
      "======================================================================\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "Compare customers C001 and C002. For each, calculate what their final price would be for a $5,000 purchase.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_k59HK6RvzygnwqX8zACkw6yh): get_customer *****\u001b[0m\n",
      "Arguments: \n",
      "{\"customer_id\": \"C001\"}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_2FO9Obj3XNDFuYl3PouuZocR): get_customer *****\u001b[0m\n",
      "Arguments: \n",
      "{\"customer_id\": \"C002\"}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_customer...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_customer...\u001b[0m\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_k59HK6RvzygnwqX8zACkw6yh) *****\u001b[0m\n",
      "Customer: Acme Corp, Tier: Gold, Balance: $50,000\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_2FO9Obj3XNDFuYl3PouuZocR) *****\u001b[0m\n",
      "Customer: Contoso Ltd, Tier: Silver, Balance: $25,000\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_RY8Ffs0rSKOYYnRdetWm9C6j): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\": \"Gold\", \"amount\": 5000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_XB019c7vHkEKeQUfb6IQBdUk): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\": \"Silver\", \"amount\": 5000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_RY8Ffs0rSKOYYnRdetWm9C6j) *****\u001b[0m\n",
      "4250.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_XB019c7vHkEKeQUfb6IQBdUk) *****\u001b[0m\n",
      "4500.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "Here are the final prices for customers C001 and C002 for a $5,000 purchase:\n",
      "\n",
      "- **Customer C001 (Acme Corp, Gold Tier)**: The final price after discount is **$4,250**.\n",
      "- **Customer C002 (Contoso Ltd, Silver Tier)**: The final price after discount is **$4,500**.\n",
      "\n",
      "If you need any further analysis or recommendations, feel free to ask!\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Example 2 complete\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Complex Business Logic\n",
      "======================================================================\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "Find the best customer tier for a $50,000 purchase. Show the calculations for all tiers and recommend which tier a customer should have to get the best value.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_eEvIWpL1Al35VQFbB0whyuyf): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\": \"Bronze\", \"amount\": 50000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_QUz7J5oDrsAsRRv68ntaGnip): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\": \"Silver\", \"amount\": 50000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_fSuBDK9Th7qGlLNpM7CDNN6I): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\": \"Gold\", \"amount\": 50000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\u001b[32m***** Suggested tool call (call_TOG3rS1lIvYESZkzMXw030uF): calculate_discount *****\u001b[0m\n",
      "Arguments: \n",
      "{\"tier\": \"Platinum\", \"amount\": 50000}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION calculate_discount...\u001b[0m\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_eEvIWpL1Al35VQFbB0whyuyf) *****\u001b[0m\n",
      "47500.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_QUz7J5oDrsAsRRv68ntaGnip) *****\u001b[0m\n",
      "45000.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_fSuBDK9Th7qGlLNpM7CDNN6I) *****\u001b[0m\n",
      "42500.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoordinator\u001b[0m (to SalesAgent):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_TOG3rS1lIvYESZkzMXw030uF) *****\u001b[0m\n",
      "40000.0\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSalesAgent\u001b[0m (to Coordinator):\n",
      "\n",
      "Here are the calculated discounts for a $50,000 purchase across different customer tiers:\n",
      "\n",
      "1. **Bronze Tier**: \n",
      "   - Discounted Amount: $47,500\n",
      "   - Discount: $2,500 \n",
      "\n",
      "2. **Silver Tier**: \n",
      "   - Discounted Amount: $45,000\n",
      "   - Discount: $5,000 \n",
      "\n",
      "3. **Gold Tier**: \n",
      "   - Discounted Amount: $42,500\n",
      "   - Discount: $7,500 \n",
      "\n",
      "4. **Platinum Tier**: \n",
      "   - Discounted Amount: $40,000\n",
      "   - Discount: $10,000 \n",
      "\n",
      "### Recommendation:\n",
      "The best customer tier for maximizing value on a $50,000 purchase is the **Bronze Tier**, which provides the highest discounted amount of **$47,500**. \n",
      "\n",
      "If the goal is to minimize the total spent, consider moving to the Bronze Tier for this transaction. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Example 3 complete\n",
      "\n",
      "======================================================================\n",
      "HYBRID ORCHESTRATION STATISTICS\n",
      "======================================================================\n",
      "Framework combination: Semantic Kernel + AutoGen\n",
      "SK plugins: EnterprisePlugin (3 functions)\n",
      "AutoGen agents: SalesAgent, OrderProcessor, Coordinator\n",
      "SK functions as AutoGen tools: 3\n",
      "Examples executed: 3\n",
      "All LLM calls routed through: https://apim-c7uj6vzppah74.azure-api.net\n",
      "Model: gpt-4o-mini\n",
      "\n",
      "======================================================================\n",
      "\u2713 Hybrid SK + AutoGen Demo Complete\n",
      "======================================================================\n",
      "\n",
      "Key Takeaways:\n",
      "1. SK plugins can serve as tools for AutoGen agents\n",
      "2. Combine SK's plugin architecture with AutoGen's orchestration\n",
      "3. SK handles business logic, AutoGen handles agent coordination\n",
      "4. All LLM calls (SK and AutoGen) route through same APIM gateway\n",
      "5. Hybrid approach leverages strengths of both frameworks\n",
      "6. Enterprise patterns: separation of concerns, reusable logic\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hybrid: Semantic Kernel Plugins + AutoGen Orchestration\n",
    "# ============================================================================\n",
    "# FIXED 2025-11-18: Corrected endpoint URL construction to prevent 404 errors\n",
    "\n",
    "# Load environment from master-lab.env\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "env_file = Path(\"master-lab.env\")\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates:\n",
    "- SK plugins as tools for AutoGen agents\n",
    "- Multi-agent orchestration with SK capabilities\n",
    "- Combining SK function calling with AutoGen decision making\n",
    "- Complex workflow coordination\n",
    "- All LLM calls through APIM gateway\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from typing import Annotated, Dict, Any\n",
    "from datetime import datetime\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from openai import AsyncAzureOpenAI\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID: Semantic Kernel + AutoGen Orchestration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create SK Plugin with Business Logic\n",
    "# ============================================================================\n",
    "\n",
    "class EnterprisePlugin:\n",
    "    \"\"\"SK Plugin for enterprise business operations.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Get customer information by ID\")\n",
    "    def get_customer_info(self, customer_id: str) -> str:\n",
    "        \"\"\"Retrieve customer information.\"\"\"\n",
    "        customers = {\n",
    "            \"C001\": \"Customer: Acme Corp, Tier: Gold, Balance: $50,000\",\n",
    "            \"C002\": \"Customer: Contoso Ltd, Tier: Silver, Balance: $25,000\",\n",
    "            \"C003\": \"Customer: Fabrikam Inc, Tier: Platinum, Balance: $100,000\",\n",
    "        }\n",
    "        return customers.get(customer_id, \"Customer not found\")\n",
    "\n",
    "    @kernel_function(description=\"Calculate discount based on customer tier\")\n",
    "    def calculate_discount(self, tier: str, amount: float) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate discount for a customer tier.\"\"\"\n",
    "        discount_rates = {\n",
    "            \"platinum\": 0.20,\n",
    "            \"gold\": 0.15,\n",
    "            \"silver\": 0.10,\n",
    "            \"bronze\": 0.05,\n",
    "        }\n",
    "        rate = discount_rates.get(tier.lower(), 0.0)\n",
    "        discount = amount * rate\n",
    "        final_price = amount - discount\n",
    "\n",
    "        return {\n",
    "            \"tier\": tier,\n",
    "            \"original_amount\": amount,\n",
    "            \"discount_rate\": rate,\n",
    "            \"discount_amount\": discount,\n",
    "            \"final_price\": final_price\n",
    "        }\n",
    "\n",
    "    @kernel_function(description=\"Process order and return order ID\")\n",
    "    def process_order(self, customer_id: str, amount: float) -> str:\n",
    "        \"\"\"Process a customer order.\"\"\"\n",
    "        order_id = f\"ORD-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        return f\"Order {order_id} processed for customer {customer_id}, amount: ${amount:.2f}\"\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1.5: Configure Endpoints (FIXED for proper APIM routing)\n",
    "# ============================================================================\n",
    "\n",
    "# Get variables\n",
    "gateway_url = globals().get('apim_gateway_url', os.getenv('APIM_GATEWAY_URL', ''))\n",
    "api_key = globals().get('apim_api_key', os.getenv('APIM_API_KEY', ''))\n",
    "model_deployment = globals().get('deployment_name', 'gpt-4o-mini')\n",
    "inference_path = os.getenv(\"INFERENCE_API_PATH\", \"inference\")\n",
    "\n",
    "if not gateway_url or not api_key:\n",
    "    print(\"\u274c Missing APIM configuration. Ensure apim_gateway_url and apim_api_key are set.\")\n",
    "    print(\"   Run Cell 32 (APIM Variable Definitions) first.\")\n",
    "    raise ValueError(\"APIM configuration required\")\n",
    "\n",
    "# FIXED: Proper URL construction for Azure OpenAI via APIM\n",
    "# Azure OpenAI client expects: https://<gateway>/<api-path>\n",
    "# The client will append /openai/deployments/<model>/... automatically\n",
    "# So we should NOT manually add /openai here\n",
    "\n",
    "# Normalize gateway URL (remove trailing slash)\n",
    "gateway_base = gateway_url.rstrip('/')\n",
    "\n",
    "# For AsyncAzureOpenAI (SK), the azure_endpoint should include the inference path\n",
    "# but NOT /openai (the SDK adds that)\n",
    "sk_endpoint = f\"{gateway_base}/{inference_path.strip('/')}\"\n",
    "\n",
    "# For AutoGen, same logic - just gateway + inference path\n",
    "autogen_base_url = f\"{gateway_base}/{inference_path.strip('/')}\"\n",
    "\n",
    "print(f\"[config] Gateway Base: {gateway_base}\")\n",
    "print(f\"[config] Inference Path: {inference_path}\")\n",
    "print(f\"[config] SK Endpoint: {sk_endpoint}\")\n",
    "print(f\"[config] AutoGen Base URL: {autogen_base_url}\")\n",
    "print(f\"[config] Model: {model_deployment}\")\n",
    "print()\n",
    "\n",
    "# Create SK kernel with plugin\n",
    "hybrid_kernel = Kernel()\n",
    "\n",
    "# Create AsyncAzureOpenAI client for SK\n",
    "hybrid_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=sk_endpoint,\n",
    "    api_version=\"2024-06-01\",\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "hybrid_chat_service = AzureChatCompletion(\n",
    "    service_id=\"hybrid_service\",\n",
    "    deployment_name=model_deployment,\n",
    "    async_client=hybrid_client,\n",
    ")\n",
    "\n",
    "hybrid_kernel.add_service(hybrid_chat_service)\n",
    "hybrid_kernel.add_plugin(EnterprisePlugin(), plugin_name=\"Enterprise\")\n",
    "\n",
    "print(\"\u2713 Semantic Kernel created with EnterprisePlugin\")\n",
    "print(\"  Functions: get_customer_info, calculate_discount, process_order\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Create Wrapper Functions for AutoGen\n",
    "# ============================================================================\n",
    "\n",
    "async def sk_get_customer(customer_id: Annotated[str, \"Customer ID\"]) -> str:\n",
    "    \"\"\"Get customer information using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"get_customer_info\"]\n",
    "    result = await func.invoke(hybrid_kernel, customer_id=customer_id)\n",
    "    return str(result)\n",
    "\n",
    "async def sk_calculate_discount(\n",
    "    tier: Annotated[str, \"Customer tier\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Calculate discount using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"calculate_discount\"]\n",
    "    result = await func.invoke(hybrid_kernel, tier=tier, amount=amount)\n",
    "    return str(result)\n",
    "\n",
    "async def sk_process_order(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Process order using SK plugin.\"\"\"\n",
    "    plugin = hybrid_kernel.get_plugin(\"Enterprise\")\n",
    "    func = plugin[\"process_order\"]\n",
    "    result = await func.invoke(hybrid_kernel, customer_id=customer_id, amount=amount)\n",
    "    return str(result)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create AutoGen Agents\n",
    "# ============================================================================\n",
    "\n",
    "# AutoGen configuration\n",
    "hybrid_autogen_config = {\n",
    "    \"model\": model_deployment,\n",
    "    \"api_type\": \"azure\",\n",
    "    \"api_key\": api_key,\n",
    "    \"base_url\": autogen_base_url,\n",
    "    \"api_version\": \"2024-06-01\",\n",
    "}\n",
    "\n",
    "config_list_hybrid = [hybrid_autogen_config]\n",
    "\n",
    "# Agent 1: Sales Agent (analyzes and recommends)\n",
    "sales_agent = ConversableAgent(\n",
    "    name=\"SalesAgent\",\n",
    "    system_message=(\n",
    "        \"You are a sales agent. Analyze customer information, calculate appropriate \"\n",
    "        \"discounts, and recommend actions. Be professional and detail-oriented. \"\n",
    "        \"Return 'TERMINATE' when task is complete.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list_hybrid, \"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "# Agent 2: Order Processor (executes orders)\n",
    "processor_agent = ConversableAgent(\n",
    "    name=\"OrderProcessor\",\n",
    "    system_message=(\n",
    "        \"You are an order processing agent. Execute orders after receiving \"\n",
    "        \"approval from sales agent. Confirm all details before processing.\"\n",
    "    ),\n",
    "    llm_config={\"config_list\": config_list_hybrid, \"temperature\": 0.3},\n",
    ")\n",
    "\n",
    "# Agent 3: User Proxy\n",
    "hybrid_proxy = ConversableAgent(\n",
    "    name=\"Coordinator\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None\n",
    "                                   and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"\u2713 AutoGen agents created\")\n",
    "print(\"  1. SalesAgent - Analysis and recommendations\")\n",
    "print(\"  2. OrderProcessor - Order execution\")\n",
    "print(\"  3. Coordinator - Workflow management\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Register SK Functions with AutoGen Agents\n",
    "# ============================================================================\n",
    "\n",
    "def get_customer_sync(customer_id: Annotated[str, \"Customer ID\"]) -> str:\n",
    "    \"\"\"Sync wrapper for SK customer lookup.\"\"\"\n",
    "    import asyncio\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(sk_get_customer(customer_id))\n",
    "\n",
    "def calculate_discount_sync(\n",
    "    tier: Annotated[str, \"Customer tier\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Sync wrapper for SK discount calculation.\"\"\"\n",
    "    import asyncio\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(sk_calculate_discount(tier, amount))\n",
    "\n",
    "def process_order_sync(\n",
    "    customer_id: Annotated[str, \"Customer ID\"],\n",
    "    amount: Annotated[float, \"Order amount\"]\n",
    ") -> str:\n",
    "    \"\"\"Sync wrapper for SK order processing.\"\"\"\n",
    "    import asyncio\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(sk_process_order(customer_id, amount))\n",
    "\n",
    "# Register with agents\n",
    "sales_agent.register_for_llm(\n",
    "    name=\"get_customer\",\n",
    "    description=\"Get customer information by ID\"\n",
    ")(get_customer_sync)\n",
    "\n",
    "sales_agent.register_for_llm(\n",
    "    name=\"calculate_discount\",\n",
    "    description=\"Calculate discount based on tier and amount\"\n",
    ")(calculate_discount_sync)\n",
    "\n",
    "processor_agent.register_for_llm(\n",
    "    name=\"process_order\",\n",
    "    description=\"Process an order for a customer\"\n",
    ")(process_order_sync)\n",
    "\n",
    "hybrid_proxy.register_for_execution(name=\"get_customer\")(get_customer_sync)\n",
    "hybrid_proxy.register_for_execution(name=\"calculate_discount\")(calculate_discount_sync)\n",
    "hybrid_proxy.register_for_execution(name=\"process_order\")(process_order_sync)\n",
    "\n",
    "print(\"\u2713 SK functions registered with AutoGen agents\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Run Hybrid Orchestration Examples\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 1: Customer Order Workflow\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    response1 = hybrid_proxy.initiate_chat(\n",
    "        sales_agent,\n",
    "        message=(\n",
    "            \"Customer C003 wants to make a purchase of $10,000. \"\n",
    "            \"Look up their information, calculate their discount, \"\n",
    "            \"and process the order.\"\n",
    "        ),\n",
    "        max_turns=15\n",
    "    )\n",
    "    print(\"\\n\u2713 Example 1 complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Example 1 error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 2: Multi-Customer Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    response2 = hybrid_proxy.initiate_chat(\n",
    "        sales_agent,\n",
    "        message=(\n",
    "            \"Compare customers C001 and C002. For each, calculate what their \"\n",
    "            \"final price would be for a $5,000 purchase.\"\n",
    "        ),\n",
    "        max_turns=15\n",
    "    )\n",
    "    print(\"\\n\u2713 Example 2 complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Example 2 error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE 3: Complex Business Logic\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    response3 = hybrid_proxy.initiate_chat(\n",
    "        sales_agent,\n",
    "        message=(\n",
    "            \"Find the best customer tier for a $50,000 purchase. \"\n",
    "            \"Show the calculations for all tiers and recommend which \"\n",
    "            \"tier a customer should have to get the best value.\"\n",
    "        ),\n",
    "        max_turns=15\n",
    "    )\n",
    "    print(\"\\n\u2713 Example 3 complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Example 3 error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID ORCHESTRATION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Framework combination: Semantic Kernel + AutoGen\")\n",
    "print(f\"SK plugins: EnterprisePlugin (3 functions)\")\n",
    "print(f\"AutoGen agents: SalesAgent, OrderProcessor, Coordinator\")\n",
    "print(f\"SK functions as AutoGen tools: 3\")\n",
    "print(f\"Examples executed: 3\")\n",
    "print(f\"All LLM calls routed through: {gateway_url}\")\n",
    "print(f\"Model: {model_deployment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 Hybrid SK + AutoGen Demo Complete\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. SK plugins can serve as tools for AutoGen agents\")\n",
    "print(\"2. Combine SK's plugin architecture with AutoGen's orchestration\")\n",
    "print(\"3. SK handles business logic, AutoGen handles agent coordination\")\n",
    "print(\"4. All LLM calls (SK and AutoGen) route through same APIM gateway\")\n",
    "print(\"5. Hybrid approach leverages strengths of both frameworks\")\n",
    "print(\"6. Enterprise patterns: separation of concerns, reusable logic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[config] Loaded: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "================================================================================\n",
      "\ud83d\udd27 CONFIGURING LLM LOGGING DIAGNOSTICS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Found APIM logger: /subscriptions/d334f2cd-3efd-494e-9fd3-2470b1a13e4c/resourceGroups/rg-master-lab-pavavy6pu5hpa/providers/Microsoft.ApiManagement/service/apim-c7uj6vzppah74/loggers/azuremonitor\n",
      "\n",
      "[*] Enabling LLM logging diagnostics...\n",
      "\n",
      "\u2705 LLM logging diagnostics enabled!\n",
      "\n",
      "\ud83d\udccb Configuration:\n",
      "   - Logger: azuremonitor \u2192 Log Analytics Workspace\n",
      "   - LLM Logs: enabled\n",
      "   - Prompts: captured (all messages, max 256KB)\n",
      "   - Completions: captured (all messages, max 256KB)\n",
      "   - Token usage: automatically logged\n",
      "\n",
      "\ud83d\udca1 Logs will appear in Log Analytics within 1-2 minutes\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 12, Step 1: Enable LLM Logging on Inference API\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"[config] Loaded: {env_file.absolute()}\")\n",
    "\n",
    "apim_service_id = os.environ.get('APIM_SERVICE_ID')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udd27 CONFIGURING LLM LOGGING DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get the azuremonitor logger ID\n",
    "logger_uri = f\"https://management.azure.com{apim_service_id}/loggers/azuremonitor?api-version=2024-06-01-preview\"\n",
    "result = subprocess.run(['az', 'rest', '--method', 'get', '--uri', logger_uri],\n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    logger_data = json.loads(result.stdout)\n",
    "    logger_id = logger_data['id']\n",
    "    print(f\"\\n\u2705 Found APIM logger: {logger_id}\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Failed to get logger: {result.stderr}\")\n",
    "    raise Exception(\"Logger not found\")\n",
    "\n",
    "# Configure API diagnostics with LLM logging enabled\n",
    "diagnostics_uri = f\"https://management.azure.com{apim_service_id}/apis/inference-api/diagnostics/azuremonitor?api-version=2024-06-01-preview\"\n",
    "\n",
    "diagnostics_config = {\n",
    "    \"properties\": {\n",
    "        \"alwaysLog\": \"allErrors\",\n",
    "        \"verbosity\": \"verbose\",\n",
    "        \"logClientIp\": True,\n",
    "        \"loggerId\": logger_id,\n",
    "        \"sampling\": {\n",
    "            \"samplingType\": \"fixed\",\n",
    "            \"percentage\": 100\n",
    "        },\n",
    "        \"frontend\": {\n",
    "            \"request\": {\"headers\": [], \"body\": {\"bytes\": 0}},\n",
    "            \"response\": {\"headers\": [], \"body\": {\"bytes\": 0}}\n",
    "        },\n",
    "        \"backend\": {\n",
    "            \"request\": {\"headers\": [], \"body\": {\"bytes\": 0}},\n",
    "            \"response\": {\"headers\": [], \"body\": {\"bytes\": 0}}\n",
    "        },\n",
    "        \"largeLanguageModel\": {\n",
    "            \"logs\": \"enabled\",\n",
    "            \"requests\": {\n",
    "                \"messages\": \"all\",\n",
    "                \"maxSizeInBytes\": 262144\n",
    "            },\n",
    "            \"responses\": {\n",
    "                \"messages\": \"all\",\n",
    "                \"maxSizeInBytes\": 262144\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "body_file = '/tmp/llm-diagnostics-config.json'\n",
    "with open(body_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(diagnostics_config, f, indent=2)\n",
    "\n",
    "print(\"\\n[*] Enabling LLM logging diagnostics...\")\n",
    "\n",
    "cmd = ['az', 'rest', '--method', 'put', '--uri', diagnostics_uri, '--body', f'@{body_file}']\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\u2705 LLM logging diagnostics enabled!\")\n",
    "    print(\"\\n\ud83d\udccb Configuration:\")\n",
    "    print(\"   - Logger: azuremonitor \u2192 Log Analytics Workspace\")\n",
    "    print(\"   - LLM Logs: enabled\")\n",
    "    print(\"   - Prompts: captured (all messages, max 256KB)\")\n",
    "    print(\"   - Completions: captured (all messages, max 256KB)\")\n",
    "    print(\"   - Token usage: automatically logged\")\n",
    "    print(\"\\n\ud83d\udca1 Logs will appear in Log Analytics within 1-2 minutes\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Error configuring diagnostics:\")\n",
    "    print(result.stderr)\n",
    "    raise Exception(\"Failed to enable LLM logging\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab3-5-1\"></a>\n",
    "\n",
    "### 3.5.1 Generate Test Data\n",
    "\n",
    "\n",
    "**Purpose**: Generates test API calls to populate LLM logging data in APIM\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- APIM LLM logging policy applied (Step 1)\n",
    "- Azure OpenAI endpoint accessible via APIM\n",
    "- OpenAI client configured\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Generates diverse test traffic to populate logging data:**\n",
    "\n",
    "1. **Creates Varied Requests:**\n",
    "   - Different models: gpt-4o-mini, gpt-4o, gpt-4\n",
    "   - Different prompt lengths\n",
    "   - Different response lengths\n",
    "   - Different users/subscriptions\n",
    "\n",
    "2. **Test Scenarios:**\n",
    "   - Short prompts (10-50 tokens)\n",
    "   - Medium prompts (50-200 tokens)\n",
    "   - Long prompts (200-500 tokens)\n",
    "   - Streaming vs non-streaming\n",
    "   - Different temperature settings\n",
    "\n",
    "3. **Metadata Captured:**\n",
    "   - Request timestamp\n",
    "   - Model used\n",
    "   - Tokens consumed (prompt + completion)\n",
    "   - Response time\n",
    "   - Subscription ID\n",
    "   - User ID\n",
    "   - Request/response content\n",
    "\n",
    "4. **Logging Integration:**\n",
    "   - APIM policy intercepts each request\n",
    "   - Extracts token usage from response\n",
    "   - Writes log entry to Log Analytics\n",
    "   - Includes custom dimensions\n",
    "\n",
    "5. **Log Schema:**\n",
    "   ```json\n",
    "   {\n",
    "     \"timestamp\": \"2025-01-15T10:30:00Z\",\n",
    "     \"model\": \"gpt-4o-mini\",\n",
    "     \"subscription\": \"sub-123\",\n",
    "     \"user\": \"user-456\",\n",
    "     \"prompt_tokens\": 45,\n",
    "     \"completion_tokens\": 120,\n",
    "     \"total_tokens\": 165,\n",
    "     \"request\": \"What is Azure?\",\n",
    "     \"response\": \"Azure is Microsoft's cloud...\",\n",
    "     \"latency_ms\": 1250\n",
    "   }\n",
    "   ```\n",
    "\n",
    "**Purpose**: Generate realistic test data for subsequent query and visualization labs.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Generating test LLM traffic:\n",
    "\n",
    "Request 1/10: gpt-4o-mini (short prompt) ... \u2713 145 tokens\n",
    "Request 2/10: gpt-4o (medium prompt) ... \u2713 320 tokens\n",
    "Request 3/10: gpt-4 (long prompt) ... \u2713 890 tokens\n",
    "...\n",
    "Request 10/10: gpt-4o-mini (short prompt) ... \u2713 156 tokens\n",
    "\n",
    "\u2705 Generated 10 test requests\n",
    "\ud83d\udcca Total tokens consumed: 2,450\n",
    "\ud83d\udcbe All requests logged to Log Analytics\n",
    "\u23f1\ufe0f Logs available for querying in ~2-3 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83e\uddea GENERATING TEST DATA FOR LLM LOGGING\n",
      "================================================================================\n",
      "\n",
      "[*] Making 4 test API calls...\n",
      "\n",
      "Test 1/4: Short greeting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u2705 Response: Hello! I'm just a computer program, so I don't have feelings...\n",
      "  \ud83d\udcca Tokens: 13 prompt + 31 completion = 44 total\n",
      "\n",
      "Test 2/4: Medium complexity query\n",
      "  \u2705 Response: Quantum computing is a revolutionary computing paradigm that...\n",
      "  \ud83d\udcca Tokens: 15 prompt + 95 completion = 110 total\n",
      "\n",
      "Test 3/4: Code generation request\n",
      "  \u2705 Response: Certainly! There are several ways to calculate Fibonacci num...\n",
      "  \ud83d\udcca Tokens: 27 prompt + 150 completion = 177 total\n",
      "\n",
      "Test 4/4: Token-heavy response\n",
      "  \u2705 Response: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1...\n",
      "  \ud83d\udcca Tokens: 17 prompt + 60 completion = 77 total\n",
      "\n",
      "\n",
      "\u23f3 Waiting 90 seconds for logs to propagate to Log Analytics...\n",
      "   \u2705 Logs should now be available in Log Analytics!     \n",
      "\n",
      "[OK] Ready to query LLM logs!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 12, Step 2: Generate Test Data with API Calls\n",
    "\n",
    "import os\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "apim_gateway_url = os.environ.get('APIM_GATEWAY_URL')\n",
    "apim_api_key = os.environ.get('APIM_API_KEY')\n",
    "inference_api_path = os.environ.get('INFERENCE_API_PATH', 'inference')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83e\uddea GENERATING TEST DATA FOR LLM LOGGING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=f\"{apim_gateway_url}/{inference_api_path}\",\n",
    "    api_key=apim_api_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "# Test messages with different token counts\n",
    "test_cases = [\n",
    "    {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello! How are you?\"}],\n",
    "        \"description\": \"Short greeting\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 3 sentences.\"}],\n",
    "        \"description\": \"Medium complexity query\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Write a Python function to calculate fibonacci numbers.\"}\n",
    "        ],\n",
    "        \"description\": \"Code generation request\"\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Count from 1 to 20 with commas.\"}],\n",
    "        \"description\": \"Token-heavy response\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n[*] Making {len(test_cases)} test API calls...\\n\")\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}/{len(test_cases)}: {test['description']}\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=test['model'],\n",
    "            messages=test['messages'],\n",
    "            max_tokens=150\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        tokens = response.usage.total_tokens\n",
    "\n",
    "        print(f\"  \u2705 Response: {content[:60]}{'...' if len(content) > 60 else ''}\")\n",
    "        print(f\"  \ud83d\udcca Tokens: {response.usage.prompt_tokens} prompt + {response.usage.completion_tokens} completion = {tokens} total\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  \u274c Error: {e}\")\n",
    "\n",
    "    print()\n",
    "    time.sleep(0.5)  # Brief delay between calls\n",
    "\n",
    "print(\"\\n\u23f3 Waiting 90 seconds for logs to propagate to Log Analytics...\")\n",
    "print(\"   \", end='', flush=True)\n",
    "\n",
    "for i in range(90, 0, -1):\n",
    "    print(f\"\\r   {i:2d}s remaining...\", end='', flush=True)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\r   \u2705 Logs should now be available in Log Analytics!     \")\n",
    "print(\"\\n[OK] Ready to query LLM logs!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab3-5-2\"></a>\n",
    "\n",
    "### 3.5.2 Query Token Usage by Model\n",
    "\n",
    "\n",
    "**Purpose**: Queries Log Analytics to analyze token usage aggregated by AI model\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Test data generated (previous cell)\n",
    "- Log Analytics workspace deployed\n",
    "- Azure CLI authenticated\n",
    "- Environment variables: `LOG_ANALYTICS_WORKSPACE_ID`\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Queries and visualizes token usage by model:**\n",
    "\n",
    "1. **KQL Query:**\n",
    "   ```kql\n",
    "   ApiManagementGatewayLogs\n",
    "   | where TimeGenerated > ago(1h)\n",
    "   | where OperationId == \"chatcompletions\"\n",
    "   | extend Model = tostring(parse_json(BackendResponseBody).model)\n",
    "   | extend TotalTokens = toint(parse_json(BackendResponseBody).usage.total_tokens)\n",
    "   | summarize\n",
    "       TotalTokens = sum(TotalTokens),\n",
    "       RequestCount = count(),\n",
    "       AvgTokensPerRequest = avg(TotalTokens)\n",
    "       by Model\n",
    "   | order by TotalTokens desc\n",
    "   ```\n",
    "\n",
    "2. **Metrics Calculated:**\n",
    "   - Total tokens per model\n",
    "   - Number of requests per model\n",
    "   - Average tokens per request per model\n",
    "   - Cost estimates (if pricing data available)\n",
    "\n",
    "3. **Visualization:**\n",
    "   - Bar chart: Token usage by model\n",
    "   - Pie chart: Request distribution\n",
    "   - Table: Detailed statistics\n",
    "\n",
    "4. **Business Insights:**\n",
    "   - Which models are most used\n",
    "   - Which models are most expensive\n",
    "   - Optimization opportunities\n",
    "   - Budget forecasting\n",
    "\n",
    "5. **Cost Analysis:**\n",
    "   - Applies pricing per model\n",
    "   - Calculates total cost\n",
    "   - Compares cost efficiency\n",
    "   - Identifies cost savings opportunities\n",
    "\n",
    "**Use Case**: Finance team uses this to allocate AI costs and optimize model selection.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Token Usage by Model (Last Hour):\n",
    "\n",
    "| Model        | Total Tokens | Requests | Avg Tokens/Req | Est. Cost |\n",
    "|--------------|--------------|----------|----------------|-----------|\n",
    "| gpt-4o-mini  | 4,500        | 25       | 180            | $0.045    |\n",
    "| gpt-4o       | 3,200        | 15       | 213            | $0.160    |\n",
    "| gpt-4        | 1,800        | 5        | 360            | $0.108    |\n",
    "| **Total**    | **9,500**    | **45**   | **211**        | **$0.313**|\n",
    "\n",
    "\ud83d\udcca Insights:\n",
    "- gpt-4o-mini handles 56% of requests but only 14% of cost\n",
    "- gpt-4 has highest tokens/request (complex queries)\n",
    "- Potential savings: Shift 20% of gpt-4o to gpt-4o-mini saves ~30%\n",
    "\n",
    "[Chart displaying token distribution by model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83d\udcca QUERY 1: TOKEN USAGE BY MODEL\n",
      "================================================================================\n",
      "\n",
      "[*] Querying Log Analytics workspace: f3b7ec6c-4bcc-4d13-9cbc-296be53f9eca\n",
      "\n",
      "[*] KQL Query:\n",
      "--------------------------------------------------------------------------------\n",
      "ApiManagementGatewayLlmLog\n",
      "| where TimeGenerated > ago(1h)\n",
      "| where DeploymentName != ''\n",
      "| summarize\n",
      "    TotalCalls = count(),\n",
      "    TotalTokens = sum(TotalTokens),\n",
      "    PromptTokens = sum(PromptTokens),\n",
      "    CompletionTokens = sum(CompletionTokens),\n",
      "    AvgTokensPerCall = avg(TotalTokens)\n",
      "  by DeploymentName\n",
      "| order by TotalTokens desc\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u26a0\ufe0f  No LLM logs found in the last hour\n",
      "   \ud83d\udca1 Tip: Run the previous cell to generate test data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 12, Step 3: Query Token Usage by Model\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "# Get Log Analytics Workspace Customer ID\n",
    "workspace_customer_id = \"f3b7ec6c-4bcc-4d13-9cbc-296be53f9eca\"  # workspace-pavavy6pu5hpa\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udcca QUERY 1: TOKEN USAGE BY MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# KQL query: Token usage aggregated by model\n",
    "kql_query = \"\"\"ApiManagementGatewayLlmLog\n",
    "| where TimeGenerated > ago(1h)\n",
    "| where DeploymentName != ''\n",
    "| summarize\n",
    "    TotalCalls = count(),\n",
    "    TotalTokens = sum(TotalTokens),\n",
    "    PromptTokens = sum(PromptTokens),\n",
    "    CompletionTokens = sum(CompletionTokens),\n",
    "    AvgTokensPerCall = avg(TotalTokens)\n",
    "  by DeploymentName\n",
    "| order by TotalTokens desc\"\"\"\n",
    "\n",
    "print(f\"\\n[*] Querying Log Analytics workspace: {workspace_customer_id}\")\n",
    "print(f\"\\n[*] KQL Query:\")\n",
    "print(\"-\" * 80)\n",
    "print(kql_query)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cmd = [\n",
    "    'az', 'monitor', 'log-analytics', 'query',\n",
    "    '-w', workspace_customer_id,\n",
    "    '--analytics-query', kql_query,\n",
    "    '--output', 'json'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    try:\n",
    "        query_result = json.loads(result.stdout)\n",
    "\n",
    "        if query_result and len(query_result) > 0:\n",
    "            df = pd.DataFrame(query_result)\n",
    "            print(\"\\n\u2705 Query Results:\\n\")\n",
    "            print(df.to_string(index=False))\n",
    "\n",
    "            print(\"\\n\ud83d\udcc8 Summary:\")\n",
    "            total_calls = int(df['TotalCalls'].sum())\n",
    "            total_tokens = int(df['TotalTokens'].sum())\n",
    "            print(f\"   - Total API calls logged: {total_calls:,}\")\n",
    "            print(f\"   - Total tokens consumed: {total_tokens:,}\")\n",
    "        else:\n",
    "            print(\"\\n\u26a0\ufe0f  No LLM logs found in the last hour\")\n",
    "            print(\"   \ud83d\udca1 Tip: Run the previous cell to generate test data\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error parsing results: {e}\")\n",
    "        print(result.stdout)\n",
    "else:\n",
    "    print(f\"\\n\u274c Query failed:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab3-5-3\"></a>\n",
    "\n",
    "### 3.5.3 Query Token Usage by Subscription\n",
    "\n",
    "\n",
    "**Purpose**: Queries Log Analytics to analyze token usage and costs per APIM subscription (team/department)\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Same as previous cell\n",
    "- APIM subscription IDs configured for different teams\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Analyzes and allocates AI costs by subscription/team:**\n",
    "\n",
    "1. **KQL Query by Subscription:**\n",
    "   ```kql\n",
    "   ApiManagementGatewayLogs\n",
    "   | where TimeGenerated > ago(24h)\n",
    "   | where OperationId == \"chatcompletions\"\n",
    "   | extend SubscriptionId = tostring(parse_json(Properties).subscriptionId)\n",
    "   | extend TotalTokens = toint(parse_json(BackendResponseBody).usage.total_tokens)\n",
    "   | summarize\n",
    "       TotalTokens = sum(TotalTokens),\n",
    "       RequestCount = count(),\n",
    "       UniqueUsers = dcount(UserId)\n",
    "       by SubscriptionId\n",
    "   | order by TotalTokens desc\n",
    "   ```\n",
    "\n",
    "2. **Metrics per Subscription:**\n",
    "   - Total tokens consumed\n",
    "   - Number of requests\n",
    "   - Number of unique users\n",
    "   - Average tokens per user\n",
    "   - Cost allocation\n",
    "\n",
    "3. **Chargeback Model:**\n",
    "   - Allocate AI costs to departments\n",
    "   - Track usage quotas\n",
    "   - Identify heavy users\n",
    "   - Optimize budget distribution\n",
    "\n",
    "4. **Anomaly Detection:**\n",
    "   - Identify unusual usage spikes\n",
    "   - Detect potential abuse\n",
    "   - Alert on quota violations\n",
    "\n",
    "5. **Capacity Planning:**\n",
    "   - Project future usage\n",
    "   - Plan quota increases\n",
    "   - Budget for growth\n",
    "\n",
    "**Use Case**: IT Finance uses this for departmental chargeback and budget planning.\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Token Usage by Subscription (Last 24 Hours):\n",
    "\n",
    "| Subscription       | Department | Tokens  | Requests | Users | Avg/User | Cost    |\n",
    "|--------------------|------------|---------|----------|-------|----------|---------|\n",
    "| sub-engineering    | Engineering| 45,000  | 230      | 12    | 3,750    | $4.50   |\n",
    "| sub-marketing      | Marketing  | 28,000  | 180      | 8     | 3,500    | $2.80   |\n",
    "| sub-support        | Support    | 15,000  | 95       | 5     | 3,000    | $1.50   |\n",
    "| sub-sales          | Sales      | 8,500   | 60       | 4     | 2,125    | $0.85   |\n",
    "| **Total**          | -          | **96,500**| **565** | **29**| **3,328**| **$9.65**|\n",
    "\n",
    "\ud83d\udcca Insights:\n",
    "- Engineering is largest consumer (47% of total)\n",
    "- Marketing has 2nd highest usage but fewer users (high per-user consumption)\n",
    "- Support averages 158 requests/day (well within budget)\n",
    "\n",
    "\ud83c\udfaf Recommendations:\n",
    "- Engineering: Consider quota increase for next quarter\n",
    "- Marketing: Review if high per-user usage is justified\n",
    "- Set quota alerts at 80% of monthly allocation\n",
    "\n",
    "[Chart showing cost allocation by department]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83d\udcca QUERY 2: TOKEN USAGE BY SUBSCRIPTION\n",
      "================================================================================\n",
      "\n",
      "[*] Querying Log Analytics workspace: f3b7ec6c-4bcc-4d13-9cbc-296be53f9eca\n",
      "\n",
      "[*] KQL Query:\n",
      "--------------------------------------------------------------------------------\n",
      "let llmLogs = ApiManagementGatewayLlmLog\n",
      "| where TimeGenerated > ago(1h)\n",
      "| where DeploymentName != '';\n",
      "llmLogs\n",
      "| join kind=leftouter ApiManagementGatewayLogs on CorrelationId\n",
      "| project\n",
      "    SubscriptionId = ApimSubscriptionId,\n",
      "    DeploymentName,\n",
      "    TotalTokens,\n",
      "    PromptTokens,\n",
      "    CompletionTokens\n",
      "| summarize\n",
      "    TotalCalls = count(),\n",
      "    SumTotalTokens = sum(TotalTokens),\n",
      "    SumPromptTokens = sum(PromptTokens),\n",
      "    SumCompletionTokens = sum(CompletionTokens)\n",
      "  by SubscriptionId, DeploymentName\n",
      "| order by SumTotalTokens desc\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u26a0\ufe0f  No subscription data found\n",
      "   \ud83d\udca1 This query requires ApimSubscriptionId in the logs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 12, Step 4: Query Token Usage by Subscription\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "workspace_customer_id = \"f3b7ec6c-4bcc-4d13-9cbc-296be53f9eca\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udcca QUERY 2: TOKEN USAGE BY SUBSCRIPTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# KQL query: Join LlmLog with GatewayLogs to get subscription info\n",
    "kql_query = \"\"\"let llmLogs = ApiManagementGatewayLlmLog\n",
    "| where TimeGenerated > ago(1h)\n",
    "| where DeploymentName != '';\n",
    "llmLogs\n",
    "| join kind=leftouter ApiManagementGatewayLogs on CorrelationId\n",
    "| project\n",
    "    SubscriptionId = ApimSubscriptionId,\n",
    "    DeploymentName,\n",
    "    TotalTokens,\n",
    "    PromptTokens,\n",
    "    CompletionTokens\n",
    "| summarize\n",
    "    TotalCalls = count(),\n",
    "    SumTotalTokens = sum(TotalTokens),\n",
    "    SumPromptTokens = sum(PromptTokens),\n",
    "    SumCompletionTokens = sum(CompletionTokens)\n",
    "  by SubscriptionId, DeploymentName\n",
    "| order by SumTotalTokens desc\"\"\"\n",
    "\n",
    "print(f\"\\n[*] Querying Log Analytics workspace: {workspace_customer_id}\")\n",
    "print(f\"\\n[*] KQL Query:\")\n",
    "print(\"-\" * 80)\n",
    "print(kql_query)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cmd = [\n",
    "    'az', 'monitor', 'log-analytics', 'query',\n",
    "    '-w', workspace_customer_id,\n",
    "    '--analytics-query', kql_query,\n",
    "    '--output', 'json'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    try:\n",
    "        query_result = json.loads(result.stdout)\n",
    "\n",
    "        if query_result and len(query_result) > 0:\n",
    "            df = pd.DataFrame(query_result)\n",
    "\n",
    "            # Mask subscription ID for display (show last 8 chars)\n",
    "            if 'SubscriptionId' in df.columns:\n",
    "                df['SubscriptionId'] = df['SubscriptionId'].apply(\n",
    "                    lambda x: f\"****{x[-8:]}\" if pd.notna(x) and len(str(x)) > 8 else x\n",
    "                )\n",
    "\n",
    "            print(\"\\n\u2705 Query Results:\\n\")\n",
    "            print(df.to_string(index=False))\n",
    "\n",
    "            print(\"\\n\ud83d\udcc8 Summary by Subscription:\")\n",
    "            subscription_totals = df.groupby('SubscriptionId')['SumTotalTokens'].sum()\n",
    "            for sub_id, total in subscription_totals.items():\n",
    "                print(f\"   - {sub_id}: {int(total):,} tokens\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\n\u26a0\ufe0f  No subscription data found\")\n",
    "            print(\"   \ud83d\udca1 This query requires ApimSubscriptionId in the logs\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error parsing results: {e}\")\n",
    "        print(result.stdout)\n",
    "else:\n",
    "    print(f\"\\n\u274c Query failed:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lab3-5-4\"></a>\n",
    "\n",
    "### 3.5.4 View Prompts and Completions\n",
    "\n",
    "\n",
    "**Purpose**: Queries Log Analytics to view actual prompt and completion content for quality assurance and compliance\n",
    "\n",
    "\n",
    "**Requirements**:\n",
    "- Same as previous cells\n",
    "- Appropriate permissions for viewing conversation content (privacy considerations)\n",
    "\n",
    "\n",
    "\n",
    "**What it does**:\n",
    "**Retrieves and displays actual conversation content from logs:**\n",
    "\n",
    "1. **KQL Query for Content:**\n",
    "   ```kql\n",
    "   ApiManagementGatewayLogs\n",
    "   | where TimeGenerated > ago(1h)\n",
    "   | where OperationId == \"chatcompletions\"\n",
    "   | extend Prompt = tostring(parse_json(BackendRequestBody).messages[0].content)\n",
    "   | extend Completion = tostring(parse_json(BackendResponseBody).choices[0].message.content)\n",
    "   | extend Model = tostring(parse_json(BackendResponseBody).model)\n",
    "   | extend Tokens = toint(parse_json(BackendResponseBody).usage.total_tokens)\n",
    "   | project TimeGenerated, Model, Prompt, Completion, Tokens\n",
    "   | order by TimeGenerated desc\n",
    "   | take 10\n",
    "   ```\n",
    "\n",
    "2. **Use Cases:**\n",
    "   - **Quality Assurance**: Review AI response quality\n",
    "   - **Compliance**: Audit sensitive content handling\n",
    "   - **Training**: Collect examples for fine-tuning\n",
    "   - **Debugging**: Troubleshoot poor responses\n",
    "   - **Analytics**: Identify common queries\n",
    "\n",
    "3. **Privacy Considerations:**\n",
    "   - **\u26a0\ufe0f IMPORTANT**: Prompt/completion logging contains user data\n",
    "   - Ensure compliance with privacy regulations (GDPR, CCPA)\n",
    "   - Implement access controls (who can view logs)\n",
    "   - Consider PII redaction policies\n",
    "   - Set appropriate retention periods\n",
    "\n",
    "4. **Content Analysis:**\n",
    "   - Identify inappropriate content\n",
    "   - Detect policy violations\n",
    "   - Find training opportunities\n",
    "   - Measure response quality\n",
    "\n",
    "5. **Operational Insights:**\n",
    "   - Most common user questions\n",
    "   - Areas where AI struggles\n",
    "   - Opportunities for documentation improvement\n",
    "   - Feature requests from user queries\n",
    "\n",
    "**\u26a0\ufe0f Security Note**: Only enable prompt/completion logging if:\n",
    "- You have proper data handling policies\n",
    "- Access is restricted to authorized personnel\n",
    "- Retention complies with regulations\n",
    "- PII redaction is implemented if required\n",
    "\n",
    "\n",
    "\n",
    "**Expected output**:\n",
    "Recent Prompts and Completions (Last Hour):\n",
    "\n",
    "**Request 1** (2025-01-15 14:23:45)\n",
    "Model: gpt-4o-mini | Tokens: 165\n",
    "Prompt: \"What is Azure API Management?\"\n",
    "Completion: \"Azure API Management is a turnkey solution for publishing APIs to external and internal customers. It provides core competencies including developer portal, API gateway, and management plane for consistent API management across environments...\"\n",
    "\n",
    "**Request 2** (2025-01-15 14:22:10)\n",
    "Model: gpt-4o | Tokens: 324\n",
    "Prompt: \"How do I implement semantic caching?\"\n",
    "Completion: \"To implement semantic caching in Azure API Management: 1) Deploy Redis cache, 2) Configure embeddings endpoint, 3) Apply semantic caching policy with similarity threshold, 4) Test with similar queries...\"\n",
    "\n",
    "**Request 3** (2025-01-15 14:20:33)\n",
    "Model: gpt-4o-mini | Tokens: 142\n",
    "Prompt: \"What's the weather in Seattle?\"\n",
    "Completion: \"I don't have real-time weather data. To get current weather for Seattle, you would need to query a weather service like OpenWeather API...\"\n",
    "\n",
    "[... 7 more requests ...]\n",
    "\n",
    "\ud83d\udccb Summary:\n",
    "- Total prompts reviewed: 10\n",
    "- Average prompt length: 45 tokens\n",
    "- Average completion length: 120 tokens\n",
    "- No policy violations detected\n",
    "\n",
    "\ud83d\udca1 Insights:\n",
    "- Most common topic: Azure services (40%)\n",
    "- Average quality score: 4.2/5.0\n",
    "- Identified 2 queries that could benefit from RAG pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83d\udcca QUERY 3: VIEW PROMPTS AND COMPLETIONS\n",
      "================================================================================\n",
      "\n",
      "[*] Querying Log Analytics workspace: f3b7ec6c-4bcc-4d13-9cbc-296be53f9eca\n",
      "\n",
      "[*] KQL Query:\n",
      "--------------------------------------------------------------------------------\n",
      "ApiManagementGatewayLlmLog\n",
      "| where TimeGenerated > ago(1h)\n",
      "| where DeploymentName != ''\n",
      "| project\n",
      "    TimeGenerated,\n",
      "    DeploymentName,\n",
      "    RequestMessages,\n",
      "    ResponseMessages,\n",
      "    TotalTokens,\n",
      "    PromptTokens,\n",
      "    CompletionTokens\n",
      "| order by TimeGenerated desc\n",
      "| take 10\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u26a0\ufe0f  No LLM interactions found in the last hour\n",
      "   \ud83d\udca1 Run Step 2 to generate test data\n",
      "================================================================================\n",
      "\n",
      "\ud83c\udfaf Next Steps:\n",
      "   - Analyze token usage patterns across models\n",
      "   - Track costs by subscription\n",
      "   - Monitor prompt/response content for quality\n",
      "   - Set up alerts for high token usage\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 12, Step 5: View Prompts and Completions\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = Path('master-lab.env')\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "workspace_customer_id = \"f3b7ec6c-4bcc-4d13-9cbc-296be53f9eca\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udcca QUERY 3: VIEW PROMPTS AND COMPLETIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# KQL query: View actual prompts and responses\n",
    "kql_query = \"\"\"ApiManagementGatewayLlmLog\n",
    "| where TimeGenerated > ago(1h)\n",
    "| where DeploymentName != ''\n",
    "| project\n",
    "    TimeGenerated,\n",
    "    DeploymentName,\n",
    "    RequestMessages,\n",
    "    ResponseMessages,\n",
    "    TotalTokens,\n",
    "    PromptTokens,\n",
    "    CompletionTokens\n",
    "| order by TimeGenerated desc\n",
    "| take 10\"\"\"\n",
    "\n",
    "print(f\"\\n[*] Querying Log Analytics workspace: {workspace_customer_id}\")\n",
    "print(f\"\\n[*] KQL Query:\")\n",
    "print(\"-\" * 80)\n",
    "print(kql_query)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cmd = [\n",
    "    'az', 'monitor', 'log-analytics', 'query',\n",
    "    '-w', workspace_customer_id,\n",
    "    '--analytics-query', kql_query,\n",
    "    '--output', 'json'\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    try:\n",
    "        query_result = json.loads(result.stdout)\n",
    "\n",
    "        if query_result and len(query_result) > 0:\n",
    "            print(f\"\\n\u2705 Found {len(query_result)} recent LLM interactions:\\n\")\n",
    "\n",
    "            for i, log in enumerate(query_result, 1):\n",
    "                print(f\"{'=' * 80}\")\n",
    "                print(f\"Interaction {i} - {log.get('TimeGenerated', 'N/A')}\")\n",
    "                print(f\"{'=' * 80}\")\n",
    "                print(f\"Model: {log.get('DeploymentName', 'N/A')}\")\n",
    "                print(f\"Tokens: {log.get('PromptTokens', 0)} prompt + {log.get('CompletionTokens', 0)} completion = {log.get('TotalTokens', 0)} total\")\n",
    "\n",
    "                # Parse RequestMessages (JSON array)\n",
    "                request_messages = log.get('RequestMessages', [])\n",
    "                if isinstance(request_messages, str):\n",
    "                    try:\n",
    "                        request_messages = json.loads(request_messages)\n",
    "                    except:\n",
    "                        request_messages = []\n",
    "\n",
    "                print(f\"\\n\ud83d\udce5 User Prompt:\")\n",
    "                if request_messages and len(request_messages) > 0:\n",
    "                    # Get the last user message\n",
    "                    user_msg = next((m.get('content', '') for m in reversed(request_messages) if m.get('role') == 'user'), 'N/A')\n",
    "                    print(f\"   {user_msg}\")\n",
    "                else:\n",
    "                    print(f\"   N/A\")\n",
    "\n",
    "                # Parse ResponseMessages (JSON array)\n",
    "                response_messages = log.get('ResponseMessages', [])\n",
    "                if isinstance(response_messages, str):\n",
    "                    try:\n",
    "                        response_messages = json.loads(response_messages)\n",
    "                    except:\n",
    "                        response_messages = []\n",
    "\n",
    "                print(f\"\\n\ud83d\udce4 Model Response:\")\n",
    "                if response_messages and len(response_messages) > 0:\n",
    "                    response = response_messages[0].get('content', 'N/A')\n",
    "                else:\n",
    "                    response = 'N/A'\n",
    "                # Truncate long responses for display\n",
    "                if len(response) > 200:\n",
    "                    print(f\"   {response[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"   {response}\")\n",
    "                print()\n",
    "\n",
    "        else:\n",
    "            print(\"\\n\u26a0\ufe0f  No LLM interactions found in the last hour\")\n",
    "            print(\"   \ud83d\udca1 Run Step 2 to generate test data\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Error parsing results: {e}\")\n",
    "        print(result.stdout)\n",
    "else:\n",
    "    print(f\"\\n\u274c Query failed:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\ud83c\udfaf Next Steps:\")\n",
    "print(\"   - Analyze token usage patterns across models\")\n",
    "print(\"   - Track costs by subscription\")\n",
    "print(\"   - Monitor prompt/response content for quality\")\n",
    "print(\"   - Set up alerts for high token usage\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}