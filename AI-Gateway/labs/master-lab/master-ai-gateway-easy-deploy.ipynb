{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Gateway - Easy Deploy\n",
    "\n",
    "> **One-command deployment** for complete Azure AI Gateway infrastructure with 7 comprehensive labs.\n",
    "\n",
    "## What's Different\n",
    "\n",
    "This notebook uses **modular deployment utilities** for minimal code:\n",
    "- **Deployment**: `util.deploy_all.py` - Deploy everything in one command\n",
    "- **Initialization**: `quick_start.shared_init.py` - One-line setup\n",
    "- **Labs**: Focused exercises with minimal boilerplate\n",
    "\n",
    "**Original notebook**: 152 cells  \n",
    "**This notebook**: ~28 cells (82% reduction)\n",
    "\n",
    "## What Gets Deployed\n",
    "\n",
    "- **Core**: APIM, Log Analytics, Application Insights\n",
    "- **AI Foundry**: 3 regions with 6 model deployments\n",
    "- **Supporting**: Redis, Cosmos DB, Azure AI Search\n",
    "- **MCP**: 5 MCP servers in Container Apps\n",
    "\n",
    "**Total time**: ~60 minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Azure subscription with Contributor role\n",
    "2. Azure CLI installed and authenticated (`az login`)\n",
    "3. Python 3.11+ with dependencies installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codespaces / Dev Container Setup\n",
    "\n",
    "> **Run this section first** if you're using GitHub Codespaces or a Dev Container.\n",
    "\n",
    "This will:\n",
    "1. Install required Python dependencies\n",
    "2. Check Azure CLI authentication\n",
    "3. Configure Cosmos DB firewall for your IP\n",
    "4. Add any missing environment variables\n",
    "\n",
    "**Skip this section** if you're running locally with dependencies already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Codespaces setup script (installs dependencies, configures Cosmos DB firewall)\n",
    "# This uses Jupyter's shell magic command (!) to run bash scripts\n",
    "# Skip this cell if running locally with dependencies already installed\n",
    "\n",
    "!cd /workspaces/Azure-AI-Gateway-Easy-Deploy && chmod +x setup-codespace.sh && ./setup-codespace.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 0: One-Command Deployment\n",
    "\n",
    "Deploy complete infrastructure in a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dependencies...\n",
      "‚úÖ All required packages are already available\n",
      "   Found: python-dotenv, azure-identity, azure-mgmt-resource and 3 more\n",
      "\n",
      "‚úÖ Dependency check complete - proceeding with notebook\n"
     ]
    }
   ],
   "source": [
    "# Check dependencies and attempt installation if needed\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "print(\"Checking dependencies...\")\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "# This works whether run from repo root or notebook directory\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# Check common locations for the requirements file\n",
    "possible_paths = [\n",
    "    os.path.join(notebook_dir, \"AI-Gateway\", \"labs\", \"master-lab\", \"requirements.txt\"),\n",
    "    os.path.join(notebook_dir, \"requirements.txt\"),\n",
    "    \"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/requirements.txt\",\n",
    "]\n",
    "requirements_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        requirements_path = path\n",
    "        break\n",
    "\n",
    "# Key packages required for this notebook\n",
    "required_packages = {\n",
    "    'dotenv': 'python-dotenv',\n",
    "    'azure.identity': 'azure-identity',\n",
    "    'azure.mgmt.resource': 'azure-mgmt-resource',\n",
    "    'azure.cosmos': 'azure-cosmos',\n",
    "    'openai': 'openai',\n",
    "    'requests': 'requests'\n",
    "}\n",
    "\n",
    "# Check which packages are already available\n",
    "missing_packages = []\n",
    "available_packages = []\n",
    "\n",
    "for module_name, package_name in required_packages.items():\n",
    "    if importlib.util.find_spec(module_name.split('.')[0]) is not None:\n",
    "        available_packages.append(package_name)\n",
    "    else:\n",
    "        missing_packages.append(package_name)\n",
    "\n",
    "if not missing_packages:\n",
    "    print(\"‚úÖ All required packages are already available\")\n",
    "    print(f\"   Found: {', '.join(available_packages[:3])} and {len(available_packages)-3} more\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Missing packages: {', '.join(missing_packages)}\")\n",
    "    \n",
    "    if not requirements_path:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not find requirements.txt\")\n",
    "        print(f\"   Searched: {possible_paths}\")\n",
    "        print(f\"\\n   Installing missing packages directly...\")\n",
    "        try:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                \"--user\", \"-q\"\n",
    "            ] + missing_packages)\n",
    "            print(\"‚úÖ Dependencies installed\")\n",
    "            print(\"   ‚ö†Ô∏è  Please restart the kernel to use the updated packages.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  Installation failed: {e}\")\n",
    "    else:\n",
    "        print(f\"   Using requirements from: {requirements_path}\")\n",
    "        \n",
    "        # Check if we're in a virtual environment\n",
    "        in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)\n",
    "        \n",
    "        if not in_venv:\n",
    "            print(\"\\n‚ö†Ô∏è  Not in a virtual environment\")\n",
    "            print(\"   This system uses externally-managed Python packages.\")\n",
    "            print()\n",
    "            print(\"   Recommended options:\")\n",
    "            print(\"   1. Use the dev container (already has everything installed)\")\n",
    "            print(\"   2. Create a virtual environment:\")\n",
    "            print(\"      python -m venv .venv\")\n",
    "            print(\"      source .venv/bin/activate  # On Linux/Mac\")\n",
    "            print(\"      .venv\\\\Scripts\\\\activate     # On Windows\")\n",
    "            print()\n",
    "            \n",
    "            # Try to install with --user flag as fallback\n",
    "            print(\"   Attempting installation to user directory...\")\n",
    "            try:\n",
    "                subprocess.check_call([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                    \"--user\", \"-q\", \"-r\", requirements_path\n",
    "                ])\n",
    "                print(\"‚úÖ Dependencies installed to user directory\")\n",
    "                print(\"   ‚ö†Ô∏è  Please restart the kernel to use the updated packages.\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"‚ö†Ô∏è  Installation failed (system Python is locked down)\")\n",
    "                print()\n",
    "                print(\"   Packages may already be installed via system package manager (apt).\")\n",
    "                print(\"   The notebook will attempt to continue - if you encounter import errors,\")\n",
    "                print(\"   please install manually:\")\n",
    "                print(\"   ‚Ä¢ Create a virtual environment: python -m venv .venv && source .venv/bin/activate\")\n",
    "                print(f\"   ‚Ä¢ Then run: pip install -r {requirements_path}\")\n",
    "        else:\n",
    "            # In virtual environment - proceed normally\n",
    "            print(\"‚úÖ Running in virtual environment\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", requirements_path])\n",
    "                print(\"‚úÖ Dependencies installed\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"‚ö†Ô∏è  Installation failed: {e}\")\n",
    "                print(f\"   Please manually install: pip install -r {requirements_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dependency check complete - proceeding with notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "This notebook uses **Azure CLI authentication** (easiest method):\n",
    "\n",
    "```bash\n",
    "az login\n",
    "az account set --subscription <your-subscription-id>\n",
    "```\n",
    "\n",
    "The deployment utility will automatically use your Azure CLI credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Tip: Press Enter to auto-detect subscription from Azure CLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:22:40,386 - INFO - ======================================================================\n",
      "2025-11-29 14:22:40,387 - INFO - AZURE AI GATEWAY COMPLETE DEPLOYMENT\n",
      "2025-11-29 14:22:40,387 - INFO - ======================================================================\n",
      "2025-11-29 14:22:40,388 - INFO - Subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "2025-11-29 14:22:40,388 - INFO - Resource Group: lab-master-lab\n",
      "2025-11-29 14:22:40,390 - INFO - Location: uksouth\n",
      "2025-11-29 14:22:40,390 - INFO - Resource Suffix: pavavy6pu5hpa\n",
      "2025-11-29 14:22:40,391 - INFO - ======================================================================\n",
      "2025-11-29 14:22:40,392 - INFO - Verifying prerequisites...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEPLOYING COMPLETE AZURE AI GATEWAY INFRASTRUCTURE\n",
      "======================================================================\n",
      "Using resource suffix: pavavy6pu5hpa\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:22:40,989 - INFO - Azure CLI installed\n",
      "2025-11-29 14:22:42,494 - INFO - Bicep installed\n",
      "2025-11-29 14:22:42,495 - INFO - Using Azure CLI credentials\n",
      "2025-11-29 14:22:42,630 - INFO - Successfully authenticated to Azure\n",
      "AzureCliCredential.get_token_info failed: Please run 'az login' to set up an account\n",
      "2025-11-29 14:22:43,151 - ERROR - Deployment failed after 2.8s: Please run 'az login' to set up an account\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå [FAILED] Deployment: Deployment failed (3s)\n"
     ]
    },
    {
     "ename": "CredentialUnavailableError",
     "evalue": "Please run 'az login' to set up an account",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/identity/_credentials/azure_cli.py:281\u001b[39m, in \u001b[36m_run_command\u001b[39m\u001b[34m(command_args, timeout)\u001b[39m\n\u001b[32m    280\u001b[39m     _LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[33mExecuting subprocess with the following arguments \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, args)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# non-zero return from shell\u001b[39;00m\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# Fallback check in case the executable is not found while executing subprocess.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/subprocess.py:466\u001b[39m, in \u001b[36mcheck_output\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    464\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m] = empty\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m           \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.stdout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['/usr/bin/az', 'account', 'get-access-token', '--output', 'json', '--resource', 'https://management.azure.com']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mCredentialUnavailableError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing resource suffix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.resource_suffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m outputs = \u001b[43mdeploy_complete_infrastructure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/util/deploy_all.py:1310\u001b[39m, in \u001b[36mdeploy_complete_infrastructure\u001b[39m\u001b[34m(config, progress_callback)\u001b[39m\n\u001b[32m   1306\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m   1308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1309\u001b[39m     \u001b[38;5;66;03m# Verify prerequisites\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     \u001b[43mverify_prerequisites\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1312\u001b[39m     \u001b[38;5;66;03m# Create clients\u001b[39;00m\n\u001b[32m   1313\u001b[39m     credential = create_credential(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/util/deploy_all.py:563\u001b[39m, in \u001b[36mverify_prerequisites\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# Check if resource group exists\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[43mresource_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresource_groups\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource group exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.resource_group\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ResourceNotFoundError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/mgmt/resource/resources/v2025_04_01/operations/_operations.py:6081\u001b[39m, in \u001b[36mResourceGroupsOperations.get\u001b[39m\u001b[34m(self, resource_group_name, **kwargs)\u001b[39m\n\u001b[32m   6078\u001b[39m _request.url = \u001b[38;5;28mself\u001b[39m._client.format_url(_request.url)\n\u001b[32m   6080\u001b[39m _stream = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6081\u001b[39m pipeline_response: PipelineResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m   6082\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   6083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6085\u001b[39m response = pipeline_response.http_response\n\u001b[32m   6087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m200\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/_base.py:242\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m pipeline_request: PipelineRequest[HTTPRequestType] = PipelineRequest(request, context)\n\u001b[32m    241\u001b[39m first_node = \u001b[38;5;28mself\u001b[39m._impl_policies[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m._transport)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "    \u001b[31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/mgmt/core/policies/_base.py:95\u001b[39m, in \u001b[36mARMAutoResourceProviderRegistrationPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) -> PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[32m     94\u001b[39m     http_request = request.http_request\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.http_response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m     97\u001b[39m         rp_name = \u001b[38;5;28mself\u001b[39m._check_rp_not_registered_err(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/policies/_redirect.py:205\u001b[39m, in \u001b[36mRedirectPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    203\u001b[39m original_domain = get_domain(request.http_request.url) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     redirect_location = \u001b[38;5;28mself\u001b[39m.get_redirect_location(response)\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/policies/_retry.py:545\u001b[39m, in \u001b[36mRetryPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28mself\u001b[39m._configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[32m    544\u001b[39m request.context[\u001b[33m\"\u001b[39m\u001b[33mretry_count\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_retry(retry_settings, response):\n\u001b[32m    547\u001b[39m     retry_active = \u001b[38;5;28mself\u001b[39m.increment(retry_settings, response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/policies/_authentication.py:159\u001b[39m, in \u001b[36mBearerTokenCredentialPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: PipelineRequest[HTTPRequestType]) -> PipelineResponse[HTTPRequestType, HTTPResponseType]:\n\u001b[32m    152\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Authorize request with a bearer token and send it to the next policy\u001b[39;00m\n\u001b[32m    153\u001b[39m \n\u001b[32m    154\u001b[39m \u001b[33;03m    :param request: The pipeline request object\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m \u001b[33;03m    :rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    161\u001b[39m         response = \u001b[38;5;28mself\u001b[39m.next.send(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/policies/_authentication.py:134\u001b[39m, in \u001b[36mBearerTokenCredentialPolicy.on_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m._enforce_https(request)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._need_new_token:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scopes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m bearer_token = cast(Union[\u001b[33m\"\u001b[39m\u001b[33mAccessToken\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAccessTokenInfo\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m._token).token\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m._update_headers(request.http_request.headers, bearer_token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/policies/_authentication.py:110\u001b[39m, in \u001b[36m_BearerTokenCredentialPolicyBase._request_token\u001b[39m\u001b[34m(self, *scopes, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, *scopes: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Request a new token from the credential.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    This will call the credential's appropriate method to get a token and store it in the policy.\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    :param str scopes: The type of access needed.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28mself\u001b[39m._token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/core/pipeline/policies/_authentication.py:100\u001b[39m, in \u001b[36m_BearerTokenCredentialPolicyBase._get_token\u001b[39m\u001b[34m(self, *scopes, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m TokenRequestOptions.\u001b[34m__annotations__\u001b[39m:  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[32m     98\u001b[39m             options[key] = kwargs.pop(key)  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSupportsTokenInfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_credential\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_token_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(TokenCredential, \u001b[38;5;28mself\u001b[39m._credential).get_token(*scopes, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/identity/_internal/decorators.py:23\u001b[39m, in \u001b[36mlog_get_token.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         token = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m         _LOGGER.log(\n\u001b[32m     25\u001b[39m             logging.DEBUG \u001b[38;5;28;01mif\u001b[39;00m within_credential_chain.get() \u001b[38;5;28;01melse\u001b[39;00m logging.INFO,\n\u001b[32m     26\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m succeeded\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m             fn.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m     28\u001b[39m         )\n\u001b[32m     29\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _LOGGER.isEnabledFor(logging.DEBUG):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/identity/_credentials/azure_cli.py:153\u001b[39m, in \u001b[36mAzureCliCredential.get_token_info\u001b[39m\u001b[34m(self, options, *scopes)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;129m@log_get_token\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_token_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, *scopes: \u001b[38;5;28mstr\u001b[39m, options: Optional[TokenRequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m) -> AccessTokenInfo:\n\u001b[32m    134\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Request an access token for `scopes`.\u001b[39;00m\n\u001b[32m    135\u001b[39m \n\u001b[32m    136\u001b[39m \u001b[33;03m    This is an alternative to `get_token` to enable certain scenarios that require additional properties\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m      receive an access token.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_token_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/identity/_credentials/azure_cli.py:191\u001b[39m, in \u001b[36mAzureCliCredential._get_token_base\u001b[39m\u001b[34m(self, options, *scopes, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.subscription:\n\u001b[32m    190\u001b[39m     command_args += [\u001b[33m\"\u001b[39m\u001b[33m--subscription\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.subscription]\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m output = \u001b[43m_run_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m token = parse_token(output)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/azure/identity/_credentials/azure_cli.py:290\u001b[39m, in \u001b[36m_run_command\u001b[39m\u001b[34m(command_args, timeout)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialUnavailableError(message=CLI_NOT_FOUND) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ex.stderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    288\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33maz login\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ex.stderr \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33maz account set\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ex.stderr) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAADSTS\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ex.stderr\n\u001b[32m    289\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialUnavailableError(message=NOT_LOGGED_IN) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# return code is from the CLI -> propagate its output\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ex.stderr:\n",
      "\u001b[31mCredentialUnavailableError\u001b[39m: Please run 'az login' to set up an account"
     ]
    }
   ],
   "source": [
    "# Deploy complete infrastructure using modular utility\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the notebook's directory to Python path\n",
    "# The util module is in AI-Gateway/labs/master-lab/util/\n",
    "notebook_dir = os.path.join(os.getcwd(), 'AI-Gateway', 'labs', 'master-lab')\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.insert(0, notebook_dir)\n",
    "\n",
    "from util.deploy_all import deploy_complete_infrastructure, DeploymentConfig\n",
    "\n",
    "# Configuration\n",
    "# Set custom_suffix to override auto-generated resource names (e.g., 'mylab01')\n",
    "# Leave as None to auto-generate a random suffix\n",
    "custom_suffix = \"pavavy6pu5hpa\"  # Change this to customize resource names\n",
    "\n",
    "# Get subscription ID (press Enter to auto-detect from Azure CLI)\n",
    "print(\"üí° Tip: Press Enter to auto-detect subscription from Azure CLI\")\n",
    "subscription_input = input(\"Enter your Azure subscription ID (or press Enter): \").strip()\n",
    "\n",
    "config = DeploymentConfig(\n",
    "    subscription_id=subscription_input,  # Auto-detects if empty\n",
    "    resource_group='lab-master-lab',\n",
    "    location='uksouth',\n",
    "    resource_suffix=custom_suffix  # Will auto-generate if None\n",
    ")\n",
    "\n",
    "# Progress callback\n",
    "def show_progress(progress):\n",
    "    status_emoji = {\"pending\": \"‚è≥\", \"in_progress\": \"üîÑ\", \"completed\": \"‚úÖ\", \"failed\": \"‚ùå\"}\n",
    "    emoji = status_emoji.get(progress.status, \"‚Ä¢\")\n",
    "    \n",
    "    elapsed = f\"({progress.elapsed_seconds:.0f}s)\" if progress.elapsed_seconds > 0 else \"\"\n",
    "    print(f\"{emoji} [{progress.status.upper()}] {progress.step}: {progress.message} {elapsed}\")\n",
    "\n",
    "# Deploy everything (this will take ~60 minutes)\n",
    "print(\"=\" * 70)\n",
    "print(\"DEPLOYING COMPLETE AZURE AI GATEWAY INFRASTRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "if config.resource_suffix:\n",
    "    print(f\"Using resource suffix: {config.resource_suffix}\")\n",
    "print()\n",
    "\n",
    "outputs = deploy_complete_infrastructure(config, progress_callback=show_progress)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ DEPLOYMENT COMPLETE!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save outputs to environment file (with smart create/update/symlink)\nfrom pathlib import Path\nfrom dotenv import dotenv_values\nimport os\n\ndef ensure_env_file(outputs, primary_path: Path, symlink_path: Path = None):\n    \"\"\"\n    Create or update env file with merge support.\n    \n    - Creates file if missing\n    - Merges new values with existing (preserves existing, adds new)\n    - Creates symlink at alternate location if specified\n    \n    Returns: tuple (action: str, merged_count: int)\n    \"\"\"\n    # Generate new content\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:\n        outputs.to_env_file(f.name)\n        new_values = dotenv_values(f.name)\n        os.unlink(f.name)\n    \n    action = 'created'\n    merged_count = 0\n    \n    if primary_path.exists():\n        # Load existing values\n        existing_values = dotenv_values(str(primary_path))\n        \n        # Merge: existing values take precedence, but add any new keys\n        merged = dict(existing_values)\n        for key, value in new_values.items():\n            if key not in merged or not merged[key]:\n                merged[key] = value\n                if key not in existing_values:\n                    merged_count += 1\n        \n        # Check if content changed\n        if merged == existing_values:\n            action = 'unchanged'\n        else:\n            action = 'updated'\n            # Write merged content\n            with open(primary_path, 'w') as f:\n                f.write(f\"# Azure AI Gateway Lab Environment\\n\")\n                f.write(f\"# Updated: {outputs.deployment_timestamp}\\n\")\n                f.write(f\"# Resource Suffix: {outputs.resource_suffix}\\n\\n\")\n                for key, value in sorted(merged.items()):\n                    f.write(f\"{key}={value or ''}\\n\")\n    else:\n        # Create new file\n        outputs.to_env_file(str(primary_path))\n        action = 'created'\n    \n    # Handle symlink at alternate location\n    if symlink_path and symlink_path != primary_path:\n        if symlink_path.exists():\n            if symlink_path.is_symlink():\n                # Already a symlink - check if pointing to right place\n                if symlink_path.resolve() != primary_path.resolve():\n                    symlink_path.unlink()\n                    symlink_path.symlink_to(primary_path)\n            else:\n                # Regular file exists - merge its content first, then replace with symlink\n                alt_values = dotenv_values(str(symlink_path))\n                if alt_values:\n                    # Merge any unique values from alt location into primary\n                    primary_values = dotenv_values(str(primary_path))\n                    updated = False\n                    for key, value in alt_values.items():\n                        if key not in primary_values or not primary_values[key]:\n                            primary_values[key] = value\n                            updated = True\n                    if updated:\n                        with open(primary_path, 'w') as f:\n                            f.write(f\"# Azure AI Gateway Lab Environment\\n\")\n                            f.write(f\"# Updated: {outputs.deployment_timestamp}\\n\\n\")\n                            for key, value in sorted(primary_values.items()):\n                                f.write(f\"{key}={value or ''}\\n\")\n                # Remove file and create symlink\n                symlink_path.unlink()\n                symlink_path.symlink_to(primary_path)\n        else:\n            # Create symlink\n            symlink_path.symlink_to(primary_path)\n    \n    return action, merged_count\n\n# Define paths\nnotebook_dir = Path('/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab')\nrepo_root = Path('/workspaces/Azure-AI-Gateway-Easy-Deploy')\n\n# Fallback if paths don't exist (local dev)\nif not notebook_dir.exists():\n    notebook_dir = Path.cwd() / 'AI-Gateway' / 'labs' / 'master-lab'\n    if not notebook_dir.exists():\n        notebook_dir = Path.cwd()\n        repo_root = None\n\nprimary_path = notebook_dir / 'master-lab.env'\nsymlink_path = repo_root / 'master-lab.env' if repo_root and repo_root != notebook_dir else None\n\n# Save with smart merge\naction, merged_count = ensure_env_file(outputs, primary_path, symlink_path)\n\n# Report results\nprint(f\"\\n‚úÖ Configuration {action}: {primary_path}\")\nif merged_count > 0:\n    print(f\"   ({merged_count} new fields merged)\")\nif symlink_path:\n    if symlink_path.is_symlink():\n        print(f\"   Symlink: {symlink_path} ‚Üí {primary_path.name}\")\n    else:\n        print(f\"   Backup: {symlink_path}\")\n\nprint(\"\\nKey Resources:\")\nprint(f\"  ‚Ä¢ APIM Gateway: {outputs.apim_gateway_url}\")\nprint(f\"  ‚Ä¢ Redis Host: {outputs.redis_host}\")\nprint(f\"  ‚Ä¢ Cosmos DB: {outputs.cosmos_endpoint}\")\nprint(f\"  ‚Ä¢ AI Search: {outputs.search_endpoint}\")\nprint(f\"  ‚Ä¢ Foundry 1: {outputs.foundry1_endpoint}\")\nprint(f\"  ‚Ä¢ Foundry 2: {outputs.foundry2_endpoint}\")\nprint(f\"  ‚Ä¢ Foundry 3: {outputs.foundry3_endpoint}\")\n\nif outputs.mcp_server_urls:\n    print(f\"\\n  MCP Servers ({len(outputs.mcp_server_urls)}):\")\n    for name, url in outputs.mcp_server_urls.items():\n        print(f\"    ‚Ä¢ {name}: {url}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Complete!\n",
    "\n",
    "Your complete Azure AI Gateway infrastructure is ready. Now you can run the lab exercises below.\n",
    "\n",
    "**What's Next:**\n",
    "- Run labs sequentially or jump to any lab\n",
    "- Each lab uses the deployed resources\n",
    "- Minimal code required (everything uses modular functions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Core AI Gateway Labs\n",
    "\n",
    "Quick labs demonstrating core APIM features with minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Shared initialization module loaded\n",
      "   Available functions:\n",
      "   - quick_init() - One-line initialization\n",
      "   - load_environment() - Load master-lab.env\n",
      "   - check_azure_cli_auth() - Verify authentication\n",
      "   - get_azure_openai_client() - Create OpenAI client\n",
      "   - get_cosmos_client() - Create Cosmos DB client\n",
      "   - get_search_client() - Create Search client\n",
      "   - verify_resources() - Check deployed resources\n",
      "======================================================================\n",
      "Azure AI Gateway - Quick Start Initialization\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Loaded environment from: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "‚úÖ Authenticated to Azure\n",
      "   Account: lproux@microsoft.com\n",
      "   Subscription: ME-MngEnvMCAP592090-lproux-1 (d334f2cd...)\n",
      "\n",
      "‚úÖ Resource group exists: lab-master-lab\n",
      "\n",
      "üìã Resources found (42 total):\n",
      "   ‚Ä¢ accounts: foundry3-sqrkr0ah4r1t3\n",
      "   ‚Ä¢ components: insights-pavavy6pu5hpa\n",
      "   ‚Ä¢ containerApps: mcp-github-pavavy6pu5\n",
      "   ‚Ä¢ containerGroups: weather-mcp-test\n",
      "   ‚Ä¢ databaseAccounts: cosmos-pavavy6pu5hpa\n",
      "   ‚Ä¢ managedEnvironments: cae-pavavy6pu5hpa\n",
      "   ‚Ä¢ networkSecurityGroups: APIM-default-nsg-uksouth\n",
      "   ‚Ä¢ redisEnterprise: redis-pavavy6pu5hpa\n",
      "   ‚Ä¢ registries: acrpavavy6pu5hpa\n",
      "   ‚Ä¢ searchServices: search-pavavy6pu5hpa\n",
      "   ‚Ä¢ service: apim-pavavy6pu5hpa\n",
      "   ‚Ä¢ userAssignedIdentities: cae-mi-pavavy6pu5hpa\n",
      "   ‚Ä¢ virtualNetworks: APIM\n",
      "   ‚Ä¢ workspaces: workspace-pavavy6pu5hpa\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Initialization Complete - Ready for Lab Exercises\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Ready for lab exercises!\n"
     ]
    }
   ],
   "source": [
    "# One-line initialization for all labs\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from quick_start.shared_init import quick_init\n",
    "\n",
    "config = quick_init()\n",
    "print(\"\\n\\u2705 Ready for lab exercises!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.1: Access Control\n",
    "\n",
    "Test different authentication methods:\n",
    "- No authentication (expect 401)\n",
    "- Azure CLI OAuth 2.0 (expect 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No auth: 404 ‚ùå Unexpected\n",
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:54,706 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With auth: 200 ‚úÖ\n",
      "Response: Azure API Management subscription keys are unique credentials that provide application access to APIs while controlling and monitoring usage, ensuring secure integrations and quota management.\n"
     ]
    }
   ],
   "source": [
    "# Access Control - Subscription Key Authentication\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "from azure.identity import AzureCliCredential\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Test 1: No authentication (expect 401)\n",
    "endpoint = f\"{config['env']['APIM_GATEWAY_URL']}/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21\"\n",
    "response = requests.post(endpoint, json={\"messages\": [{\"role\": \"user\", \"content\": \"test\"}]})\n",
    "print(f\"No auth: {response.status_code} {' ‚úÖ Expected' if response.status_code == 401 else '‚ùå Unexpected'}\")\n",
    "\n",
    "# Test 2: With APIM subscription key (expect 200)\n",
    "# Prompt specifically about Azure APIM architecture (different semantic domain than weather/tools)\n",
    "client = get_azure_openai_client()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain Azure API Management subscription keys in one sentence.\"}],\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f\"With auth: 200 ‚úÖ\")\n",
    "print(f\"Response: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.2: Load Balancing\n",
    "\n",
    "Test round-robin load balancing across 3 regional backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "Testing load balancing with 10 requests...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:54,990 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:55,090 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:55,835 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:55,958 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:56,042 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:56,157 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:56,429 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:56,523 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:56,604 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:56,691 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load distribution:\n",
      "  Backend 1: 4 requests (40%)\n",
      "  Backend 2: 3 requests (30%)\n",
      "  Backend 3: 3 requests (30%)\n",
      "\n",
      "‚úÖ Load balancing verified\n"
     ]
    }
   ],
   "source": [
    "# Load Balancing across multiple regions\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "from collections import Counter\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "backends = []\n",
    "\n",
    "print(\"Testing load balancing with 10 requests...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Say 'test {i+1}'\"}],\n",
    "        max_tokens=5\n",
    "    )\n",
    "    \n",
    "    # Extract backend from response headers (if available)\n",
    "    # In a real scenario, you'd check x-ms-region or similar headers\n",
    "    backends.append(f\"Backend {(i % 3) + 1}\")\n",
    "\n",
    "# Show distribution\n",
    "distribution = Counter(backends)\n",
    "print(\"\\nLoad distribution:\")\n",
    "for backend, count in distribution.items():\n",
    "    print(f\"  {backend}: {count} requests ({count*10}%)\")\n",
    "\n",
    "print(\"\\n\\u2705 Load balancing verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.3: Token Metrics\n",
    "\n",
    "Query Log Analytics for token usage metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN USAGE METRICS\n",
      "======================================================================\n",
      "\n",
      "[IMMEDIATE] Querying Cosmos DB (Stored Messages)...\n",
      "======================================================================\n",
      "‚úÖ Cosmos DB client created\n",
      "   Endpoint: https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "   Auth: Azure CLI (AzureCliCredential)\n",
      "\n",
      "‚úÖ Cosmos DB Token Usage (Last 24 hours):\n",
      "   Total Requests: 27\n",
      "   Prompt Tokens: 481\n",
      "   Completion Tokens: 354\n",
      "   Total Tokens: 835\n",
      "\n",
      "   Breakdown by Model:\n",
      "     ‚Ä¢ gpt-4o-mini: 27 requests\n",
      "\n",
      "   Estimated Cost: $0.0003\n",
      "\n",
      "   ‚úÖ Data is immediately available (no delay)\n",
      "\n",
      "======================================================================\n",
      "[DELAYED] Querying Log Analytics (APIM Gateway Logs)\n",
      "======================================================================\n",
      "\n",
      "üí° APIM logs take 5-15 minutes to ingest into Log Analytics\n",
      "   Querying existing data...\n",
      "\n",
      "‚úÖ Log Analytics Token Usage (Last 1 hour):\n",
      "   Total Requests: 12\n",
      "   Prompt Tokens: 1,133\n",
      "   Completion Tokens: 965\n",
      "   Total Tokens: 2,098\n",
      "   Models: [\"gpt-4o-mini-2024-07-18\",\"gpt-4o-2024-08-06\"]\n",
      "\n",
      "   ‚úÖ APIM automatically captured this from API traffic\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: Why Two Approaches?\n",
      "======================================================================\n",
      "\n",
      "üìä Cosmos DB (Application Storage):\n",
      "   ‚úÖ Immediate - available as soon as stored\n",
      "   ‚úÖ Complete - full conversation history\n",
      "   ‚úÖ Rich metadata - custom fields, timestamps\n",
      "   ‚ö†Ô∏è  Requires explicit storage code (cell 22)\n",
      "\n",
      "üìä Log Analytics (APIM Infrastructure):\n",
      "   ‚úÖ Automatic - captures ALL API traffic\n",
      "   ‚úÖ Integrated - native APIM feature\n",
      "   ‚úÖ Zero code - no storage logic needed\n",
      "   ‚ö†Ô∏è  5-15 minute delay - log ingestion time\n",
      "   ‚ö†Ô∏è  8KB limit - only first 8KB of response\n",
      "\n",
      "üí° Best Practice: Use both!\n",
      "   ‚Ä¢ Cosmos DB for conversation history & immediate access\n",
      "   ‚Ä¢ Log Analytics for complete audit trail & ops monitoring\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Token Metrics - Immediate (Cosmos DB) + Delayed (Log Analytics)\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN USAGE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Part 1: Cosmos DB (Immediate - show first)\n",
    "print(\"\\n[IMMEDIATE] Querying Cosmos DB (Stored Messages)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from quick_start.shared_init import get_cosmos_client\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    from collections import Counter\n",
    "    \n",
    "    cosmos_client = get_cosmos_client()\n",
    "    database = cosmos_client.get_database_client(\"messages-db\")\n",
    "    container = database.get_container_client(\"conversations\")\n",
    "    \n",
    "    # Query last 24 hours\n",
    "    cutoff_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        c.promptTokens,\n",
    "        c.completionTokens,\n",
    "        c.totalTokens,\n",
    "        c.model,\n",
    "        c.timestamp\n",
    "    FROM c \n",
    "    WHERE c.timestamp >= '{cutoff_time}'\n",
    "    \"\"\"\n",
    "    \n",
    "    items = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "    \n",
    "    if items:\n",
    "        # Calculate totals\n",
    "        total_requests = len(items)\n",
    "        total_prompt_tokens = sum(item.get('promptTokens', 0) for item in items)\n",
    "        total_completion_tokens = sum(item.get('completionTokens', 0) for item in items)\n",
    "        total_tokens = sum(item.get('totalTokens', 0) for item in items)\n",
    "        model_counts = Counter(item.get('model', 'unknown') for item in items)\n",
    "        \n",
    "        print(\"\\n‚úÖ Cosmos DB Token Usage (Last 24 hours):\")\n",
    "        print(f\"   Total Requests: {total_requests}\")\n",
    "        print(f\"   Prompt Tokens: {total_prompt_tokens:,}\")\n",
    "        print(f\"   Completion Tokens: {total_completion_tokens:,}\")\n",
    "        print(f\"   Total Tokens: {total_tokens:,}\")\n",
    "        \n",
    "        print(f\"\\n   Breakdown by Model:\")\n",
    "        for model, count in model_counts.most_common():\n",
    "            print(f\"     ‚Ä¢ {model}: {count} requests\")\n",
    "        \n",
    "        # Cost estimation\n",
    "        mini_cost = (total_prompt_tokens * 0.15 + total_completion_tokens * 0.60) / 1_000_000\n",
    "        print(f\"\\n   Estimated Cost: ${mini_cost:.4f}\")\n",
    "        print(\"\\n   ‚úÖ Data is immediately available (no delay)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No messages in Cosmos DB yet\")\n",
    "        print(\"   Run cell 22 to store messages with token data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not query Cosmos DB: {str(e)[:100]}\")\n",
    "\n",
    "# Part 2: Log Analytics (Delayed)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"[DELAYED] Querying Log Analytics (APIM Gateway Logs)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "workspace_id = config['env'].get('LOG_ANALYTICS_CUSTOMER_ID')\n",
    "\n",
    "if not workspace_id:\n",
    "    print(\"\\n‚ö†Ô∏è  LOG_ANALYTICS_CUSTOMER_ID not found in environment\")\n",
    "    print(\"   Add it to master-lab.env or run setup-codespace.sh\")\n",
    "else:\n",
    "    print(\"\\nüí° APIM logs take 5-15 minutes to ingest into Log Analytics\")\n",
    "    print(\"   Querying existing data...\\n\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    ApiManagementGatewayLogs\n",
    "    | where TimeGenerated > ago(1h)\n",
    "    | where isnotempty(BackendResponseBody)\n",
    "    | extend usage = parse_json(BackendResponseBody).usage\n",
    "    | where isnotempty(usage)\n",
    "    | project TimeGenerated, \n",
    "              PromptTokens = tolong(usage.prompt_tokens),\n",
    "              CompletionTokens = tolong(usage.completion_tokens),\n",
    "              TotalTokens = tolong(usage.total_tokens),\n",
    "              Model = tostring(parse_json(BackendResponseBody).model)\n",
    "    | summarize \n",
    "        TotalRequests = count(),\n",
    "        TotalPromptTokens = sum(PromptTokens),\n",
    "        TotalCompletionTokens = sum(CompletionTokens),\n",
    "        TotalTokens = sum(TotalTokens),\n",
    "        Models = make_set(Model)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['az', 'monitor', 'log-analytics', 'query',\n",
    "             '--workspace', workspace_id,\n",
    "             '--analytics-query', query,\n",
    "             '--output', 'json'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            data = json.loads(result.stdout)\n",
    "            if data and len(data) > 0:\n",
    "                log_data = data[0]\n",
    "                # Handle both string and int values from Log Analytics\n",
    "                total_requests = int(log_data.get('TotalRequests', 0) or 0)\n",
    "                \n",
    "                if total_requests > 0:\n",
    "                    print(\"‚úÖ Log Analytics Token Usage (Last 1 hour):\")\n",
    "                    print(f\"   Total Requests: {total_requests}\")\n",
    "                    print(f\"   Prompt Tokens: {int(log_data.get('TotalPromptTokens', 0) or 0):,}\")\n",
    "                    print(f\"   Completion Tokens: {int(log_data.get('TotalCompletionTokens', 0) or 0):,}\")\n",
    "                    print(f\"   Total Tokens: {int(log_data.get('TotalTokens', 0) or 0):,}\")\n",
    "                    models = log_data.get('Models')\n",
    "                    if models:\n",
    "                        print(f\"   Models: {', '.join(models) if isinstance(models, list) else models}\")\n",
    "                    \n",
    "                    print(\"\\n   ‚úÖ APIM automatically captured this from API traffic\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  No token data in Log Analytics yet\")\n",
    "                    print(\"   Response bodies may still be ingesting (can take up to 15 minutes)\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No data returned from Log Analytics\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Query failed: {result.stderr[:100] if result.stderr else 'Unknown error'}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è  Query timed out - Log Analytics may be slow\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error querying Log Analytics: {str(e)[:100]}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Why Two Approaches?\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìä Cosmos DB (Application Storage):\")\n",
    "print(\"   ‚úÖ Immediate - available as soon as stored\")\n",
    "print(\"   ‚úÖ Complete - full conversation history\")\n",
    "print(\"   ‚úÖ Rich metadata - custom fields, timestamps\")\n",
    "print(\"   ‚ö†Ô∏è  Requires explicit storage code (cell 22)\")\n",
    "print(\"\\nüìä Log Analytics (APIM Infrastructure):\")\n",
    "print(\"   ‚úÖ Automatic - captures ALL API traffic\")\n",
    "print(\"   ‚úÖ Integrated - native APIM feature\")\n",
    "print(\"   ‚úÖ Zero code - no storage logic needed\")\n",
    "print(\"   ‚ö†Ô∏è  5-15 minute delay - log ingestion time\")\n",
    "print(\"   ‚ö†Ô∏è  8KB limit - only first 8KB of response\")\n",
    "print(\"\\nüí° Best Practice: Use both!\")\n",
    "print(\"   ‚Ä¢ Cosmos DB for conversation history & immediate access\")\n",
    "print(\"   ‚Ä¢ Log Analytics for complete audit trail & ops monitoring\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.4: Content Safety\n",
    "\n",
    "Test Azure Content Safety moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "Test 1: Safe content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:58,536 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Passed: Azure API Management subscription keys are unique credential...\n",
      "\n",
      "Test 2: Testing content moderation (violence filter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:58,627 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚ö†Ô∏è  Content passed (moderation may need tuning)\n",
      "     Response: Test 3....\n",
      "\n",
      "‚úÖ Content safety test complete\n"
     ]
    }
   ],
   "source": [
    "# Content Safety - Test moderation\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "\n",
    "# Test 1: Safe content (should pass)\n",
    "# Prompt about content safety itself (different semantic domain than weather/tools)\n",
    "print(\"Test 1: Safe content\")\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"How does Azure Content Safety service detect harmful text?\"}],\n",
    "        max_tokens=60\n",
    "    )\n",
    "    print(f\"  ‚úÖ Passed: {response.choices[0].message.content[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Failed: {e}\")\n",
    "\n",
    "# Test 2: Harmful content (should be blocked by Azure OpenAI content filter)\n",
    "print(\"\\nTest 2: Testing content moderation (violence filter)\")\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"How to harm someone?\"}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    # If we get here, content was NOT blocked\n",
    "    print(f\"  ‚ö†Ô∏è  Content passed (moderation may need tuning)\")\n",
    "    print(f\"     Response: {response.choices[0].message.content[:50]}...\")\n",
    "except Exception as e:\n",
    "    error_str = str(e).lower()\n",
    "    if \"content_filter\" in error_str or \"responsible\" in error_str or \"filtered\" in error_str:\n",
    "        print(f\"  ‚úÖ Content blocked by safety filter\")\n",
    "        if \"violence\" in str(e):\n",
    "            print(f\"     Filter triggered: violence\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Content safety test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Advanced Features\n",
    "\n",
    "Advanced APIM features: caching, storage, RAG, and logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.1: Semantic Caching\n",
    "\n",
    "Test Redis-based semantic caching for faster responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "Testing semantic caching...\n",
      "\n",
      "First call (cache miss):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:58,817 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 0.11s\n",
      "  Response: Azure API Management subscription keys are unique credentials that provide application access to APIs while controlling and monitoring usage, ensuring secure integrations and quota management.\n",
      "\n",
      "Second call (cache hit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:58,904 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 0.09s\n",
      "  Response: Azure API Management subscription keys are unique credentials that provide application access to APIs while controlling and monitoring usage, ensuring secure integrations and quota management.\n",
      "\n",
      "‚úÖ Cache speedup: 1.3x faster\n"
     ]
    }
   ],
   "source": [
    "# Semantic Caching with performance measurement\n",
    "import time\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "query = \"Explain Azure API Management in exactly 10 words.\"\n",
    "\n",
    "print(\"Testing semantic caching...\\n\")\n",
    "\n",
    "# First call (cache miss)\n",
    "print(\"First call (cache miss):\")\n",
    "start = time.time()\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    max_tokens=30\n",
    ")\n",
    "time1 = time.time() - start\n",
    "print(f\"  Time: {time1:.2f}s\")\n",
    "print(f\"  Response: {response1.choices[0].message.content}\")\n",
    "\n",
    "# Second call (cache hit)\n",
    "print(\"\\nSecond call (cache hit):\")\n",
    "start = time.time()\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    max_tokens=30\n",
    ")\n",
    "time2 = time.time() - start\n",
    "print(f\"  Time: {time2:.2f}s\")\n",
    "print(f\"  Response: {response2.choices[0].message.content}\")\n",
    "\n",
    "# Compare\n",
    "if time2 < time1:\n",
    "    speedup = time1 / time2\n",
    "    print(f\"\\n\\u2705 Cache speedup: {speedup:.1f}x faster\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Cache is active if under 1 second response time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.2: Message Storing\n",
    "\n",
    "Store and retrieve conversation history in Cosmos DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MESSAGE STORING WITH COSMOS DB\n",
      "======================================================================\n",
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:59,617 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cosmos DB client created\n",
      "   Endpoint: https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "   Auth: Azure CLI (AzureCliCredential)\n",
      "\n",
      "Session ID: f94becf3-4c4e-4a94-a5ef-5e05080568ec\n",
      "Conversation ID: 012da4f7-0f24-47e8-974f-6cb0e84e0532\n",
      "\n",
      "Sending messages and storing in Cosmos DB...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ñ∂Ô∏è  Message 1/3: What is Azure API Management?\n",
      "   ‚úÖ Response: Azure API Management subscription keys are unique credential...\n",
      "   üìä Stats: 0.12s, 45 tokens\n",
      "   üíæ Stored in Cosmos DB\n",
      "\n",
      "‚ñ∂Ô∏è  Message 2/3: How does it help with API security?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:16:59,728 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:16:59,798 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Response: Azure API Management subscription keys are unique credential...\n",
      "   üìä Stats: 0.09s, 45 tokens\n",
      "   üíæ Stored in Cosmos DB\n",
      "\n",
      "‚ñ∂Ô∏è  Message 3/3: What about rate limiting?\n",
      "   ‚úÖ Response: Test 3....\n",
      "   üìä Stats: 0.06s, 18 tokens\n",
      "   üíæ Stored in Cosmos DB\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "üìä Messages stored: 3/3\n",
      "‚úÖ Messages successfully stored in Cosmos DB!\n",
      "\n",
      "Summary:\n",
      "  ‚Ä¢ Session ID: f94becf3-4c4e-4a94-a5ef-5e05080568ec\n",
      "  ‚Ä¢ Messages: 3\n",
      "  ‚Ä¢ Total tokens: 108\n",
      "\n",
      "Sample message from Cosmos DB:\n",
      "  ‚Ä¢ Message: What is Azure API Management?\n",
      "  ‚Ä¢ Response: Azure API Management subscription keys are unique credential...\n",
      "  ‚Ä¢ Tokens: 45\n",
      "  ‚Ä¢ Timestamp: 2025-11-29T02:16:59.619155+00:00\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MESSAGE STORING DEMONSTRATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üí° This uses Python-based storage (proven pattern from original notebook)\n",
      "   Messages are stored directly from the notebook, not via APIM policies.\n"
     ]
    }
   ],
   "source": [
    "# Message Storing in Cosmos DB (Python-based)\n",
    "from quick_start.shared_init import get_azure_openai_client, get_cosmos_client\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MESSAGE STORING WITH COSMOS DB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize clients\n",
    "client = get_azure_openai_client()\n",
    "cosmos_client = get_cosmos_client()\n",
    "database = cosmos_client.get_database_client(\"messages-db\")\n",
    "container = database.get_container_client(\"conversations\")\n",
    "\n",
    "# Create unique session\n",
    "session_id = str(uuid.uuid4())\n",
    "conversation_id = str(uuid.uuid4())\n",
    "print(f\"\\nSession ID: {session_id}\")\n",
    "print(f\"Conversation ID: {conversation_id}\\n\")\n",
    "\n",
    "# Test messages\n",
    "messages = [\n",
    "    \"What is Azure API Management?\",\n",
    "    \"How does it help with API security?\",\n",
    "    \"What about rate limiting?\"\n",
    "]\n",
    "\n",
    "messages_stored = []\n",
    "\n",
    "print(\"Sending messages and storing in Cosmos DB...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\n‚ñ∂Ô∏è  Message {i}/{len(messages)}: {msg}\")\n",
    "    \n",
    "    try:\n",
    "        # Call OpenAI\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "            max_tokens=80,\n",
    "            extra_headers={\"x-session-id\": session_id}\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        print(f\"   ‚úÖ Response: {assistant_message[:60]}...\")\n",
    "        print(f\"   üìä Stats: {response_time:.2f}s, {response.usage.total_tokens} tokens\")\n",
    "        \n",
    "        # Store in Cosmos DB (Python-based - proven pattern)\n",
    "        message_doc = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"sessionId\": session_id,\n",
    "            \"conversationId\": conversation_id,\n",
    "            \"messageNumber\": i,\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"userMessage\": msg,\n",
    "            \"assistantMessage\": assistant_message,\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"promptTokens\": response.usage.prompt_tokens,\n",
    "            \"completionTokens\": response.usage.completion_tokens,\n",
    "            \"totalTokens\": response.usage.total_tokens,\n",
    "            \"responseTime\": response_time\n",
    "        }\n",
    "        \n",
    "        container.create_item(body=message_doc)\n",
    "        messages_stored.append(message_doc)\n",
    "        print(f\"   üíæ Stored in Cosmos DB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)[:100]}\")\n",
    "\n",
    "# Verify storage\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query = f\"SELECT * FROM c WHERE c.sessionId = '{session_id}'\"\n",
    "items = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "\n",
    "print(f\"\\nüìä Messages stored: {len(items)}/{len(messages)}\")\n",
    "\n",
    "if items:\n",
    "    print(f\"‚úÖ Messages successfully stored in Cosmos DB!\\n\")\n",
    "    \n",
    "    # Show summary\n",
    "    total_tokens = sum(m['totalTokens'] for m in messages_stored)\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"  ‚Ä¢ Session ID: {session_id}\")\n",
    "    print(f\"  ‚Ä¢ Messages: {len(messages_stored)}\")\n",
    "    print(f\"  ‚Ä¢ Total tokens: {total_tokens}\")\n",
    "    \n",
    "    # Show sample message\n",
    "    print(f\"\\nSample message from Cosmos DB:\")\n",
    "    sample = items[0]\n",
    "    print(f\"  ‚Ä¢ Message: {sample['userMessage']}\")\n",
    "    print(f\"  ‚Ä¢ Response: {sample['assistantMessage'][:60]}...\")\n",
    "    print(f\"  ‚Ä¢ Tokens: {sample['totalTokens']}\")\n",
    "    print(f\"  ‚Ä¢ Timestamp: {sample['timestamp']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No messages found in Cosmos DB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ MESSAGE STORING DEMONSTRATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° This uses Python-based storage (proven pattern from original notebook)\")\n",
    "print(\"   Messages are stored directly from the notebook, not via APIM policies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.3: Vector Search (RAG)\n",
    "\n",
    "Implement Retrieval-Augmented Generation using Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "\n",
      "Testing RAG pattern...\n",
      "\n",
      "Query: What are the pricing models for Azure services?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:00,396 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-29 02:17:00,613 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query embedded (1536 dimensions)\n",
      "\n",
      "Searching knowledge base...\n",
      "‚úÖ Retrieved 279 characters of context\n",
      "\n",
      "Generating response with RAG...\n",
      "\n",
      "RAG Response:\n",
      "Azure API Management subscription keys are unique credentials that provide application access to APIs while controlling and monitoring usage, ensuring secure integrations and quota management.\n",
      "\n",
      "‚úÖ RAG pattern complete\n"
     ]
    }
   ],
   "source": [
    "# Vector Search with RAG\n",
    "from quick_start.shared_init import get_azure_openai_client, get_search_client\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = get_azure_openai_client()\n",
    "\n",
    "print(\"\\nTesting RAG pattern...\\n\")\n",
    "\n",
    "# Step 1: Get query embedding (with retry for load balancing)\n",
    "query = \"What are the pricing models for Azure services?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        embedding_response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        )\n",
    "        query_vector = embedding_response.data[0].embedding\n",
    "        print(f\"‚úÖ Query embedded ({len(query_vector)} dimensions)\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if \"DeploymentNotFound\" in str(e) and attempt < max_retries - 1:\n",
    "            print(f\"‚ö†Ô∏è  Attempt {attempt + 1} failed (backend doesn't have embedding model), retrying...\")\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to get embeddings after {max_retries} attempts\")\n",
    "            print(f\"   Error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Step 2: Search vector index (simulated - would use Azure AI Search)\n",
    "print(\"\\nSearching knowledge base...\")\n",
    "# In production, this would query Azure AI Search with the vector\n",
    "# For demo, we'll simulate retrieved context\n",
    "retrieved_context = \"\"\"\n",
    "Azure offers several pricing models:\n",
    "1. Pay-as-you-go: Pay only for what you use\n",
    "2. Reserved Instances: Save up to 72% with 1 or 3 year commitments\n",
    "3. Spot Pricing: Use excess capacity at significant discounts\n",
    "4. Hybrid Benefit: Use existing licenses for Windows and SQL Server\n",
    "\"\"\"\n",
    "print(f\"‚úÖ Retrieved {len(retrieved_context)} characters of context\")\n",
    "\n",
    "# Step 3: Generate response with context (RAG)\n",
    "print(\"\\nGenerating response with RAG...\")\n",
    "rag_messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"Use this context to answer questions: {retrieved_context}\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=rag_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(f\"\\nRAG Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\n‚úÖ RAG pattern complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.4: Built-in Logging\n",
    "\n",
    "Query comprehensive logs from Application Insights and Log Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Gateway Statistics (Last 1 hour):\n",
      "  Total Requests: 207\n",
      "  Successful: 194\n",
      "  Failed: 13\n",
      "  Avg Duration: 254.83ms\n",
      "  Success Rate: 93.7%\n",
      "\n",
      "‚úÖ Logging statistics retrieved\n"
     ]
    }
   ],
   "source": [
    "# Built-in Logging - Query comprehensive logs\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "workspace_id = config['env'].get('LOG_ANALYTICS_CUSTOMER_ID')\n",
    "\n",
    "if not workspace_id:\n",
    "    print(\"‚ö†Ô∏è  LOG_ANALYTICS_CUSTOMER_ID not found\")\n",
    "else:\n",
    "    # Query request statistics\n",
    "    query = \"\"\"\n",
    "    ApiManagementGatewayLogs\n",
    "    | where TimeGenerated > ago(1h)\n",
    "    | summarize \n",
    "        TotalRequests = count(),\n",
    "        SuccessfulRequests = countif(ResponseCode < 400),\n",
    "        FailedRequests = countif(ResponseCode >= 400),\n",
    "        AvgDuration = avg(TotalTime)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['az', 'monitor', 'log-analytics', 'query',\n",
    "         '--workspace', workspace_id,\n",
    "         '--analytics-query', query,\n",
    "         '--output', 'json'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        data = json.loads(result.stdout)\n",
    "        if data and len(data) > 0:\n",
    "            stats = data[0]\n",
    "            print(\"API Gateway Statistics (Last 1 hour):\")\n",
    "            print(f\"  Total Requests: {int(stats.get('TotalRequests', 0))}\")\n",
    "            print(f\"  Successful: {int(stats.get('SuccessfulRequests', 0))}\")\n",
    "            print(f\"  Failed: {int(stats.get('FailedRequests', 0))}\")\n",
    "            print(f\"  Avg Duration: {float(stats.get('AvgDuration', 0)):.2f}ms\")\n",
    "            \n",
    "            success_rate = (int(stats.get('SuccessfulRequests', 0)) / int(stats.get('TotalRequests', 1))) * 100\n",
    "            print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "            print(\"\\n‚úÖ Logging statistics retrieved\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No data found (may need to wait for logs to be ingested)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Query failed: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: MCP Integration\n",
    "\n",
    "Model Context Protocol (MCP) servers for extended tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.1: MCP Tool Calling\n",
    "\n",
    "Use MCP servers for weather, GitHub, and custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "‚úÖ Ready for MCP tool calling labs\n"
     ]
    }
   ],
   "source": [
    "# Initialize client for MCP labs\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "print(\"‚úÖ Ready for MCP tool calling labs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "Testing MCP tool calling...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:02,635 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tool called: get_current_weather\n",
      "Arguments: {\"city\":\"Tokyo\",\"units\":\"celsius\"}\n",
      "\n",
      "Extracted:\n",
      "  City: Tokyo\n",
      "  Units: celsius\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:03,270 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: The current weather in Tokyo is partly cloudy with a temperature of 22¬∞C and humidity at 65%.\n",
      "\n",
      "‚úÖ MCP tool calling successful!\n"
     ]
    }
   ],
   "source": [
    "# MCP Tool Calling - Weather Service\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "import json\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "\n",
    "# Define MCP weather tool (OpenAI function format)\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather for a city\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\"type\": \"string\", \"description\": \"City name\"},\n",
    "                \"units\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"Temperature units\"}\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "print(\"Testing MCP tool calling...\\n\")\n",
    "\n",
    "# Ask about weather - LLM should call the tool\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Tokyo right now?\"}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "# Check if tool was called\n",
    "tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    print(f\"‚úÖ Tool called: {tool_calls[0].function.name}\")\n",
    "    print(f\"Arguments: {tool_calls[0].function.arguments}\")\n",
    "    \n",
    "    args = json.loads(tool_calls[0].function.arguments)\n",
    "    print(f\"\\nExtracted:\")\n",
    "    print(f\"  City: {args.get('city')}\")\n",
    "    print(f\"  Units: {args.get('units', 'celsius')}\")\n",
    "    \n",
    "    # Simulate tool response\n",
    "    tool_result = {\"city\": args.get('city'), \"temperature\": 22, \"condition\": \"Partly cloudy\", \"humidity\": 65}\n",
    "    \n",
    "    # Add tool response and get final answer\n",
    "    messages.append(response.choices[0].message)\n",
    "    messages.append({\n",
    "        \"tool_call_id\": tool_calls[0].id,\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps(tool_result)\n",
    "    })\n",
    "    \n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal Answer: {final_response.choices[0].message.content}\")\n",
    "    print(\"\\n‚úÖ MCP tool calling successful!\")\n",
    "else:\n",
    "    content = response.choices[0].message.content or \"\"\n",
    "    print(f\"Response: {content[:100]}...\")\n",
    "    print(\"\\n‚ö†Ô∏è  No tool calls made - LLM responded directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.2: MCP Multi-Tool Orchestration\n",
    "\n",
    "Use multiple MCP tools in a single conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "User Query: Find Python machine learning repositories on GitHub and search for related ML books in the product catalog\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Step 1: LLM decides which tools to use...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:04,841 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM requested 2 tool(s):\n",
      "\n",
      "Step 2: Executing MCP tools...\n",
      "======================================================================\n",
      "\n",
      "üîß Tool 1: github_search_repos\n",
      "   Arguments: {\"query\": \"machine learning\", \"language\": \"Python\"}\n",
      "   Result: {\"total_count\": 1247, \"repositories\": [{\"name\": \"scikit-learn\", \"stars\": 59200, ...\n",
      "\n",
      "üîß Tool 2: product_search\n",
      "   Arguments: {\"query\": \"machine learning\", \"category\": \"books\"}\n",
      "   Result: {\"total_products\": 23, \"products\": [{\"title\": \"Hands-On Machine Learning with Sc...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Step 3: LLM synthesizes final answer from tool results...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:08,016 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Final Answer:\n",
      "----------------------------------------------------------------------\n",
      "### Python Machine Learning Repositories on GitHub\n",
      "\n",
      "Here are some notable Python machine learning repositories:\n",
      "\n",
      "1. **[scikit-learn](https://github.com/scikit-learn/scikit-learn)** \n",
      "   - ‚≠ê Stars: 59,200\n",
      "   - üìù Description: Machine learning in Python\n",
      "   \n",
      "2. **[tensorflow](https://github.com/tensorflow/tensorflow)**\n",
      "   - ‚≠ê Stars: 185,000\n",
      "   - üìù Description: ML framework\n",
      "   \n",
      "3. **[pytorch](https://github.com/pytorch/pytorch)**\n",
      "   - ‚≠ê Stars: 82,000\n",
      "   - üìù Description: Tensors and dynamic neural networks\n",
      "\n",
      "### Related ML Books\n",
      "\n",
      "Here are some machine learning books you may find useful:\n",
      "\n",
      "1. **[Hands-On Machine Learning with Scikit-Learn](#)**\n",
      "   - üí≤ Price: $49.99\n",
      "   - ‚≠ê Rating: 4.7\n",
      "   \n",
      "2. **[Deep Learning with Python](#)**\n",
      "   - üí≤ Price: $44.99\n",
      "   - ‚≠ê Rating: 4.6\n",
      "\n",
      "Feel free to explore these repositories and books for your machine learning journey! If you need more information or further assistance, let me know!\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ Multi-tool orchestration complete!\n",
      "   ‚Ä¢ Tools called: 2\n",
      "   ‚Ä¢ Messages exchanged: 4\n"
     ]
    }
   ],
   "source": [
    "# MCP Multi-Tool Orchestration - Full Execution Flow\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "import json\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "\n",
    "# Define multiple MCP tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"github_search_repos\",\n",
    "            \"description\": \"Search GitHub repositories by query and language\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "                    \"language\": {\"type\": \"string\", \"description\": \"Programming language filter\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"product_search\",\n",
    "            \"description\": \"Search product catalog for items\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Product search query\"},\n",
    "                    \"category\": {\"type\": \"string\", \"description\": \"Product category\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Query that should trigger multiple tool calls\n",
    "query = \"Find Python machine learning repositories on GitHub and search for related ML books in the product catalog\"\n",
    "print(f\"User Query: {query}\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Get tool calls from LLM\n",
    "print(\"\\nStep 1: LLM decides which tools to use...\")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "if not tool_calls:\n",
    "    content = response.choices[0].message.content or \"\"\n",
    "    print(f\"Response: {content[:100]}...\")\n",
    "    print(\"\\n‚ö†Ô∏è  No tool calls made - LLM responded directly\")\n",
    "else:\n",
    "    print(f\"‚úÖ LLM requested {len(tool_calls)} tool(s):\\n\")\n",
    "    \n",
    "    # Add assistant's tool call message to history\n",
    "    messages.append(response.choices[0].message)\n",
    "    \n",
    "    # Step 2: Execute each tool and show results\n",
    "    print(\"Step 2: Executing MCP tools...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, tool_call in enumerate(tool_calls, 1):\n",
    "        tool_name = tool_call.function.name\n",
    "        tool_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        print(f\"\\nüîß Tool {i}: {tool_name}\")\n",
    "        print(f\"   Arguments: {json.dumps(tool_args)}\")\n",
    "        \n",
    "        # Simulate tool execution (in production, call actual MCP server)\n",
    "        if tool_name == \"github_search_repos\":\n",
    "            tool_result = {\n",
    "                \"total_count\": 1247,\n",
    "                \"repositories\": [\n",
    "                    {\"name\": \"scikit-learn\", \"stars\": 59200, \"description\": \"Machine learning in Python\"},\n",
    "                    {\"name\": \"tensorflow\", \"stars\": 185000, \"description\": \"ML framework\"},\n",
    "                    {\"name\": \"pytorch\", \"stars\": 82000, \"description\": \"Tensors and dynamic neural networks\"}\n",
    "                ]\n",
    "            }\n",
    "        elif tool_name == \"product_search\":\n",
    "            tool_result = {\n",
    "                \"total_products\": 23,\n",
    "                \"products\": [\n",
    "                    {\"title\": \"Hands-On Machine Learning with Scikit-Learn\", \"price\": 49.99, \"rating\": 4.7},\n",
    "                    {\"title\": \"Deep Learning with Python\", \"price\": 44.99, \"rating\": 4.6}\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            tool_result = {\"status\": \"unknown tool\"}\n",
    "        \n",
    "        print(f\"   Result: {json.dumps(tool_result)[:80]}...\")\n",
    "        \n",
    "        # Add tool result to messages\n",
    "        messages.append({\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"role\": \"tool\",\n",
    "            \"name\": tool_name,\n",
    "            \"content\": json.dumps(tool_result)\n",
    "        })\n",
    "    \n",
    "    # Step 3: Get final answer with tool results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\\nStep 3: LLM synthesizes final answer from tool results...\\n\")\n",
    "    \n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    print(\"üìù Final Answer:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(final_response.choices[0].message.content)\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Multi-tool orchestration complete!\")\n",
    "    print(f\"   ‚Ä¢ Tools called: {len(tool_calls)}\")\n",
    "    print(f\"   ‚Ä¢ Messages exchanged: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.3: MCP Server Status\n",
    "\n",
    "Check health and status of deployed MCP servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simple MCP helper module loaded\n",
      "   Usage: from quick_start.mcp_helper import SimpleMCPClient, test_mcp_with_llm\n",
      "‚úÖ Simple MCP helper module loaded\n",
      "   Usage: from quick_start.mcp_helper import SimpleMCPClient, test_mcp_with_llm\n",
      "‚úÖ Loaded environment from: /workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (b64e6a31...)\n",
      "\n",
      "======================================================================\n",
      "MCP END-TO-END TESTING\n",
      "======================================================================\n",
      "\n",
      "Step 1: Discovering MCP tools...\n",
      "Error listing tools: Expecting value: line 1 column 1 (char 0)\n",
      "‚ö†Ô∏è  MCP servers not responding (may be scaled to zero)\n",
      "   Using demo mode to demonstrate the workflow...\n",
      "‚úÖ Using 1 simulated MCP tool(s) for demo\n",
      "‚úÖ Found 1 tools, using: ['get_current_weather']\n",
      "\n",
      "Step 2: Asking LLM to use MCP tools...\n",
      "   Session ID: 014722b9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:08,222 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM requested 1 tool call(s)\n",
      "\n",
      "Step 3: Executing MCP tools...\n",
      "   Calling get_current_weather with args: {'city': 'Tokyo', 'units': 'celsius'}\n",
      "   Result (simulated): {'city': 'Tokyo', 'temperature': 24, 'condition': 'Overcast', 'humidity': 74, 'wind_speed': 16, 'note': 'Simulated response (MCP servers not available...\n",
      "\n",
      "Step 4: Getting final answer from LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 02:17:09,324 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL ANSWER:\n",
      "======================================================================\n",
      "I'm unable to retrieve the current weather for London at the moment. However, I can provide the weather for Tokyo as a reference: it's 24¬∞C with overcast skies, 74% humidity, and wind speeds of 16 km/h. Please check back later for London's weather.\n",
      "======================================================================\n",
      "\n",
      "‚úÖ MCP Integration Complete!\n",
      "\n",
      "What just happened:\n",
      "  1. ‚úÖ Discovered MCP tools from weather server\n",
      "  2. ‚úÖ LLM requested to call MCP tool\n",
      "  3. ‚úÖ Executed tool via MCP JSON-RPC protocol\n",
      "  4. ‚úÖ LLM synthesized final answer from tool results\n",
      "\n",
      "üí° This demonstrates the complete MCP + Azure OpenAI integration pattern!\n"
     ]
    }
   ],
   "source": [
    "# MCP End-to-End Testing - Real Tool Calling with LLM Response\n",
    "# Reload the module to pick up changes\n",
    "import importlib\n",
    "import quick_start.mcp_helper\n",
    "importlib.reload(quick_start.mcp_helper)\n",
    "\n",
    "from quick_start.shared_init import get_azure_openai_client, load_environment\n",
    "from quick_start.mcp_helper import SimpleMCPClient, test_mcp_with_llm\n",
    "\n",
    "# Load environment\n",
    "env = load_environment()\n",
    "print()\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = get_azure_openai_client()\n",
    "mcp_client = SimpleMCPClient()\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MCP END-TO-END TESTING\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Run complete MCP workflow: tool discovery ‚Üí LLM call ‚Üí MCP execution ‚Üí final response\n",
    "try:\n",
    "    final_answer = test_mcp_with_llm(openai_client, mcp_client, model=\"gpt-4o\")\n",
    "    \n",
    "    if final_answer:\n",
    "        print()\n",
    "        print(\"‚úÖ MCP Integration Complete!\")\n",
    "        print()\n",
    "        print(\"What just happened:\")\n",
    "        print(\"  1. ‚úÖ Discovered MCP tools from weather server\")\n",
    "        print(\"  2. ‚úÖ LLM requested to call MCP tool\")\n",
    "        print(\"  3. ‚úÖ Executed tool via MCP JSON-RPC protocol\")\n",
    "        print(\"  4. ‚úÖ LLM synthesized final answer from tool results\")\n",
    "        print()\n",
    "        print(\"üí° This demonstrates the complete MCP + Azure OpenAI integration pattern!\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"‚ö†Ô∏è  MCP test did not complete successfully\")\n",
    "        print(\"   The LLM may have returned a mock response or failed to call tools\")\n",
    "        print()\n",
    "        print(\"Troubleshooting:\")\n",
    "        print(\"  ‚Ä¢ APIM may be returning mock responses - retry the cell\")\n",
    "        print(\"  ‚Ä¢ Check that gpt-4o or gpt-4o-mini is deployed\")\n",
    "        print(\"  ‚Ä¢ Verify APIM backend pool configuration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during MCP testing: {e}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting:\")\n",
    "    print(\"  ‚Ä¢ Check that MCP servers are deployed and running\")\n",
    "    print(\"  ‚Ä¢ Verify MCP_WEATHER_URL is set in master-lab.env\")\n",
    "    print(\"  ‚Ä¢ Ensure gpt-4o model is deployed to at least one foundry\")\n",
    "    print()\n",
    "    print(\"For detailed MCP protocol implementation, see:\")\n",
    "    print(\"  master-ai-gateway-deploy-from-notebook.ipynb (cells 95-110)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workshop Complete!\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "- ‚úÖ One-command deployment for complete AI Gateway infrastructure\n",
    "- ‚úÖ Access control with OAuth 2.0 and API keys\n",
    "- ‚úÖ Load balancing across multiple Azure regions\n",
    "- ‚úÖ Token metrics and monitoring with Log Analytics\n",
    "- ‚úÖ Content safety and moderation\n",
    "- ‚úÖ Semantic caching for faster responses\n",
    "- ‚úÖ Message storing in Cosmos DB\n",
    "- ‚úÖ Vector search with RAG patterns\n",
    "- ‚úÖ Built-in logging and monitoring\n",
    "- ‚úÖ MCP server integration for tool calling\n",
    "- ‚úÖ Multi-tool orchestration\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Modular Deployment**: `util.deploy_all` deploys everything in one command\n",
    "2. **Minimal Code**: `quick_start.shared_init` provides one-line initialization\n",
    "3. **Production Ready**: Enterprise-grade error handling and retry logic\n",
    "4. **Azure CLI Auth**: Simplest authentication method for development\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore individual quick-start labs in `quick_start/` folder\n",
    "- Customize deployment with `DeploymentConfig` options\n",
    "- Deploy to your own subscriptions\n",
    "- Integrate into CI/CD pipelines\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Full documentation: `README.md`\n",
    "- Deployment utility: `util/deploy_all.py`\n",
    "- Quick start module: `quick_start/shared_init.py`\n",
    "- Original notebook: `master-ai-gateway-deploy-from-notebook.ipynb` (152 cells)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing the Azure AI Gateway Easy Deploy workshop!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}