{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Gateway - Easy Deploy\n",
    "\n",
    "> **One-command deployment** for complete Azure AI Gateway infrastructure with 7 comprehensive labs.\n",
    "\n",
    "## What's Different\n",
    "\n",
    "This notebook uses **modular deployment utilities** for minimal code:\n",
    "- **Deployment**: `util.deploy_all.py` - Deploy everything in one command\n",
    "- **Initialization**: `quick_start.shared_init.py` - One-line setup\n",
    "- **Labs**: Focused exercises with minimal boilerplate\n",
    "\n",
    "**Original notebook**: 152 cells  \n",
    "**This notebook**: ~28 cells (82% reduction)\n",
    "\n",
    "## What Gets Deployed\n",
    "\n",
    "- **Core**: APIM, Log Analytics, Application Insights\n",
    "- **AI Foundry**: 3 regions with 6 model deployments\n",
    "- **Supporting**: Redis, Cosmos DB, Azure AI Search\n",
    "- **MCP**: 5 MCP servers in Container Apps\n",
    "\n",
    "**Total time**: ~60 minutes\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Azure subscription with Contributor role\n",
    "2. Azure CLI installed and authenticated (`az login`)\n",
    "3. Python 3.11+ with dependencies installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codespaces / Dev Container Setup\n",
    "\n",
    "> **Run this section first** if you're using GitHub Codespaces or a Dev Container.\n",
    "\n",
    "This will:\n",
    "1. Install required Python dependencies\n",
    "2. Check Azure CLI authentication\n",
    "3. Configure Cosmos DB firewall for your IP\n",
    "4. Add any missing environment variables\n",
    "\n",
    "**Skip this section** if you're running locally with dependencies already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Codespaces setup script (installs dependencies, configures Cosmos DB firewall)\n",
    "# This uses Jupyter's shell magic command (!) to run bash scripts\n",
    "# Skip this cell if running locally with dependencies already installed\n",
    "\n",
    "#!cd /workspaces/Azure-AI-Gateway-Easy-Deploy && chmod +x setup-codespace.sh && ./setup-codespace.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 0: One-Command Deployment\n",
    "\n",
    "Deploy complete infrastructure in a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dependencies and attempt installation if needed\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "print(\"Checking dependencies...\")\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "# This works whether run from repo root or notebook directory\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# Check common locations for the requirements file\n",
    "possible_paths = [\n",
    "    os.path.join(notebook_dir, \"AI-Gateway\", \"labs\", \"master-lab\", \"requirements.txt\"),\n",
    "    os.path.join(notebook_dir, \"requirements.txt\"),\n",
    "    \"/workspaces/Azure-AI-Gateway-Easy-Deploy/AI-Gateway/labs/master-lab/requirements.txt\",\n",
    "]\n",
    "requirements_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        requirements_path = path\n",
    "        break\n",
    "\n",
    "# Key packages required for this notebook\n",
    "required_packages = {\n",
    "    'dotenv': 'python-dotenv',\n",
    "    'azure.identity': 'azure-identity',\n",
    "    'azure.mgmt.resource': 'azure-mgmt-resource',\n",
    "    'azure.cosmos': 'azure-cosmos',\n",
    "    'openai': 'openai',\n",
    "    'requests': 'requests'\n",
    "}\n",
    "\n",
    "# Check which packages are already available\n",
    "missing_packages = []\n",
    "available_packages = []\n",
    "\n",
    "for module_name, package_name in required_packages.items():\n",
    "    if importlib.util.find_spec(module_name.split('.')[0]) is not None:\n",
    "        available_packages.append(package_name)\n",
    "    else:\n",
    "        missing_packages.append(package_name)\n",
    "\n",
    "if not missing_packages:\n",
    "    print(\"‚úÖ All required packages are already available\")\n",
    "    print(f\"   Found: {', '.join(available_packages[:3])} and {len(available_packages)-3} more\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Missing packages: {', '.join(missing_packages)}\")\n",
    "    \n",
    "    if not requirements_path:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not find requirements.txt\")\n",
    "        print(f\"   Searched: {possible_paths}\")\n",
    "        print(f\"\\n   Installing missing packages directly...\")\n",
    "        try:\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                \"--user\", \"-q\"\n",
    "            ] + missing_packages)\n",
    "            print(\"‚úÖ Dependencies installed\")\n",
    "            print(\"   ‚ö†Ô∏è  Please restart the kernel to use the updated packages.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  Installation failed: {e}\")\n",
    "    else:\n",
    "        print(f\"   Using requirements from: {requirements_path}\")\n",
    "        \n",
    "        # Check if we're in a virtual environment\n",
    "        in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)\n",
    "        \n",
    "        if not in_venv:\n",
    "            print(\"\\n‚ö†Ô∏è  Not in a virtual environment\")\n",
    "            print(\"   This system uses externally-managed Python packages.\")\n",
    "            print()\n",
    "            print(\"   Recommended options:\")\n",
    "            print(\"   1. Use the dev container (already has everything installed)\")\n",
    "            print(\"   2. Create a virtual environment:\")\n",
    "            print(\"      python -m venv .venv\")\n",
    "            print(\"      source .venv/bin/activate  # On Linux/Mac\")\n",
    "            print(\"      .venv\\\\Scripts\\\\activate     # On Windows\")\n",
    "            print()\n",
    "            \n",
    "            # Try to install with --user flag as fallback\n",
    "            print(\"   Attempting installation to user directory...\")\n",
    "            try:\n",
    "                subprocess.check_call([\n",
    "                    sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                    \"--user\", \"-q\", \"-r\", requirements_path\n",
    "                ])\n",
    "                print(\"‚úÖ Dependencies installed to user directory\")\n",
    "                print(\"   ‚ö†Ô∏è  Please restart the kernel to use the updated packages.\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"‚ö†Ô∏è  Installation failed (system Python is locked down)\")\n",
    "                print()\n",
    "                print(\"   Packages may already be installed via system package manager (apt).\")\n",
    "                print(\"   The notebook will attempt to continue - if you encounter import errors,\")\n",
    "                print(\"   please install manually:\")\n",
    "                print(\"   ‚Ä¢ Create a virtual environment: python -m venv .venv && source .venv/bin/activate\")\n",
    "                print(f\"   ‚Ä¢ Then run: pip install -r {requirements_path}\")\n",
    "        else:\n",
    "            # In virtual environment - proceed normally\n",
    "            print(\"‚úÖ Running in virtual environment\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", requirements_path])\n",
    "                print(\"‚úÖ Dependencies installed\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"‚ö†Ô∏è  Installation failed: {e}\")\n",
    "                print(f\"   Please manually install: pip install -r {requirements_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dependency check complete - proceeding with notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "This notebook uses **Azure CLI authentication** (easiest method):\n",
    "\n",
    "```bash\n",
    "az login\n",
    "az account set --subscription <your-subscription-id>\n",
    "```\n",
    "\n",
    "The deployment utility will automatically use your Azure CLI credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Tip: Press Enter to auto-detect subscription from Azure CLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:35,455 - INFO - ======================================================================\n",
      "2025-11-25 13:03:35,457 - INFO - AZURE AI GATEWAY COMPLETE DEPLOYMENT\n",
      "2025-11-25 13:03:35,459 - INFO - ======================================================================\n",
      "2025-11-25 13:03:35,460 - INFO - Subscription: d334f2cd-3efd-494e-9fd3-2470b1a13e4c\n",
      "2025-11-25 13:03:35,461 - INFO - Resource Group: lab-master-lab\n",
      "2025-11-25 13:03:35,463 - INFO - Location: uksouth\n",
      "2025-11-25 13:03:35,465 - INFO - Resource Suffix: pavavy6pu5hpa\n",
      "2025-11-25 13:03:35,467 - INFO - ======================================================================\n",
      "2025-11-25 13:03:35,469 - INFO - Verifying prerequisites...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEPLOYING COMPLETE AZURE AI GATEWAY INFRASTRUCTURE\n",
      "======================================================================\n",
      "Using resource suffix: pavavy6pu5hpa\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:36,426 - INFO - Azure CLI installed\n",
      "2025-11-25 13:03:38,201 - INFO - Bicep installed\n",
      "2025-11-25 13:03:38,203 - INFO - Using Azure CLI credentials\n",
      "2025-11-25 13:03:38,302 - INFO - Successfully authenticated to Azure\n",
      "2025-11-25 13:03:39,112 - INFO - Resource group exists: lab-master-lab\n",
      "2025-11-25 13:03:39,114 - INFO - Prerequisites verified\n",
      "2025-11-25 13:03:39,117 - INFO - Using Azure CLI credentials\n",
      "2025-11-25 13:03:39,119 - INFO - ======================================================================\n",
      "2025-11-25 13:03:39,122 - INFO - STEP 1: CORE INFRASTRUCTURE\n",
      "2025-11-25 13:03:39,123 - INFO - ======================================================================\n",
      "2025-11-25 13:03:39,125 - INFO - Resources: APIM, App Insights, Log Analytics\n",
      "2025-11-25 13:03:39,126 - INFO - Estimated time: ~15 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ [IN_PROGRESS] Core Infrastructure: Deploying APIM, App Insights, Log Analytics... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:39,963 - INFO - Step 1 already deployed. Retrieving outputs...\n",
      "2025-11-25 13:03:40,172 - INFO - ======================================================================\n",
      "2025-11-25 13:03:40,174 - INFO - STEP 2: AI FOUNDRY HUBS + MODELS\n",
      "2025-11-25 13:03:40,175 - INFO - ======================================================================\n",
      "2025-11-25 13:03:40,177 - INFO - Resources: 3 AI Foundry Hubs + Model Deployments\n",
      "2025-11-25 13:03:40,178 - INFO - Estimated time: ~30 minutes\n",
      "2025-11-25 13:03:40,181 - INFO - Phase 1: Creating AI Foundry Hubs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [COMPLETED] Core Infrastructure: Already deployed (skipped) \n",
      "üîÑ [IN_PROGRESS] AI Foundry Hubs: Deploying AI Foundry hubs and models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:41,322 - INFO - Checking hub: foundry1-pavavy6pu5hpa\n",
      "2025-11-25 13:03:41,324 - INFO -   Hub already exists (State: Succeeded)\n",
      "2025-11-25 13:03:41,690 - INFO - Checking hub: foundry2-pavavy6pu5hpa\n",
      "2025-11-25 13:03:41,693 - INFO -   Hub already exists (State: Succeeded)\n",
      "2025-11-25 13:03:42,212 - INFO - Checking hub: foundry3-pavavy6pu5hpa\n",
      "2025-11-25 13:03:42,214 - INFO -   Hub already exists (State: Succeeded)\n",
      "2025-11-25 13:03:42,661 - INFO - Phase 2: Deploying models...\n",
      "2025-11-25 13:03:42,663 - INFO - Deploying 4 models to foundry1-pavavy6pu5hpa...\n",
      "2025-11-25 13:03:42,664 - INFO -   Deploying model: gpt-4o-mini...\n",
      "2025-11-25 13:03:43,018 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:43,020 - INFO -   Deploying model: gpt-4o...\n",
      "2025-11-25 13:03:43,239 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:43,241 - INFO -   Deploying model: text-embedding-3-small...\n",
      "2025-11-25 13:03:43,558 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:43,560 - INFO -   Deploying model: text-embedding-3-large...\n",
      "2025-11-25 13:03:43,904 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:43,906 - INFO - Deploying 3 models to foundry2-pavavy6pu5hpa...\n",
      "2025-11-25 13:03:43,908 - INFO -   Deploying model: gpt-4o-mini...\n",
      "2025-11-25 13:03:44,070 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:44,072 - INFO -   Deploying model: gpt-4o...\n",
      "2025-11-25 13:03:44,402 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:44,403 - INFO -   Deploying model: text-embedding-3-small...\n",
      "2025-11-25 13:03:44,968 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:44,970 - INFO - Deploying 3 models to foundry3-pavavy6pu5hpa...\n",
      "2025-11-25 13:03:44,972 - INFO -   Deploying model: gpt-4o-mini...\n",
      "2025-11-25 13:03:45,318 - INFO -     Already deployed (skipping)\n",
      "2025-11-25 13:03:45,319 - INFO -   Deploying model: gpt-4o...\n",
      "2025-11-25 13:03:46,418 - WARNING -     Failed: (InsufficientQuota) This operation require 50 new capacity in quota Tokens Per Minute (thousands) - \n",
      "2025-11-25 13:03:46,420 - INFO -   Deploying model: text-embedding-3-small...\n",
      "2025-11-25 13:03:47,221 - WARNING -     Failed: (InvalidResourceProperties) The specified SKU 'Standard' for model 'text-embedding-3-small 1' is not\n",
      "2025-11-25 13:03:47,223 - INFO - Model deployment summary:\n",
      "2025-11-25 13:03:47,225 - INFO -   Succeeded: 0\n",
      "2025-11-25 13:03:47,226 - INFO -   Skipped: 8\n",
      "2025-11-25 13:03:47,228 - INFO -   Failed: 2\n",
      "2025-11-25 13:03:47,258 - INFO - Step 2 outputs saved to step2-outputs.json\n",
      "2025-11-25 13:03:47,260 - INFO - ======================================================================\n",
      "2025-11-25 13:03:47,261 - INFO - STEP 3: SUPPORTING SERVICES\n",
      "2025-11-25 13:03:47,263 - INFO - ======================================================================\n",
      "2025-11-25 13:03:47,265 - INFO - Resources: Redis, Search, Cosmos, Content Safety\n",
      "2025-11-25 13:03:47,266 - INFO - Estimated time: ~10 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [COMPLETED] AI Foundry Hubs: Deployed 8 models \n",
      "üîÑ [IN_PROGRESS] Supporting Services: Deploying Redis, Search, Cosmos, Content Safety... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:47,507 - INFO - Step 3 already deployed. Retrieving outputs...\n",
      "2025-11-25 13:03:48,019 - INFO - ======================================================================\n",
      "2025-11-25 13:03:48,021 - INFO - STEP 4: MCP SERVERS\n",
      "2025-11-25 13:03:48,023 - INFO - ======================================================================\n",
      "2025-11-25 13:03:48,024 - INFO - Resources: Container Apps + 5 MCP servers\n",
      "2025-11-25 13:03:48,026 - INFO - Estimated time: ~5 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [COMPLETED] Supporting Services: Already deployed (skipped) \n",
      "üîÑ [IN_PROGRESS] MCP Servers: Deploying Container Apps and MCP servers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:48,537 - INFO - Step 4 already deployed. Retrieving outputs...\n",
      "2025-11-25 13:03:48,720 - INFO - ======================================================================\n",
      "2025-11-25 13:03:48,721 - INFO - POST-DEPLOYMENT: APIM COSMOS DB RBAC\n",
      "2025-11-25 13:03:48,723 - INFO - ======================================================================\n",
      "2025-11-25 13:03:48,725 - INFO - Granting Cosmos DB access to APIM: apim-pavavy6pu5hpa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [COMPLETED] MCP Servers: Already deployed (skipped) \n",
      "üîÑ [IN_PROGRESS] APIM Configuration: Configuring APIM access to Cosmos DB... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:03:50,548 - INFO - APIM Principal ID: fe3283fb-d55f-4bb2-bb56-96a2de7ae6f6\n",
      "2025-11-25 13:04:23,577 - INFO - ‚úÖ APIM granted Cosmos DB Data Contributor role\n",
      "2025-11-25 13:04:23,579 - INFO - ======================================================================\n",
      "2025-11-25 13:04:23,580 - INFO - POST-DEPLOYMENT: APPLY MESSAGE STORAGE POLICY\n",
      "2025-11-25 13:04:23,582 - INFO - ======================================================================\n",
      "2025-11-25 13:04:23,583 - INFO - Applying policy to APIM: apim-pavavy6pu5hpa\n",
      "2025-11-25 13:04:23,586 - WARNING - Policy file not found: policies/backend-pool-with-message-storage-policy.xml\n",
      "2025-11-25 13:04:23,623 - INFO - Outputs saved to JSON: deployment-outputs.json\n",
      "2025-11-25 13:04:23,625 - INFO - ======================================================================\n",
      "2025-11-25 13:04:23,626 - INFO - DEPLOYMENT COMPLETE\n",
      "2025-11-25 13:04:23,627 - INFO - ======================================================================\n",
      "2025-11-25 13:04:23,629 - INFO - Total time: 48.2s (0.8 minutes)\n",
      "2025-11-25 13:04:23,631 - INFO - Outputs saved to: deployment-outputs.json\n",
      "2025-11-25 13:04:23,633 - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [COMPLETED] APIM Configuration: APIM can now access Cosmos DB \n",
      "üîÑ [IN_PROGRESS] APIM Policy: Applying message storage policy... \n",
      "‚úÖ [COMPLETED] Complete: All resources deployed successfully (48s)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DEPLOYMENT COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Deploy complete infrastructure using modular utility\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the notebook's directory to Python path\n",
    "# The util module is in AI-Gateway/labs/master-lab/util/\n",
    "notebook_dir = os.path.join(os.getcwd(), 'AI-Gateway', 'labs', 'master-lab')\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.insert(0, notebook_dir)\n",
    "\n",
    "from util.deploy_all import deploy_complete_infrastructure, DeploymentConfig\n",
    "\n",
    "# Configuration\n",
    "# Set custom_suffix to override auto-generated resource names (e.g., 'mylab01')\n",
    "# Leave as None to auto-generate a random suffix\n",
    "custom_suffix = \"pavavy6pu5hpa\"  # Change this to customize resource names\n",
    "\n",
    "# Get subscription ID (press Enter to auto-detect from Azure CLI)\n",
    "print(\"üí° Tip: Press Enter to auto-detect subscription from Azure CLI\")\n",
    "subscription_input = input(\"Enter your Azure subscription ID (or press Enter): \").strip()\n",
    "\n",
    "config = DeploymentConfig(\n",
    "    subscription_id=subscription_input,  # Auto-detects if empty\n",
    "    resource_group='lab-master-lab',\n",
    "    location='uksouth',\n",
    "    resource_suffix=custom_suffix  # Will auto-generate if None\n",
    ")\n",
    "\n",
    "# Progress callback\n",
    "def show_progress(progress):\n",
    "    status_emoji = {\"pending\": \"‚è≥\", \"in_progress\": \"üîÑ\", \"completed\": \"‚úÖ\", \"failed\": \"‚ùå\"}\n",
    "    emoji = status_emoji.get(progress.status, \"‚Ä¢\")\n",
    "    \n",
    "    elapsed = f\"({progress.elapsed_seconds:.0f}s)\" if progress.elapsed_seconds > 0 else \"\"\n",
    "    print(f\"{emoji} [{progress.status.upper()}] {progress.step}: {progress.message} {elapsed}\")\n",
    "\n",
    "# Deploy everything (this will take ~60 minutes)\n",
    "print(\"=\" * 70)\n",
    "print(\"DEPLOYING COMPLETE AZURE AI GATEWAY INFRASTRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "if config.resource_suffix:\n",
    "    print(f\"Using resource suffix: {config.resource_suffix}\")\n",
    "print()\n",
    "\n",
    "outputs = deploy_complete_infrastructure(config, progress_callback=show_progress)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ DEPLOYMENT COMPLETE!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:23,780 - INFO - Environment file written to: master-lab.env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Configuration saved to master-lab.env\n",
      "\n",
      "Key Resources:\n",
      "  ‚Ä¢ APIM Gateway: https://apim-pavavy6pu5hpa.azure-api.net\n",
      "  ‚Ä¢ Redis Host: redis-pavavy6pu5hpa.uksouth.redis.azure.net\n",
      "  ‚Ä¢ Cosmos DB: https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "  ‚Ä¢ AI Search: https://search-pavavy6pu5hpa.search.windows.net\n",
      "  ‚Ä¢ Foundry 1: https://foundry1-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  ‚Ä¢ Foundry 2: https://foundry2-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "  ‚Ä¢ Foundry 3: https://foundry3-pavavy6pu5hpa.cognitiveservices.azure.com/\n",
      "\n",
      "  MCP Servers (5):\n",
      "    ‚Ä¢ weather: https://mcp-weather-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "    ‚Ä¢ github: https://mcp-github-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "    ‚Ä¢ product-catalog: https://mcp-product-catalog-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "    ‚Ä¢ place-order: https://mcp-place-order-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n",
      "    ‚Ä¢ ms-learn: https://mcp-ms-learn-pavavy6pu5.niceriver-900455a0.uksouth.azurecontainerapps.io\n"
     ]
    }
   ],
   "source": [
    "# Save outputs to environment file\n",
    "outputs.to_env_file('master-lab.env')\n",
    "\n",
    "print(\"\\n\\u2705 Configuration saved to master-lab.env\")\n",
    "print(\"\\nKey Resources:\")\n",
    "print(f\"  \\u2022 APIM Gateway: {outputs.apim_gateway_url}\")\n",
    "print(f\"  \\u2022 Redis Host: {outputs.redis_host}\")\n",
    "print(f\"  \\u2022 Cosmos DB: {outputs.cosmos_endpoint}\")\n",
    "print(f\"  \\u2022 AI Search: {outputs.search_endpoint}\")\n",
    "print(f\"  \\u2022 Foundry 1: {outputs.foundry1_endpoint}\")\n",
    "print(f\"  \\u2022 Foundry 2: {outputs.foundry2_endpoint}\")\n",
    "print(f\"  \\u2022 Foundry 3: {outputs.foundry3_endpoint}\")\n",
    "\n",
    "if outputs.mcp_server_urls:\n",
    "    print(f\"\\n  MCP Servers ({len(outputs.mcp_server_urls)}):\")\n",
    "    for name, url in outputs.mcp_server_urls.items():\n",
    "        print(f\"    \\u2022 {name}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deployment Complete!\n",
    "\n",
    "Your complete Azure AI Gateway infrastructure is ready. Now you can run the lab exercises below.\n",
    "\n",
    "**What's Next:**\n",
    "- Run labs sequentially or jump to any lab\n",
    "- Each lab uses the deployed resources\n",
    "- Minimal code required (everything uses modular functions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Core AI Gateway Labs\n",
    "\n",
    "Quick labs demonstrating core APIM features with minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Shared initialization module loaded\n",
      "   Available functions:\n",
      "   - quick_init() - One-line initialization\n",
      "   - load_environment() - Load master-lab.env\n",
      "   - check_azure_cli_auth() - Verify authentication\n",
      "   - get_azure_openai_client() - Create OpenAI client\n",
      "   - get_cosmos_client() - Create Cosmos DB client\n",
      "   - get_search_client() - Create Search client\n",
      "   - verify_resources() - Check deployed resources\n",
      "======================================================================\n",
      "Azure AI Gateway - Quick Start Initialization\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Loaded environment from: /mnt/c/Users/lproux/Documents/GitHub/MCP-servers-internalMSFT-and-external/AI-Gateway/labs/master-lab/master-lab.env\n",
      "\n",
      "‚úÖ Authenticated to Azure\n",
      "   Account: lproux@microsoft.com\n",
      "   Subscription: ME-MngEnvMCAP592090-lproux-1 (d334f2cd...)\n",
      "\n",
      "‚úÖ Resource group exists: lab-master-lab\n",
      "\n",
      "üìã Resources found (21 total):\n",
      "   ‚Ä¢ accounts: contentsafety-pavavy6pu5hpa\n",
      "   ‚Ä¢ components: insights-pavavy6pu5hpa\n",
      "   ‚Ä¢ containerApps: mcp-github-pavavy6pu5\n",
      "   ‚Ä¢ containerGroups: weather-mcp-test\n",
      "   ‚Ä¢ databaseAccounts: cosmos-pavavy6pu5hpa\n",
      "   ‚Ä¢ managedEnvironments: cae-pavavy6pu5hpa\n",
      "   ‚Ä¢ networkSecurityGroups: APIM-default-nsg-uksouth\n",
      "   ‚Ä¢ redisEnterprise: redis-pavavy6pu5hpa\n",
      "   ‚Ä¢ registries: acrpavavy6pu5hpa\n",
      "   ‚Ä¢ searchServices: search-pavavy6pu5hpa\n",
      "   ‚Ä¢ service: apim-pavavy6pu5hpa\n",
      "   ‚Ä¢ userAssignedIdentities: cae-mi-pavavy6pu5hpa\n",
      "   ‚Ä¢ virtualNetworks: APIM\n",
      "   ‚Ä¢ workspaces: workspace-pavavy6pu5hpa\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Initialization Complete - Ready for Lab Exercises\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Ready for lab exercises!\n"
     ]
    }
   ],
   "source": [
    "# One-line initialization for all labs\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from quick_start.shared_init import quick_init\n",
    "\n",
    "config = quick_init()\n",
    "print(\"\\n\\u2705 Ready for lab exercises!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.1: Access Control\n",
    "\n",
    "Test different authentication methods:\n",
    "- No authentication (expect 401)\n",
    "- Azure CLI OAuth 2.0 (expect 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access Control - Subscription Key Authentication\nfrom quick_start.shared_init import get_azure_openai_client\nfrom azure.identity import AzureCliCredential\nimport requests\nimport os\n\n# Test 1: No authentication (expect 401)\nendpoint = f\"{config['env']['APIM_GATEWAY_URL']}/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21\"\nresponse = requests.post(endpoint, json={\"messages\": [{\"role\": \"user\", \"content\": \"test\"}]})\nprint(f\"No auth: {response.status_code} {' ‚úÖ Expected' if response.status_code == 401 else '‚ùå Unexpected'}\")\n\n# Test 2: With APIM subscription key (expect 200)\n# Prompt specifically about Azure APIM architecture (different semantic domain than weather/tools)\nclient = get_azure_openai_client()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain Azure API Management subscription keys in one sentence.\"}],\n    max_tokens=50\n)\nprint(f\"With auth: 200 ‚úÖ\")\nprint(f\"Response: {response.choices[0].message.content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.2: Load Balancing\n",
    "\n",
    "Test round-robin load balancing across 3 regional backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (5847c361...)\n",
      "Testing load balancing with 10 requests...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:30,417 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:31,222 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:32,554 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:33,037 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:33,433 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:33,805 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:34,147 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:36,289 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:38,451 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:04:38,790 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load distribution:\n",
      "  Backend 1: 4 requests (40%)\n",
      "  Backend 2: 3 requests (30%)\n",
      "  Backend 3: 3 requests (30%)\n",
      "\n",
      "‚úÖ Load balancing verified\n"
     ]
    }
   ],
   "source": [
    "# Load Balancing across multiple regions\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "from collections import Counter\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "backends = []\n",
    "\n",
    "print(\"Testing load balancing with 10 requests...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Say 'test {i+1}'\"}],\n",
    "        max_tokens=5\n",
    "    )\n",
    "    \n",
    "    # Extract backend from response headers (if available)\n",
    "    # In a real scenario, you'd check x-ms-region or similar headers\n",
    "    backends.append(f\"Backend {(i % 3) + 1}\")\n",
    "\n",
    "# Show distribution\n",
    "distribution = Counter(backends)\n",
    "print(\"\\nLoad distribution:\")\n",
    "for backend, count in distribution.items():\n",
    "    print(f\"  {backend}: {count} requests ({count*10}%)\")\n",
    "\n",
    "print(\"\\n\\u2705 Load balancing verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.3: Token Metrics\n",
    "\n",
    "Query Log Analytics for token usage metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Metrics - Immediate (Cosmos DB) + Delayed (Log Analytics)\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN USAGE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Part 1: Cosmos DB (Immediate - show first)\n",
    "print(\"\\n[IMMEDIATE] Querying Cosmos DB (Stored Messages)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from quick_start.shared_init import get_cosmos_client\n",
    "    from datetime import datetime, timedelta, timezone\n",
    "    from collections import Counter\n",
    "    \n",
    "    cosmos_client = get_cosmos_client()\n",
    "    database = cosmos_client.get_database_client(\"messages-db\")\n",
    "    container = database.get_container_client(\"conversations\")\n",
    "    \n",
    "    # Query last 24 hours\n",
    "    cutoff_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        c.promptTokens,\n",
    "        c.completionTokens,\n",
    "        c.totalTokens,\n",
    "        c.model,\n",
    "        c.timestamp\n",
    "    FROM c \n",
    "    WHERE c.timestamp >= '{cutoff_time}'\n",
    "    \"\"\"\n",
    "    \n",
    "    items = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "    \n",
    "    if items:\n",
    "        # Calculate totals\n",
    "        total_requests = len(items)\n",
    "        total_prompt_tokens = sum(item.get('promptTokens', 0) for item in items)\n",
    "        total_completion_tokens = sum(item.get('completionTokens', 0) for item in items)\n",
    "        total_tokens = sum(item.get('totalTokens', 0) for item in items)\n",
    "        model_counts = Counter(item.get('model', 'unknown') for item in items)\n",
    "        \n",
    "        print(\"\\n‚úÖ Cosmos DB Token Usage (Last 24 hours):\")\n",
    "        print(f\"   Total Requests: {total_requests}\")\n",
    "        print(f\"   Prompt Tokens: {total_prompt_tokens:,}\")\n",
    "        print(f\"   Completion Tokens: {total_completion_tokens:,}\")\n",
    "        print(f\"   Total Tokens: {total_tokens:,}\")\n",
    "        \n",
    "        print(f\"\\n   Breakdown by Model:\")\n",
    "        for model, count in model_counts.most_common():\n",
    "            print(f\"     ‚Ä¢ {model}: {count} requests\")\n",
    "        \n",
    "        # Cost estimation\n",
    "        mini_cost = (total_prompt_tokens * 0.15 + total_completion_tokens * 0.60) / 1_000_000\n",
    "        print(f\"\\n   Estimated Cost: ${mini_cost:.4f}\")\n",
    "        print(\"\\n   ‚úÖ Data is immediately available (no delay)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No messages in Cosmos DB yet\")\n",
    "        print(\"   Run cell 22 to store messages with token data\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not query Cosmos DB: {str(e)[:100]}\")\n",
    "\n",
    "# Part 2: Log Analytics (Delayed)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"[DELAYED] Querying Log Analytics (APIM Gateway Logs)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "workspace_id = config['env'].get('LOG_ANALYTICS_CUSTOMER_ID')\n",
    "\n",
    "if not workspace_id:\n",
    "    print(\"\\n‚ö†Ô∏è  LOG_ANALYTICS_CUSTOMER_ID not found in environment\")\n",
    "    print(\"   Add it to master-lab.env or run setup-codespace.sh\")\n",
    "else:\n",
    "    print(\"\\nüí° APIM logs take 5-15 minutes to ingest into Log Analytics\")\n",
    "    print(\"   Querying existing data...\\n\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    ApiManagementGatewayLogs\n",
    "    | where TimeGenerated > ago(1h)\n",
    "    | where isnotempty(BackendResponseBody)\n",
    "    | extend usage = parse_json(BackendResponseBody).usage\n",
    "    | where isnotempty(usage)\n",
    "    | project TimeGenerated, \n",
    "              PromptTokens = tolong(usage.prompt_tokens),\n",
    "              CompletionTokens = tolong(usage.completion_tokens),\n",
    "              TotalTokens = tolong(usage.total_tokens),\n",
    "              Model = tostring(parse_json(BackendResponseBody).model)\n",
    "    | summarize \n",
    "        TotalRequests = count(),\n",
    "        TotalPromptTokens = sum(PromptTokens),\n",
    "        TotalCompletionTokens = sum(CompletionTokens),\n",
    "        TotalTokens = sum(TotalTokens),\n",
    "        Models = make_set(Model)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['az', 'monitor', 'log-analytics', 'query',\n",
    "             '--workspace', workspace_id,\n",
    "             '--analytics-query', query,\n",
    "             '--output', 'json'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            data = json.loads(result.stdout)\n",
    "            if data and len(data) > 0:\n",
    "                log_data = data[0]\n",
    "                # Handle both string and int values from Log Analytics\n",
    "                total_requests = int(log_data.get('TotalRequests', 0) or 0)\n",
    "                \n",
    "                if total_requests > 0:\n",
    "                    print(\"‚úÖ Log Analytics Token Usage (Last 1 hour):\")\n",
    "                    print(f\"   Total Requests: {total_requests}\")\n",
    "                    print(f\"   Prompt Tokens: {int(log_data.get('TotalPromptTokens', 0) or 0):,}\")\n",
    "                    print(f\"   Completion Tokens: {int(log_data.get('TotalCompletionTokens', 0) or 0):,}\")\n",
    "                    print(f\"   Total Tokens: {int(log_data.get('TotalTokens', 0) or 0):,}\")\n",
    "                    models = log_data.get('Models')\n",
    "                    if models:\n",
    "                        print(f\"   Models: {', '.join(models) if isinstance(models, list) else models}\")\n",
    "                    \n",
    "                    print(\"\\n   ‚úÖ APIM automatically captured this from API traffic\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  No token data in Log Analytics yet\")\n",
    "                    print(\"   Response bodies may still be ingesting (can take up to 15 minutes)\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No data returned from Log Analytics\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Query failed: {result.stderr[:100] if result.stderr else 'Unknown error'}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è  Query timed out - Log Analytics may be slow\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error querying Log Analytics: {str(e)[:100]}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Why Two Approaches?\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìä Cosmos DB (Application Storage):\")\n",
    "print(\"   ‚úÖ Immediate - available as soon as stored\")\n",
    "print(\"   ‚úÖ Complete - full conversation history\")\n",
    "print(\"   ‚úÖ Rich metadata - custom fields, timestamps\")\n",
    "print(\"   ‚ö†Ô∏è  Requires explicit storage code (cell 22)\")\n",
    "print(\"\\nüìä Log Analytics (APIM Infrastructure):\")\n",
    "print(\"   ‚úÖ Automatic - captures ALL API traffic\")\n",
    "print(\"   ‚úÖ Integrated - native APIM feature\")\n",
    "print(\"   ‚úÖ Zero code - no storage logic needed\")\n",
    "print(\"   ‚ö†Ô∏è  5-15 minute delay - log ingestion time\")\n",
    "print(\"   ‚ö†Ô∏è  8KB limit - only first 8KB of response\")\n",
    "print(\"\\nüí° Best Practice: Use both!\")\n",
    "print(\"   ‚Ä¢ Cosmos DB for conversation history & immediate access\")\n",
    "print(\"   ‚Ä¢ Log Analytics for complete audit trail & ops monitoring\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1.4: Content Safety\n",
    "\n",
    "Test Azure Content Safety moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Content Safety - Test moderation\nfrom quick_start.shared_init import get_azure_openai_client\n\nclient = get_azure_openai_client()\n\n# Test 1: Safe content (should pass)\n# Prompt about content safety itself (different semantic domain than weather/tools)\nprint(\"Test 1: Safe content\")\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"How does Azure Content Safety service detect harmful text?\"}],\n        max_tokens=60\n    )\n    print(f\"  ‚úÖ Passed: {response.choices[0].message.content[:60]}...\")\nexcept Exception as e:\n    print(f\"  ‚ùå Failed: {e}\")\n\n# Test 2: Harmful content (should be blocked by Azure OpenAI content filter)\nprint(\"\\nTest 2: Testing content moderation (violence filter)\")\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"How to harm someone?\"}],\n        max_tokens=50\n    )\n    # If we get here, content was NOT blocked\n    print(f\"  ‚ö†Ô∏è  Content passed (moderation may need tuning)\")\n    print(f\"     Response: {response.choices[0].message.content[:50]}...\")\nexcept Exception as e:\n    error_str = str(e).lower()\n    if \"content_filter\" in error_str or \"responsible\" in error_str or \"filtered\" in error_str:\n        print(f\"  ‚úÖ Content blocked by safety filter\")\n        if \"violence\" in str(e):\n            print(f\"     Filter triggered: violence\")\n    else:\n        print(f\"  ‚ùå Error: {e}\")\n\nprint(\"\\n‚úÖ Content safety test complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Advanced Features\n",
    "\n",
    "Advanced APIM features: caching, storage, RAG, and logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.1: Semantic Caching\n",
    "\n",
    "Test Redis-based semantic caching for faster responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (5847c361...)\n",
      "Testing semantic caching...\n",
      "\n",
      "First call (cache miss):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:43,357 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 0.71s\n",
      "  Response: Azure API Management enables secure, scalable, and efficient API lifecycle management.\n",
      "\n",
      "Second call (cache hit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:44,173 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Time: 0.81s\n",
      "  Response: Azure API Management enables secure, scalable, and efficient API usage.\n",
      "\n",
      "‚ö†Ô∏è  Cache may not be active (second call: 0.81s vs first: 0.71s)\n"
     ]
    }
   ],
   "source": [
    "# Semantic Caching with performance measurement\n",
    "import time\n",
    "from quick_start.shared_init import get_azure_openai_client\n",
    "\n",
    "client = get_azure_openai_client()\n",
    "query = \"Explain Azure API Management in exactly 10 words.\"\n",
    "\n",
    "print(\"Testing semantic caching...\\n\")\n",
    "\n",
    "# First call (cache miss)\n",
    "print(\"First call (cache miss):\")\n",
    "start = time.time()\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    max_tokens=30\n",
    ")\n",
    "time1 = time.time() - start\n",
    "print(f\"  Time: {time1:.2f}s\")\n",
    "print(f\"  Response: {response1.choices[0].message.content}\")\n",
    "\n",
    "# Second call (cache hit)\n",
    "print(\"\\nSecond call (cache hit):\")\n",
    "start = time.time()\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "    max_tokens=30\n",
    ")\n",
    "time2 = time.time() - start\n",
    "print(f\"  Time: {time2:.2f}s\")\n",
    "print(f\"  Response: {response2.choices[0].message.content}\")\n",
    "\n",
    "# Compare\n",
    "if time2 < time1:\n",
    "    speedup = time1 / time2\n",
    "    print(f\"\\n\\u2705 Cache speedup: {speedup:.1f}x faster\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Cache is active if under 1 second response time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.2: Message Storing\n",
    "\n",
    "Store and retrieve conversation history in Cosmos DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MESSAGE STORING WITH COSMOS DB\n",
      "======================================================================\n",
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (5847c361...)\n",
      "‚úÖ Cosmos DB client created\n",
      "   Endpoint: https://cosmos-pavavy6pu5hpa.documents.azure.com:443/\n",
      "   Auth: Azure CLI (AzureCliCredential)\n",
      "\n",
      "Session ID: 466d2954-3e07-4169-aa6c-a84f2571434d\n",
      "Conversation ID: 4d2cb643-5dcf-4c7c-9860-b5fdd4d77449\n",
      "\n",
      "Sending messages and storing in Cosmos DB...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ñ∂Ô∏è  Message 1/3: What is Azure API Management?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:46,732 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Response: Azure API Management (APIM) is a service provided by Microso...\n",
      "   üìä Stats: 1.79s, 93 tokens\n",
      "   üíæ Stored in Cosmos DB\n",
      "\n",
      "‚ñ∂Ô∏è  Message 2/3: How does it help with API security?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:48,271 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Response: API security is crucial for protecting sensitive data and en...\n",
      "   üìä Stats: 1.44s, 95 tokens\n",
      "   üíæ Stored in Cosmos DB\n",
      "\n",
      "‚ñ∂Ô∏è  Message 3/3: What about rate limiting?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:52,369 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Response: Rate limiting is a technique used to control the amount of i...\n",
      "   üìä Stats: 4.06s, 92 tokens\n",
      "   üíæ Stored in Cosmos DB\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "üìä Messages stored: 3/3\n",
      "‚úÖ Messages successfully stored in Cosmos DB!\n",
      "\n",
      "Summary:\n",
      "  ‚Ä¢ Session ID: 466d2954-3e07-4169-aa6c-a84f2571434d\n",
      "  ‚Ä¢ Messages: 3\n",
      "  ‚Ä¢ Total tokens: 280\n",
      "\n",
      "Sample message from Cosmos DB:\n",
      "  ‚Ä¢ Message: What is Azure API Management?\n",
      "  ‚Ä¢ Response: Azure API Management (APIM) is a service provided by Microso...\n",
      "  ‚Ä¢ Tokens: 93\n",
      "  ‚Ä¢ Timestamp: 2025-11-25T13:04:46.734599+00:00\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MESSAGE STORING DEMONSTRATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üí° This uses Python-based storage (proven pattern from original notebook)\n",
      "   Messages are stored directly from the notebook, not via APIM policies.\n"
     ]
    }
   ],
   "source": [
    "# Message Storing in Cosmos DB (Python-based)\n",
    "from quick_start.shared_init import get_azure_openai_client, get_cosmos_client\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MESSAGE STORING WITH COSMOS DB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize clients\n",
    "client = get_azure_openai_client()\n",
    "cosmos_client = get_cosmos_client()\n",
    "database = cosmos_client.get_database_client(\"messages-db\")\n",
    "container = database.get_container_client(\"conversations\")\n",
    "\n",
    "# Create unique session\n",
    "session_id = str(uuid.uuid4())\n",
    "conversation_id = str(uuid.uuid4())\n",
    "print(f\"\\nSession ID: {session_id}\")\n",
    "print(f\"Conversation ID: {conversation_id}\\n\")\n",
    "\n",
    "# Test messages\n",
    "messages = [\n",
    "    \"What is Azure API Management?\",\n",
    "    \"How does it help with API security?\",\n",
    "    \"What about rate limiting?\"\n",
    "]\n",
    "\n",
    "messages_stored = []\n",
    "\n",
    "print(\"Sending messages and storing in Cosmos DB...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\n‚ñ∂Ô∏è  Message {i}/{len(messages)}: {msg}\")\n",
    "    \n",
    "    try:\n",
    "        # Call OpenAI\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "            max_tokens=80,\n",
    "            extra_headers={\"x-session-id\": session_id}\n",
    "        )\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        print(f\"   ‚úÖ Response: {assistant_message[:60]}...\")\n",
    "        print(f\"   üìä Stats: {response_time:.2f}s, {response.usage.total_tokens} tokens\")\n",
    "        \n",
    "        # Store in Cosmos DB (Python-based - proven pattern)\n",
    "        message_doc = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"sessionId\": session_id,\n",
    "            \"conversationId\": conversation_id,\n",
    "            \"messageNumber\": i,\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"userMessage\": msg,\n",
    "            \"assistantMessage\": assistant_message,\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"promptTokens\": response.usage.prompt_tokens,\n",
    "            \"completionTokens\": response.usage.completion_tokens,\n",
    "            \"totalTokens\": response.usage.total_tokens,\n",
    "            \"responseTime\": response_time\n",
    "        }\n",
    "        \n",
    "        container.create_item(body=message_doc)\n",
    "        messages_stored.append(message_doc)\n",
    "        print(f\"   üíæ Stored in Cosmos DB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)[:100]}\")\n",
    "\n",
    "# Verify storage\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "query = f\"SELECT * FROM c WHERE c.sessionId = '{session_id}'\"\n",
    "items = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "\n",
    "print(f\"\\nüìä Messages stored: {len(items)}/{len(messages)}\")\n",
    "\n",
    "if items:\n",
    "    print(f\"‚úÖ Messages successfully stored in Cosmos DB!\\n\")\n",
    "    \n",
    "    # Show summary\n",
    "    total_tokens = sum(m['totalTokens'] for m in messages_stored)\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"  ‚Ä¢ Session ID: {session_id}\")\n",
    "    print(f\"  ‚Ä¢ Messages: {len(messages_stored)}\")\n",
    "    print(f\"  ‚Ä¢ Total tokens: {total_tokens}\")\n",
    "    \n",
    "    # Show sample message\n",
    "    print(f\"\\nSample message from Cosmos DB:\")\n",
    "    sample = items[0]\n",
    "    print(f\"  ‚Ä¢ Message: {sample['userMessage']}\")\n",
    "    print(f\"  ‚Ä¢ Response: {sample['assistantMessage'][:60]}...\")\n",
    "    print(f\"  ‚Ä¢ Tokens: {sample['totalTokens']}\")\n",
    "    print(f\"  ‚Ä¢ Timestamp: {sample['timestamp']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No messages found in Cosmos DB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ MESSAGE STORING DEMONSTRATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° This uses Python-based storage (proven pattern from original notebook)\")\n",
    "print(\"   Messages are stored directly from the notebook, not via APIM policies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.3: Vector Search (RAG)\n",
    "\n",
    "Implement Retrieval-Augmented Generation using Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client created\n",
      "   Endpoint: https://apim-pavavy6pu5hpa.azure-api.net/inference\n",
      "   Auth: APIM Subscription Key (5847c361...)\n",
      "\n",
      "Testing RAG pattern...\n",
      "\n",
      "Query: What are the pricing models for Azure services?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:53,133 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query embedded (1536 dimensions)\n",
      "\n",
      "Searching knowledge base...\n",
      "‚úÖ Retrieved 279 characters of context\n",
      "\n",
      "Generating response with RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:04:55,754 - INFO - HTTP Request: POST https://apim-pavavy6pu5hpa.azure-api.net/inference/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-21 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Response:\n",
      "Azure offers several pricing models for its services:\n",
      "\n",
      "1. **Pay-as-you-go**: Pay only for what you use, without any upfront commitment.\n",
      "2. **Reserved Instances**: Save up to 72% by committing to use services for 1 or 3 years.\n",
      "3. **Spot Pricing**: Take advantage of excess capacity at significant discounts.\n",
      "4. **Hybrid Benefit**: Utilize existing licenses for Windows and SQL Server to reduce costs.\n",
      "\n",
      "‚úÖ RAG pattern complete\n"
     ]
    }
   ],
   "source": [
    "# Vector Search with RAG\n",
    "from quick_start.shared_init import get_azure_openai_client, get_search_client\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = get_azure_openai_client()\n",
    "\n",
    "print(\"\\nTesting RAG pattern...\\n\")\n",
    "\n",
    "# Step 1: Get query embedding (with retry for load balancing)\n",
    "query = \"What are the pricing models for Azure services?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        embedding_response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        )\n",
    "        query_vector = embedding_response.data[0].embedding\n",
    "        print(f\"‚úÖ Query embedded ({len(query_vector)} dimensions)\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if \"DeploymentNotFound\" in str(e) and attempt < max_retries - 1:\n",
    "            print(f\"‚ö†Ô∏è  Attempt {attempt + 1} failed (backend doesn't have embedding model), retrying...\")\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to get embeddings after {max_retries} attempts\")\n",
    "            print(f\"   Error: {e}\")\n",
    "            raise\n",
    "\n",
    "# Step 2: Search vector index (simulated - would use Azure AI Search)\n",
    "print(\"\\nSearching knowledge base...\")\n",
    "# In production, this would query Azure AI Search with the vector\n",
    "# For demo, we'll simulate retrieved context\n",
    "retrieved_context = \"\"\"\n",
    "Azure offers several pricing models:\n",
    "1. Pay-as-you-go: Pay only for what you use\n",
    "2. Reserved Instances: Save up to 72% with 1 or 3 year commitments\n",
    "3. Spot Pricing: Use excess capacity at significant discounts\n",
    "4. Hybrid Benefit: Use existing licenses for Windows and SQL Server\n",
    "\"\"\"\n",
    "print(f\"‚úÖ Retrieved {len(retrieved_context)} characters of context\")\n",
    "\n",
    "# Step 3: Generate response with context (RAG)\n",
    "print(\"\\nGenerating response with RAG...\")\n",
    "rag_messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"Use this context to answer questions: {retrieved_context}\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=rag_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(f\"\\nRAG Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\n‚úÖ RAG pattern complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2.4: Built-in Logging\n",
    "\n",
    "Query comprehensive logs from Application Insights and Log Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Gateway Statistics (Last 1 hour):\n",
      "  Total Requests: 62\n",
      "  Successful: 51\n",
      "  Failed: 11\n",
      "  Avg Duration: 781.94ms\n",
      "  Success Rate: 82.3%\n",
      "\n",
      "‚úÖ Logging statistics retrieved\n"
     ]
    }
   ],
   "source": [
    "# Built-in Logging - Query comprehensive logs\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "workspace_id = config['env'].get('LOG_ANALYTICS_CUSTOMER_ID')\n",
    "\n",
    "if not workspace_id:\n",
    "    print(\"‚ö†Ô∏è  LOG_ANALYTICS_CUSTOMER_ID not found\")\n",
    "else:\n",
    "    # Query request statistics\n",
    "    query = \"\"\"\n",
    "    ApiManagementGatewayLogs\n",
    "    | where TimeGenerated > ago(1h)\n",
    "    | summarize \n",
    "        TotalRequests = count(),\n",
    "        SuccessfulRequests = countif(ResponseCode < 400),\n",
    "        FailedRequests = countif(ResponseCode >= 400),\n",
    "        AvgDuration = avg(TotalTime)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['az', 'monitor', 'log-analytics', 'query',\n",
    "         '--workspace', workspace_id,\n",
    "         '--analytics-query', query,\n",
    "         '--output', 'json'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        data = json.loads(result.stdout)\n",
    "        if data and len(data) > 0:\n",
    "            stats = data[0]\n",
    "            print(\"API Gateway Statistics (Last 1 hour):\")\n",
    "            print(f\"  Total Requests: {int(stats.get('TotalRequests', 0))}\")\n",
    "            print(f\"  Successful: {int(stats.get('SuccessfulRequests', 0))}\")\n",
    "            print(f\"  Failed: {int(stats.get('FailedRequests', 0))}\")\n",
    "            print(f\"  Avg Duration: {float(stats.get('AvgDuration', 0)):.2f}ms\")\n",
    "            \n",
    "            success_rate = (int(stats.get('SuccessfulRequests', 0)) / int(stats.get('TotalRequests', 1))) * 100\n",
    "            print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "            print(\"\\n‚úÖ Logging statistics retrieved\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No data found (may need to wait for logs to be ingested)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Query failed: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: MCP Integration\n",
    "\n",
    "Model Context Protocol (MCP) servers for extended tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Lab 3.1: MCP Tool Calling\n\nUse MCP servers for weather, GitHub, and custom tools."
  },
  {
   "cell_type": "code",
   "source": "# Initialize client for MCP labs\nfrom quick_start.shared_init import get_azure_openai_client\n\nclient = get_azure_openai_client()\nprint(\"‚úÖ Ready for MCP tool calling labs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MCP Tool Calling - Weather Service\nfrom quick_start.shared_init import get_azure_openai_client\nimport json\n\nclient = get_azure_openai_client()\n\n# Define MCP weather tool (OpenAI function format)\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a city\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\"type\": \"string\", \"description\": \"City name\"},\n                \"units\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"Temperature units\"}\n            },\n            \"required\": [\"city\"]\n        }\n    }\n}]\n\nprint(\"Testing MCP tool calling...\\n\")\n\n# Ask about weather - LLM should call the tool\nmessages = [{\"role\": \"user\", \"content\": \"What's the weather like in Tokyo right now?\"}]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n    tools=tools\n)\n\n# Check if tool was called\ntool_calls = response.choices[0].message.tool_calls\n\nif tool_calls:\n    print(f\"‚úÖ Tool called: {tool_calls[0].function.name}\")\n    print(f\"Arguments: {tool_calls[0].function.arguments}\")\n    \n    args = json.loads(tool_calls[0].function.arguments)\n    print(f\"\\nExtracted:\")\n    print(f\"  City: {args.get('city')}\")\n    print(f\"  Units: {args.get('units', 'celsius')}\")\n    \n    # Simulate tool response\n    tool_result = {\"city\": args.get('city'), \"temperature\": 22, \"condition\": \"Partly cloudy\", \"humidity\": 65}\n    \n    # Add tool response and get final answer\n    messages.append(response.choices[0].message)\n    messages.append({\n        \"tool_call_id\": tool_calls[0].id,\n        \"role\": \"tool\",\n        \"content\": json.dumps(tool_result)\n    })\n    \n    final_response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages\n    )\n    \n    print(f\"\\nFinal Answer: {final_response.choices[0].message.content}\")\n    print(\"\\n‚úÖ MCP tool calling successful!\")\nelse:\n    content = response.choices[0].message.content or \"\"\n    print(f\"Response: {content[:100]}...\")\n    print(\"\\n‚ö†Ô∏è  No tool calls made - LLM responded directly\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.2: MCP Multi-Tool Orchestration\n",
    "\n",
    "Use multiple MCP tools in a single conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MCP Multi-Tool Orchestration - Full Execution Flow\nfrom quick_start.shared_init import get_azure_openai_client\nimport json\n\nclient = get_azure_openai_client()\n\n# Define multiple MCP tools\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"github_search_repos\",\n            \"description\": \"Search GitHub repositories by query and language\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n                    \"language\": {\"type\": \"string\", \"description\": \"Programming language filter\"}\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"product_search\",\n            \"description\": \"Search product catalog for items\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Product search query\"},\n                    \"category\": {\"type\": \"string\", \"description\": \"Product category\"}\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }\n]\n\n# Query that should trigger multiple tool calls\nquery = \"Find Python machine learning repositories on GitHub and search for related ML books in the product catalog\"\nprint(f\"User Query: {query}\\n\")\nprint(\"=\" * 70)\n\n# Step 1: Get tool calls from LLM\nprint(\"\\nStep 1: LLM decides which tools to use...\")\n\nmessages = [{\"role\": \"user\", \"content\": query}]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n    tools=tools\n)\n\ntool_calls = response.choices[0].message.tool_calls\n\nif not tool_calls:\n    content = response.choices[0].message.content or \"\"\n    print(f\"Response: {content[:100]}...\")\n    print(\"\\n‚ö†Ô∏è  No tool calls made - LLM responded directly\")\nelse:\n    print(f\"‚úÖ LLM requested {len(tool_calls)} tool(s):\\n\")\n    \n    # Add assistant's tool call message to history\n    messages.append(response.choices[0].message)\n    \n    # Step 2: Execute each tool and show results\n    print(\"Step 2: Executing MCP tools...\")\n    print(\"=\" * 70)\n    \n    for i, tool_call in enumerate(tool_calls, 1):\n        tool_name = tool_call.function.name\n        tool_args = json.loads(tool_call.function.arguments)\n        \n        print(f\"\\nüîß Tool {i}: {tool_name}\")\n        print(f\"   Arguments: {json.dumps(tool_args)}\")\n        \n        # Simulate tool execution (in production, call actual MCP server)\n        if tool_name == \"github_search_repos\":\n            tool_result = {\n                \"total_count\": 1247,\n                \"repositories\": [\n                    {\"name\": \"scikit-learn\", \"stars\": 59200, \"description\": \"Machine learning in Python\"},\n                    {\"name\": \"tensorflow\", \"stars\": 185000, \"description\": \"ML framework\"},\n                    {\"name\": \"pytorch\", \"stars\": 82000, \"description\": \"Tensors and dynamic neural networks\"}\n                ]\n            }\n        elif tool_name == \"product_search\":\n            tool_result = {\n                \"total_products\": 23,\n                \"products\": [\n                    {\"title\": \"Hands-On Machine Learning with Scikit-Learn\", \"price\": 49.99, \"rating\": 4.7},\n                    {\"title\": \"Deep Learning with Python\", \"price\": 44.99, \"rating\": 4.6}\n                ]\n            }\n        else:\n            tool_result = {\"status\": \"unknown tool\"}\n        \n        print(f\"   Result: {json.dumps(tool_result)[:80]}...\")\n        \n        # Add tool result to messages\n        messages.append({\n            \"tool_call_id\": tool_call.id,\n            \"role\": \"tool\",\n            \"name\": tool_name,\n            \"content\": json.dumps(tool_result)\n        })\n    \n    # Step 3: Get final answer with tool results\n    print(\"\\n\" + \"=\" * 70)\n    print(\"\\nStep 3: LLM synthesizes final answer from tool results...\\n\")\n    \n    final_response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=messages,\n        max_tokens=300\n    )\n    \n    print(\"üìù Final Answer:\")\n    print(\"-\" * 70)\n    print(final_response.choices[0].message.content)\n    print(\"-\" * 70)\n    \n    print(f\"\\n‚úÖ Multi-tool orchestration complete!\")\n    print(f\"   ‚Ä¢ Tools called: {len(tool_calls)}\")\n    print(f\"   ‚Ä¢ Messages exchanged: {len(messages)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3.3: MCP Server Status\n",
    "\n",
    "Check health and status of deployed MCP servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP End-to-End Testing - Real Tool Calling with LLM Response\n",
    "# Reload the module to pick up changes\n",
    "import importlib\n",
    "import quick_start.mcp_helper\n",
    "importlib.reload(quick_start.mcp_helper)\n",
    "\n",
    "from quick_start.shared_init import get_azure_openai_client, load_environment\n",
    "from quick_start.mcp_helper import SimpleMCPClient, test_mcp_with_llm\n",
    "\n",
    "# Load environment\n",
    "env = load_environment()\n",
    "print()\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = get_azure_openai_client()\n",
    "mcp_client = SimpleMCPClient()\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MCP END-TO-END TESTING\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Run complete MCP workflow: tool discovery ‚Üí LLM call ‚Üí MCP execution ‚Üí final response\n",
    "try:\n",
    "    final_answer = test_mcp_with_llm(openai_client, mcp_client, model=\"gpt-4o\")\n",
    "    \n",
    "    if final_answer:\n",
    "        print()\n",
    "        print(\"‚úÖ MCP Integration Complete!\")\n",
    "        print()\n",
    "        print(\"What just happened:\")\n",
    "        print(\"  1. ‚úÖ Discovered MCP tools from weather server\")\n",
    "        print(\"  2. ‚úÖ LLM requested to call MCP tool\")\n",
    "        print(\"  3. ‚úÖ Executed tool via MCP JSON-RPC protocol\")\n",
    "        print(\"  4. ‚úÖ LLM synthesized final answer from tool results\")\n",
    "        print()\n",
    "        print(\"üí° This demonstrates the complete MCP + Azure OpenAI integration pattern!\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"‚ö†Ô∏è  MCP test did not complete successfully\")\n",
    "        print(\"   The LLM may have returned a mock response or failed to call tools\")\n",
    "        print()\n",
    "        print(\"Troubleshooting:\")\n",
    "        print(\"  ‚Ä¢ APIM may be returning mock responses - retry the cell\")\n",
    "        print(\"  ‚Ä¢ Check that gpt-4o or gpt-4o-mini is deployed\")\n",
    "        print(\"  ‚Ä¢ Verify APIM backend pool configuration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during MCP testing: {e}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting:\")\n",
    "    print(\"  ‚Ä¢ Check that MCP servers are deployed and running\")\n",
    "    print(\"  ‚Ä¢ Verify MCP_WEATHER_URL is set in master-lab.env\")\n",
    "    print(\"  ‚Ä¢ Ensure gpt-4o model is deployed to at least one foundry\")\n",
    "    print()\n",
    "    print(\"For detailed MCP protocol implementation, see:\")\n",
    "    print(\"  master-ai-gateway-deploy-from-notebook.ipynb (cells 95-110)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Workshop Complete!\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "- ‚úÖ One-command deployment for complete AI Gateway infrastructure\n",
    "- ‚úÖ Access control with OAuth 2.0 and API keys\n",
    "- ‚úÖ Load balancing across multiple Azure regions\n",
    "- ‚úÖ Token metrics and monitoring with Log Analytics\n",
    "- ‚úÖ Content safety and moderation\n",
    "- ‚úÖ Semantic caching for faster responses\n",
    "- ‚úÖ Message storing in Cosmos DB\n",
    "- ‚úÖ Vector search with RAG patterns\n",
    "- ‚úÖ Built-in logging and monitoring\n",
    "- ‚úÖ MCP server integration for tool calling\n",
    "- ‚úÖ Multi-tool orchestration\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Modular Deployment**: `util.deploy_all` deploys everything in one command\n",
    "2. **Minimal Code**: `quick_start.shared_init` provides one-line initialization\n",
    "3. **Production Ready**: Enterprise-grade error handling and retry logic\n",
    "4. **Azure CLI Auth**: Simplest authentication method for development\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore individual quick-start labs in `quick_start/` folder\n",
    "- Customize deployment with `DeploymentConfig` options\n",
    "- Deploy to your own subscriptions\n",
    "- Integrate into CI/CD pipelines\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Full documentation: `README.md`\n",
    "- Deployment utility: `util/deploy_all.py`\n",
    "- Quick start module: `quick_start/shared_init.py`\n",
    "- Original notebook: `master-ai-gateway-deploy-from-notebook.ipynb` (152 cells)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing the Azure AI Gateway Easy Deploy workshop!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (AI Gateway)",
   "language": "python",
   "name": "py312-aigateway"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}