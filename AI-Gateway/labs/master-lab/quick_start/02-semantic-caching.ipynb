{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Semantic Caching with Redis\n",
    "\n",
    "Implement intelligent caching to reduce latency and costs for repeated queries.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand semantic caching vs traditional caching\n",
    "- Configure Redis for LLM response caching\n",
    "- Measure cache hit/miss performance\n",
    "- Optimize caching strategies\n",
    "\n",
    "## Prerequisites\n",
    "- Azure CLI authenticated (`az login`)\n",
    "- Resources deployed via main notebook\n",
    "- Redis Cache instance created\n",
    "- APIM with semantic caching policy configured\n",
    "\n",
    "**Duration:** ~15 minutes  \n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from quick_start.shared_init import quick_init, get_azure_openai_client\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "config = quick_init()\n",
    "\n",
    "# Get Redis configuration\n",
    "redis_host = config['env']['REDIS_HOST']\n",
    "print(f\"\\nüîó Redis Host: {redis_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Redis Connection\n",
    "\n",
    "Redis is used by APIM's semantic caching policy to store LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Redis library\n",
    "try:\n",
    "    import redis\n",
    "    print(\"‚úÖ Redis library available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing redis library...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"redis\"])\n",
    "    import redis\n",
    "    print(\"‚úÖ Redis library installed\")\n",
    "\n",
    "# Get Redis access key from Azure\n",
    "from azure.identity import AzureCliCredential\n",
    "import subprocess\n",
    "\n",
    "resource_group = config['resource_group']\n",
    "redis_name = redis_host.split('.')[0]  # Extract name from hostname\n",
    "\n",
    "# Get Redis access key\n",
    "result = subprocess.run(\n",
    "    ['az', 'redis', 'list-keys', '--name', redis_name, '--resource-group', resource_group, '--query', 'primaryKey', '-o', 'tsv'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    redis_key = result.stdout.strip()\n",
    "    print(\"‚úÖ Retrieved Redis access key\")\n",
    "    \n",
    "    # Create Redis client\n",
    "    redis_client = redis.StrictRedis(\n",
    "        host=redis_host,\n",
    "        port=6380,\n",
    "        password=redis_key,\n",
    "        ssl=True,\n",
    "        decode_responses=True\n",
    "    )\n",
    "    \n",
    "    # Test connection\n",
    "    redis_client.ping()\n",
    "    print(\"‚úÖ Connected to Redis\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to retrieve Redis key: {result.stderr}\")\n",
    "    redis_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: First Query (Cache Miss)\n",
    "\n",
    "Send a query and measure the response time. This will be a cache miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenAI client\n",
    "client = get_azure_openai_client()\n",
    "\n",
    "# Define test query\n",
    "test_query = \"What are the key benefits of using Azure AI Gateway?\"\n",
    "\n",
    "print(f\"\\nüîç Query: {test_query}\")\n",
    "print(\"\\n‚è±Ô∏è Sending first request (cache miss expected)...\\n\")\n",
    "\n",
    "# Measure time\n",
    "start_time = time.time()\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": test_query}\n",
    "    ],\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "elapsed_time_1 = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Response received in {elapsed_time_1:.3f} seconds\")\n",
    "print(f\"\\nResponse:\\n{response1.choices[0].message.content}\")\n",
    "print(f\"\\nüìä Cache Status: MISS (first request)\")\n",
    "print(f\"   Response Time: {elapsed_time_1:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Same Query (Cache Hit)\n",
    "\n",
    "Send the exact same query and compare the response time. This should be a cache hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment to ensure first request is cached\n",
    "print(\"‚è≥ Waiting 2 seconds for cache to propagate...\")\n",
    "time.sleep(2)\n",
    "\n",
    "print(f\"\\nüîç Query: {test_query}\")\n",
    "print(\"\\n‚è±Ô∏è Sending second request (cache hit expected)...\\n\")\n",
    "\n",
    "# Measure time\n",
    "start_time = time.time()\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": test_query}\n",
    "    ],\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "elapsed_time_2 = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Response received in {elapsed_time_2:.3f} seconds\")\n",
    "print(f\"\\nResponse:\\n{response2.choices[0].message.content}\")\n",
    "\n",
    "# Compare results\n",
    "speedup = elapsed_time_1 / elapsed_time_2 if elapsed_time_2 > 0 else 0\n",
    "time_saved = elapsed_time_1 - elapsed_time_2\n",
    "\n",
    "print(f\"\\nüìä Cache Status: HIT (cached response)\")\n",
    "print(f\"   Response Time: {elapsed_time_2:.3f}s\")\n",
    "print(f\"   Time Saved: {time_saved:.3f}s ({speedup:.1f}x faster)\")\n",
    "\n",
    "# Verify responses are identical\n",
    "if response1.choices[0].message.content == response2.choices[0].message.content:\n",
    "    print(f\"\\n‚úÖ Responses match - cache working correctly\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Responses differ - cache may not be enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Different Query (Cache Miss)\n",
    "\n",
    "Send a different query to verify cache miss behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different query\n",
    "test_query_2 = \"Explain the purpose of API Management in Azure.\"\n",
    "\n",
    "print(f\"\\nüîç Query: {test_query_2}\")\n",
    "print(\"\\n‚è±Ô∏è Sending new request (cache miss expected)...\\n\")\n",
    "\n",
    "# Measure time\n",
    "start_time = time.time()\n",
    "\n",
    "response3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": test_query_2}\n",
    "    ],\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "elapsed_time_3 = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Response received in {elapsed_time_3:.3f} seconds\")\n",
    "print(f\"\\nResponse:\\n{response3.choices[0].message.content}\")\n",
    "print(f\"\\nüìä Cache Status: MISS (new query)\")\n",
    "print(f\"   Response Time: {elapsed_time_3:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Cache in Redis (Optional)\n",
    "\n",
    "Inspect cached keys directly in Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if redis_client:\n",
    "    # List all keys (limit to 10)\n",
    "    keys = redis_client.keys('*')[:10]\n",
    "    \n",
    "    print(f\"\\nüì¶ Cached Keys in Redis ({len(keys)} shown):\")\n",
    "    for i, key in enumerate(keys, 1):\n",
    "        # Get TTL\n",
    "        ttl = redis_client.ttl(key)\n",
    "        ttl_str = f\"{ttl}s\" if ttl > 0 else \"no expiry\"\n",
    "        \n",
    "        print(f\"   {i}. {key[:80]}... (TTL: {ttl_str})\")\n",
    "    \n",
    "    # Get cache statistics\n",
    "    info = redis_client.info('stats')\n",
    "    print(f\"\\nüìä Redis Statistics:\")\n",
    "    print(f\"   Total Connections: {info.get('total_connections_received', 'N/A')}\")\n",
    "    print(f\"   Total Commands: {info.get('total_commands_processed', 'N/A')}\")\n",
    "    print(f\"   Keyspace Hits: {info.get('keyspace_hits', 'N/A')}\")\n",
    "    print(f\"   Keyspace Misses: {info.get('keyspace_misses', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Redis client not available - skip cache inspection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "\n",
    "Compare all test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Test':<40} {'Time (s)':<12} {'Status':<10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Test 1: First query (cache miss)':<40} {elapsed_time_1:>8.3f}     {'MISS':<10}\")\n",
    "print(f\"{'Test 2: Same query (cache hit)':<40} {elapsed_time_2:>8.3f}     {'HIT':<10}\")\n",
    "print(f\"{'Test 3: Different query (cache miss)':<40} {elapsed_time_3:>8.3f}     {'MISS':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_miss_time = (elapsed_time_1 + elapsed_time_3) / 2\n",
    "cache_speedup = avg_miss_time / elapsed_time_2 if elapsed_time_2 > 0 else 0\n",
    "time_saved_pct = ((avg_miss_time - elapsed_time_2) / avg_miss_time * 100) if avg_miss_time > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Key Metrics:\")\n",
    "print(f\"   Average cache miss time: {avg_miss_time:.3f}s\")\n",
    "print(f\"   Cache hit time: {elapsed_time_2:.3f}s\")\n",
    "print(f\"   Cache speedup: {cache_speedup:.1f}x faster\")\n",
    "print(f\"   Time saved: {time_saved_pct:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Learned\n",
    "\n",
    "1. **Semantic caching reduces latency** - Cache hits are significantly faster\n",
    "2. **Cost optimization** - Cached responses don't consume OpenAI tokens\n",
    "3. **Cache key generation** - APIM creates cache keys from request content\n",
    "4. **Redis integration** - APIM policies manage cache storage automatically\n",
    "5. **Performance gains** - Typical speedups of 3-10x for cached queries\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **Set appropriate TTL** - Balance freshness vs cache hit rate\n",
    "- **Monitor cache metrics** - Track hit/miss ratios in Redis\n",
    "- **Use for stable queries** - FAQs, documentation, common questions\n",
    "- **Avoid for dynamic content** - Real-time data, personalized responses\n",
    "- **Configure cache size** - Ensure Redis has sufficient capacity\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Adjust cache TTL in APIM policy (default: 3600s)\n",
    "- Configure cache eviction policies in Redis\n",
    "- Monitor cache hit rates in Azure Monitor\n",
    "- Implement cache warming for common queries\n",
    "- Set up cache invalidation strategies\n",
    "\n",
    "**Next Lab:** `03-message-storing.ipynb` - Store conversations in Cosmos DB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
